{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "PEDRO0 ANGELO MARTINS\n",
        "\n",
        "Considerando a base de dados Titanic (https://www.kaggle.com/datasets/yasserh/titanicdataset):\n",
        "\n",
        "1. Aplicar a separação do conjunto treino (com 70%) e teste (30%);\n",
        " * Classificadores: serão utilizados os classificadores Decision Tree,\n",
        " * Random Forest, KNN e Redes Neurais (biliotecas scikit-learn e Keras).\n",
        "\n",
        "2. Etapas do processo:\n",
        "\n",
        " * Limpeza e pré-processamento dos dados(Remoção de dados faltantes NaN,\n",
        "   transformação de dados categóricos e normalização);\n",
        " * Balanceamento das classes;\n",
        " * Comparação com todos os classificadores descritos anteriormente;\n",
        " * Ajuste dos hiperparâmetros com GridSearchCV e RandonSearchCV;\n",
        " * Amostragem por validação cruzada estratificada (10 folds);\n",
        " * Apresentação da Acurácia nos conjuntos treino (10 folds) e também conjunto\n",
        "   teste;\n",
        " * Apresentação da Matriz confusão;"
      ],
      "metadata": {
        "id": "SZQ_KadWcXuX"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5Thvv9ghArq"
      },
      "source": [
        "## Tratamento de dados para o Titanic Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV, RandomizedSearchCV, StratifiedShuffleSplit\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "import statistics  as sts\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from imblearn.over_sampling import SMOTE"
      ],
      "metadata": {
        "id": "PlCUPznsnEIR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Carregar os dados do arquivo CSV\n",
        "df = pd.read_csv('/content/Titanic-Dataset.csv')\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "a1MY_Vp_dEgy",
        "outputId": "2720de57-8255-47c6-e09b-7e615c288064"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     PassengerId  Survived  Pclass  \\\n",
              "0              1         0       3   \n",
              "1              2         1       1   \n",
              "2              3         1       3   \n",
              "3              4         1       1   \n",
              "4              5         0       3   \n",
              "..           ...       ...     ...   \n",
              "886          887         0       2   \n",
              "887          888         1       1   \n",
              "888          889         0       3   \n",
              "889          890         1       1   \n",
              "890          891         0       3   \n",
              "\n",
              "                                                  Name     Sex   Age  SibSp  \\\n",
              "0                              Braund, Mr. Owen Harris    male  22.0      1   \n",
              "1    Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
              "2                               Heikkinen, Miss. Laina  female  26.0      0   \n",
              "3         Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
              "4                             Allen, Mr. William Henry    male  35.0      0   \n",
              "..                                                 ...     ...   ...    ...   \n",
              "886                              Montvila, Rev. Juozas    male  27.0      0   \n",
              "887                       Graham, Miss. Margaret Edith  female  19.0      0   \n",
              "888           Johnston, Miss. Catherine Helen \"Carrie\"  female   NaN      1   \n",
              "889                              Behr, Mr. Karl Howell    male  26.0      0   \n",
              "890                                Dooley, Mr. Patrick    male  32.0      0   \n",
              "\n",
              "     Parch            Ticket     Fare Cabin Embarked  \n",
              "0        0         A/5 21171   7.2500   NaN        S  \n",
              "1        0          PC 17599  71.2833   C85        C  \n",
              "2        0  STON/O2. 3101282   7.9250   NaN        S  \n",
              "3        0            113803  53.1000  C123        S  \n",
              "4        0            373450   8.0500   NaN        S  \n",
              "..     ...               ...      ...   ...      ...  \n",
              "886      0            211536  13.0000   NaN        S  \n",
              "887      0            112053  30.0000   B42        S  \n",
              "888      2        W./C. 6607  23.4500   NaN        S  \n",
              "889      0            111369  30.0000  C148        C  \n",
              "890      0            370376   7.7500   NaN        Q  \n",
              "\n",
              "[891 rows x 12 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4afa72b7-844a-4a3f-b813-f1a38c8fa776\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PassengerId</th>\n",
              "      <th>Survived</th>\n",
              "      <th>Pclass</th>\n",
              "      <th>Name</th>\n",
              "      <th>Sex</th>\n",
              "      <th>Age</th>\n",
              "      <th>SibSp</th>\n",
              "      <th>Parch</th>\n",
              "      <th>Ticket</th>\n",
              "      <th>Fare</th>\n",
              "      <th>Cabin</th>\n",
              "      <th>Embarked</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>Braund, Mr. Owen Harris</td>\n",
              "      <td>male</td>\n",
              "      <td>22.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>A/5 21171</td>\n",
              "      <td>7.2500</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
              "      <td>female</td>\n",
              "      <td>38.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>PC 17599</td>\n",
              "      <td>71.2833</td>\n",
              "      <td>C85</td>\n",
              "      <td>C</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>Heikkinen, Miss. Laina</td>\n",
              "      <td>female</td>\n",
              "      <td>26.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>STON/O2. 3101282</td>\n",
              "      <td>7.9250</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
              "      <td>female</td>\n",
              "      <td>35.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>113803</td>\n",
              "      <td>53.1000</td>\n",
              "      <td>C123</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>Allen, Mr. William Henry</td>\n",
              "      <td>male</td>\n",
              "      <td>35.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>373450</td>\n",
              "      <td>8.0500</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>886</th>\n",
              "      <td>887</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>Montvila, Rev. Juozas</td>\n",
              "      <td>male</td>\n",
              "      <td>27.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>211536</td>\n",
              "      <td>13.0000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>887</th>\n",
              "      <td>888</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Graham, Miss. Margaret Edith</td>\n",
              "      <td>female</td>\n",
              "      <td>19.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>112053</td>\n",
              "      <td>30.0000</td>\n",
              "      <td>B42</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>888</th>\n",
              "      <td>889</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>Johnston, Miss. Catherine Helen \"Carrie\"</td>\n",
              "      <td>female</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>W./C. 6607</td>\n",
              "      <td>23.4500</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>889</th>\n",
              "      <td>890</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Behr, Mr. Karl Howell</td>\n",
              "      <td>male</td>\n",
              "      <td>26.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>111369</td>\n",
              "      <td>30.0000</td>\n",
              "      <td>C148</td>\n",
              "      <td>C</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>890</th>\n",
              "      <td>891</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>Dooley, Mr. Patrick</td>\n",
              "      <td>male</td>\n",
              "      <td>32.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>370376</td>\n",
              "      <td>7.7500</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Q</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>891 rows × 12 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4afa72b7-844a-4a3f-b813-f1a38c8fa776')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4afa72b7-844a-4a3f-b813-f1a38c8fa776 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4afa72b7-844a-4a3f-b813-f1a38c8fa776');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DSeJBhmWdWXx",
        "outputId": "a4ac4fd2-ce8e-4a35-e80d-df16081889bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 891 entries, 0 to 890\n",
            "Data columns (total 12 columns):\n",
            " #   Column       Non-Null Count  Dtype  \n",
            "---  ------       --------------  -----  \n",
            " 0   PassengerId  891 non-null    int64  \n",
            " 1   Survived     891 non-null    int64  \n",
            " 2   Pclass       891 non-null    int64  \n",
            " 3   Name         891 non-null    object \n",
            " 4   Sex          891 non-null    object \n",
            " 5   Age          714 non-null    float64\n",
            " 6   SibSp        891 non-null    int64  \n",
            " 7   Parch        891 non-null    int64  \n",
            " 8   Ticket       891 non-null    object \n",
            " 9   Fare         891 non-null    float64\n",
            " 10  Cabin        204 non-null    object \n",
            " 11  Embarked     889 non-null    object \n",
            "dtypes: float64(2), int64(5), object(5)\n",
            "memory usage: 83.7+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop([\"Ticket\",\"Name\",\"PassengerId\"],axis=1)"
      ],
      "metadata": {
        "id": "Myn7P-Yny-Cb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "E3eR9XhO2ut4",
        "outputId": "59a67a44-037a-4881-df32-a592a0d1aab5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     Survived  Pclass     Sex   Age  SibSp  Parch     Fare Cabin Embarked\n",
              "0           0       3    male  22.0      1      0   7.2500   NaN        S\n",
              "1           1       1  female  38.0      1      0  71.2833   C85        C\n",
              "2           1       3  female  26.0      0      0   7.9250   NaN        S\n",
              "3           1       1  female  35.0      1      0  53.1000  C123        S\n",
              "4           0       3    male  35.0      0      0   8.0500   NaN        S\n",
              "..        ...     ...     ...   ...    ...    ...      ...   ...      ...\n",
              "886         0       2    male  27.0      0      0  13.0000   NaN        S\n",
              "887         1       1  female  19.0      0      0  30.0000   B42        S\n",
              "888         0       3  female   NaN      1      2  23.4500   NaN        S\n",
              "889         1       1    male  26.0      0      0  30.0000  C148        C\n",
              "890         0       3    male  32.0      0      0   7.7500   NaN        Q\n",
              "\n",
              "[891 rows x 9 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9b3ad586-dfdc-431b-ae4d-7be2794b79fd\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Survived</th>\n",
              "      <th>Pclass</th>\n",
              "      <th>Sex</th>\n",
              "      <th>Age</th>\n",
              "      <th>SibSp</th>\n",
              "      <th>Parch</th>\n",
              "      <th>Fare</th>\n",
              "      <th>Cabin</th>\n",
              "      <th>Embarked</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>male</td>\n",
              "      <td>22.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>7.2500</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>female</td>\n",
              "      <td>38.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>71.2833</td>\n",
              "      <td>C85</td>\n",
              "      <td>C</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>female</td>\n",
              "      <td>26.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>7.9250</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>female</td>\n",
              "      <td>35.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>53.1000</td>\n",
              "      <td>C123</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>male</td>\n",
              "      <td>35.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>8.0500</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>886</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>male</td>\n",
              "      <td>27.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>13.0000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>887</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>female</td>\n",
              "      <td>19.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>30.0000</td>\n",
              "      <td>B42</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>888</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>female</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>23.4500</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>889</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>male</td>\n",
              "      <td>26.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>30.0000</td>\n",
              "      <td>C148</td>\n",
              "      <td>C</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>890</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>male</td>\n",
              "      <td>32.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>7.7500</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Q</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>891 rows × 9 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9b3ad586-dfdc-431b-ae4d-7be2794b79fd')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9b3ad586-dfdc-431b-ae4d-7be2794b79fd button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9b3ad586-dfdc-431b-ae4d-7be2794b79fd');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print (\"Linhas: \" , df.shape[0])\n",
        "print (\"Colunas: \" , df.shape[1])\n",
        "print (\"\\nAtributos : \\n\" , df.columns.tolist())\n",
        "print (\"\\nValores faltantes :  \", df.isnull().sum().values.sum())\n",
        "print (\"\\nValores únicos :  \\n\",df.nunique())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pUal2-eCdt-s",
        "outputId": "6e14af81-855d-43ae-ff01-8de53f422a76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linhas:  891\n",
            "Colunas:  9\n",
            "\n",
            "Atributos : \n",
            " ['Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Cabin', 'Embarked']\n",
            "\n",
            "Valores faltantes :   866\n",
            "\n",
            "Valores únicos :  \n",
            " Survived      2\n",
            "Pclass        3\n",
            "Sex           2\n",
            "Age          88\n",
            "SibSp         7\n",
            "Parch         7\n",
            "Fare        248\n",
            "Cabin       147\n",
            "Embarked      3\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['Survived'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YvgWEdwySe8u",
        "outputId": "7869a109-5cbd-4de6-bc7c-8de4c59e17f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    549\n",
              "1    342\n",
              "Name: Survived, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['Pclass'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9yD2DaDdf1ZU",
        "outputId": "88041c30-489b-4819-bf56-9c260cc79f65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3    491\n",
              "1    216\n",
              "2    184\n",
              "Name: Pclass, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"Sex\"].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rS4LG6H93ISU",
        "outputId": "421c2875-9524-45bd-d928-8ace46ba890c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "male      577\n",
              "female    314\n",
              "Name: Sex, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Ver inconsistências no preenchimento dos dados\n",
        "df.isna().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rIy3W-fhD-IR",
        "outputId": "7d359425-1e2c-43c1-f306-8bb1c0b6bf1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Survived      0\n",
              "Pclass        0\n",
              "Sex           0\n",
              "Age         177\n",
              "SibSp         0\n",
              "Parch         0\n",
              "Fare          0\n",
              "Cabin       687\n",
              "Embarked      2\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Tentativa para preencher dados de idade com a mediana (funcionou)\n",
        "\n",
        "from sklearn.impute import SimpleImputer\n",
        "imputer = SimpleImputer(missing_values=np.NaN, strategy='median')\n",
        "\n",
        "df['Age'] = imputer.fit_transform(df['Age'].values.reshape(-1,1))[:,0]"
      ],
      "metadata": {
        "id": "PQqOvpefCusS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Utlizando moda para sanar o problema com \"Embarked\" (funcionou)\n",
        "mode_value = sts.mode(df['Embarked'])\n",
        "df['Embarked'].fillna(mode_value, inplace=True)\n",
        "df = pd.get_dummies(data=df, columns=['Embarked'])"
      ],
      "metadata": {
        "id": "uydTjo5mDR0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Binarizando \"Pclass\"\n",
        "df = pd.get_dummies(data=df, columns=['Pclass'])"
      ],
      "metadata": {
        "id": "iZO30ginFa-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Guarda uma cópia do data frame, para a realização do segundo teste,\n",
        "# sem a classe \"Cabin\".\n",
        "df_re_teste = df.copy()"
      ],
      "metadata": {
        "id": "nq4ysaNmFY1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Método de tratamento da coluna \"Cabin\", que cria uma cópia\n",
        "# do data frame, filtra toda a base de dados com NaN nas cabines\n",
        "# e calcula a moda entre elas, armazenando o resultado numa variavel.\n",
        "dfa = df.copy()\n",
        "dfa.dropna(subset=['Cabin'], inplace=True)\n",
        "dfa['Cabin'] = df['Cabin'].str[0]\n",
        "mode_value = sts.mode(dfa['Cabin'])\n",
        "print(mode_value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2T7QnR8ZmlMi",
        "outputId": "efc60b2b-c466-4b3e-92c7-c6118005373a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Aplicando a moda presente em cabin, e preeenchendo nos campos faltantes\n",
        "df['Cabin'] = df['Cabin'].str[0]\n",
        "df['Cabin'].fillna(mode_value, inplace=True)\n",
        "df = pd.get_dummies(data=df, columns=['Cabin'])"
      ],
      "metadata": {
        "id": "CkwEF9GBHyqH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Transformando a coluna \"Sex\" em numérico (note que,\n",
        "# devido a natureza da coluna, seus dados se comportarão\n",
        "# de forma binária).\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "labelencoder = LabelEncoder()\n",
        "df[[\"Sex\"]] = \\\n",
        "df[[\"Sex\"]].apply(labelencoder.fit_transform)"
      ],
      "metadata": {
        "id": "PpsmDB_sgFiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Ver inconsistências no preenchimento dos dados\n",
        "df.isna().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0xWf1wu8SIUQ",
        "outputId": "8c183a17-76b3-4b06-f238-e7b65e5197b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Survived      0\n",
              "Sex           0\n",
              "Age           0\n",
              "SibSp         0\n",
              "Parch         0\n",
              "Fare          0\n",
              "Embarked_C    0\n",
              "Embarked_Q    0\n",
              "Embarked_S    0\n",
              "Pclass_1      0\n",
              "Pclass_2      0\n",
              "Pclass_3      0\n",
              "Cabin_A       0\n",
              "Cabin_B       0\n",
              "Cabin_C       0\n",
              "Cabin_D       0\n",
              "Cabin_E       0\n",
              "Cabin_F       0\n",
              "Cabin_G       0\n",
              "Cabin_T       0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dj5ZFamz6kOI",
        "outputId": "c170f1cf-ee2e-44a6-c90a-31533aebc9e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 891 entries, 0 to 890\n",
            "Data columns (total 20 columns):\n",
            " #   Column      Non-Null Count  Dtype  \n",
            "---  ------      --------------  -----  \n",
            " 0   Survived    891 non-null    int64  \n",
            " 1   Sex         891 non-null    int64  \n",
            " 2   Age         891 non-null    float64\n",
            " 3   SibSp       891 non-null    int64  \n",
            " 4   Parch       891 non-null    int64  \n",
            " 5   Fare        891 non-null    float64\n",
            " 6   Embarked_C  891 non-null    uint8  \n",
            " 7   Embarked_Q  891 non-null    uint8  \n",
            " 8   Embarked_S  891 non-null    uint8  \n",
            " 9   Pclass_1    891 non-null    uint8  \n",
            " 10  Pclass_2    891 non-null    uint8  \n",
            " 11  Pclass_3    891 non-null    uint8  \n",
            " 12  Cabin_A     891 non-null    uint8  \n",
            " 13  Cabin_B     891 non-null    uint8  \n",
            " 14  Cabin_C     891 non-null    uint8  \n",
            " 15  Cabin_D     891 non-null    uint8  \n",
            " 16  Cabin_E     891 non-null    uint8  \n",
            " 17  Cabin_F     891 non-null    uint8  \n",
            " 18  Cabin_G     891 non-null    uint8  \n",
            " 19  Cabin_T     891 non-null    uint8  \n",
            "dtypes: float64(2), int64(4), uint8(14)\n",
            "memory usage: 54.1 KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "WoY02DQs7WI-",
        "outputId": "ad2c242f-d909-4617-a452-051d0cea6693"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     Survived  Sex   Age  SibSp  Parch     Fare  Embarked_C  Embarked_Q  \\\n",
              "0           0    1  22.0      1      0   7.2500           0           0   \n",
              "1           1    0  38.0      1      0  71.2833           1           0   \n",
              "2           1    0  26.0      0      0   7.9250           0           0   \n",
              "3           1    0  35.0      1      0  53.1000           0           0   \n",
              "4           0    1  35.0      0      0   8.0500           0           0   \n",
              "..        ...  ...   ...    ...    ...      ...         ...         ...   \n",
              "886         0    1  27.0      0      0  13.0000           0           0   \n",
              "887         1    0  19.0      0      0  30.0000           0           0   \n",
              "888         0    0  28.0      1      2  23.4500           0           0   \n",
              "889         1    1  26.0      0      0  30.0000           1           0   \n",
              "890         0    1  32.0      0      0   7.7500           0           1   \n",
              "\n",
              "     Embarked_S  Pclass_1  Pclass_2  Pclass_3  Cabin_A  Cabin_B  Cabin_C  \\\n",
              "0             1         0         0         1        0        0        1   \n",
              "1             0         1         0         0        0        0        1   \n",
              "2             1         0         0         1        0        0        1   \n",
              "3             1         1         0         0        0        0        1   \n",
              "4             1         0         0         1        0        0        1   \n",
              "..          ...       ...       ...       ...      ...      ...      ...   \n",
              "886           1         0         1         0        0        0        1   \n",
              "887           1         1         0         0        0        1        0   \n",
              "888           1         0         0         1        0        0        1   \n",
              "889           0         1         0         0        0        0        1   \n",
              "890           0         0         0         1        0        0        1   \n",
              "\n",
              "     Cabin_D  Cabin_E  Cabin_F  Cabin_G  Cabin_T  \n",
              "0          0        0        0        0        0  \n",
              "1          0        0        0        0        0  \n",
              "2          0        0        0        0        0  \n",
              "3          0        0        0        0        0  \n",
              "4          0        0        0        0        0  \n",
              "..       ...      ...      ...      ...      ...  \n",
              "886        0        0        0        0        0  \n",
              "887        0        0        0        0        0  \n",
              "888        0        0        0        0        0  \n",
              "889        0        0        0        0        0  \n",
              "890        0        0        0        0        0  \n",
              "\n",
              "[891 rows x 20 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-25f6bb4b-4659-400c-86a7-99a2878538d9\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Survived</th>\n",
              "      <th>Sex</th>\n",
              "      <th>Age</th>\n",
              "      <th>SibSp</th>\n",
              "      <th>Parch</th>\n",
              "      <th>Fare</th>\n",
              "      <th>Embarked_C</th>\n",
              "      <th>Embarked_Q</th>\n",
              "      <th>Embarked_S</th>\n",
              "      <th>Pclass_1</th>\n",
              "      <th>Pclass_2</th>\n",
              "      <th>Pclass_3</th>\n",
              "      <th>Cabin_A</th>\n",
              "      <th>Cabin_B</th>\n",
              "      <th>Cabin_C</th>\n",
              "      <th>Cabin_D</th>\n",
              "      <th>Cabin_E</th>\n",
              "      <th>Cabin_F</th>\n",
              "      <th>Cabin_G</th>\n",
              "      <th>Cabin_T</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>22.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>7.2500</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>38.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>71.2833</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>26.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>7.9250</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>53.1000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>35.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>8.0500</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>886</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>27.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>13.0000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>887</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>30.0000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>888</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>28.0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>23.4500</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>889</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>26.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>30.0000</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>890</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>32.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>7.7500</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>891 rows × 20 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-25f6bb4b-4659-400c-86a7-99a2878538d9')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-25f6bb4b-4659-400c-86a7-99a2878538d9 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-25f6bb4b-4659-400c-86a7-99a2878538d9');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#mulheres = 0 ; homens = 1\n",
        "df[\"Sex\"].value_counts()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25qs-Scb9T72",
        "outputId": "2b7d5a55-c42f-42e7-800f-2249f138d665"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    577\n",
              "0    314\n",
              "Name: Sex, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Normalização de Age e Fare, que possuem discrepância alta em distribuição de dados.\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "df[['Age', 'Fare']] = scaler.fit_transform(df[['Age', 'Fare']])"
      ],
      "metadata": {
        "id": "np25B9Ll2gMS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "GzeuqIXz88uI",
        "outputId": "aa823c40-8b3d-4a1e-ff3a-d0d533bd8724"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     Survived  Sex       Age  SibSp  Parch      Fare  Embarked_C  Embarked_Q  \\\n",
              "0           0    1  0.271174      1      0  0.014151           0           0   \n",
              "1           1    0  0.472229      1      0  0.139136           1           0   \n",
              "2           1    0  0.321438      0      0  0.015469           0           0   \n",
              "3           1    0  0.434531      1      0  0.103644           0           0   \n",
              "4           0    1  0.434531      0      0  0.015713           0           0   \n",
              "..        ...  ...       ...    ...    ...       ...         ...         ...   \n",
              "886         0    1  0.334004      0      0  0.025374           0           0   \n",
              "887         1    0  0.233476      0      0  0.058556           0           0   \n",
              "888         0    0  0.346569      1      2  0.045771           0           0   \n",
              "889         1    1  0.321438      0      0  0.058556           1           0   \n",
              "890         0    1  0.396833      0      0  0.015127           0           1   \n",
              "\n",
              "     Embarked_S  Pclass_1  Pclass_2  Pclass_3  Cabin_A  Cabin_B  Cabin_C  \\\n",
              "0             1         0         0         1        0        0        1   \n",
              "1             0         1         0         0        0        0        1   \n",
              "2             1         0         0         1        0        0        1   \n",
              "3             1         1         0         0        0        0        1   \n",
              "4             1         0         0         1        0        0        1   \n",
              "..          ...       ...       ...       ...      ...      ...      ...   \n",
              "886           1         0         1         0        0        0        1   \n",
              "887           1         1         0         0        0        1        0   \n",
              "888           1         0         0         1        0        0        1   \n",
              "889           0         1         0         0        0        0        1   \n",
              "890           0         0         0         1        0        0        1   \n",
              "\n",
              "     Cabin_D  Cabin_E  Cabin_F  Cabin_G  Cabin_T  \n",
              "0          0        0        0        0        0  \n",
              "1          0        0        0        0        0  \n",
              "2          0        0        0        0        0  \n",
              "3          0        0        0        0        0  \n",
              "4          0        0        0        0        0  \n",
              "..       ...      ...      ...      ...      ...  \n",
              "886        0        0        0        0        0  \n",
              "887        0        0        0        0        0  \n",
              "888        0        0        0        0        0  \n",
              "889        0        0        0        0        0  \n",
              "890        0        0        0        0        0  \n",
              "\n",
              "[891 rows x 20 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-de4b2c14-136d-4c97-a45a-7353b4a70208\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Survived</th>\n",
              "      <th>Sex</th>\n",
              "      <th>Age</th>\n",
              "      <th>SibSp</th>\n",
              "      <th>Parch</th>\n",
              "      <th>Fare</th>\n",
              "      <th>Embarked_C</th>\n",
              "      <th>Embarked_Q</th>\n",
              "      <th>Embarked_S</th>\n",
              "      <th>Pclass_1</th>\n",
              "      <th>Pclass_2</th>\n",
              "      <th>Pclass_3</th>\n",
              "      <th>Cabin_A</th>\n",
              "      <th>Cabin_B</th>\n",
              "      <th>Cabin_C</th>\n",
              "      <th>Cabin_D</th>\n",
              "      <th>Cabin_E</th>\n",
              "      <th>Cabin_F</th>\n",
              "      <th>Cabin_G</th>\n",
              "      <th>Cabin_T</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.271174</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.014151</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.472229</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.139136</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.321438</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.015469</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.434531</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.103644</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.434531</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.015713</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>886</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.334004</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.025374</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>887</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.233476</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.058556</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>888</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.346569</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0.045771</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>889</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.321438</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.058556</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>890</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.396833</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.015127</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>891 rows × 20 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-de4b2c14-136d-4c97-a45a-7353b4a70208')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-de4b2c14-136d-4c97-a45a-7353b4a70208 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-de4b2c14-136d-4c97-a45a-7353b4a70208');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-sSM0_6kmt6F"
      },
      "source": [
        "## Distribuição e balanceamento\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.drop('Survived', axis=1)\n",
        "y = df['Survived']\n",
        "# Dividir os dados em conjunto de treinamento e teste (70 - 30)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
        "# Aplicar balanceamento nas classes usando SMOTE\n",
        "sm = SMOTE()\n",
        "X_train_oversampled, y_train_oversampled = sm.fit_resample(X_train, y_train)\n",
        "print(X_train_oversampled.shape)\n",
        "print(X_train.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t__kpH6IpWJi",
        "outputId": "2547eaef-3d2b-41f0-dbfc-724a847b599e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(786, 19)\n",
            "(623, 19)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.drop('Survived', axis=1)\n",
        "y = df['Survived']"
      ],
      "metadata": {
        "id": "iuVu77eGTR55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dividir os dados em conjunto de treinamento e teste (70 - 30)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
      ],
      "metadata": {
        "id": "Elzoa9OMnHi4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aplicar balanceamento nas classes usando SMOTE\n",
        "sm = SMOTE()\n",
        "X_train_oversampled, y_train_oversampled = sm.fit_resample(X_train, y_train)"
      ],
      "metadata": {
        "id": "6Dgw1UvknIp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train_oversampled.shape)\n",
        "print(X_train.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3GvMmvndxYRD",
        "outputId": "cbe698a0-4778-46f5-ebfb-6f4489b263bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(760, 19)\n",
            "(623, 19)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WeDP4ddZnVwX"
      },
      "source": [
        "## Decision Tree\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inicializando variavel para manipular\n",
        "from sklearn import tree\n",
        "clf = tree.DecisionTreeClassifier(criterion = 'entropy')\n",
        "clf = clf.fit(X_train_oversampled, y_train_oversampled)"
      ],
      "metadata": {
        "id": "6YvnY39VCsG4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The predictions are stored in X_pred\n",
        "y_pred = clf.predict(X_test)"
      ],
      "metadata": {
        "id": "fQvs6DUxCzUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_proba = clf.predict_proba(X_test)"
      ],
      "metadata": {
        "id": "rC6hTP_jC7KV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(clf.feature_importances_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CjGRzRCSDB7R",
        "outputId": "6a44167c-6646-4283-d712-666a35504f0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.27454083 0.25572275 0.03908089 0.01665965 0.25918659 0.01172377\n",
            " 0.         0.00433209 0.00169243 0.02122181 0.08004934 0.01441439\n",
            " 0.01098807 0.00646314 0.         0.0035959  0.00032836 0.\n",
            " 0.        ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(clf.feature_names_in_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Z_s_jgKDDvo",
        "outputId": "6c6a6943-240f-4fb6-bf6f-f35f1b19f8f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Sex' 'Age' 'SibSp' 'Parch' 'Fare' 'Embarked_C' 'Embarked_Q' 'Embarked_S'\n",
            " 'Pclass_1' 'Pclass_2' 'Pclass_3' 'Cabin_A' 'Cabin_B' 'Cabin_C' 'Cabin_D'\n",
            " 'Cabin_E' 'Cabin_F' 'Cabin_G' 'Cabin_T']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_tree = accuracy_score(y_test, y_pred)\n",
        "confusion_tree = confusion_matrix(y_test, y_pred)\n",
        "print(\"Acurácia do modelo de árvore de decisão:\", accuracy_tree)\n",
        "print(\"Matriz confusao:\")\n",
        "print(confusion_tree)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KyAP1I-ZENcU",
        "outputId": "2653706a-ee30-46ca-cb93-f7bf72cf171c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acurácia do modelo de árvore de decisão: 0.7649253731343284\n",
            "Matriz confusao:\n",
            "[[141  28]\n",
            " [ 35  64]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ctmL-iZdDk8d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GridSearch"
      ],
      "metadata": {
        "id": "jEsx-qpYnpRp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Grid Search\n",
        "param_grid = {\n",
        "    'criterion': ['gini', 'entropy'],\n",
        "    'max_depth': [None, 5, 10, 15],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(estimator=clf, param_grid=param_grid, cv=10)\n",
        "grid_search.fit(X_train_oversampled, y_train_oversampled)\n",
        "\n",
        "best_params = grid_search.best_params_\n",
        "best_score = grid_search.best_score_\n",
        "\n",
        "print(\"Melhores hiperparâmetros encontrados através do Grid Search:\")\n",
        "print(best_params)\n",
        "print(\"Melhor pontuação (acurácia) encontrada através do Grid Search:\", best_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FPC9BuEnDGcg",
        "outputId": "552df9f4-b8b3-4f16-c2e3-7d70da284d6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Melhores hiperparâmetros encontrados através do Grid Search:\n",
            "{'criterion': 'gini', 'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
            "Melhor pontuação (acurácia) encontrada através do Grid Search: 0.8368421052631578\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross Validation com Grid Search\n",
        "grid_scores = cross_val_score(grid_search.best_estimator_, X_train_oversampled, y_train_oversampled, cv=10)\n",
        "Acurácias_Tree_Grid = np.mean(grid_scores)\n",
        "print(\"Média das acurácias alcançadas no Cross Validation com Grid Search:\", np.mean(grid_scores))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Z8XfL-7DQsS",
        "outputId": "8e949eb6-1c4f-4e05-8474-e420b889f11d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Média das acurácias alcançadas no Cross Validation com Grid Search: 0.8355263157894737\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Search"
      ],
      "metadata": {
        "id": "JgesF4VQ0wHu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Random Search\n",
        "param_dist = {\n",
        "    'criterion': ['gini', 'entropy'],\n",
        "    'max_depth': [None, 5, 10, 15],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "random_search = RandomizedSearchCV(estimator=clf, param_distributions=param_dist, cv=10)\n",
        "random_search.fit(X_train_oversampled, y_train_oversampled)\n",
        "\n",
        "best_params = random_search.best_params_\n",
        "best_score = random_search.best_score_\n",
        "\n",
        "print(\"Melhores hiperparâmetros encontrados através do Random Search:\")\n",
        "print(best_params)\n",
        "print(\"Melhor pontuação (acurácia) encontrada através do Random Search:\", best_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-0XNfpvkDMcK",
        "outputId": "3837061a-f480-4864-f6ac-8fa38b0927bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Melhores hiperparâmetros encontrados através do Random Search:\n",
            "{'min_samples_split': 2, 'min_samples_leaf': 4, 'max_depth': 10, 'criterion': 'gini'}\n",
            "Melhor pontuação (acurácia) encontrada através do Random Search: 0.8249999999999998\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross Validation com Random Search\n",
        "random_scores = cross_val_score(random_search.best_estimator_, X_train_oversampled, y_train_oversampled, cv=10)\n",
        "Acurácias_Tree_Random = np.mean(random_scores)\n",
        "print(\"Média das acurácias alcançadas no Cross Validation com Random Search:\", np.mean(random_scores))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYERBPxzDTKS",
        "outputId": "fb04e0da-b649-46f2-9f6c-93ff310d1c3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Média das acurácias alcançadas no Cross Validation com Random Search: 0.8263157894736841\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNP3L9Htnwss"
      },
      "source": [
        "## KNN\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Treinar o classificador KNN\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "knn=KNeighborsClassifier(n_neighbors=1)\n",
        "knn.fit(X_train_oversampled, y_train_oversampled)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ufrh-C_7kGZw",
        "outputId": "17e6089e-c275-4d2b-a888-7b6d258325a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "KNeighborsClassifier(n_neighbors=1)"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KNeighborsClassifier(n_neighbors=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsClassifier</label><div class=\"sk-toggleable__content\"><pre>KNeighborsClassifier(n_neighbors=1)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred=knn.predict(X_test)\n",
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "cm_knn = confusion_matrix(y_test,pred)\n",
        "print(cm_knn)\n",
        "print(classification_report(y_test,pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2IJh75WAkG9M",
        "outputId": "84a89c67-2017-4716-fab7-f1ef3841cd48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[128  41]\n",
            " [ 36  63]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.76      0.77       169\n",
            "           1       0.61      0.64      0.62        99\n",
            "\n",
            "    accuracy                           0.71       268\n",
            "   macro avg       0.69      0.70      0.69       268\n",
            "weighted avg       0.72      0.71      0.71       268\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "acuracia_knn = accuracy_score(y_test, pred)"
      ],
      "metadata": {
        "id": "eJoiQZiNsU4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inicialização do classificador KNN\n",
        "knn = KNeighborsClassifier()\n"
      ],
      "metadata": {
        "id": "FEOh47b-mnyv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GridSearch"
      ],
      "metadata": {
        "id": "xfuKMT7xmp5o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Grid Search\n",
        "param_grid = {\n",
        "    'n_neighbors': [1, 3, 5, 7],\n",
        "    'weights': ['uniform', 'distance'],\n",
        "    'p': [1, 2]\n",
        "}\n",
        "g_search = GridSearchCV(estimator=knn, param_grid=param_grid, cv=10)\n",
        "g_search.fit(X_train_oversampled, y_train_oversampled)\n",
        "\n",
        "best_params_grid = g_search.best_params_\n",
        "best_score_grid = g_search.best_score_\n",
        "\n",
        "print(\"Melhores hiperparâmetros Grid Search:\")\n",
        "print(best_params_grid)\n",
        "print(\"Melhor pontuação (acurácia) Grid Search:\", best_score_grid)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KSQB0ydrn3cp",
        "outputId": "041499e7-31ed-4ecc-87b2-954de8153d53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Melhores hiperparâmetros Grid Search:\n",
            "{'n_neighbors': 3, 'p': 1, 'weights': 'uniform'}\n",
            "Melhor pontuação (acurácia) Grid Search: 0.8355263157894737\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Carrega todos os dados do GridSearch em um Dataframe\n",
        "g_results =  pd.DataFrame(g_search.cv_results_)\n",
        "# Obtém a média das acurácias (10 folds) referente ao conjunto treino\n",
        "knn_grid = g_results.loc[g_search.best_index_,'mean_test_score']\n",
        "print(knn_grid)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9Z51w3opTz0",
        "outputId": "3f2f77d4-cbe5-4555-b1a7-2f7582614a33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8355263157894737\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RandomSearch"
      ],
      "metadata": {
        "id": "3FL4KBXPpV5u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Random Search\n",
        "param_dist = {\n",
        "    'n_neighbors': [1, 3, 5, 7],\n",
        "    'weights': ['uniform', 'distance'],\n",
        "    'p': [1, 2]\n",
        "}\n",
        "\n",
        "r_search = RandomizedSearchCV(estimator=knn, param_distributions=param_dist, cv=10)\n",
        "r_search.fit(X_train_oversampled, y_train_oversampled)\n",
        "\n",
        "best_params_random = r_search.best_params_\n",
        "best_score_random = r_search.best_score_\n",
        "\n",
        "print(\"Melhores hiperparâmetros Random Search:\")\n",
        "print(best_params_random)\n",
        "print(\"Melhor pontuação (acurácia) Random Search:\", best_score_random)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RQ0Fp4eLm6NV",
        "outputId": "643c8700-4ad6-4e7b-de4e-84a45843de96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Melhores hiperparâmetros Random Search:\n",
            "{'weights': 'uniform', 'p': 2, 'n_neighbors': 3}\n",
            "Melhor pontuação (acurácia) Random Search: 0.8342105263157894\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Carrega todos os dados do GridSearch em um Dataframe\n",
        "r_results =  pd.DataFrame(r_search.cv_results_)\n",
        "# Obtém a média das acurácias (10 folds) referente ao conjunto treino\n",
        "knn_random = r_results.loc[r_search.best_index_,'mean_test_score']\n",
        "print(knn_random)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4WuyASMsmGo",
        "outputId": "e5a9d54c-7d20-43f8-b0ca-958a2004b24b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8342105263157894\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCAAsmLwn3sD"
      },
      "source": [
        "## Random Forest\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Treinar o classificador Random Forest\n",
        "forest = RandomForestClassifier(random_state=42)\n",
        "forest.fit(X_train_oversampled, y_train_oversampled)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x6K5I0ZenUa8",
        "outputId": "96ae6dc4-b0d5-4b85-e39f-9372e887bb13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(random_state=42)"
            ],
            "text/html": [
              "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=42)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Avaliar o desempenho do classificador Random Forest\n",
        "forest_score = forest.score(X_train_oversampled, y_train_oversampled)\n",
        "forest_test = forest.score(X_test, y_test)\n",
        "y_pred_forest = forest.predict(X_test)\n",
        "cm_forest = confusion_matrix(y_test, y_pred_forest)"
      ],
      "metadata": {
        "id": "Kb49z0kRnYT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exibir os resultados\n",
        "print('Random Forest:')\n",
        "print('Training Score:', forest_score)\n",
        "print('Testing Score:', forest_test)\n",
        "print('Confusion Matrix:\\n', cm_forest)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iSyHt1G0ncR5",
        "outputId": "eb82d1dc-0d5a-43d0-cf69-f351b71db4cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest:\n",
            "Training Score: 0.9881578947368421\n",
            "Testing Score: 0.7649253731343284\n",
            "Confusion Matrix:\n",
            " [[141  28]\n",
            " [ 35  64]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "forest = RandomForestClassifier()\n",
        "param_grid = {'criterion': ['gini', 'entropy', 'log_loss'],\n",
        "              'max_features':['sqrt','log2'],\n",
        "              'n_estimators': [10, 20, 30, 60]}"
      ],
      "metadata": {
        "id": "TEyKTRDmziWc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cria o objeto g_search\n",
        "g_search = GridSearchCV(estimator = forest, param_grid = param_grid,\n",
        "                        refit=True, scoring='accuracy', cv = 10)"
      ],
      "metadata": {
        "id": "h8jUcnu8zX6e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Faz o treinamento\n",
        "g_search.fit(X_train_oversampled, y_train_oversampled);\n",
        "print(g_search.best_params_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nbYftLv1_r3x",
        "outputId": "57220130-e1df-4cb7-d3dd-59f01ff5c24e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'criterion': 'gini', 'max_features': 'sqrt', 'n_estimators': 60}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Carrega todos os dados do GridSearch em um Dataframe\n",
        "g_results =  pd.DataFrame(g_search.cv_results_)\n",
        "g_search.cv_results_.keys()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GuuyNpvW_wbU",
        "outputId": "374de6a5-1b58-412e-e0de-124c37db9b1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['mean_fit_time', 'std_fit_time', 'mean_score_time', 'std_score_time', 'param_criterion', 'param_max_features', 'param_n_estimators', 'params', 'split0_test_score', 'split1_test_score', 'split2_test_score', 'split3_test_score', 'split4_test_score', 'split5_test_score', 'split6_test_score', 'split7_test_score', 'split8_test_score', 'split9_test_score', 'mean_test_score', 'std_test_score', 'rank_test_score'])"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtém a média das acurácias (10 folds) referente ao conjunto treino\n",
        "forest_grid = g_results.loc[g_search.best_index_,'mean_test_score']\n",
        "print(forest_grid)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KHM-_LS5_x_c",
        "outputId": "9ce2d4e9-7034-453f-c4ef-99141f00ce2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.843421052631579\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Avalia o conjunto teste  com o melhor conjunto de parâmetros encontrado\n",
        "# best_estimator_ .Para tanto, o parâmetro refit precisa ser igual a True\n",
        "model = g_search.best_estimator_\n",
        "model.score(X_test,y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iCjVsG6j1BH_",
        "outputId": "b3967ef2-788e-4ac6-c752-282c5d9b1d3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7723880597014925"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RandomSearchCV"
      ],
      "metadata": {
        "id": "MhXagW-T_3rB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cria o objeto r_search\n",
        "r_search = RandomizedSearchCV(estimator = forest, param_distributions = param_grid,\n",
        "                        n_iter= 10, cv = 10, return_train_score=True)"
      ],
      "metadata": {
        "id": "Q0SGJxiqALo1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sm = SMOTE()\n",
        "x_train_oversampled, y_train_oversampled = sm.fit_resample(X_train, y_train)"
      ],
      "metadata": {
        "id": "sG8LWZPB0hSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Faz o treinamento\n",
        "r_search.fit(x_train_oversampled, y_train_oversampled);\n",
        "print(r_search.best_params_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FwjRdKE5ASV4",
        "outputId": "50ddd063-57d6-41ff-eb9b-cdd858685968"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'n_estimators': 60, 'max_features': 'log2', 'criterion': 'gini'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtém a média das acurácias (10 folds) referente ao conjunto treino\n",
        "forest_random = r_search.cv_results_['mean_test_score'][r_search.best_index_]\n",
        "print(forest_random)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "do86ri1r0n22",
        "outputId": "46fe6f76-4fda-4ab4-ac5e-0ece3e9c4625"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8328947368421054\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Avalia o conjunto teste  com o melhor conjunto de parâmetros encontrado\n",
        "# best_estimator_ .Para tanto, o parâmetro refit precisa ser igual a True\n",
        "model = r_search.best_estimator_\n",
        "model.score(X_test,y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GiAE4QOvn-nV",
        "outputId": "630a87ea-78a1-435c-bd28-cdfb32f03b7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7723880597014925"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7uZEV2OOcBm"
      },
      "source": [
        "## Rede Neural (Scikit) - 1 camada\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "REDE NEURAL SCIKIT - 1 CAMADA"
      ],
      "metadata": {
        "id": "eafcXQQTErsa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "rna = MLPClassifier(hidden_layer_sizes=(3,), activation='relu', solver='sgd', max_iter=1000,\n",
        "                    tol=0.0001, random_state=3, verbose=True)"
      ],
      "metadata": {
        "id": "SfonYc60ErXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rna.fit(X_train_oversampled, y_train_oversampled)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2e9PMBcBj6yR",
        "outputId": "e571b692-3ea7-4a97-eb90-b13c8d93b133"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 0.70811314\n",
            "Iteration 2, loss = 0.70779564\n",
            "Iteration 3, loss = 0.70727795\n",
            "Iteration 4, loss = 0.70671937\n",
            "Iteration 5, loss = 0.70605760\n",
            "Iteration 6, loss = 0.70537506\n",
            "Iteration 7, loss = 0.70471013\n",
            "Iteration 8, loss = 0.70399217\n",
            "Iteration 9, loss = 0.70329075\n",
            "Iteration 10, loss = 0.70260582\n",
            "Iteration 11, loss = 0.70197382\n",
            "Iteration 12, loss = 0.70134424\n",
            "Iteration 13, loss = 0.70064070\n",
            "Iteration 14, loss = 0.70001321\n",
            "Iteration 15, loss = 0.69938523\n",
            "Iteration 16, loss = 0.69882038\n",
            "Iteration 17, loss = 0.69818148\n",
            "Iteration 18, loss = 0.69761449\n",
            "Iteration 19, loss = 0.69701386\n",
            "Iteration 20, loss = 0.69643609\n",
            "Iteration 21, loss = 0.69586792\n",
            "Iteration 22, loss = 0.69528799\n",
            "Iteration 23, loss = 0.69468313\n",
            "Iteration 24, loss = 0.69406001\n",
            "Iteration 25, loss = 0.69350887\n",
            "Iteration 26, loss = 0.69295252\n",
            "Iteration 27, loss = 0.69236209\n",
            "Iteration 28, loss = 0.69178408\n",
            "Iteration 29, loss = 0.69123900\n",
            "Iteration 30, loss = 0.69070662\n",
            "Iteration 31, loss = 0.69018766\n",
            "Iteration 32, loss = 0.68965340\n",
            "Iteration 33, loss = 0.68914579\n",
            "Iteration 34, loss = 0.68862421\n",
            "Iteration 35, loss = 0.68816746\n",
            "Iteration 36, loss = 0.68766698\n",
            "Iteration 37, loss = 0.68717985\n",
            "Iteration 38, loss = 0.68664308\n",
            "Iteration 39, loss = 0.68617842\n",
            "Iteration 40, loss = 0.68571376\n",
            "Iteration 41, loss = 0.68525222\n",
            "Iteration 42, loss = 0.68478559\n",
            "Iteration 43, loss = 0.68433218\n",
            "Iteration 44, loss = 0.68387629\n",
            "Iteration 45, loss = 0.68343087\n",
            "Iteration 46, loss = 0.68304698\n",
            "Iteration 47, loss = 0.68259084\n",
            "Iteration 48, loss = 0.68218816\n",
            "Iteration 49, loss = 0.68180439\n",
            "Iteration 50, loss = 0.68139402\n",
            "Iteration 51, loss = 0.68102990\n",
            "Iteration 52, loss = 0.68065296\n",
            "Iteration 53, loss = 0.68030991\n",
            "Iteration 54, loss = 0.67991271\n",
            "Iteration 55, loss = 0.67954578\n",
            "Iteration 56, loss = 0.67917861\n",
            "Iteration 57, loss = 0.67885141\n",
            "Iteration 58, loss = 0.67850058\n",
            "Iteration 59, loss = 0.67814521\n",
            "Iteration 60, loss = 0.67780481\n",
            "Iteration 61, loss = 0.67746388\n",
            "Iteration 62, loss = 0.67711338\n",
            "Iteration 63, loss = 0.67680344\n",
            "Iteration 64, loss = 0.67645202\n",
            "Iteration 65, loss = 0.67613070\n",
            "Iteration 66, loss = 0.67578325\n",
            "Iteration 67, loss = 0.67545491\n",
            "Iteration 68, loss = 0.67513739\n",
            "Iteration 69, loss = 0.67480696\n",
            "Iteration 70, loss = 0.67447828\n",
            "Iteration 71, loss = 0.67415640\n",
            "Iteration 72, loss = 0.67385522\n",
            "Iteration 73, loss = 0.67351035\n",
            "Iteration 74, loss = 0.67319646\n",
            "Iteration 75, loss = 0.67286766\n",
            "Iteration 76, loss = 0.67254870\n",
            "Iteration 77, loss = 0.67222888\n",
            "Iteration 78, loss = 0.67191665\n",
            "Iteration 79, loss = 0.67159801\n",
            "Iteration 80, loss = 0.67127256\n",
            "Iteration 81, loss = 0.67095500\n",
            "Iteration 82, loss = 0.67063219\n",
            "Iteration 83, loss = 0.67030596\n",
            "Iteration 84, loss = 0.66997760\n",
            "Iteration 85, loss = 0.66967261\n",
            "Iteration 86, loss = 0.66934227\n",
            "Iteration 87, loss = 0.66902326\n",
            "Iteration 88, loss = 0.66870643\n",
            "Iteration 89, loss = 0.66838059\n",
            "Iteration 90, loss = 0.66806839\n",
            "Iteration 91, loss = 0.66775396\n",
            "Iteration 92, loss = 0.66742158\n",
            "Iteration 93, loss = 0.66710744\n",
            "Iteration 94, loss = 0.66679599\n",
            "Iteration 95, loss = 0.66647958\n",
            "Iteration 96, loss = 0.66616030\n",
            "Iteration 97, loss = 0.66583800\n",
            "Iteration 98, loss = 0.66550573\n",
            "Iteration 99, loss = 0.66518623\n",
            "Iteration 100, loss = 0.66489015\n",
            "Iteration 101, loss = 0.66455371\n",
            "Iteration 102, loss = 0.66420491\n",
            "Iteration 103, loss = 0.66390158\n",
            "Iteration 104, loss = 0.66356133\n",
            "Iteration 105, loss = 0.66324381\n",
            "Iteration 106, loss = 0.66292356\n",
            "Iteration 107, loss = 0.66259670\n",
            "Iteration 108, loss = 0.66227413\n",
            "Iteration 109, loss = 0.66193685\n",
            "Iteration 110, loss = 0.66162516\n",
            "Iteration 111, loss = 0.66130355\n",
            "Iteration 112, loss = 0.66097703\n",
            "Iteration 113, loss = 0.66066529\n",
            "Iteration 114, loss = 0.66033743\n",
            "Iteration 115, loss = 0.66002216\n",
            "Iteration 116, loss = 0.65968271\n",
            "Iteration 117, loss = 0.65936919\n",
            "Iteration 118, loss = 0.65904506\n",
            "Iteration 119, loss = 0.65872226\n",
            "Iteration 120, loss = 0.65839995\n",
            "Iteration 121, loss = 0.65809963\n",
            "Iteration 122, loss = 0.65776398\n",
            "Iteration 123, loss = 0.65744704\n",
            "Iteration 124, loss = 0.65710317\n",
            "Iteration 125, loss = 0.65679966\n",
            "Iteration 126, loss = 0.65647078\n",
            "Iteration 127, loss = 0.65614086\n",
            "Iteration 128, loss = 0.65582876\n",
            "Iteration 129, loss = 0.65549759\n",
            "Iteration 130, loss = 0.65515482\n",
            "Iteration 131, loss = 0.65482899\n",
            "Iteration 132, loss = 0.65449789\n",
            "Iteration 133, loss = 0.65416893\n",
            "Iteration 134, loss = 0.65382294\n",
            "Iteration 135, loss = 0.65348952\n",
            "Iteration 136, loss = 0.65315464\n",
            "Iteration 137, loss = 0.65282224\n",
            "Iteration 138, loss = 0.65248990\n",
            "Iteration 139, loss = 0.65213941\n",
            "Iteration 140, loss = 0.65182384\n",
            "Iteration 141, loss = 0.65148239\n",
            "Iteration 142, loss = 0.65115541\n",
            "Iteration 143, loss = 0.65082682\n",
            "Iteration 144, loss = 0.65048654\n",
            "Iteration 145, loss = 0.65016909\n",
            "Iteration 146, loss = 0.64982949\n",
            "Iteration 147, loss = 0.64949604\n",
            "Iteration 148, loss = 0.64917147\n",
            "Iteration 149, loss = 0.64883327\n",
            "Iteration 150, loss = 0.64850429\n",
            "Iteration 151, loss = 0.64819420\n",
            "Iteration 152, loss = 0.64783964\n",
            "Iteration 153, loss = 0.64751245\n",
            "Iteration 154, loss = 0.64719532\n",
            "Iteration 155, loss = 0.64683877\n",
            "Iteration 156, loss = 0.64651674\n",
            "Iteration 157, loss = 0.64619524\n",
            "Iteration 158, loss = 0.64586758\n",
            "Iteration 159, loss = 0.64553624\n",
            "Iteration 160, loss = 0.64520010\n",
            "Iteration 161, loss = 0.64487193\n",
            "Iteration 162, loss = 0.64454484\n",
            "Iteration 163, loss = 0.64421218\n",
            "Iteration 164, loss = 0.64387893\n",
            "Iteration 165, loss = 0.64355306\n",
            "Iteration 166, loss = 0.64322666\n",
            "Iteration 167, loss = 0.64287941\n",
            "Iteration 168, loss = 0.64254617\n",
            "Iteration 169, loss = 0.64223441\n",
            "Iteration 170, loss = 0.64189255\n",
            "Iteration 171, loss = 0.64155030\n",
            "Iteration 172, loss = 0.64121494\n",
            "Iteration 173, loss = 0.64087449\n",
            "Iteration 174, loss = 0.64054955\n",
            "Iteration 175, loss = 0.64022683\n",
            "Iteration 176, loss = 0.63988580\n",
            "Iteration 177, loss = 0.63956132\n",
            "Iteration 178, loss = 0.63922813\n",
            "Iteration 179, loss = 0.63891025\n",
            "Iteration 180, loss = 0.63857590\n",
            "Iteration 181, loss = 0.63823936\n",
            "Iteration 182, loss = 0.63790220\n",
            "Iteration 183, loss = 0.63758503\n",
            "Iteration 184, loss = 0.63724697\n",
            "Iteration 185, loss = 0.63692311\n",
            "Iteration 186, loss = 0.63659277\n",
            "Iteration 187, loss = 0.63626247\n",
            "Iteration 188, loss = 0.63592623\n",
            "Iteration 189, loss = 0.63559044\n",
            "Iteration 190, loss = 0.63527046\n",
            "Iteration 191, loss = 0.63493331\n",
            "Iteration 192, loss = 0.63461128\n",
            "Iteration 193, loss = 0.63426992\n",
            "Iteration 194, loss = 0.63394756\n",
            "Iteration 195, loss = 0.63360771\n",
            "Iteration 196, loss = 0.63327690\n",
            "Iteration 197, loss = 0.63295429\n",
            "Iteration 198, loss = 0.63263054\n",
            "Iteration 199, loss = 0.63229913\n",
            "Iteration 200, loss = 0.63196926\n",
            "Iteration 201, loss = 0.63164838\n",
            "Iteration 202, loss = 0.63132066\n",
            "Iteration 203, loss = 0.63100015\n",
            "Iteration 204, loss = 0.63066454\n",
            "Iteration 205, loss = 0.63034815\n",
            "Iteration 206, loss = 0.63002469\n",
            "Iteration 207, loss = 0.62970702\n",
            "Iteration 208, loss = 0.62938677\n",
            "Iteration 209, loss = 0.62907985\n",
            "Iteration 210, loss = 0.62874243\n",
            "Iteration 211, loss = 0.62842794\n",
            "Iteration 212, loss = 0.62810900\n",
            "Iteration 213, loss = 0.62777452\n",
            "Iteration 214, loss = 0.62744468\n",
            "Iteration 215, loss = 0.62710768\n",
            "Iteration 216, loss = 0.62677516\n",
            "Iteration 217, loss = 0.62644300\n",
            "Iteration 218, loss = 0.62610200\n",
            "Iteration 219, loss = 0.62576215\n",
            "Iteration 220, loss = 0.62541802\n",
            "Iteration 221, loss = 0.62507946\n",
            "Iteration 222, loss = 0.62476323\n",
            "Iteration 223, loss = 0.62439131\n",
            "Iteration 224, loss = 0.62406966\n",
            "Iteration 225, loss = 0.62372738\n",
            "Iteration 226, loss = 0.62338572\n",
            "Iteration 227, loss = 0.62304653\n",
            "Iteration 228, loss = 0.62271207\n",
            "Iteration 229, loss = 0.62237446\n",
            "Iteration 230, loss = 0.62202858\n",
            "Iteration 231, loss = 0.62169183\n",
            "Iteration 232, loss = 0.62135521\n",
            "Iteration 233, loss = 0.62100832\n",
            "Iteration 234, loss = 0.62066269\n",
            "Iteration 235, loss = 0.62031616\n",
            "Iteration 236, loss = 0.61994036\n",
            "Iteration 237, loss = 0.61958169\n",
            "Iteration 238, loss = 0.61923418\n",
            "Iteration 239, loss = 0.61886131\n",
            "Iteration 240, loss = 0.61848917\n",
            "Iteration 241, loss = 0.61812630\n",
            "Iteration 242, loss = 0.61777183\n",
            "Iteration 243, loss = 0.61740536\n",
            "Iteration 244, loss = 0.61702860\n",
            "Iteration 245, loss = 0.61668065\n",
            "Iteration 246, loss = 0.61629538\n",
            "Iteration 247, loss = 0.61594310\n",
            "Iteration 248, loss = 0.61556818\n",
            "Iteration 249, loss = 0.61522249\n",
            "Iteration 250, loss = 0.61484872\n",
            "Iteration 251, loss = 0.61448978\n",
            "Iteration 252, loss = 0.61410757\n",
            "Iteration 253, loss = 0.61373821\n",
            "Iteration 254, loss = 0.61338853\n",
            "Iteration 255, loss = 0.61300269\n",
            "Iteration 256, loss = 0.61264567\n",
            "Iteration 257, loss = 0.61226197\n",
            "Iteration 258, loss = 0.61189415\n",
            "Iteration 259, loss = 0.61152686\n",
            "Iteration 260, loss = 0.61117590\n",
            "Iteration 261, loss = 0.61078874\n",
            "Iteration 262, loss = 0.61042790\n",
            "Iteration 263, loss = 0.61005914\n",
            "Iteration 264, loss = 0.60968095\n",
            "Iteration 265, loss = 0.60932201\n",
            "Iteration 266, loss = 0.60893242\n",
            "Iteration 267, loss = 0.60855001\n",
            "Iteration 268, loss = 0.60818886\n",
            "Iteration 269, loss = 0.60781989\n",
            "Iteration 270, loss = 0.60742827\n",
            "Iteration 271, loss = 0.60706160\n",
            "Iteration 272, loss = 0.60667484\n",
            "Iteration 273, loss = 0.60628098\n",
            "Iteration 274, loss = 0.60589891\n",
            "Iteration 275, loss = 0.60550429\n",
            "Iteration 276, loss = 0.60510447\n",
            "Iteration 277, loss = 0.60471023\n",
            "Iteration 278, loss = 0.60432510\n",
            "Iteration 279, loss = 0.60393620\n",
            "Iteration 280, loss = 0.60352529\n",
            "Iteration 281, loss = 0.60313330\n",
            "Iteration 282, loss = 0.60274100\n",
            "Iteration 283, loss = 0.60235473\n",
            "Iteration 284, loss = 0.60195382\n",
            "Iteration 285, loss = 0.60155865\n",
            "Iteration 286, loss = 0.60115836\n",
            "Iteration 287, loss = 0.60077747\n",
            "Iteration 288, loss = 0.60038124\n",
            "Iteration 289, loss = 0.59998189\n",
            "Iteration 290, loss = 0.59959445\n",
            "Iteration 291, loss = 0.59921668\n",
            "Iteration 292, loss = 0.59881072\n",
            "Iteration 293, loss = 0.59842785\n",
            "Iteration 294, loss = 0.59804350\n",
            "Iteration 295, loss = 0.59764593\n",
            "Iteration 296, loss = 0.59726667\n",
            "Iteration 297, loss = 0.59687343\n",
            "Iteration 298, loss = 0.59645239\n",
            "Iteration 299, loss = 0.59604168\n",
            "Iteration 300, loss = 0.59562151\n",
            "Iteration 301, loss = 0.59518980\n",
            "Iteration 302, loss = 0.59478628\n",
            "Iteration 303, loss = 0.59433570\n",
            "Iteration 304, loss = 0.59392102\n",
            "Iteration 305, loss = 0.59349067\n",
            "Iteration 306, loss = 0.59304544\n",
            "Iteration 307, loss = 0.59262579\n",
            "Iteration 308, loss = 0.59216132\n",
            "Iteration 309, loss = 0.59173513\n",
            "Iteration 310, loss = 0.59128326\n",
            "Iteration 311, loss = 0.59083548\n",
            "Iteration 312, loss = 0.59039698\n",
            "Iteration 313, loss = 0.58995074\n",
            "Iteration 314, loss = 0.58950127\n",
            "Iteration 315, loss = 0.58906713\n",
            "Iteration 316, loss = 0.58861835\n",
            "Iteration 317, loss = 0.58816998\n",
            "Iteration 318, loss = 0.58771595\n",
            "Iteration 319, loss = 0.58729287\n",
            "Iteration 320, loss = 0.58685192\n",
            "Iteration 321, loss = 0.58639652\n",
            "Iteration 322, loss = 0.58595462\n",
            "Iteration 323, loss = 0.58550075\n",
            "Iteration 324, loss = 0.58506115\n",
            "Iteration 325, loss = 0.58461929\n",
            "Iteration 326, loss = 0.58417537\n",
            "Iteration 327, loss = 0.58373658\n",
            "Iteration 328, loss = 0.58329345\n",
            "Iteration 329, loss = 0.58285386\n",
            "Iteration 330, loss = 0.58239611\n",
            "Iteration 331, loss = 0.58195375\n",
            "Iteration 332, loss = 0.58150649\n",
            "Iteration 333, loss = 0.58106578\n",
            "Iteration 334, loss = 0.58060874\n",
            "Iteration 335, loss = 0.58015956\n",
            "Iteration 336, loss = 0.57971898\n",
            "Iteration 337, loss = 0.57927914\n",
            "Iteration 338, loss = 0.57881341\n",
            "Iteration 339, loss = 0.57839390\n",
            "Iteration 340, loss = 0.57793776\n",
            "Iteration 341, loss = 0.57748703\n",
            "Iteration 342, loss = 0.57703499\n",
            "Iteration 343, loss = 0.57659157\n",
            "Iteration 344, loss = 0.57614965\n",
            "Iteration 345, loss = 0.57570015\n",
            "Iteration 346, loss = 0.57525416\n",
            "Iteration 347, loss = 0.57481669\n",
            "Iteration 348, loss = 0.57436564\n",
            "Iteration 349, loss = 0.57391658\n",
            "Iteration 350, loss = 0.57350332\n",
            "Iteration 351, loss = 0.57304031\n",
            "Iteration 352, loss = 0.57260797\n",
            "Iteration 353, loss = 0.57217592\n",
            "Iteration 354, loss = 0.57174004\n",
            "Iteration 355, loss = 0.57131903\n",
            "Iteration 356, loss = 0.57088045\n",
            "Iteration 357, loss = 0.57045965\n",
            "Iteration 358, loss = 0.57002891\n",
            "Iteration 359, loss = 0.56960001\n",
            "Iteration 360, loss = 0.56917389\n",
            "Iteration 361, loss = 0.56873882\n",
            "Iteration 362, loss = 0.56831682\n",
            "Iteration 363, loss = 0.56790034\n",
            "Iteration 364, loss = 0.56746762\n",
            "Iteration 365, loss = 0.56704080\n",
            "Iteration 366, loss = 0.56660775\n",
            "Iteration 367, loss = 0.56614591\n",
            "Iteration 368, loss = 0.56570276\n",
            "Iteration 369, loss = 0.56522707\n",
            "Iteration 370, loss = 0.56479332\n",
            "Iteration 371, loss = 0.56431514\n",
            "Iteration 372, loss = 0.56387419\n",
            "Iteration 373, loss = 0.56339823\n",
            "Iteration 374, loss = 0.56295410\n",
            "Iteration 375, loss = 0.56249425\n",
            "Iteration 376, loss = 0.56203804\n",
            "Iteration 377, loss = 0.56158457\n",
            "Iteration 378, loss = 0.56112855\n",
            "Iteration 379, loss = 0.56068028\n",
            "Iteration 380, loss = 0.56021531\n",
            "Iteration 381, loss = 0.55976766\n",
            "Iteration 382, loss = 0.55931844\n",
            "Iteration 383, loss = 0.55885657\n",
            "Iteration 384, loss = 0.55841325\n",
            "Iteration 385, loss = 0.55797766\n",
            "Iteration 386, loss = 0.55752422\n",
            "Iteration 387, loss = 0.55708164\n",
            "Iteration 388, loss = 0.55662868\n",
            "Iteration 389, loss = 0.55620003\n",
            "Iteration 390, loss = 0.55574649\n",
            "Iteration 391, loss = 0.55534441\n",
            "Iteration 392, loss = 0.55487709\n",
            "Iteration 393, loss = 0.55442769\n",
            "Iteration 394, loss = 0.55399933\n",
            "Iteration 395, loss = 0.55357314\n",
            "Iteration 396, loss = 0.55312421\n",
            "Iteration 397, loss = 0.55269692\n",
            "Iteration 398, loss = 0.55225215\n",
            "Iteration 399, loss = 0.55181466\n",
            "Iteration 400, loss = 0.55138684\n",
            "Iteration 401, loss = 0.55095621\n",
            "Iteration 402, loss = 0.55050990\n",
            "Iteration 403, loss = 0.55009134\n",
            "Iteration 404, loss = 0.54966030\n",
            "Iteration 405, loss = 0.54922747\n",
            "Iteration 406, loss = 0.54879077\n",
            "Iteration 407, loss = 0.54838125\n",
            "Iteration 408, loss = 0.54794708\n",
            "Iteration 409, loss = 0.54751855\n",
            "Iteration 410, loss = 0.54710159\n",
            "Iteration 411, loss = 0.54668271\n",
            "Iteration 412, loss = 0.54624268\n",
            "Iteration 413, loss = 0.54582003\n",
            "Iteration 414, loss = 0.54541089\n",
            "Iteration 415, loss = 0.54497974\n",
            "Iteration 416, loss = 0.54455695\n",
            "Iteration 417, loss = 0.54414285\n",
            "Iteration 418, loss = 0.54372292\n",
            "Iteration 419, loss = 0.54331222\n",
            "Iteration 420, loss = 0.54288855\n",
            "Iteration 421, loss = 0.54246263\n",
            "Iteration 422, loss = 0.54205098\n",
            "Iteration 423, loss = 0.54163464\n",
            "Iteration 424, loss = 0.54122952\n",
            "Iteration 425, loss = 0.54082386\n",
            "Iteration 426, loss = 0.54039034\n",
            "Iteration 427, loss = 0.53998909\n",
            "Iteration 428, loss = 0.53957671\n",
            "Iteration 429, loss = 0.53915950\n",
            "Iteration 430, loss = 0.53875801\n",
            "Iteration 431, loss = 0.53835249\n",
            "Iteration 432, loss = 0.53794146\n",
            "Iteration 433, loss = 0.53753552\n",
            "Iteration 434, loss = 0.53715901\n",
            "Iteration 435, loss = 0.53672850\n",
            "Iteration 436, loss = 0.53633618\n",
            "Iteration 437, loss = 0.53594384\n",
            "Iteration 438, loss = 0.53554115\n",
            "Iteration 439, loss = 0.53515181\n",
            "Iteration 440, loss = 0.53474909\n",
            "Iteration 441, loss = 0.53438747\n",
            "Iteration 442, loss = 0.53396879\n",
            "Iteration 443, loss = 0.53358293\n",
            "Iteration 444, loss = 0.53318915\n",
            "Iteration 445, loss = 0.53280663\n",
            "Iteration 446, loss = 0.53241388\n",
            "Iteration 447, loss = 0.53202698\n",
            "Iteration 448, loss = 0.53164309\n",
            "Iteration 449, loss = 0.53125866\n",
            "Iteration 450, loss = 0.53086240\n",
            "Iteration 451, loss = 0.53048476\n",
            "Iteration 452, loss = 0.53009768\n",
            "Iteration 453, loss = 0.52971678\n",
            "Iteration 454, loss = 0.52933271\n",
            "Iteration 455, loss = 0.52894040\n",
            "Iteration 456, loss = 0.52856824\n",
            "Iteration 457, loss = 0.52817975\n",
            "Iteration 458, loss = 0.52781109\n",
            "Iteration 459, loss = 0.52743340\n",
            "Iteration 460, loss = 0.52704045\n",
            "Iteration 461, loss = 0.52666510\n",
            "Iteration 462, loss = 0.52627895\n",
            "Iteration 463, loss = 0.52593309\n",
            "Iteration 464, loss = 0.52554732\n",
            "Iteration 465, loss = 0.52518240\n",
            "Iteration 466, loss = 0.52479804\n",
            "Iteration 467, loss = 0.52444067\n",
            "Iteration 468, loss = 0.52407709\n",
            "Iteration 469, loss = 0.52370418\n",
            "Iteration 470, loss = 0.52333892\n",
            "Iteration 471, loss = 0.52298375\n",
            "Iteration 472, loss = 0.52262650\n",
            "Iteration 473, loss = 0.52225672\n",
            "Iteration 474, loss = 0.52189527\n",
            "Iteration 475, loss = 0.52154358\n",
            "Iteration 476, loss = 0.52116523\n",
            "Iteration 477, loss = 0.52083124\n",
            "Iteration 478, loss = 0.52045660\n",
            "Iteration 479, loss = 0.52011238\n",
            "Iteration 480, loss = 0.51974792\n",
            "Iteration 481, loss = 0.51938336\n",
            "Iteration 482, loss = 0.51904318\n",
            "Iteration 483, loss = 0.51868063\n",
            "Iteration 484, loss = 0.51835158\n",
            "Iteration 485, loss = 0.51798239\n",
            "Iteration 486, loss = 0.51762609\n",
            "Iteration 487, loss = 0.51728042\n",
            "Iteration 488, loss = 0.51693287\n",
            "Iteration 489, loss = 0.51660503\n",
            "Iteration 490, loss = 0.51624021\n",
            "Iteration 491, loss = 0.51590199\n",
            "Iteration 492, loss = 0.51555441\n",
            "Iteration 493, loss = 0.51522969\n",
            "Iteration 494, loss = 0.51487376\n",
            "Iteration 495, loss = 0.51454935\n",
            "Iteration 496, loss = 0.51419996\n",
            "Iteration 497, loss = 0.51387217\n",
            "Iteration 498, loss = 0.51354762\n",
            "Iteration 499, loss = 0.51321431\n",
            "Iteration 500, loss = 0.51287579\n",
            "Iteration 501, loss = 0.51254479\n",
            "Iteration 502, loss = 0.51222569\n",
            "Iteration 503, loss = 0.51188881\n",
            "Iteration 504, loss = 0.51156082\n",
            "Iteration 505, loss = 0.51121094\n",
            "Iteration 506, loss = 0.51089562\n",
            "Iteration 507, loss = 0.51056811\n",
            "Iteration 508, loss = 0.51023074\n",
            "Iteration 509, loss = 0.50991507\n",
            "Iteration 510, loss = 0.50959376\n",
            "Iteration 511, loss = 0.50925943\n",
            "Iteration 512, loss = 0.50894067\n",
            "Iteration 513, loss = 0.50862448\n",
            "Iteration 514, loss = 0.50830227\n",
            "Iteration 515, loss = 0.50798934\n",
            "Iteration 516, loss = 0.50767500\n",
            "Iteration 517, loss = 0.50735597\n",
            "Iteration 518, loss = 0.50703889\n",
            "Iteration 519, loss = 0.50672017\n",
            "Iteration 520, loss = 0.50641008\n",
            "Iteration 521, loss = 0.50609844\n",
            "Iteration 522, loss = 0.50577300\n",
            "Iteration 523, loss = 0.50547354\n",
            "Iteration 524, loss = 0.50516209\n",
            "Iteration 525, loss = 0.50486098\n",
            "Iteration 526, loss = 0.50454580\n",
            "Iteration 527, loss = 0.50424366\n",
            "Iteration 528, loss = 0.50393298\n",
            "Iteration 529, loss = 0.50363215\n",
            "Iteration 530, loss = 0.50333468\n",
            "Iteration 531, loss = 0.50305817\n",
            "Iteration 532, loss = 0.50273405\n",
            "Iteration 533, loss = 0.50244557\n",
            "Iteration 534, loss = 0.50214877\n",
            "Iteration 535, loss = 0.50187367\n",
            "Iteration 536, loss = 0.50156664\n",
            "Iteration 537, loss = 0.50127535\n",
            "Iteration 538, loss = 0.50098304\n",
            "Iteration 539, loss = 0.50069655\n",
            "Iteration 540, loss = 0.50041787\n",
            "Iteration 541, loss = 0.50013060\n",
            "Iteration 542, loss = 0.49983376\n",
            "Iteration 543, loss = 0.49955763\n",
            "Iteration 544, loss = 0.49928026\n",
            "Iteration 545, loss = 0.49899722\n",
            "Iteration 546, loss = 0.49870558\n",
            "Iteration 547, loss = 0.49843289\n",
            "Iteration 548, loss = 0.49815462\n",
            "Iteration 549, loss = 0.49787039\n",
            "Iteration 550, loss = 0.49760286\n",
            "Iteration 551, loss = 0.49732785\n",
            "Iteration 552, loss = 0.49705710\n",
            "Iteration 553, loss = 0.49678496\n",
            "Iteration 554, loss = 0.49650429\n",
            "Iteration 555, loss = 0.49622635\n",
            "Iteration 556, loss = 0.49595595\n",
            "Iteration 557, loss = 0.49568720\n",
            "Iteration 558, loss = 0.49540951\n",
            "Iteration 559, loss = 0.49513848\n",
            "Iteration 560, loss = 0.49486147\n",
            "Iteration 561, loss = 0.49459702\n",
            "Iteration 562, loss = 0.49432955\n",
            "Iteration 563, loss = 0.49406245\n",
            "Iteration 564, loss = 0.49380170\n",
            "Iteration 565, loss = 0.49353601\n",
            "Iteration 566, loss = 0.49325861\n",
            "Iteration 567, loss = 0.49299694\n",
            "Iteration 568, loss = 0.49272543\n",
            "Iteration 569, loss = 0.49247487\n",
            "Iteration 570, loss = 0.49221875\n",
            "Iteration 571, loss = 0.49194573\n",
            "Iteration 572, loss = 0.49168705\n",
            "Iteration 573, loss = 0.49143473\n",
            "Iteration 574, loss = 0.49116307\n",
            "Iteration 575, loss = 0.49091627\n",
            "Iteration 576, loss = 0.49066439\n",
            "Iteration 577, loss = 0.49040290\n",
            "Iteration 578, loss = 0.49015760\n",
            "Iteration 579, loss = 0.48989879\n",
            "Iteration 580, loss = 0.48964316\n",
            "Iteration 581, loss = 0.48938241\n",
            "Iteration 582, loss = 0.48915512\n",
            "Iteration 583, loss = 0.48890449\n",
            "Iteration 584, loss = 0.48863970\n",
            "Iteration 585, loss = 0.48839967\n",
            "Iteration 586, loss = 0.48814496\n",
            "Iteration 587, loss = 0.48791333\n",
            "Iteration 588, loss = 0.48765749\n",
            "Iteration 589, loss = 0.48741239\n",
            "Iteration 590, loss = 0.48716542\n",
            "Iteration 591, loss = 0.48692319\n",
            "Iteration 592, loss = 0.48668581\n",
            "Iteration 593, loss = 0.48644853\n",
            "Iteration 594, loss = 0.48620093\n",
            "Iteration 595, loss = 0.48596722\n",
            "Iteration 596, loss = 0.48574161\n",
            "Iteration 597, loss = 0.48550302\n",
            "Iteration 598, loss = 0.48524993\n",
            "Iteration 599, loss = 0.48502290\n",
            "Iteration 600, loss = 0.48479023\n",
            "Iteration 601, loss = 0.48456361\n",
            "Iteration 602, loss = 0.48431933\n",
            "Iteration 603, loss = 0.48409184\n",
            "Iteration 604, loss = 0.48388122\n",
            "Iteration 605, loss = 0.48362867\n",
            "Iteration 606, loss = 0.48340886\n",
            "Iteration 607, loss = 0.48318375\n",
            "Iteration 608, loss = 0.48294837\n",
            "Iteration 609, loss = 0.48273307\n",
            "Iteration 610, loss = 0.48251450\n",
            "Iteration 611, loss = 0.48230846\n",
            "Iteration 612, loss = 0.48207081\n",
            "Iteration 613, loss = 0.48184768\n",
            "Iteration 614, loss = 0.48161802\n",
            "Iteration 615, loss = 0.48141371\n",
            "Iteration 616, loss = 0.48118202\n",
            "Iteration 617, loss = 0.48096315\n",
            "Iteration 618, loss = 0.48076272\n",
            "Iteration 619, loss = 0.48054022\n",
            "Iteration 620, loss = 0.48031876\n",
            "Iteration 621, loss = 0.48010713\n",
            "Iteration 622, loss = 0.47990250\n",
            "Iteration 623, loss = 0.47968199\n",
            "Iteration 624, loss = 0.47946638\n",
            "Iteration 625, loss = 0.47925354\n",
            "Iteration 626, loss = 0.47905370\n",
            "Iteration 627, loss = 0.47883368\n",
            "Iteration 628, loss = 0.47862853\n",
            "Iteration 629, loss = 0.47843856\n",
            "Iteration 630, loss = 0.47820782\n",
            "Iteration 631, loss = 0.47800230\n",
            "Iteration 632, loss = 0.47780930\n",
            "Iteration 633, loss = 0.47759804\n",
            "Iteration 634, loss = 0.47738485\n",
            "Iteration 635, loss = 0.47719227\n",
            "Iteration 636, loss = 0.47698472\n",
            "Iteration 637, loss = 0.47678899\n",
            "Iteration 638, loss = 0.47660496\n",
            "Iteration 639, loss = 0.47638572\n",
            "Iteration 640, loss = 0.47620350\n",
            "Iteration 641, loss = 0.47599541\n",
            "Iteration 642, loss = 0.47580194\n",
            "Iteration 643, loss = 0.47560335\n",
            "Iteration 644, loss = 0.47540796\n",
            "Iteration 645, loss = 0.47521013\n",
            "Iteration 646, loss = 0.47499973\n",
            "Iteration 647, loss = 0.47482427\n",
            "Iteration 648, loss = 0.47462323\n",
            "Iteration 649, loss = 0.47443280\n",
            "Iteration 650, loss = 0.47424012\n",
            "Iteration 651, loss = 0.47405321\n",
            "Iteration 652, loss = 0.47386414\n",
            "Iteration 653, loss = 0.47367042\n",
            "Iteration 654, loss = 0.47349022\n",
            "Iteration 655, loss = 0.47329886\n",
            "Iteration 656, loss = 0.47309985\n",
            "Iteration 657, loss = 0.47293220\n",
            "Iteration 658, loss = 0.47273754\n",
            "Iteration 659, loss = 0.47254693\n",
            "Iteration 660, loss = 0.47237177\n",
            "Iteration 661, loss = 0.47218179\n",
            "Iteration 662, loss = 0.47200550\n",
            "Iteration 663, loss = 0.47182392\n",
            "Iteration 664, loss = 0.47163700\n",
            "Iteration 665, loss = 0.47145089\n",
            "Iteration 666, loss = 0.47127391\n",
            "Iteration 667, loss = 0.47109530\n",
            "Iteration 668, loss = 0.47091844\n",
            "Iteration 669, loss = 0.47073969\n",
            "Iteration 670, loss = 0.47057842\n",
            "Iteration 671, loss = 0.47038828\n",
            "Iteration 672, loss = 0.47020995\n",
            "Iteration 673, loss = 0.47005360\n",
            "Iteration 674, loss = 0.46985603\n",
            "Iteration 675, loss = 0.46969013\n",
            "Iteration 676, loss = 0.46951294\n",
            "Iteration 677, loss = 0.46934203\n",
            "Iteration 678, loss = 0.46918674\n",
            "Iteration 679, loss = 0.46900661\n",
            "Iteration 680, loss = 0.46883218\n",
            "Iteration 681, loss = 0.46866711\n",
            "Iteration 682, loss = 0.46851015\n",
            "Iteration 683, loss = 0.46833721\n",
            "Iteration 684, loss = 0.46816752\n",
            "Iteration 685, loss = 0.46800322\n",
            "Iteration 686, loss = 0.46784177\n",
            "Iteration 687, loss = 0.46769137\n",
            "Iteration 688, loss = 0.46751806\n",
            "Iteration 689, loss = 0.46734363\n",
            "Iteration 690, loss = 0.46719501\n",
            "Iteration 691, loss = 0.46702782\n",
            "Iteration 692, loss = 0.46685843\n",
            "Iteration 693, loss = 0.46670633\n",
            "Iteration 694, loss = 0.46655344\n",
            "Iteration 695, loss = 0.46639522\n",
            "Iteration 696, loss = 0.46622852\n",
            "Iteration 697, loss = 0.46608796\n",
            "Iteration 698, loss = 0.46591031\n",
            "Iteration 699, loss = 0.46576410\n",
            "Iteration 700, loss = 0.46560886\n",
            "Iteration 701, loss = 0.46545061\n",
            "Iteration 702, loss = 0.46530121\n",
            "Iteration 703, loss = 0.46514845\n",
            "Iteration 704, loss = 0.46499896\n",
            "Iteration 705, loss = 0.46484940\n",
            "Iteration 706, loss = 0.46468939\n",
            "Iteration 707, loss = 0.46454671\n",
            "Iteration 708, loss = 0.46439077\n",
            "Iteration 709, loss = 0.46424305\n",
            "Iteration 710, loss = 0.46409737\n",
            "Iteration 711, loss = 0.46397216\n",
            "Iteration 712, loss = 0.46379331\n",
            "Iteration 713, loss = 0.46365004\n",
            "Iteration 714, loss = 0.46350778\n",
            "Iteration 715, loss = 0.46337319\n",
            "Iteration 716, loss = 0.46322898\n",
            "Iteration 717, loss = 0.46307437\n",
            "Iteration 718, loss = 0.46295862\n",
            "Iteration 719, loss = 0.46278590\n",
            "Iteration 720, loss = 0.46266595\n",
            "Iteration 721, loss = 0.46250916\n",
            "Iteration 722, loss = 0.46237618\n",
            "Iteration 723, loss = 0.46222769\n",
            "Iteration 724, loss = 0.46208123\n",
            "Iteration 725, loss = 0.46196083\n",
            "Iteration 726, loss = 0.46181797\n",
            "Iteration 727, loss = 0.46169212\n",
            "Iteration 728, loss = 0.46154058\n",
            "Iteration 729, loss = 0.46142948\n",
            "Iteration 730, loss = 0.46127882\n",
            "Iteration 731, loss = 0.46113367\n",
            "Iteration 732, loss = 0.46100142\n",
            "Iteration 733, loss = 0.46086434\n",
            "Iteration 734, loss = 0.46073958\n",
            "Iteration 735, loss = 0.46059998\n",
            "Iteration 736, loss = 0.46048366\n",
            "Iteration 737, loss = 0.46033875\n",
            "Iteration 738, loss = 0.46021260\n",
            "Iteration 739, loss = 0.46007624\n",
            "Iteration 740, loss = 0.45995927\n",
            "Iteration 741, loss = 0.45983347\n",
            "Iteration 742, loss = 0.45969630\n",
            "Iteration 743, loss = 0.45956235\n",
            "Iteration 744, loss = 0.45943914\n",
            "Iteration 745, loss = 0.45930990\n",
            "Iteration 746, loss = 0.45918556\n",
            "Iteration 747, loss = 0.45908110\n",
            "Iteration 748, loss = 0.45893194\n",
            "Iteration 749, loss = 0.45881062\n",
            "Iteration 750, loss = 0.45869987\n",
            "Iteration 751, loss = 0.45855589\n",
            "Iteration 752, loss = 0.45844154\n",
            "Iteration 753, loss = 0.45830746\n",
            "Iteration 754, loss = 0.45820233\n",
            "Iteration 755, loss = 0.45806874\n",
            "Iteration 756, loss = 0.45795423\n",
            "Iteration 757, loss = 0.45784167\n",
            "Iteration 758, loss = 0.45770991\n",
            "Iteration 759, loss = 0.45759326\n",
            "Iteration 760, loss = 0.45746816\n",
            "Iteration 761, loss = 0.45734922\n",
            "Iteration 762, loss = 0.45724836\n",
            "Iteration 763, loss = 0.45711706\n",
            "Iteration 764, loss = 0.45699408\n",
            "Iteration 765, loss = 0.45688632\n",
            "Iteration 766, loss = 0.45676512\n",
            "Iteration 767, loss = 0.45663845\n",
            "Iteration 768, loss = 0.45652683\n",
            "Iteration 769, loss = 0.45641855\n",
            "Iteration 770, loss = 0.45629363\n",
            "Iteration 771, loss = 0.45617437\n",
            "Iteration 772, loss = 0.45607061\n",
            "Iteration 773, loss = 0.45596477\n",
            "Iteration 774, loss = 0.45584224\n",
            "Iteration 775, loss = 0.45572852\n",
            "Iteration 776, loss = 0.45561866\n",
            "Iteration 777, loss = 0.45550357\n",
            "Iteration 778, loss = 0.45540526\n",
            "Iteration 779, loss = 0.45529240\n",
            "Iteration 780, loss = 0.45517781\n",
            "Iteration 781, loss = 0.45506714\n",
            "Iteration 782, loss = 0.45495781\n",
            "Iteration 783, loss = 0.45487717\n",
            "Iteration 784, loss = 0.45474907\n",
            "Iteration 785, loss = 0.45464292\n",
            "Iteration 786, loss = 0.45453682\n",
            "Iteration 787, loss = 0.45443473\n",
            "Iteration 788, loss = 0.45433821\n",
            "Iteration 789, loss = 0.45421639\n",
            "Iteration 790, loss = 0.45410588\n",
            "Iteration 791, loss = 0.45399855\n",
            "Iteration 792, loss = 0.45390484\n",
            "Iteration 793, loss = 0.45381023\n",
            "Iteration 794, loss = 0.45369534\n",
            "Iteration 795, loss = 0.45358565\n",
            "Iteration 796, loss = 0.45348810\n",
            "Iteration 797, loss = 0.45338389\n",
            "Iteration 798, loss = 0.45327890\n",
            "Iteration 799, loss = 0.45318615\n",
            "Iteration 800, loss = 0.45308189\n",
            "Iteration 801, loss = 0.45297595\n",
            "Iteration 802, loss = 0.45287679\n",
            "Iteration 803, loss = 0.45277636\n",
            "Iteration 804, loss = 0.45268915\n",
            "Iteration 805, loss = 0.45256808\n",
            "Iteration 806, loss = 0.45247473\n",
            "Iteration 807, loss = 0.45237861\n",
            "Iteration 808, loss = 0.45228400\n",
            "Iteration 809, loss = 0.45218270\n",
            "Iteration 810, loss = 0.45208514\n",
            "Iteration 811, loss = 0.45200218\n",
            "Iteration 812, loss = 0.45189470\n",
            "Iteration 813, loss = 0.45181745\n",
            "Iteration 814, loss = 0.45170187\n",
            "Iteration 815, loss = 0.45161189\n",
            "Iteration 816, loss = 0.45150876\n",
            "Iteration 817, loss = 0.45141565\n",
            "Iteration 818, loss = 0.45132496\n",
            "Iteration 819, loss = 0.45124213\n",
            "Iteration 820, loss = 0.45114177\n",
            "Iteration 821, loss = 0.45104757\n",
            "Iteration 822, loss = 0.45096036\n",
            "Iteration 823, loss = 0.45085906\n",
            "Iteration 824, loss = 0.45077248\n",
            "Iteration 825, loss = 0.45068557\n",
            "Iteration 826, loss = 0.45058485\n",
            "Iteration 827, loss = 0.45049978\n",
            "Iteration 828, loss = 0.45040065\n",
            "Iteration 829, loss = 0.45031285\n",
            "Iteration 830, loss = 0.45022743\n",
            "Iteration 831, loss = 0.45013103\n",
            "Iteration 832, loss = 0.45005142\n",
            "Iteration 833, loss = 0.44998578\n",
            "Iteration 834, loss = 0.44986600\n",
            "Iteration 835, loss = 0.44977677\n",
            "Iteration 836, loss = 0.44968606\n",
            "Iteration 837, loss = 0.44960805\n",
            "Iteration 838, loss = 0.44950722\n",
            "Iteration 839, loss = 0.44943033\n",
            "Iteration 840, loss = 0.44933718\n",
            "Iteration 841, loss = 0.44925898\n",
            "Iteration 842, loss = 0.44916845\n",
            "Iteration 843, loss = 0.44907826\n",
            "Iteration 844, loss = 0.44898602\n",
            "Iteration 845, loss = 0.44891469\n",
            "Iteration 846, loss = 0.44881790\n",
            "Iteration 847, loss = 0.44873650\n",
            "Iteration 848, loss = 0.44865509\n",
            "Iteration 849, loss = 0.44856785\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(hidden_layer_sizes=(3,), max_iter=1000, random_state=3,\n",
              "              solver='sgd', verbose=True)"
            ],
            "text/html": [
              "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(hidden_layer_sizes=(3,), max_iter=1000, random_state=3,\n",
              "              solver=&#x27;sgd&#x27;, verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(hidden_layer_sizes=(3,), max_iter=1000, random_state=3,\n",
              "              solver=&#x27;sgd&#x27;, verbose=True)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "previsoes = rna.predict(X_test)"
      ],
      "metadata": {
        "id": "RWUw3HtWF5at"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rna_teste = accuracy_score(y_test, previsoes)"
      ],
      "metadata": {
        "id": "Ta1YmEA9zZ6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(rna_teste)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25SkQ47AzuHb",
        "outputId": "aab198fe-816a-4652-bbea-0b022f390bf0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7276119402985075\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cm_rna_teste = confusion_matrix(y_test, previsoes)"
      ],
      "metadata": {
        "id": "5P5yF1_eGDZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_test, previsoes))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0b9ep6YhGEy0",
        "outputId": "4c19dfe3-2eaa-4145-ab03-64f92a8f1fe6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.73      0.77       169\n",
            "           1       0.61      0.72      0.66        99\n",
            "\n",
            "    accuracy                           0.73       268\n",
            "   macro avg       0.71      0.73      0.72       268\n",
            "weighted avg       0.74      0.73      0.73       268\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "previsoes_treino = rna.predict(X_train_oversampled)"
      ],
      "metadata": {
        "id": "g_v3es4OlcNE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rna_treino = accuracy_score(y_train_oversampled, previsoes_treino)"
      ],
      "metadata": {
        "id": "6-73nIgeGKRd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm_rna_treino = confusion_matrix(y_train_oversampled, previsoes_treino)"
      ],
      "metadata": {
        "id": "qFWIxXiEGLf0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GridSearch"
      ],
      "metadata": {
        "id": "wqzk3XhWI17y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Grid Search\n",
        "param_grid = {\n",
        "    'activation': ['relu', 'tanh', 'logistic'],\n",
        "    'solver': ['sgd', 'adam'],\n",
        "    'max_iter': [1000, 2000],\n",
        "    'tol': [0.0001, 0.001],\n",
        "}\n",
        "\n",
        "g_search = GridSearchCV(estimator=rna, param_grid=param_grid, cv=10)\n",
        "g_search.fit(X_train_oversampled, y_train_oversampled)\n",
        "\n",
        "best_params_grid = g_search.best_params_\n",
        "best_score_grid = g_search.best_score_\n",
        "\n",
        "print(\"Melhores hiperparâmetros encontrados através do Grid Search:\")\n",
        "print(best_params_grid)\n",
        "print(\"Melhor pontuação (acurácia) encontrada através do Grid Search:\", best_score_grid)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dUzRlrPDI09u",
        "outputId": "96ad8503-aa1f-443b-e706-4945b74e679a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mA saída de streaming foi truncada nas últimas 5000 linhas.\u001b[0m\n",
            "Iteration 559, loss = 0.42903897\n",
            "Iteration 560, loss = 0.42893093\n",
            "Iteration 561, loss = 0.42884357\n",
            "Iteration 562, loss = 0.42871120\n",
            "Iteration 563, loss = 0.42860910\n",
            "Iteration 564, loss = 0.42851204\n",
            "Iteration 565, loss = 0.42840491\n",
            "Iteration 566, loss = 0.42829176\n",
            "Iteration 567, loss = 0.42819392\n",
            "Iteration 568, loss = 0.42807726\n",
            "Iteration 569, loss = 0.42798807\n",
            "Iteration 570, loss = 0.42787385\n",
            "Iteration 571, loss = 0.42778170\n",
            "Iteration 572, loss = 0.42770481\n",
            "Iteration 573, loss = 0.42759068\n",
            "Iteration 574, loss = 0.42749812\n",
            "Iteration 575, loss = 0.42739982\n",
            "Iteration 576, loss = 0.42732154\n",
            "Iteration 577, loss = 0.42721431\n",
            "Iteration 578, loss = 0.42710931\n",
            "Iteration 579, loss = 0.42700288\n",
            "Iteration 580, loss = 0.42689798\n",
            "Iteration 581, loss = 0.42679535\n",
            "Iteration 582, loss = 0.42670491\n",
            "Iteration 583, loss = 0.42659154\n",
            "Iteration 584, loss = 0.42652455\n",
            "Iteration 585, loss = 0.42640744\n",
            "Iteration 586, loss = 0.42630599\n",
            "Iteration 587, loss = 0.42621587\n",
            "Iteration 588, loss = 0.42614265\n",
            "Iteration 589, loss = 0.42603140\n",
            "Iteration 590, loss = 0.42595503\n",
            "Iteration 591, loss = 0.42587575\n",
            "Iteration 592, loss = 0.42576551\n",
            "Iteration 593, loss = 0.42569917\n",
            "Iteration 594, loss = 0.42559256\n",
            "Iteration 595, loss = 0.42552079\n",
            "Iteration 596, loss = 0.42542213\n",
            "Iteration 597, loss = 0.42541147\n",
            "Iteration 598, loss = 0.42526506\n",
            "Iteration 599, loss = 0.42519688\n",
            "Iteration 600, loss = 0.42509854\n",
            "Iteration 601, loss = 0.42502126\n",
            "Iteration 602, loss = 0.42493484\n",
            "Iteration 603, loss = 0.42485176\n",
            "Iteration 604, loss = 0.42477295\n",
            "Iteration 605, loss = 0.42471513\n",
            "Iteration 606, loss = 0.42463146\n",
            "Iteration 607, loss = 0.42453653\n",
            "Iteration 608, loss = 0.42448492\n",
            "Iteration 609, loss = 0.42439002\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69470200\n",
            "Iteration 2, loss = 0.69428016\n",
            "Iteration 3, loss = 0.69389849\n",
            "Iteration 4, loss = 0.69353959\n",
            "Iteration 5, loss = 0.69316858\n",
            "Iteration 6, loss = 0.69279835\n",
            "Iteration 7, loss = 0.69246205\n",
            "Iteration 8, loss = 0.69212603\n",
            "Iteration 9, loss = 0.69180658\n",
            "Iteration 10, loss = 0.69145865\n",
            "Iteration 11, loss = 0.69111892\n",
            "Iteration 12, loss = 0.69077270\n",
            "Iteration 13, loss = 0.69047500\n",
            "Iteration 14, loss = 0.69011399\n",
            "Iteration 15, loss = 0.68979448\n",
            "Iteration 16, loss = 0.68942047\n",
            "Iteration 17, loss = 0.68908574\n",
            "Iteration 18, loss = 0.68873309\n",
            "Iteration 19, loss = 0.68840513\n",
            "Iteration 20, loss = 0.68806392\n",
            "Iteration 21, loss = 0.68771633\n",
            "Iteration 22, loss = 0.68736401\n",
            "Iteration 23, loss = 0.68701376\n",
            "Iteration 24, loss = 0.68665385\n",
            "Iteration 25, loss = 0.68628687\n",
            "Iteration 26, loss = 0.68589809\n",
            "Iteration 27, loss = 0.68551202\n",
            "Iteration 28, loss = 0.68512342\n",
            "Iteration 29, loss = 0.68475524\n",
            "Iteration 30, loss = 0.68432664\n",
            "Iteration 31, loss = 0.68391784\n",
            "Iteration 32, loss = 0.68351444\n",
            "Iteration 33, loss = 0.68305558\n",
            "Iteration 34, loss = 0.68262375\n",
            "Iteration 35, loss = 0.68218985\n",
            "Iteration 36, loss = 0.68173792\n",
            "Iteration 37, loss = 0.68126134\n",
            "Iteration 38, loss = 0.68082237\n",
            "Iteration 39, loss = 0.68033656\n",
            "Iteration 40, loss = 0.67985848\n",
            "Iteration 41, loss = 0.67935708\n",
            "Iteration 42, loss = 0.67886703\n",
            "Iteration 43, loss = 0.67833669\n",
            "Iteration 44, loss = 0.67780766\n",
            "Iteration 45, loss = 0.67726713\n",
            "Iteration 46, loss = 0.67671049\n",
            "Iteration 47, loss = 0.67616372\n",
            "Iteration 48, loss = 0.67558361\n",
            "Iteration 49, loss = 0.67501667\n",
            "Iteration 50, loss = 0.67442734\n",
            "Iteration 51, loss = 0.67383272\n",
            "Iteration 52, loss = 0.67325814\n",
            "Iteration 53, loss = 0.67264496\n",
            "Iteration 54, loss = 0.67202663\n",
            "Iteration 55, loss = 0.67137932\n",
            "Iteration 56, loss = 0.67074061\n",
            "Iteration 57, loss = 0.67007650\n",
            "Iteration 58, loss = 0.66940169\n",
            "Iteration 59, loss = 0.66873007\n",
            "Iteration 60, loss = 0.66803808\n",
            "Iteration 61, loss = 0.66738297\n",
            "Iteration 62, loss = 0.66664727\n",
            "Iteration 63, loss = 0.66592830\n",
            "Iteration 64, loss = 0.66521093\n",
            "Iteration 65, loss = 0.66450602\n",
            "Iteration 66, loss = 0.66377690\n",
            "Iteration 67, loss = 0.66301353\n",
            "Iteration 68, loss = 0.66224727\n",
            "Iteration 69, loss = 0.66148184\n",
            "Iteration 70, loss = 0.66069224\n",
            "Iteration 71, loss = 0.65989084\n",
            "Iteration 72, loss = 0.65909550\n",
            "Iteration 73, loss = 0.65827251\n",
            "Iteration 74, loss = 0.65744379\n",
            "Iteration 75, loss = 0.65662424\n",
            "Iteration 76, loss = 0.65579438\n",
            "Iteration 77, loss = 0.65494318\n",
            "Iteration 78, loss = 0.65407581\n",
            "Iteration 79, loss = 0.65321680\n",
            "Iteration 80, loss = 0.65235278\n",
            "Iteration 81, loss = 0.65147839\n",
            "Iteration 82, loss = 0.65057749\n",
            "Iteration 83, loss = 0.64966905\n",
            "Iteration 84, loss = 0.64877146\n",
            "Iteration 85, loss = 0.64784087\n",
            "Iteration 86, loss = 0.64694317\n",
            "Iteration 87, loss = 0.64600815\n",
            "Iteration 88, loss = 0.64508997\n",
            "Iteration 89, loss = 0.64413587\n",
            "Iteration 90, loss = 0.64321654\n",
            "Iteration 91, loss = 0.64227530\n",
            "Iteration 92, loss = 0.64132794\n",
            "Iteration 93, loss = 0.64042471\n",
            "Iteration 94, loss = 0.63945429\n",
            "Iteration 95, loss = 0.63850704\n",
            "Iteration 96, loss = 0.63758727\n",
            "Iteration 97, loss = 0.63659673\n",
            "Iteration 98, loss = 0.63566600\n",
            "Iteration 99, loss = 0.63467501\n",
            "Iteration 100, loss = 0.63372696\n",
            "Iteration 101, loss = 0.63273772\n",
            "Iteration 102, loss = 0.63176290\n",
            "Iteration 103, loss = 0.63078959\n",
            "Iteration 104, loss = 0.62978614\n",
            "Iteration 105, loss = 0.62882016\n",
            "Iteration 106, loss = 0.62782235\n",
            "Iteration 107, loss = 0.62685124\n",
            "Iteration 108, loss = 0.62583265\n",
            "Iteration 109, loss = 0.62487731\n",
            "Iteration 110, loss = 0.62388602\n",
            "Iteration 111, loss = 0.62288948\n",
            "Iteration 112, loss = 0.62200535\n",
            "Iteration 113, loss = 0.62099409\n",
            "Iteration 114, loss = 0.62003540\n",
            "Iteration 115, loss = 0.61906746\n",
            "Iteration 116, loss = 0.61808363\n",
            "Iteration 117, loss = 0.61715896\n",
            "Iteration 118, loss = 0.61621152\n",
            "Iteration 119, loss = 0.61524712\n",
            "Iteration 120, loss = 0.61430757\n",
            "Iteration 121, loss = 0.61337662\n",
            "Iteration 122, loss = 0.61242493\n",
            "Iteration 123, loss = 0.61148385\n",
            "Iteration 124, loss = 0.61055326\n",
            "Iteration 125, loss = 0.60955740\n",
            "Iteration 126, loss = 0.60864501\n",
            "Iteration 127, loss = 0.60768404\n",
            "Iteration 128, loss = 0.60673977\n",
            "Iteration 129, loss = 0.60579138\n",
            "Iteration 130, loss = 0.60487248\n",
            "Iteration 131, loss = 0.60391729\n",
            "Iteration 132, loss = 0.60300033\n",
            "Iteration 133, loss = 0.60208702\n",
            "Iteration 134, loss = 0.60113713\n",
            "Iteration 135, loss = 0.60024070\n",
            "Iteration 136, loss = 0.59930649\n",
            "Iteration 137, loss = 0.59840360\n",
            "Iteration 138, loss = 0.59750783\n",
            "Iteration 139, loss = 0.59656222\n",
            "Iteration 140, loss = 0.59567583\n",
            "Iteration 141, loss = 0.59477267\n",
            "Iteration 142, loss = 0.59386616\n",
            "Iteration 143, loss = 0.59296805\n",
            "Iteration 144, loss = 0.59207307\n",
            "Iteration 145, loss = 0.59112634\n",
            "Iteration 146, loss = 0.59033095\n",
            "Iteration 147, loss = 0.58939508\n",
            "Iteration 148, loss = 0.58853916\n",
            "Iteration 149, loss = 0.58765927\n",
            "Iteration 150, loss = 0.58683569\n",
            "Iteration 151, loss = 0.58596803\n",
            "Iteration 152, loss = 0.58512472\n",
            "Iteration 153, loss = 0.58428997\n",
            "Iteration 154, loss = 0.58340895\n",
            "Iteration 155, loss = 0.58257942\n",
            "Iteration 156, loss = 0.58173226\n",
            "Iteration 157, loss = 0.58088781\n",
            "Iteration 158, loss = 0.58007715\n",
            "Iteration 159, loss = 0.57926905\n",
            "Iteration 160, loss = 0.57849385\n",
            "Iteration 161, loss = 0.57764908\n",
            "Iteration 162, loss = 0.57682438\n",
            "Iteration 163, loss = 0.57601749\n",
            "Iteration 164, loss = 0.57524050\n",
            "Iteration 165, loss = 0.57444205\n",
            "Iteration 166, loss = 0.57360292\n",
            "Iteration 167, loss = 0.57281051\n",
            "Iteration 168, loss = 0.57201101\n",
            "Iteration 169, loss = 0.57124005\n",
            "Iteration 170, loss = 0.57043921\n",
            "Iteration 171, loss = 0.56965103\n",
            "Iteration 172, loss = 0.56889651\n",
            "Iteration 173, loss = 0.56809456\n",
            "Iteration 174, loss = 0.56735693\n",
            "Iteration 175, loss = 0.56654486\n",
            "Iteration 176, loss = 0.56577002\n",
            "Iteration 177, loss = 0.56500867\n",
            "Iteration 178, loss = 0.56424937\n",
            "Iteration 179, loss = 0.56347180\n",
            "Iteration 180, loss = 0.56273199\n",
            "Iteration 181, loss = 0.56198999\n",
            "Iteration 182, loss = 0.56121904\n",
            "Iteration 183, loss = 0.56049565\n",
            "Iteration 184, loss = 0.55973006\n",
            "Iteration 185, loss = 0.55901600\n",
            "Iteration 186, loss = 0.55825194\n",
            "Iteration 187, loss = 0.55751128\n",
            "Iteration 188, loss = 0.55681587\n",
            "Iteration 189, loss = 0.55607658\n",
            "Iteration 190, loss = 0.55537074\n",
            "Iteration 191, loss = 0.55463384\n",
            "Iteration 192, loss = 0.55394823\n",
            "Iteration 193, loss = 0.55322824\n",
            "Iteration 194, loss = 0.55251551\n",
            "Iteration 195, loss = 0.55182599\n",
            "Iteration 196, loss = 0.55112675\n",
            "Iteration 197, loss = 0.55039251\n",
            "Iteration 198, loss = 0.54972566\n",
            "Iteration 199, loss = 0.54900987\n",
            "Iteration 200, loss = 0.54831507\n",
            "Iteration 201, loss = 0.54764383\n",
            "Iteration 202, loss = 0.54695191\n",
            "Iteration 203, loss = 0.54625466\n",
            "Iteration 204, loss = 0.54558897\n",
            "Iteration 205, loss = 0.54489734\n",
            "Iteration 206, loss = 0.54425320\n",
            "Iteration 207, loss = 0.54360811\n",
            "Iteration 208, loss = 0.54288591\n",
            "Iteration 209, loss = 0.54223591\n",
            "Iteration 210, loss = 0.54158849\n",
            "Iteration 211, loss = 0.54092901\n",
            "Iteration 212, loss = 0.54026579\n",
            "Iteration 213, loss = 0.53962603\n",
            "Iteration 214, loss = 0.53895267\n",
            "Iteration 215, loss = 0.53829348\n",
            "Iteration 216, loss = 0.53763166\n",
            "Iteration 217, loss = 0.53700256\n",
            "Iteration 218, loss = 0.53633975\n",
            "Iteration 219, loss = 0.53571178\n",
            "Iteration 220, loss = 0.53509826\n",
            "Iteration 221, loss = 0.53444786\n",
            "Iteration 222, loss = 0.53383590\n",
            "Iteration 223, loss = 0.53320426\n",
            "Iteration 224, loss = 0.53259587\n",
            "Iteration 225, loss = 0.53197201\n",
            "Iteration 226, loss = 0.53136400\n",
            "Iteration 227, loss = 0.53072615\n",
            "Iteration 228, loss = 0.53011841\n",
            "Iteration 229, loss = 0.52949555\n",
            "Iteration 230, loss = 0.52887687\n",
            "Iteration 231, loss = 0.52831903\n",
            "Iteration 232, loss = 0.52767240\n",
            "Iteration 233, loss = 0.52704084\n",
            "Iteration 234, loss = 0.52646480\n",
            "Iteration 235, loss = 0.52584975\n",
            "Iteration 236, loss = 0.52523794\n",
            "Iteration 237, loss = 0.52463250\n",
            "Iteration 238, loss = 0.52405132\n",
            "Iteration 239, loss = 0.52347490\n",
            "Iteration 240, loss = 0.52283965\n",
            "Iteration 241, loss = 0.52228627\n",
            "Iteration 242, loss = 0.52169027\n",
            "Iteration 243, loss = 0.52113992\n",
            "Iteration 244, loss = 0.52052946\n",
            "Iteration 245, loss = 0.51993835\n",
            "Iteration 246, loss = 0.51936563\n",
            "Iteration 247, loss = 0.51880041\n",
            "Iteration 248, loss = 0.51822642\n",
            "Iteration 249, loss = 0.51764540\n",
            "Iteration 250, loss = 0.51704619\n",
            "Iteration 251, loss = 0.51649427\n",
            "Iteration 252, loss = 0.51591856\n",
            "Iteration 253, loss = 0.51534705\n",
            "Iteration 254, loss = 0.51476345\n",
            "Iteration 255, loss = 0.51420603\n",
            "Iteration 256, loss = 0.51363376\n",
            "Iteration 257, loss = 0.51306914\n",
            "Iteration 258, loss = 0.51250719\n",
            "Iteration 259, loss = 0.51192471\n",
            "Iteration 260, loss = 0.51135307\n",
            "Iteration 261, loss = 0.51080213\n",
            "Iteration 262, loss = 0.51024541\n",
            "Iteration 263, loss = 0.50968309\n",
            "Iteration 264, loss = 0.50916341\n",
            "Iteration 265, loss = 0.50858274\n",
            "Iteration 266, loss = 0.50808108\n",
            "Iteration 267, loss = 0.50751343\n",
            "Iteration 268, loss = 0.50693719\n",
            "Iteration 269, loss = 0.50642391\n",
            "Iteration 270, loss = 0.50587542\n",
            "Iteration 271, loss = 0.50534839\n",
            "Iteration 272, loss = 0.50484870\n",
            "Iteration 273, loss = 0.50430100\n",
            "Iteration 274, loss = 0.50379206\n",
            "Iteration 275, loss = 0.50327172\n",
            "Iteration 276, loss = 0.50272036\n",
            "Iteration 277, loss = 0.50220034\n",
            "Iteration 278, loss = 0.50166834\n",
            "Iteration 279, loss = 0.50115363\n",
            "Iteration 280, loss = 0.50062063\n",
            "Iteration 281, loss = 0.50009967\n",
            "Iteration 282, loss = 0.49960410\n",
            "Iteration 283, loss = 0.49908105\n",
            "Iteration 284, loss = 0.49852612\n",
            "Iteration 285, loss = 0.49805209\n",
            "Iteration 286, loss = 0.49752215\n",
            "Iteration 287, loss = 0.49701048\n",
            "Iteration 288, loss = 0.49649861\n",
            "Iteration 289, loss = 0.49598203\n",
            "Iteration 290, loss = 0.49549069\n",
            "Iteration 291, loss = 0.49498367\n",
            "Iteration 292, loss = 0.49446661\n",
            "Iteration 293, loss = 0.49396943\n",
            "Iteration 294, loss = 0.49345070\n",
            "Iteration 295, loss = 0.49295582\n",
            "Iteration 296, loss = 0.49243848\n",
            "Iteration 297, loss = 0.49200574\n",
            "Iteration 298, loss = 0.49146855\n",
            "Iteration 299, loss = 0.49102378\n",
            "Iteration 300, loss = 0.49050122\n",
            "Iteration 301, loss = 0.49004076\n",
            "Iteration 302, loss = 0.48957132\n",
            "Iteration 303, loss = 0.48909353\n",
            "Iteration 304, loss = 0.48859363\n",
            "Iteration 305, loss = 0.48813459\n",
            "Iteration 306, loss = 0.48765739\n",
            "Iteration 307, loss = 0.48717300\n",
            "Iteration 308, loss = 0.48671819\n",
            "Iteration 309, loss = 0.48624033\n",
            "Iteration 310, loss = 0.48578343\n",
            "Iteration 311, loss = 0.48531241\n",
            "Iteration 312, loss = 0.48487200\n",
            "Iteration 313, loss = 0.48440426\n",
            "Iteration 314, loss = 0.48396572\n",
            "Iteration 315, loss = 0.48350174\n",
            "Iteration 316, loss = 0.48306509\n",
            "Iteration 317, loss = 0.48260182\n",
            "Iteration 318, loss = 0.48214837\n",
            "Iteration 319, loss = 0.48169470\n",
            "Iteration 320, loss = 0.48123044\n",
            "Iteration 321, loss = 0.48076425\n",
            "Iteration 322, loss = 0.48032392\n",
            "Iteration 323, loss = 0.47987413\n",
            "Iteration 324, loss = 0.47942132\n",
            "Iteration 325, loss = 0.47896967\n",
            "Iteration 326, loss = 0.47851274\n",
            "Iteration 327, loss = 0.47804517\n",
            "Iteration 328, loss = 0.47757663\n",
            "Iteration 329, loss = 0.47714014\n",
            "Iteration 330, loss = 0.47671835\n",
            "Iteration 331, loss = 0.47623680\n",
            "Iteration 332, loss = 0.47580358\n",
            "Iteration 333, loss = 0.47538591\n",
            "Iteration 334, loss = 0.47494338\n",
            "Iteration 335, loss = 0.47454080\n",
            "Iteration 336, loss = 0.47409121\n",
            "Iteration 337, loss = 0.47368952\n",
            "Iteration 338, loss = 0.47322051\n",
            "Iteration 339, loss = 0.47278727\n",
            "Iteration 340, loss = 0.47236343\n",
            "Iteration 341, loss = 0.47194293\n",
            "Iteration 342, loss = 0.47152205\n",
            "Iteration 343, loss = 0.47109960\n",
            "Iteration 344, loss = 0.47067729\n",
            "Iteration 345, loss = 0.47025207\n",
            "Iteration 346, loss = 0.46983824\n",
            "Iteration 347, loss = 0.46942109\n",
            "Iteration 348, loss = 0.46898792\n",
            "Iteration 349, loss = 0.46858996\n",
            "Iteration 350, loss = 0.46816776\n",
            "Iteration 351, loss = 0.46780235\n",
            "Iteration 352, loss = 0.46738394\n",
            "Iteration 353, loss = 0.46701865\n",
            "Iteration 354, loss = 0.46659155\n",
            "Iteration 355, loss = 0.46623976\n",
            "Iteration 356, loss = 0.46585355\n",
            "Iteration 357, loss = 0.46546890\n",
            "Iteration 358, loss = 0.46511748\n",
            "Iteration 359, loss = 0.46469560\n",
            "Iteration 360, loss = 0.46432322\n",
            "Iteration 361, loss = 0.46394765\n",
            "Iteration 362, loss = 0.46358592\n",
            "Iteration 363, loss = 0.46324065\n",
            "Iteration 364, loss = 0.46280747\n",
            "Iteration 365, loss = 0.46245070\n",
            "Iteration 366, loss = 0.46204239\n",
            "Iteration 367, loss = 0.46167183\n",
            "Iteration 368, loss = 0.46127206\n",
            "Iteration 369, loss = 0.46089724\n",
            "Iteration 370, loss = 0.46057728\n",
            "Iteration 371, loss = 0.46016431\n",
            "Iteration 372, loss = 0.45977631\n",
            "Iteration 373, loss = 0.45942850\n",
            "Iteration 374, loss = 0.45905741\n",
            "Iteration 375, loss = 0.45871226\n",
            "Iteration 376, loss = 0.45834487\n",
            "Iteration 377, loss = 0.45800237\n",
            "Iteration 378, loss = 0.45762112\n",
            "Iteration 379, loss = 0.45727717\n",
            "Iteration 380, loss = 0.45693936\n",
            "Iteration 381, loss = 0.45655933\n",
            "Iteration 382, loss = 0.45620678\n",
            "Iteration 383, loss = 0.45587926\n",
            "Iteration 384, loss = 0.45552460\n",
            "Iteration 385, loss = 0.45518549\n",
            "Iteration 386, loss = 0.45481754\n",
            "Iteration 387, loss = 0.45450095\n",
            "Iteration 388, loss = 0.45415979\n",
            "Iteration 389, loss = 0.45384162\n",
            "Iteration 390, loss = 0.45351247\n",
            "Iteration 391, loss = 0.45319076\n",
            "Iteration 392, loss = 0.45286295\n",
            "Iteration 393, loss = 0.45255156\n",
            "Iteration 394, loss = 0.45221814\n",
            "Iteration 395, loss = 0.45189034\n",
            "Iteration 396, loss = 0.45155674\n",
            "Iteration 397, loss = 0.45124932\n",
            "Iteration 398, loss = 0.45091946\n",
            "Iteration 399, loss = 0.45061095\n",
            "Iteration 400, loss = 0.45026694\n",
            "Iteration 401, loss = 0.44993088\n",
            "Iteration 402, loss = 0.44963091\n",
            "Iteration 403, loss = 0.44929268\n",
            "Iteration 404, loss = 0.44898440\n",
            "Iteration 405, loss = 0.44867078\n",
            "Iteration 406, loss = 0.44835192\n",
            "Iteration 407, loss = 0.44805671\n",
            "Iteration 408, loss = 0.44772766\n",
            "Iteration 409, loss = 0.44742710\n",
            "Iteration 410, loss = 0.44711784\n",
            "Iteration 411, loss = 0.44682422\n",
            "Iteration 412, loss = 0.44654466\n",
            "Iteration 413, loss = 0.44623477\n",
            "Iteration 414, loss = 0.44597103\n",
            "Iteration 415, loss = 0.44568225\n",
            "Iteration 416, loss = 0.44544660\n",
            "Iteration 417, loss = 0.44513387\n",
            "Iteration 418, loss = 0.44489482\n",
            "Iteration 419, loss = 0.44457336\n",
            "Iteration 420, loss = 0.44433325\n",
            "Iteration 421, loss = 0.44401998\n",
            "Iteration 422, loss = 0.44373411\n",
            "Iteration 423, loss = 0.44344669\n",
            "Iteration 424, loss = 0.44321260\n",
            "Iteration 425, loss = 0.44288773\n",
            "Iteration 426, loss = 0.44259163\n",
            "Iteration 427, loss = 0.44232933\n",
            "Iteration 428, loss = 0.44206661\n",
            "Iteration 429, loss = 0.44177143\n",
            "Iteration 430, loss = 0.44150591\n",
            "Iteration 431, loss = 0.44123113\n",
            "Iteration 432, loss = 0.44097903\n",
            "Iteration 433, loss = 0.44072036\n",
            "Iteration 434, loss = 0.44043802\n",
            "Iteration 435, loss = 0.44018862\n",
            "Iteration 436, loss = 0.43995114\n",
            "Iteration 437, loss = 0.43965879\n",
            "Iteration 438, loss = 0.43939577\n",
            "Iteration 439, loss = 0.43913504\n",
            "Iteration 440, loss = 0.43894482\n",
            "Iteration 441, loss = 0.43862718\n",
            "Iteration 442, loss = 0.43837510\n",
            "Iteration 443, loss = 0.43811388\n",
            "Iteration 444, loss = 0.43787442\n",
            "Iteration 445, loss = 0.43762115\n",
            "Iteration 446, loss = 0.43736571\n",
            "Iteration 447, loss = 0.43715485\n",
            "Iteration 448, loss = 0.43690873\n",
            "Iteration 449, loss = 0.43667367\n",
            "Iteration 450, loss = 0.43644612\n",
            "Iteration 451, loss = 0.43617876\n",
            "Iteration 452, loss = 0.43595011\n",
            "Iteration 453, loss = 0.43574171\n",
            "Iteration 454, loss = 0.43549098\n",
            "Iteration 455, loss = 0.43527569\n",
            "Iteration 456, loss = 0.43504929\n",
            "Iteration 457, loss = 0.43480933\n",
            "Iteration 458, loss = 0.43458676\n",
            "Iteration 459, loss = 0.43437530\n",
            "Iteration 460, loss = 0.43414293\n",
            "Iteration 461, loss = 0.43392414\n",
            "Iteration 462, loss = 0.43372828\n",
            "Iteration 463, loss = 0.43349671\n",
            "Iteration 464, loss = 0.43327986\n",
            "Iteration 465, loss = 0.43307338\n",
            "Iteration 466, loss = 0.43285018\n",
            "Iteration 467, loss = 0.43265300\n",
            "Iteration 468, loss = 0.43245478\n",
            "Iteration 469, loss = 0.43223083\n",
            "Iteration 470, loss = 0.43203855\n",
            "Iteration 471, loss = 0.43184529\n",
            "Iteration 472, loss = 0.43162323\n",
            "Iteration 473, loss = 0.43143246\n",
            "Iteration 474, loss = 0.43119457\n",
            "Iteration 475, loss = 0.43100667\n",
            "Iteration 476, loss = 0.43079974\n",
            "Iteration 477, loss = 0.43060010\n",
            "Iteration 478, loss = 0.43042571\n",
            "Iteration 479, loss = 0.43022739\n",
            "Iteration 480, loss = 0.43005082\n",
            "Iteration 481, loss = 0.42984083\n",
            "Iteration 482, loss = 0.42965687\n",
            "Iteration 483, loss = 0.42945743\n",
            "Iteration 484, loss = 0.42927883\n",
            "Iteration 485, loss = 0.42908467\n",
            "Iteration 486, loss = 0.42891383\n",
            "Iteration 487, loss = 0.42873718\n",
            "Iteration 488, loss = 0.42852198\n",
            "Iteration 489, loss = 0.42831980\n",
            "Iteration 490, loss = 0.42813891\n",
            "Iteration 491, loss = 0.42793259\n",
            "Iteration 492, loss = 0.42777054\n",
            "Iteration 493, loss = 0.42773251\n",
            "Iteration 494, loss = 0.42752843\n",
            "Iteration 495, loss = 0.42734732\n",
            "Iteration 496, loss = 0.42718949\n",
            "Iteration 497, loss = 0.42699807\n",
            "Iteration 498, loss = 0.42679291\n",
            "Iteration 499, loss = 0.42661566\n",
            "Iteration 500, loss = 0.42639823\n",
            "Iteration 501, loss = 0.42622044\n",
            "Iteration 502, loss = 0.42606447\n",
            "Iteration 503, loss = 0.42588116\n",
            "Iteration 504, loss = 0.42569643\n",
            "Iteration 505, loss = 0.42551221\n",
            "Iteration 506, loss = 0.42535026\n",
            "Iteration 507, loss = 0.42518039\n",
            "Iteration 508, loss = 0.42500379\n",
            "Iteration 509, loss = 0.42485634\n",
            "Iteration 510, loss = 0.42468383\n",
            "Iteration 511, loss = 0.42452437\n",
            "Iteration 512, loss = 0.42437449\n",
            "Iteration 513, loss = 0.42422553\n",
            "Iteration 514, loss = 0.42405715\n",
            "Iteration 515, loss = 0.42391317\n",
            "Iteration 516, loss = 0.42377285\n",
            "Iteration 517, loss = 0.42360793\n",
            "Iteration 518, loss = 0.42346332\n",
            "Iteration 519, loss = 0.42329997\n",
            "Iteration 520, loss = 0.42317218\n",
            "Iteration 521, loss = 0.42303837\n",
            "Iteration 522, loss = 0.42288708\n",
            "Iteration 523, loss = 0.42274307\n",
            "Iteration 524, loss = 0.42259946\n",
            "Iteration 525, loss = 0.42245577\n",
            "Iteration 526, loss = 0.42231302\n",
            "Iteration 527, loss = 0.42216194\n",
            "Iteration 528, loss = 0.42204540\n",
            "Iteration 529, loss = 0.42187335\n",
            "Iteration 530, loss = 0.42175993\n",
            "Iteration 531, loss = 0.42159832\n",
            "Iteration 532, loss = 0.42147764\n",
            "Iteration 533, loss = 0.42131817\n",
            "Iteration 534, loss = 0.42117123\n",
            "Iteration 535, loss = 0.42105681\n",
            "Iteration 536, loss = 0.42090598\n",
            "Iteration 537, loss = 0.42076623\n",
            "Iteration 538, loss = 0.42062806\n",
            "Iteration 539, loss = 0.42050105\n",
            "Iteration 540, loss = 0.42038150\n",
            "Iteration 541, loss = 0.42022555\n",
            "Iteration 542, loss = 0.42012310\n",
            "Iteration 543, loss = 0.42000812\n",
            "Iteration 544, loss = 0.41989216\n",
            "Iteration 545, loss = 0.41973402\n",
            "Iteration 546, loss = 0.41962843\n",
            "Iteration 547, loss = 0.41950177\n",
            "Iteration 548, loss = 0.41939999\n",
            "Iteration 549, loss = 0.41924996\n",
            "Iteration 550, loss = 0.41914561\n",
            "Iteration 551, loss = 0.41904333\n",
            "Iteration 552, loss = 0.41895308\n",
            "Iteration 553, loss = 0.41883259\n",
            "Iteration 554, loss = 0.41870879\n",
            "Iteration 555, loss = 0.41859522\n",
            "Iteration 556, loss = 0.41852937\n",
            "Iteration 557, loss = 0.41835618\n",
            "Iteration 558, loss = 0.41827456\n",
            "Iteration 559, loss = 0.41815756\n",
            "Iteration 560, loss = 0.41803414\n",
            "Iteration 561, loss = 0.41792131\n",
            "Iteration 562, loss = 0.41778401\n",
            "Iteration 563, loss = 0.41768036\n",
            "Iteration 564, loss = 0.41753335\n",
            "Iteration 565, loss = 0.41746263\n",
            "Iteration 566, loss = 0.41732829\n",
            "Iteration 567, loss = 0.41724106\n",
            "Iteration 568, loss = 0.41711401\n",
            "Iteration 569, loss = 0.41701033\n",
            "Iteration 570, loss = 0.41691186\n",
            "Iteration 571, loss = 0.41682369\n",
            "Iteration 572, loss = 0.41671516\n",
            "Iteration 573, loss = 0.41660953\n",
            "Iteration 574, loss = 0.41650971\n",
            "Iteration 575, loss = 0.41640110\n",
            "Iteration 576, loss = 0.41630734\n",
            "Iteration 577, loss = 0.41624198\n",
            "Iteration 578, loss = 0.41608577\n",
            "Iteration 579, loss = 0.41598814\n",
            "Iteration 580, loss = 0.41588160\n",
            "Iteration 581, loss = 0.41577532\n",
            "Iteration 582, loss = 0.41568123\n",
            "Iteration 583, loss = 0.41557104\n",
            "Iteration 584, loss = 0.41548643\n",
            "Iteration 585, loss = 0.41539665\n",
            "Iteration 586, loss = 0.41528461\n",
            "Iteration 587, loss = 0.41520565\n",
            "Iteration 588, loss = 0.41509294\n",
            "Iteration 589, loss = 0.41499701\n",
            "Iteration 590, loss = 0.41491522\n",
            "Iteration 591, loss = 0.41482459\n",
            "Iteration 592, loss = 0.41472130\n",
            "Iteration 593, loss = 0.41464164\n",
            "Iteration 594, loss = 0.41456794\n",
            "Iteration 595, loss = 0.41447186\n",
            "Iteration 596, loss = 0.41438354\n",
            "Iteration 597, loss = 0.41431021\n",
            "Iteration 598, loss = 0.41421626\n",
            "Iteration 599, loss = 0.41411604\n",
            "Iteration 600, loss = 0.41404881\n",
            "Iteration 601, loss = 0.41395148\n",
            "Iteration 602, loss = 0.41387590\n",
            "Iteration 603, loss = 0.41377439\n",
            "Iteration 604, loss = 0.41370286\n",
            "Iteration 605, loss = 0.41365715\n",
            "Iteration 606, loss = 0.41358817\n",
            "Iteration 607, loss = 0.41345991\n",
            "Iteration 608, loss = 0.41337201\n",
            "Iteration 609, loss = 0.41329825\n",
            "Iteration 610, loss = 0.41322680\n",
            "Iteration 611, loss = 0.41313448\n",
            "Iteration 612, loss = 0.41303570\n",
            "Iteration 613, loss = 0.41294279\n",
            "Iteration 614, loss = 0.41286996\n",
            "Iteration 615, loss = 0.41278238\n",
            "Iteration 616, loss = 0.41269726\n",
            "Iteration 617, loss = 0.41262029\n",
            "Iteration 618, loss = 0.41252536\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69486222\n",
            "Iteration 2, loss = 0.69440173\n",
            "Iteration 3, loss = 0.69405436\n",
            "Iteration 4, loss = 0.69370789\n",
            "Iteration 5, loss = 0.69334119\n",
            "Iteration 6, loss = 0.69298727\n",
            "Iteration 7, loss = 0.69265666\n",
            "Iteration 8, loss = 0.69231392\n",
            "Iteration 9, loss = 0.69199990\n",
            "Iteration 10, loss = 0.69163630\n",
            "Iteration 11, loss = 0.69130766\n",
            "Iteration 12, loss = 0.69097020\n",
            "Iteration 13, loss = 0.69063535\n",
            "Iteration 14, loss = 0.69029416\n",
            "Iteration 15, loss = 0.68999162\n",
            "Iteration 16, loss = 0.68960811\n",
            "Iteration 17, loss = 0.68927633\n",
            "Iteration 18, loss = 0.68892301\n",
            "Iteration 19, loss = 0.68858637\n",
            "Iteration 20, loss = 0.68823076\n",
            "Iteration 21, loss = 0.68788673\n",
            "Iteration 22, loss = 0.68752940\n",
            "Iteration 23, loss = 0.68716832\n",
            "Iteration 24, loss = 0.68681561\n",
            "Iteration 25, loss = 0.68644259\n",
            "Iteration 26, loss = 0.68605503\n",
            "Iteration 27, loss = 0.68566582\n",
            "Iteration 28, loss = 0.68529241\n",
            "Iteration 29, loss = 0.68492465\n",
            "Iteration 30, loss = 0.68448627\n",
            "Iteration 31, loss = 0.68409181\n",
            "Iteration 32, loss = 0.68367629\n",
            "Iteration 33, loss = 0.68323665\n",
            "Iteration 34, loss = 0.68281024\n",
            "Iteration 35, loss = 0.68238058\n",
            "Iteration 36, loss = 0.68193220\n",
            "Iteration 37, loss = 0.68145363\n",
            "Iteration 38, loss = 0.68099828\n",
            "Iteration 39, loss = 0.68051308\n",
            "Iteration 40, loss = 0.68001917\n",
            "Iteration 41, loss = 0.67951822\n",
            "Iteration 42, loss = 0.67902536\n",
            "Iteration 43, loss = 0.67850727\n",
            "Iteration 44, loss = 0.67798582\n",
            "Iteration 45, loss = 0.67745037\n",
            "Iteration 46, loss = 0.67689986\n",
            "Iteration 47, loss = 0.67637442\n",
            "Iteration 48, loss = 0.67577898\n",
            "Iteration 49, loss = 0.67521002\n",
            "Iteration 50, loss = 0.67462223\n",
            "Iteration 51, loss = 0.67403274\n",
            "Iteration 52, loss = 0.67343192\n",
            "Iteration 53, loss = 0.67284157\n",
            "Iteration 54, loss = 0.67220009\n",
            "Iteration 55, loss = 0.67155939\n",
            "Iteration 56, loss = 0.67093210\n",
            "Iteration 57, loss = 0.67026513\n",
            "Iteration 58, loss = 0.66958778\n",
            "Iteration 59, loss = 0.66892240\n",
            "Iteration 60, loss = 0.66822687\n",
            "Iteration 61, loss = 0.66754141\n",
            "Iteration 62, loss = 0.66682007\n",
            "Iteration 63, loss = 0.66609800\n",
            "Iteration 64, loss = 0.66535672\n",
            "Iteration 65, loss = 0.66463604\n",
            "Iteration 66, loss = 0.66389185\n",
            "Iteration 67, loss = 0.66312179\n",
            "Iteration 68, loss = 0.66236174\n",
            "Iteration 69, loss = 0.66158801\n",
            "Iteration 70, loss = 0.66078564\n",
            "Iteration 71, loss = 0.65998232\n",
            "Iteration 72, loss = 0.65916647\n",
            "Iteration 73, loss = 0.65836054\n",
            "Iteration 74, loss = 0.65752380\n",
            "Iteration 75, loss = 0.65670182\n",
            "Iteration 76, loss = 0.65588960\n",
            "Iteration 77, loss = 0.65504293\n",
            "Iteration 78, loss = 0.65417992\n",
            "Iteration 79, loss = 0.65332975\n",
            "Iteration 80, loss = 0.65248116\n",
            "Iteration 81, loss = 0.65161581\n",
            "Iteration 82, loss = 0.65076586\n",
            "Iteration 83, loss = 0.64984805\n",
            "Iteration 84, loss = 0.64896509\n",
            "Iteration 85, loss = 0.64806343\n",
            "Iteration 86, loss = 0.64715804\n",
            "Iteration 87, loss = 0.64625488\n",
            "Iteration 88, loss = 0.64536264\n",
            "Iteration 89, loss = 0.64442838\n",
            "Iteration 90, loss = 0.64352382\n",
            "Iteration 91, loss = 0.64263742\n",
            "Iteration 92, loss = 0.64169668\n",
            "Iteration 93, loss = 0.64080425\n",
            "Iteration 94, loss = 0.63987703\n",
            "Iteration 95, loss = 0.63893961\n",
            "Iteration 96, loss = 0.63801039\n",
            "Iteration 97, loss = 0.63705413\n",
            "Iteration 98, loss = 0.63613822\n",
            "Iteration 99, loss = 0.63517579\n",
            "Iteration 100, loss = 0.63422981\n",
            "Iteration 101, loss = 0.63329872\n",
            "Iteration 102, loss = 0.63232979\n",
            "Iteration 103, loss = 0.63140961\n",
            "Iteration 104, loss = 0.63041801\n",
            "Iteration 105, loss = 0.62949117\n",
            "Iteration 106, loss = 0.62852763\n",
            "Iteration 107, loss = 0.62759012\n",
            "Iteration 108, loss = 0.62660999\n",
            "Iteration 109, loss = 0.62566294\n",
            "Iteration 110, loss = 0.62474107\n",
            "Iteration 111, loss = 0.62376161\n",
            "Iteration 112, loss = 0.62287405\n",
            "Iteration 113, loss = 0.62190843\n",
            "Iteration 114, loss = 0.62100535\n",
            "Iteration 115, loss = 0.62005290\n",
            "Iteration 116, loss = 0.61907200\n",
            "Iteration 117, loss = 0.61816129\n",
            "Iteration 118, loss = 0.61726344\n",
            "Iteration 119, loss = 0.61631661\n",
            "Iteration 120, loss = 0.61539662\n",
            "Iteration 121, loss = 0.61446846\n",
            "Iteration 122, loss = 0.61353806\n",
            "Iteration 123, loss = 0.61263923\n",
            "Iteration 124, loss = 0.61170053\n",
            "Iteration 125, loss = 0.61074212\n",
            "Iteration 126, loss = 0.60985495\n",
            "Iteration 127, loss = 0.60891853\n",
            "Iteration 128, loss = 0.60799461\n",
            "Iteration 129, loss = 0.60707795\n",
            "Iteration 130, loss = 0.60617739\n",
            "Iteration 131, loss = 0.60525263\n",
            "Iteration 132, loss = 0.60435790\n",
            "Iteration 133, loss = 0.60342707\n",
            "Iteration 134, loss = 0.60253859\n",
            "Iteration 135, loss = 0.60164247\n",
            "Iteration 136, loss = 0.60073749\n",
            "Iteration 137, loss = 0.59985626\n",
            "Iteration 138, loss = 0.59899446\n",
            "Iteration 139, loss = 0.59809800\n",
            "Iteration 140, loss = 0.59720853\n",
            "Iteration 141, loss = 0.59634057\n",
            "Iteration 142, loss = 0.59544246\n",
            "Iteration 143, loss = 0.59457376\n",
            "Iteration 144, loss = 0.59371647\n",
            "Iteration 145, loss = 0.59277791\n",
            "Iteration 146, loss = 0.59195216\n",
            "Iteration 147, loss = 0.59107611\n",
            "Iteration 148, loss = 0.59021717\n",
            "Iteration 149, loss = 0.58937376\n",
            "Iteration 150, loss = 0.58854976\n",
            "Iteration 151, loss = 0.58772131\n",
            "Iteration 152, loss = 0.58690837\n",
            "Iteration 153, loss = 0.58607184\n",
            "Iteration 154, loss = 0.58522572\n",
            "Iteration 155, loss = 0.58440030\n",
            "Iteration 156, loss = 0.58355016\n",
            "Iteration 157, loss = 0.58273685\n",
            "Iteration 158, loss = 0.58193364\n",
            "Iteration 159, loss = 0.58111307\n",
            "Iteration 160, loss = 0.58035114\n",
            "Iteration 161, loss = 0.57953986\n",
            "Iteration 162, loss = 0.57872420\n",
            "Iteration 163, loss = 0.57790967\n",
            "Iteration 164, loss = 0.57715179\n",
            "Iteration 165, loss = 0.57636799\n",
            "Iteration 166, loss = 0.57557673\n",
            "Iteration 167, loss = 0.57479634\n",
            "Iteration 168, loss = 0.57403154\n",
            "Iteration 169, loss = 0.57327089\n",
            "Iteration 170, loss = 0.57251082\n",
            "Iteration 171, loss = 0.57175216\n",
            "Iteration 172, loss = 0.57103062\n",
            "Iteration 173, loss = 0.57027721\n",
            "Iteration 174, loss = 0.56959211\n",
            "Iteration 175, loss = 0.56879605\n",
            "Iteration 176, loss = 0.56807808\n",
            "Iteration 177, loss = 0.56736667\n",
            "Iteration 178, loss = 0.56664661\n",
            "Iteration 179, loss = 0.56589520\n",
            "Iteration 180, loss = 0.56522535\n",
            "Iteration 181, loss = 0.56452110\n",
            "Iteration 182, loss = 0.56378151\n",
            "Iteration 183, loss = 0.56308672\n",
            "Iteration 184, loss = 0.56239549\n",
            "Iteration 185, loss = 0.56172040\n",
            "Iteration 186, loss = 0.56099421\n",
            "Iteration 187, loss = 0.56028202\n",
            "Iteration 188, loss = 0.55959590\n",
            "Iteration 189, loss = 0.55890288\n",
            "Iteration 190, loss = 0.55822574\n",
            "Iteration 191, loss = 0.55754880\n",
            "Iteration 192, loss = 0.55684787\n",
            "Iteration 193, loss = 0.55616420\n",
            "Iteration 194, loss = 0.55548399\n",
            "Iteration 195, loss = 0.55481696\n",
            "Iteration 196, loss = 0.55414518\n",
            "Iteration 197, loss = 0.55347642\n",
            "Iteration 198, loss = 0.55282577\n",
            "Iteration 199, loss = 0.55213983\n",
            "Iteration 200, loss = 0.55149047\n",
            "Iteration 201, loss = 0.55084313\n",
            "Iteration 202, loss = 0.55018183\n",
            "Iteration 203, loss = 0.54955832\n",
            "Iteration 204, loss = 0.54890698\n",
            "Iteration 205, loss = 0.54826334\n",
            "Iteration 206, loss = 0.54765733\n",
            "Iteration 207, loss = 0.54702552\n",
            "Iteration 208, loss = 0.54638075\n",
            "Iteration 209, loss = 0.54575870\n",
            "Iteration 210, loss = 0.54515499\n",
            "Iteration 211, loss = 0.54451578\n",
            "Iteration 212, loss = 0.54390866\n",
            "Iteration 213, loss = 0.54331477\n",
            "Iteration 214, loss = 0.54268350\n",
            "Iteration 215, loss = 0.54206083\n",
            "Iteration 216, loss = 0.54145717\n",
            "Iteration 217, loss = 0.54087071\n",
            "Iteration 218, loss = 0.54027146\n",
            "Iteration 219, loss = 0.53964760\n",
            "Iteration 220, loss = 0.53908092\n",
            "Iteration 221, loss = 0.53848040\n",
            "Iteration 222, loss = 0.53789269\n",
            "Iteration 223, loss = 0.53729922\n",
            "Iteration 224, loss = 0.53670406\n",
            "Iteration 225, loss = 0.53612376\n",
            "Iteration 226, loss = 0.53557856\n",
            "Iteration 227, loss = 0.53496728\n",
            "Iteration 228, loss = 0.53437420\n",
            "Iteration 229, loss = 0.53381604\n",
            "Iteration 230, loss = 0.53322181\n",
            "Iteration 231, loss = 0.53267707\n",
            "Iteration 232, loss = 0.53210749\n",
            "Iteration 233, loss = 0.53152317\n",
            "Iteration 234, loss = 0.53098736\n",
            "Iteration 235, loss = 0.53042547\n",
            "Iteration 236, loss = 0.52986653\n",
            "Iteration 237, loss = 0.52929172\n",
            "Iteration 238, loss = 0.52876279\n",
            "Iteration 239, loss = 0.52820406\n",
            "Iteration 240, loss = 0.52762963\n",
            "Iteration 241, loss = 0.52711492\n",
            "Iteration 242, loss = 0.52655567\n",
            "Iteration 243, loss = 0.52604354\n",
            "Iteration 244, loss = 0.52546297\n",
            "Iteration 245, loss = 0.52493047\n",
            "Iteration 246, loss = 0.52439002\n",
            "Iteration 247, loss = 0.52387118\n",
            "Iteration 248, loss = 0.52337285\n",
            "Iteration 249, loss = 0.52281061\n",
            "Iteration 250, loss = 0.52227155\n",
            "Iteration 251, loss = 0.52175906\n",
            "Iteration 252, loss = 0.52123291\n",
            "Iteration 253, loss = 0.52071920\n",
            "Iteration 254, loss = 0.52016912\n",
            "Iteration 255, loss = 0.51966776\n",
            "Iteration 256, loss = 0.51915512\n",
            "Iteration 257, loss = 0.51864183\n",
            "Iteration 258, loss = 0.51812987\n",
            "Iteration 259, loss = 0.51758512\n",
            "Iteration 260, loss = 0.51708147\n",
            "Iteration 261, loss = 0.51658051\n",
            "Iteration 262, loss = 0.51605820\n",
            "Iteration 263, loss = 0.51557125\n",
            "Iteration 264, loss = 0.51508403\n",
            "Iteration 265, loss = 0.51457973\n",
            "Iteration 266, loss = 0.51409716\n",
            "Iteration 267, loss = 0.51357882\n",
            "Iteration 268, loss = 0.51307121\n",
            "Iteration 269, loss = 0.51261274\n",
            "Iteration 270, loss = 0.51212039\n",
            "Iteration 271, loss = 0.51162470\n",
            "Iteration 272, loss = 0.51116667\n",
            "Iteration 273, loss = 0.51067381\n",
            "Iteration 274, loss = 0.51021192\n",
            "Iteration 275, loss = 0.50976318\n",
            "Iteration 276, loss = 0.50925464\n",
            "Iteration 277, loss = 0.50879691\n",
            "Iteration 278, loss = 0.50833403\n",
            "Iteration 279, loss = 0.50785084\n",
            "Iteration 280, loss = 0.50737466\n",
            "Iteration 281, loss = 0.50691087\n",
            "Iteration 282, loss = 0.50645187\n",
            "Iteration 283, loss = 0.50599421\n",
            "Iteration 284, loss = 0.50549798\n",
            "Iteration 285, loss = 0.50506722\n",
            "Iteration 286, loss = 0.50459380\n",
            "Iteration 287, loss = 0.50412852\n",
            "Iteration 288, loss = 0.50364442\n",
            "Iteration 289, loss = 0.50320008\n",
            "Iteration 290, loss = 0.50275018\n",
            "Iteration 291, loss = 0.50229068\n",
            "Iteration 292, loss = 0.50181722\n",
            "Iteration 293, loss = 0.50138889\n",
            "Iteration 294, loss = 0.50090729\n",
            "Iteration 295, loss = 0.50047753\n",
            "Iteration 296, loss = 0.50001090\n",
            "Iteration 297, loss = 0.49960391\n",
            "Iteration 298, loss = 0.49911909\n",
            "Iteration 299, loss = 0.49873517\n",
            "Iteration 300, loss = 0.49825491\n",
            "Iteration 301, loss = 0.49782059\n",
            "Iteration 302, loss = 0.49740755\n",
            "Iteration 303, loss = 0.49695264\n",
            "Iteration 304, loss = 0.49652458\n",
            "Iteration 305, loss = 0.49611697\n",
            "Iteration 306, loss = 0.49567409\n",
            "Iteration 307, loss = 0.49524964\n",
            "Iteration 308, loss = 0.49485825\n",
            "Iteration 309, loss = 0.49444258\n",
            "Iteration 310, loss = 0.49401429\n",
            "Iteration 311, loss = 0.49360168\n",
            "Iteration 312, loss = 0.49321154\n",
            "Iteration 313, loss = 0.49280022\n",
            "Iteration 314, loss = 0.49239435\n",
            "Iteration 315, loss = 0.49198368\n",
            "Iteration 316, loss = 0.49157069\n",
            "Iteration 317, loss = 0.49117148\n",
            "Iteration 318, loss = 0.49075881\n",
            "Iteration 319, loss = 0.49036465\n",
            "Iteration 320, loss = 0.48995665\n",
            "Iteration 321, loss = 0.48954147\n",
            "Iteration 322, loss = 0.48912899\n",
            "Iteration 323, loss = 0.48872573\n",
            "Iteration 324, loss = 0.48834091\n",
            "Iteration 325, loss = 0.48791531\n",
            "Iteration 326, loss = 0.48750912\n",
            "Iteration 327, loss = 0.48708739\n",
            "Iteration 328, loss = 0.48670112\n",
            "Iteration 329, loss = 0.48629849\n",
            "Iteration 330, loss = 0.48590133\n",
            "Iteration 331, loss = 0.48548900\n",
            "Iteration 332, loss = 0.48508771\n",
            "Iteration 333, loss = 0.48473896\n",
            "Iteration 334, loss = 0.48431739\n",
            "Iteration 335, loss = 0.48396542\n",
            "Iteration 336, loss = 0.48357356\n",
            "Iteration 337, loss = 0.48321477\n",
            "Iteration 338, loss = 0.48280688\n",
            "Iteration 339, loss = 0.48240157\n",
            "Iteration 340, loss = 0.48204241\n",
            "Iteration 341, loss = 0.48170514\n",
            "Iteration 342, loss = 0.48129190\n",
            "Iteration 343, loss = 0.48091526\n",
            "Iteration 344, loss = 0.48056765\n",
            "Iteration 345, loss = 0.48019449\n",
            "Iteration 346, loss = 0.47980709\n",
            "Iteration 347, loss = 0.47944494\n",
            "Iteration 348, loss = 0.47906532\n",
            "Iteration 349, loss = 0.47870997\n",
            "Iteration 350, loss = 0.47836855\n",
            "Iteration 351, loss = 0.47800120\n",
            "Iteration 352, loss = 0.47765429\n",
            "Iteration 353, loss = 0.47734979\n",
            "Iteration 354, loss = 0.47695848\n",
            "Iteration 355, loss = 0.47663606\n",
            "Iteration 356, loss = 0.47627576\n",
            "Iteration 357, loss = 0.47593316\n",
            "Iteration 358, loss = 0.47561642\n",
            "Iteration 359, loss = 0.47524671\n",
            "Iteration 360, loss = 0.47490459\n",
            "Iteration 361, loss = 0.47459157\n",
            "Iteration 362, loss = 0.47424310\n",
            "Iteration 363, loss = 0.47396421\n",
            "Iteration 364, loss = 0.47356348\n",
            "Iteration 365, loss = 0.47325196\n",
            "Iteration 366, loss = 0.47293096\n",
            "Iteration 367, loss = 0.47256737\n",
            "Iteration 368, loss = 0.47224965\n",
            "Iteration 369, loss = 0.47192652\n",
            "Iteration 370, loss = 0.47162330\n",
            "Iteration 371, loss = 0.47128168\n",
            "Iteration 372, loss = 0.47093888\n",
            "Iteration 373, loss = 0.47062239\n",
            "Iteration 374, loss = 0.47029450\n",
            "Iteration 375, loss = 0.46997650\n",
            "Iteration 376, loss = 0.46964754\n",
            "Iteration 377, loss = 0.46934397\n",
            "Iteration 378, loss = 0.46900095\n",
            "Iteration 379, loss = 0.46868589\n",
            "Iteration 380, loss = 0.46840545\n",
            "Iteration 381, loss = 0.46807425\n",
            "Iteration 382, loss = 0.46775141\n",
            "Iteration 383, loss = 0.46744076\n",
            "Iteration 384, loss = 0.46713432\n",
            "Iteration 385, loss = 0.46683559\n",
            "Iteration 386, loss = 0.46650386\n",
            "Iteration 387, loss = 0.46622081\n",
            "Iteration 388, loss = 0.46592467\n",
            "Iteration 389, loss = 0.46564119\n",
            "Iteration 390, loss = 0.46534193\n",
            "Iteration 391, loss = 0.46505747\n",
            "Iteration 392, loss = 0.46475751\n",
            "Iteration 393, loss = 0.46447070\n",
            "Iteration 394, loss = 0.46418837\n",
            "Iteration 395, loss = 0.46388929\n",
            "Iteration 396, loss = 0.46358918\n",
            "Iteration 397, loss = 0.46332980\n",
            "Iteration 398, loss = 0.46305331\n",
            "Iteration 399, loss = 0.46275156\n",
            "Iteration 400, loss = 0.46245626\n",
            "Iteration 401, loss = 0.46218200\n",
            "Iteration 402, loss = 0.46189847\n",
            "Iteration 403, loss = 0.46161865\n",
            "Iteration 404, loss = 0.46133684\n",
            "Iteration 405, loss = 0.46105607\n",
            "Iteration 406, loss = 0.46078148\n",
            "Iteration 407, loss = 0.46052019\n",
            "Iteration 408, loss = 0.46021786\n",
            "Iteration 409, loss = 0.45996838\n",
            "Iteration 410, loss = 0.45971226\n",
            "Iteration 411, loss = 0.45942223\n",
            "Iteration 412, loss = 0.45919341\n",
            "Iteration 413, loss = 0.45892118\n",
            "Iteration 414, loss = 0.45868453\n",
            "Iteration 415, loss = 0.45845998\n",
            "Iteration 416, loss = 0.45818574\n",
            "Iteration 417, loss = 0.45797578\n",
            "Iteration 418, loss = 0.45772147\n",
            "Iteration 419, loss = 0.45744304\n",
            "Iteration 420, loss = 0.45728816\n",
            "Iteration 421, loss = 0.45694385\n",
            "Iteration 422, loss = 0.45667956\n",
            "Iteration 423, loss = 0.45642753\n",
            "Iteration 424, loss = 0.45620736\n",
            "Iteration 425, loss = 0.45594758\n",
            "Iteration 426, loss = 0.45569765\n",
            "Iteration 427, loss = 0.45544860\n",
            "Iteration 428, loss = 0.45519993\n",
            "Iteration 429, loss = 0.45493926\n",
            "Iteration 430, loss = 0.45472740\n",
            "Iteration 431, loss = 0.45448100\n",
            "Iteration 432, loss = 0.45423134\n",
            "Iteration 433, loss = 0.45401153\n",
            "Iteration 434, loss = 0.45375187\n",
            "Iteration 435, loss = 0.45352515\n",
            "Iteration 436, loss = 0.45334717\n",
            "Iteration 437, loss = 0.45309200\n",
            "Iteration 438, loss = 0.45284528\n",
            "Iteration 439, loss = 0.45261340\n",
            "Iteration 440, loss = 0.45242684\n",
            "Iteration 441, loss = 0.45214730\n",
            "Iteration 442, loss = 0.45191500\n",
            "Iteration 443, loss = 0.45169215\n",
            "Iteration 444, loss = 0.45148042\n",
            "Iteration 445, loss = 0.45125123\n",
            "Iteration 446, loss = 0.45103423\n",
            "Iteration 447, loss = 0.45082740\n",
            "Iteration 448, loss = 0.45059133\n",
            "Iteration 449, loss = 0.45038146\n",
            "Iteration 450, loss = 0.45024097\n",
            "Iteration 451, loss = 0.44995344\n",
            "Iteration 452, loss = 0.44975186\n",
            "Iteration 453, loss = 0.44955488\n",
            "Iteration 454, loss = 0.44932836\n",
            "Iteration 455, loss = 0.44912854\n",
            "Iteration 456, loss = 0.44891915\n",
            "Iteration 457, loss = 0.44873070\n",
            "Iteration 458, loss = 0.44853150\n",
            "Iteration 459, loss = 0.44834298\n",
            "Iteration 460, loss = 0.44812080\n",
            "Iteration 461, loss = 0.44793933\n",
            "Iteration 462, loss = 0.44774546\n",
            "Iteration 463, loss = 0.44753128\n",
            "Iteration 464, loss = 0.44737127\n",
            "Iteration 465, loss = 0.44718415\n",
            "Iteration 466, loss = 0.44699811\n",
            "Iteration 467, loss = 0.44682068\n",
            "Iteration 468, loss = 0.44664383\n",
            "Iteration 469, loss = 0.44645620\n",
            "Iteration 470, loss = 0.44626469\n",
            "Iteration 471, loss = 0.44607677\n",
            "Iteration 472, loss = 0.44590998\n",
            "Iteration 473, loss = 0.44570876\n",
            "Iteration 474, loss = 0.44552204\n",
            "Iteration 475, loss = 0.44534401\n",
            "Iteration 476, loss = 0.44515303\n",
            "Iteration 477, loss = 0.44496295\n",
            "Iteration 478, loss = 0.44480920\n",
            "Iteration 479, loss = 0.44462201\n",
            "Iteration 480, loss = 0.44446078\n",
            "Iteration 481, loss = 0.44428226\n",
            "Iteration 482, loss = 0.44413823\n",
            "Iteration 483, loss = 0.44393899\n",
            "Iteration 484, loss = 0.44377436\n",
            "Iteration 485, loss = 0.44360837\n",
            "Iteration 486, loss = 0.44344172\n",
            "Iteration 487, loss = 0.44326591\n",
            "Iteration 488, loss = 0.44309643\n",
            "Iteration 489, loss = 0.44293792\n",
            "Iteration 490, loss = 0.44277277\n",
            "Iteration 491, loss = 0.44258921\n",
            "Iteration 492, loss = 0.44249100\n",
            "Iteration 493, loss = 0.44233088\n",
            "Iteration 494, loss = 0.44215328\n",
            "Iteration 495, loss = 0.44198698\n",
            "Iteration 496, loss = 0.44185076\n",
            "Iteration 497, loss = 0.44168649\n",
            "Iteration 498, loss = 0.44154092\n",
            "Iteration 499, loss = 0.44136685\n",
            "Iteration 500, loss = 0.44119136\n",
            "Iteration 501, loss = 0.44103073\n",
            "Iteration 502, loss = 0.44086420\n",
            "Iteration 503, loss = 0.44068778\n",
            "Iteration 504, loss = 0.44051430\n",
            "Iteration 505, loss = 0.44035772\n",
            "Iteration 506, loss = 0.44020152\n",
            "Iteration 507, loss = 0.44005262\n",
            "Iteration 508, loss = 0.43990156\n",
            "Iteration 509, loss = 0.43976845\n",
            "Iteration 510, loss = 0.43960097\n",
            "Iteration 511, loss = 0.43947372\n",
            "Iteration 512, loss = 0.43930924\n",
            "Iteration 513, loss = 0.43918023\n",
            "Iteration 514, loss = 0.43902819\n",
            "Iteration 515, loss = 0.43888377\n",
            "Iteration 516, loss = 0.43874770\n",
            "Iteration 517, loss = 0.43861464\n",
            "Iteration 518, loss = 0.43847835\n",
            "Iteration 519, loss = 0.43832700\n",
            "Iteration 520, loss = 0.43819166\n",
            "Iteration 521, loss = 0.43806756\n",
            "Iteration 522, loss = 0.43793866\n",
            "Iteration 523, loss = 0.43779386\n",
            "Iteration 524, loss = 0.43765353\n",
            "Iteration 525, loss = 0.43753515\n",
            "Iteration 526, loss = 0.43739224\n",
            "Iteration 527, loss = 0.43725784\n",
            "Iteration 528, loss = 0.43713243\n",
            "Iteration 529, loss = 0.43698230\n",
            "Iteration 530, loss = 0.43686546\n",
            "Iteration 531, loss = 0.43671840\n",
            "Iteration 532, loss = 0.43658779\n",
            "Iteration 533, loss = 0.43644721\n",
            "Iteration 534, loss = 0.43637631\n",
            "Iteration 535, loss = 0.43622369\n",
            "Iteration 536, loss = 0.43609547\n",
            "Iteration 537, loss = 0.43596796\n",
            "Iteration 538, loss = 0.43584455\n",
            "Iteration 539, loss = 0.43575090\n",
            "Iteration 540, loss = 0.43561401\n",
            "Iteration 541, loss = 0.43546973\n",
            "Iteration 542, loss = 0.43535861\n",
            "Iteration 543, loss = 0.43527279\n",
            "Iteration 544, loss = 0.43515744\n",
            "Iteration 545, loss = 0.43502947\n",
            "Iteration 546, loss = 0.43492125\n",
            "Iteration 547, loss = 0.43480824\n",
            "Iteration 548, loss = 0.43469271\n",
            "Iteration 549, loss = 0.43456673\n",
            "Iteration 550, loss = 0.43446083\n",
            "Iteration 551, loss = 0.43434327\n",
            "Iteration 552, loss = 0.43422805\n",
            "Iteration 553, loss = 0.43413169\n",
            "Iteration 554, loss = 0.43401605\n",
            "Iteration 555, loss = 0.43391995\n",
            "Iteration 556, loss = 0.43382856\n",
            "Iteration 557, loss = 0.43369569\n",
            "Iteration 558, loss = 0.43360068\n",
            "Iteration 559, loss = 0.43349171\n",
            "Iteration 560, loss = 0.43339828\n",
            "Iteration 561, loss = 0.43329454\n",
            "Iteration 562, loss = 0.43315265\n",
            "Iteration 563, loss = 0.43304607\n",
            "Iteration 564, loss = 0.43292356\n",
            "Iteration 565, loss = 0.43286025\n",
            "Iteration 566, loss = 0.43270203\n",
            "Iteration 567, loss = 0.43261852\n",
            "Iteration 568, loss = 0.43251256\n",
            "Iteration 569, loss = 0.43242621\n",
            "Iteration 570, loss = 0.43231710\n",
            "Iteration 571, loss = 0.43221770\n",
            "Iteration 572, loss = 0.43211668\n",
            "Iteration 573, loss = 0.43200960\n",
            "Iteration 574, loss = 0.43191647\n",
            "Iteration 575, loss = 0.43181413\n",
            "Iteration 576, loss = 0.43171734\n",
            "Iteration 577, loss = 0.43162593\n",
            "Iteration 578, loss = 0.43151466\n",
            "Iteration 579, loss = 0.43140435\n",
            "Iteration 580, loss = 0.43131418\n",
            "Iteration 581, loss = 0.43120917\n",
            "Iteration 582, loss = 0.43110642\n",
            "Iteration 583, loss = 0.43101156\n",
            "Iteration 584, loss = 0.43091507\n",
            "Iteration 585, loss = 0.43085060\n",
            "Iteration 586, loss = 0.43073067\n",
            "Iteration 587, loss = 0.43064615\n",
            "Iteration 588, loss = 0.43056323\n",
            "Iteration 589, loss = 0.43045018\n",
            "Iteration 590, loss = 0.43038117\n",
            "Iteration 591, loss = 0.43032417\n",
            "Iteration 592, loss = 0.43019866\n",
            "Iteration 593, loss = 0.43012752\n",
            "Iteration 594, loss = 0.43002215\n",
            "Iteration 595, loss = 0.42993861\n",
            "Iteration 596, loss = 0.42983969\n",
            "Iteration 597, loss = 0.42977016\n",
            "Iteration 598, loss = 0.42966131\n",
            "Iteration 599, loss = 0.42957356\n",
            "Iteration 600, loss = 0.42950207\n",
            "Iteration 601, loss = 0.42941312\n",
            "Iteration 602, loss = 0.42933551\n",
            "Iteration 603, loss = 0.42925264\n",
            "Iteration 604, loss = 0.42917790\n",
            "Iteration 605, loss = 0.42915854\n",
            "Iteration 606, loss = 0.42900335\n",
            "Iteration 607, loss = 0.42892552\n",
            "Iteration 608, loss = 0.42884108\n",
            "Iteration 609, loss = 0.42876557\n",
            "Iteration 610, loss = 0.42867483\n",
            "Iteration 611, loss = 0.42860660\n",
            "Iteration 612, loss = 0.42850962\n",
            "Iteration 613, loss = 0.42840884\n",
            "Iteration 614, loss = 0.42835246\n",
            "Iteration 615, loss = 0.42824407\n",
            "Iteration 616, loss = 0.42816138\n",
            "Iteration 617, loss = 0.42810321\n",
            "Iteration 618, loss = 0.42799984\n",
            "Iteration 619, loss = 0.42791419\n",
            "Iteration 620, loss = 0.42782713\n",
            "Iteration 621, loss = 0.42775202\n",
            "Iteration 622, loss = 0.42767261\n",
            "Iteration 623, loss = 0.42760904\n",
            "Iteration 624, loss = 0.42753023\n",
            "Iteration 625, loss = 0.42747803\n",
            "Iteration 626, loss = 0.42741538\n",
            "Iteration 627, loss = 0.42732822\n",
            "Iteration 628, loss = 0.42724426\n",
            "Iteration 629, loss = 0.42719140\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69456510\n",
            "Iteration 2, loss = 0.69413215\n",
            "Iteration 3, loss = 0.69375891\n",
            "Iteration 4, loss = 0.69343161\n",
            "Iteration 5, loss = 0.69305278\n",
            "Iteration 6, loss = 0.69270355\n",
            "Iteration 7, loss = 0.69240020\n",
            "Iteration 8, loss = 0.69205971\n",
            "Iteration 9, loss = 0.69173530\n",
            "Iteration 10, loss = 0.69139766\n",
            "Iteration 11, loss = 0.69107223\n",
            "Iteration 12, loss = 0.69073003\n",
            "Iteration 13, loss = 0.69039622\n",
            "Iteration 14, loss = 0.69004414\n",
            "Iteration 15, loss = 0.68978450\n",
            "Iteration 16, loss = 0.68937262\n",
            "Iteration 17, loss = 0.68901630\n",
            "Iteration 18, loss = 0.68868044\n",
            "Iteration 19, loss = 0.68833158\n",
            "Iteration 20, loss = 0.68799484\n",
            "Iteration 21, loss = 0.68765028\n",
            "Iteration 22, loss = 0.68727475\n",
            "Iteration 23, loss = 0.68692317\n",
            "Iteration 24, loss = 0.68655318\n",
            "Iteration 25, loss = 0.68618747\n",
            "Iteration 26, loss = 0.68581353\n",
            "Iteration 27, loss = 0.68540396\n",
            "Iteration 28, loss = 0.68502880\n",
            "Iteration 29, loss = 0.68465957\n",
            "Iteration 30, loss = 0.68423985\n",
            "Iteration 31, loss = 0.68384295\n",
            "Iteration 32, loss = 0.68342286\n",
            "Iteration 33, loss = 0.68298745\n",
            "Iteration 34, loss = 0.68256247\n",
            "Iteration 35, loss = 0.68212640\n",
            "Iteration 36, loss = 0.68167501\n",
            "Iteration 37, loss = 0.68118781\n",
            "Iteration 38, loss = 0.68073055\n",
            "Iteration 39, loss = 0.68024207\n",
            "Iteration 40, loss = 0.67975534\n",
            "Iteration 41, loss = 0.67923535\n",
            "Iteration 42, loss = 0.67874020\n",
            "Iteration 43, loss = 0.67819801\n",
            "Iteration 44, loss = 0.67767662\n",
            "Iteration 45, loss = 0.67713978\n",
            "Iteration 46, loss = 0.67658667\n",
            "Iteration 47, loss = 0.67602261\n",
            "Iteration 48, loss = 0.67544206\n",
            "Iteration 49, loss = 0.67484111\n",
            "Iteration 50, loss = 0.67423753\n",
            "Iteration 51, loss = 0.67362206\n",
            "Iteration 52, loss = 0.67301579\n",
            "Iteration 53, loss = 0.67240564\n",
            "Iteration 54, loss = 0.67172643\n",
            "Iteration 55, loss = 0.67106835\n",
            "Iteration 56, loss = 0.67039912\n",
            "Iteration 57, loss = 0.66969725\n",
            "Iteration 58, loss = 0.66899266\n",
            "Iteration 59, loss = 0.66830455\n",
            "Iteration 60, loss = 0.66757694\n",
            "Iteration 61, loss = 0.66687078\n",
            "Iteration 62, loss = 0.66610552\n",
            "Iteration 63, loss = 0.66539106\n",
            "Iteration 64, loss = 0.66461605\n",
            "Iteration 65, loss = 0.66386389\n",
            "Iteration 66, loss = 0.66311192\n",
            "Iteration 67, loss = 0.66231519\n",
            "Iteration 68, loss = 0.66151348\n",
            "Iteration 69, loss = 0.66073650\n",
            "Iteration 70, loss = 0.65992215\n",
            "Iteration 71, loss = 0.65908630\n",
            "Iteration 72, loss = 0.65827111\n",
            "Iteration 73, loss = 0.65742970\n",
            "Iteration 74, loss = 0.65659279\n",
            "Iteration 75, loss = 0.65574551\n",
            "Iteration 76, loss = 0.65489565\n",
            "Iteration 77, loss = 0.65403568\n",
            "Iteration 78, loss = 0.65318466\n",
            "Iteration 79, loss = 0.65230208\n",
            "Iteration 80, loss = 0.65140743\n",
            "Iteration 81, loss = 0.65053602\n",
            "Iteration 82, loss = 0.64961306\n",
            "Iteration 83, loss = 0.64870861\n",
            "Iteration 84, loss = 0.64781585\n",
            "Iteration 85, loss = 0.64689817\n",
            "Iteration 86, loss = 0.64598941\n",
            "Iteration 87, loss = 0.64505077\n",
            "Iteration 88, loss = 0.64416555\n",
            "Iteration 89, loss = 0.64325062\n",
            "Iteration 90, loss = 0.64230482\n",
            "Iteration 91, loss = 0.64137164\n",
            "Iteration 92, loss = 0.64044552\n",
            "Iteration 93, loss = 0.63952692\n",
            "Iteration 94, loss = 0.63857258\n",
            "Iteration 95, loss = 0.63759604\n",
            "Iteration 96, loss = 0.63665356\n",
            "Iteration 97, loss = 0.63569728\n",
            "Iteration 98, loss = 0.63473811\n",
            "Iteration 99, loss = 0.63377683\n",
            "Iteration 100, loss = 0.63279329\n",
            "Iteration 101, loss = 0.63182230\n",
            "Iteration 102, loss = 0.63088501\n",
            "Iteration 103, loss = 0.62991226\n",
            "Iteration 104, loss = 0.62891167\n",
            "Iteration 105, loss = 0.62796427\n",
            "Iteration 106, loss = 0.62701148\n",
            "Iteration 107, loss = 0.62607386\n",
            "Iteration 108, loss = 0.62511977\n",
            "Iteration 109, loss = 0.62418664\n",
            "Iteration 110, loss = 0.62325606\n",
            "Iteration 111, loss = 0.62231859\n",
            "Iteration 112, loss = 0.62141591\n",
            "Iteration 113, loss = 0.62045318\n",
            "Iteration 114, loss = 0.61959771\n",
            "Iteration 115, loss = 0.61860210\n",
            "Iteration 116, loss = 0.61764018\n",
            "Iteration 117, loss = 0.61675146\n",
            "Iteration 118, loss = 0.61584062\n",
            "Iteration 119, loss = 0.61491846\n",
            "Iteration 120, loss = 0.61399889\n",
            "Iteration 121, loss = 0.61304102\n",
            "Iteration 122, loss = 0.61214233\n",
            "Iteration 123, loss = 0.61124187\n",
            "Iteration 124, loss = 0.61031073\n",
            "Iteration 125, loss = 0.60936492\n",
            "Iteration 126, loss = 0.60851718\n",
            "Iteration 127, loss = 0.60759128\n",
            "Iteration 128, loss = 0.60668577\n",
            "Iteration 129, loss = 0.60581856\n",
            "Iteration 130, loss = 0.60491120\n",
            "Iteration 131, loss = 0.60400180\n",
            "Iteration 132, loss = 0.60313671\n",
            "Iteration 133, loss = 0.60225828\n",
            "Iteration 134, loss = 0.60138539\n",
            "Iteration 135, loss = 0.60055471\n",
            "Iteration 136, loss = 0.59963608\n",
            "Iteration 137, loss = 0.59880783\n",
            "Iteration 138, loss = 0.59797864\n",
            "Iteration 139, loss = 0.59711178\n",
            "Iteration 140, loss = 0.59623166\n",
            "Iteration 141, loss = 0.59542713\n",
            "Iteration 142, loss = 0.59457372\n",
            "Iteration 143, loss = 0.59373603\n",
            "Iteration 144, loss = 0.59289562\n",
            "Iteration 145, loss = 0.59200850\n",
            "Iteration 146, loss = 0.59123342\n",
            "Iteration 147, loss = 0.59038341\n",
            "Iteration 148, loss = 0.58956728\n",
            "Iteration 149, loss = 0.58876240\n",
            "Iteration 150, loss = 0.58796031\n",
            "Iteration 151, loss = 0.58714714\n",
            "Iteration 152, loss = 0.58639037\n",
            "Iteration 153, loss = 0.58557067\n",
            "Iteration 154, loss = 0.58476189\n",
            "Iteration 155, loss = 0.58401815\n",
            "Iteration 156, loss = 0.58321066\n",
            "Iteration 157, loss = 0.58246023\n",
            "Iteration 158, loss = 0.58170215\n",
            "Iteration 159, loss = 0.58095722\n",
            "Iteration 160, loss = 0.58021081\n",
            "Iteration 161, loss = 0.57942484\n",
            "Iteration 162, loss = 0.57868648\n",
            "Iteration 163, loss = 0.57790558\n",
            "Iteration 164, loss = 0.57715088\n",
            "Iteration 165, loss = 0.57641660\n",
            "Iteration 166, loss = 0.57564619\n",
            "Iteration 167, loss = 0.57493960\n",
            "Iteration 168, loss = 0.57415736\n",
            "Iteration 169, loss = 0.57343634\n",
            "Iteration 170, loss = 0.57269904\n",
            "Iteration 171, loss = 0.57195046\n",
            "Iteration 172, loss = 0.57124832\n",
            "Iteration 173, loss = 0.57054380\n",
            "Iteration 174, loss = 0.56982907\n",
            "Iteration 175, loss = 0.56911101\n",
            "Iteration 176, loss = 0.56840192\n",
            "Iteration 177, loss = 0.56773287\n",
            "Iteration 178, loss = 0.56702707\n",
            "Iteration 179, loss = 0.56630294\n",
            "Iteration 180, loss = 0.56563133\n",
            "Iteration 181, loss = 0.56495317\n",
            "Iteration 182, loss = 0.56426856\n",
            "Iteration 183, loss = 0.56360716\n",
            "Iteration 184, loss = 0.56293260\n",
            "Iteration 185, loss = 0.56232614\n",
            "Iteration 186, loss = 0.56160326\n",
            "Iteration 187, loss = 0.56092973\n",
            "Iteration 188, loss = 0.56026683\n",
            "Iteration 189, loss = 0.55959161\n",
            "Iteration 190, loss = 0.55894902\n",
            "Iteration 191, loss = 0.55829071\n",
            "Iteration 192, loss = 0.55762419\n",
            "Iteration 193, loss = 0.55695400\n",
            "Iteration 194, loss = 0.55632003\n",
            "Iteration 195, loss = 0.55567546\n",
            "Iteration 196, loss = 0.55501265\n",
            "Iteration 197, loss = 0.55438499\n",
            "Iteration 198, loss = 0.55377247\n",
            "Iteration 199, loss = 0.55313238\n",
            "Iteration 200, loss = 0.55247739\n",
            "Iteration 201, loss = 0.55182960\n",
            "Iteration 202, loss = 0.55123957\n",
            "Iteration 203, loss = 0.55062764\n",
            "Iteration 204, loss = 0.55001919\n",
            "Iteration 205, loss = 0.54943808\n",
            "Iteration 206, loss = 0.54885305\n",
            "Iteration 207, loss = 0.54827431\n",
            "Iteration 208, loss = 0.54763512\n",
            "Iteration 209, loss = 0.54707290\n",
            "Iteration 210, loss = 0.54648283\n",
            "Iteration 211, loss = 0.54590083\n",
            "Iteration 212, loss = 0.54530914\n",
            "Iteration 213, loss = 0.54473427\n",
            "Iteration 214, loss = 0.54414802\n",
            "Iteration 215, loss = 0.54355946\n",
            "Iteration 216, loss = 0.54296620\n",
            "Iteration 217, loss = 0.54238562\n",
            "Iteration 218, loss = 0.54180351\n",
            "Iteration 219, loss = 0.54122337\n",
            "Iteration 220, loss = 0.54063449\n",
            "Iteration 221, loss = 0.54007608\n",
            "Iteration 222, loss = 0.53950974\n",
            "Iteration 223, loss = 0.53895064\n",
            "Iteration 224, loss = 0.53839115\n",
            "Iteration 225, loss = 0.53782094\n",
            "Iteration 226, loss = 0.53728247\n",
            "Iteration 227, loss = 0.53669821\n",
            "Iteration 228, loss = 0.53614500\n",
            "Iteration 229, loss = 0.53559622\n",
            "Iteration 230, loss = 0.53502090\n",
            "Iteration 231, loss = 0.53449196\n",
            "Iteration 232, loss = 0.53393241\n",
            "Iteration 233, loss = 0.53338829\n",
            "Iteration 234, loss = 0.53284416\n",
            "Iteration 235, loss = 0.53232618\n",
            "Iteration 236, loss = 0.53176508\n",
            "Iteration 237, loss = 0.53121476\n",
            "Iteration 238, loss = 0.53069975\n",
            "Iteration 239, loss = 0.53017360\n",
            "Iteration 240, loss = 0.52961299\n",
            "Iteration 241, loss = 0.52909211\n",
            "Iteration 242, loss = 0.52856102\n",
            "Iteration 243, loss = 0.52803830\n",
            "Iteration 244, loss = 0.52749956\n",
            "Iteration 245, loss = 0.52696257\n",
            "Iteration 246, loss = 0.52642573\n",
            "Iteration 247, loss = 0.52591325\n",
            "Iteration 248, loss = 0.52543539\n",
            "Iteration 249, loss = 0.52486801\n",
            "Iteration 250, loss = 0.52435326\n",
            "Iteration 251, loss = 0.52383221\n",
            "Iteration 252, loss = 0.52333285\n",
            "Iteration 253, loss = 0.52282305\n",
            "Iteration 254, loss = 0.52227077\n",
            "Iteration 255, loss = 0.52174814\n",
            "Iteration 256, loss = 0.52125854\n",
            "Iteration 257, loss = 0.52073757\n",
            "Iteration 258, loss = 0.52024182\n",
            "Iteration 259, loss = 0.51970591\n",
            "Iteration 260, loss = 0.51919342\n",
            "Iteration 261, loss = 0.51870283\n",
            "Iteration 262, loss = 0.51818244\n",
            "Iteration 263, loss = 0.51768446\n",
            "Iteration 264, loss = 0.51720147\n",
            "Iteration 265, loss = 0.51669506\n",
            "Iteration 266, loss = 0.51619217\n",
            "Iteration 267, loss = 0.51568973\n",
            "Iteration 268, loss = 0.51516913\n",
            "Iteration 269, loss = 0.51470092\n",
            "Iteration 270, loss = 0.51422570\n",
            "Iteration 271, loss = 0.51373529\n",
            "Iteration 272, loss = 0.51324957\n",
            "Iteration 273, loss = 0.51276520\n",
            "Iteration 274, loss = 0.51229607\n",
            "Iteration 275, loss = 0.51188652\n",
            "Iteration 276, loss = 0.51136139\n",
            "Iteration 277, loss = 0.51087516\n",
            "Iteration 278, loss = 0.51041374\n",
            "Iteration 279, loss = 0.50991410\n",
            "Iteration 280, loss = 0.50943149\n",
            "Iteration 281, loss = 0.50895799\n",
            "Iteration 282, loss = 0.50850159\n",
            "Iteration 283, loss = 0.50805298\n",
            "Iteration 284, loss = 0.50755071\n",
            "Iteration 285, loss = 0.50711181\n",
            "Iteration 286, loss = 0.50663079\n",
            "Iteration 287, loss = 0.50615738\n",
            "Iteration 288, loss = 0.50570038\n",
            "Iteration 289, loss = 0.50523458\n",
            "Iteration 290, loss = 0.50475859\n",
            "Iteration 291, loss = 0.50431398\n",
            "Iteration 292, loss = 0.50382159\n",
            "Iteration 293, loss = 0.50337740\n",
            "Iteration 294, loss = 0.50290173\n",
            "Iteration 295, loss = 0.50243077\n",
            "Iteration 296, loss = 0.50197712\n",
            "Iteration 297, loss = 0.50155290\n",
            "Iteration 298, loss = 0.50107179\n",
            "Iteration 299, loss = 0.50065886\n",
            "Iteration 300, loss = 0.50019124\n",
            "Iteration 301, loss = 0.49974943\n",
            "Iteration 302, loss = 0.49933234\n",
            "Iteration 303, loss = 0.49887340\n",
            "Iteration 304, loss = 0.49844274\n",
            "Iteration 305, loss = 0.49801912\n",
            "Iteration 306, loss = 0.49758192\n",
            "Iteration 307, loss = 0.49715119\n",
            "Iteration 308, loss = 0.49674044\n",
            "Iteration 309, loss = 0.49632616\n",
            "Iteration 310, loss = 0.49589162\n",
            "Iteration 311, loss = 0.49547784\n",
            "Iteration 312, loss = 0.49507718\n",
            "Iteration 313, loss = 0.49463342\n",
            "Iteration 314, loss = 0.49423231\n",
            "Iteration 315, loss = 0.49378976\n",
            "Iteration 316, loss = 0.49336804\n",
            "Iteration 317, loss = 0.49295107\n",
            "Iteration 318, loss = 0.49253822\n",
            "Iteration 319, loss = 0.49210301\n",
            "Iteration 320, loss = 0.49169142\n",
            "Iteration 321, loss = 0.49125268\n",
            "Iteration 322, loss = 0.49083315\n",
            "Iteration 323, loss = 0.49042000\n",
            "Iteration 324, loss = 0.48999619\n",
            "Iteration 325, loss = 0.48957842\n",
            "Iteration 326, loss = 0.48913250\n",
            "Iteration 327, loss = 0.48870291\n",
            "Iteration 328, loss = 0.48827433\n",
            "Iteration 329, loss = 0.48785202\n",
            "Iteration 330, loss = 0.48746281\n",
            "Iteration 331, loss = 0.48701285\n",
            "Iteration 332, loss = 0.48659028\n",
            "Iteration 333, loss = 0.48624643\n",
            "Iteration 334, loss = 0.48578700\n",
            "Iteration 335, loss = 0.48538205\n",
            "Iteration 336, loss = 0.48499246\n",
            "Iteration 337, loss = 0.48457522\n",
            "Iteration 338, loss = 0.48417656\n",
            "Iteration 339, loss = 0.48376389\n",
            "Iteration 340, loss = 0.48339490\n",
            "Iteration 341, loss = 0.48299617\n",
            "Iteration 342, loss = 0.48259586\n",
            "Iteration 343, loss = 0.48218961\n",
            "Iteration 344, loss = 0.48179427\n",
            "Iteration 345, loss = 0.48140056\n",
            "Iteration 346, loss = 0.48100372\n",
            "Iteration 347, loss = 0.48063552\n",
            "Iteration 348, loss = 0.48023253\n",
            "Iteration 349, loss = 0.47983417\n",
            "Iteration 350, loss = 0.47948412\n",
            "Iteration 351, loss = 0.47910022\n",
            "Iteration 352, loss = 0.47871045\n",
            "Iteration 353, loss = 0.47837396\n",
            "Iteration 354, loss = 0.47793660\n",
            "Iteration 355, loss = 0.47757648\n",
            "Iteration 356, loss = 0.47720562\n",
            "Iteration 357, loss = 0.47682514\n",
            "Iteration 358, loss = 0.47647524\n",
            "Iteration 359, loss = 0.47609054\n",
            "Iteration 360, loss = 0.47571485\n",
            "Iteration 361, loss = 0.47538737\n",
            "Iteration 362, loss = 0.47501565\n",
            "Iteration 363, loss = 0.47464791\n",
            "Iteration 364, loss = 0.47427737\n",
            "Iteration 365, loss = 0.47393912\n",
            "Iteration 366, loss = 0.47360289\n",
            "Iteration 367, loss = 0.47323844\n",
            "Iteration 368, loss = 0.47290203\n",
            "Iteration 369, loss = 0.47254474\n",
            "Iteration 370, loss = 0.47217660\n",
            "Iteration 371, loss = 0.47182820\n",
            "Iteration 372, loss = 0.47147739\n",
            "Iteration 373, loss = 0.47111105\n",
            "Iteration 374, loss = 0.47076178\n",
            "Iteration 375, loss = 0.47041549\n",
            "Iteration 376, loss = 0.47005214\n",
            "Iteration 377, loss = 0.46973420\n",
            "Iteration 378, loss = 0.46939821\n",
            "Iteration 379, loss = 0.46904306\n",
            "Iteration 380, loss = 0.46872648\n",
            "Iteration 381, loss = 0.46836653\n",
            "Iteration 382, loss = 0.46805076\n",
            "Iteration 383, loss = 0.46773133\n",
            "Iteration 384, loss = 0.46740087\n",
            "Iteration 385, loss = 0.46709501\n",
            "Iteration 386, loss = 0.46675959\n",
            "Iteration 387, loss = 0.46645382\n",
            "Iteration 388, loss = 0.46615285\n",
            "Iteration 389, loss = 0.46583110\n",
            "Iteration 390, loss = 0.46552784\n",
            "Iteration 391, loss = 0.46523187\n",
            "Iteration 392, loss = 0.46491624\n",
            "Iteration 393, loss = 0.46460950\n",
            "Iteration 394, loss = 0.46431522\n",
            "Iteration 395, loss = 0.46402040\n",
            "Iteration 396, loss = 0.46370428\n",
            "Iteration 397, loss = 0.46340558\n",
            "Iteration 398, loss = 0.46308130\n",
            "Iteration 399, loss = 0.46277253\n",
            "Iteration 400, loss = 0.46245736\n",
            "Iteration 401, loss = 0.46215037\n",
            "Iteration 402, loss = 0.46184073\n",
            "Iteration 403, loss = 0.46153167\n",
            "Iteration 404, loss = 0.46123068\n",
            "Iteration 405, loss = 0.46090603\n",
            "Iteration 406, loss = 0.46062929\n",
            "Iteration 407, loss = 0.46034205\n",
            "Iteration 408, loss = 0.46000988\n",
            "Iteration 409, loss = 0.45973240\n",
            "Iteration 410, loss = 0.45942941\n",
            "Iteration 411, loss = 0.45913715\n",
            "Iteration 412, loss = 0.45885707\n",
            "Iteration 413, loss = 0.45856323\n",
            "Iteration 414, loss = 0.45829277\n",
            "Iteration 415, loss = 0.45806630\n",
            "Iteration 416, loss = 0.45773183\n",
            "Iteration 417, loss = 0.45746922\n",
            "Iteration 418, loss = 0.45716937\n",
            "Iteration 419, loss = 0.45688956\n",
            "Iteration 420, loss = 0.45671249\n",
            "Iteration 421, loss = 0.45631811\n",
            "Iteration 422, loss = 0.45605574\n",
            "Iteration 423, loss = 0.45576680\n",
            "Iteration 424, loss = 0.45549702\n",
            "Iteration 425, loss = 0.45525073\n",
            "Iteration 426, loss = 0.45495586\n",
            "Iteration 427, loss = 0.45467406\n",
            "Iteration 428, loss = 0.45440494\n",
            "Iteration 429, loss = 0.45412850\n",
            "Iteration 430, loss = 0.45389468\n",
            "Iteration 431, loss = 0.45361858\n",
            "Iteration 432, loss = 0.45335546\n",
            "Iteration 433, loss = 0.45310500\n",
            "Iteration 434, loss = 0.45284825\n",
            "Iteration 435, loss = 0.45258862\n",
            "Iteration 436, loss = 0.45234269\n",
            "Iteration 437, loss = 0.45207762\n",
            "Iteration 438, loss = 0.45182836\n",
            "Iteration 439, loss = 0.45157747\n",
            "Iteration 440, loss = 0.45133930\n",
            "Iteration 441, loss = 0.45107386\n",
            "Iteration 442, loss = 0.45083534\n",
            "Iteration 443, loss = 0.45059238\n",
            "Iteration 444, loss = 0.45035979\n",
            "Iteration 445, loss = 0.45011153\n",
            "Iteration 446, loss = 0.44985675\n",
            "Iteration 447, loss = 0.44962022\n",
            "Iteration 448, loss = 0.44938000\n",
            "Iteration 449, loss = 0.44915077\n",
            "Iteration 450, loss = 0.44897067\n",
            "Iteration 451, loss = 0.44867338\n",
            "Iteration 452, loss = 0.44845521\n",
            "Iteration 453, loss = 0.44826375\n",
            "Iteration 454, loss = 0.44801645\n",
            "Iteration 455, loss = 0.44776761\n",
            "Iteration 456, loss = 0.44756206\n",
            "Iteration 457, loss = 0.44734329\n",
            "Iteration 458, loss = 0.44714537\n",
            "Iteration 459, loss = 0.44693387\n",
            "Iteration 460, loss = 0.44670758\n",
            "Iteration 461, loss = 0.44650197\n",
            "Iteration 462, loss = 0.44628646\n",
            "Iteration 463, loss = 0.44607171\n",
            "Iteration 464, loss = 0.44587976\n",
            "Iteration 465, loss = 0.44565782\n",
            "Iteration 466, loss = 0.44546710\n",
            "Iteration 467, loss = 0.44526310\n",
            "Iteration 468, loss = 0.44507211\n",
            "Iteration 469, loss = 0.44485485\n",
            "Iteration 470, loss = 0.44464274\n",
            "Iteration 471, loss = 0.44443250\n",
            "Iteration 472, loss = 0.44425875\n",
            "Iteration 473, loss = 0.44401901\n",
            "Iteration 474, loss = 0.44383608\n",
            "Iteration 475, loss = 0.44364060\n",
            "Iteration 476, loss = 0.44342490\n",
            "Iteration 477, loss = 0.44321836\n",
            "Iteration 478, loss = 0.44306115\n",
            "Iteration 479, loss = 0.44286592\n",
            "Iteration 480, loss = 0.44268139\n",
            "Iteration 481, loss = 0.44246958\n",
            "Iteration 482, loss = 0.44228285\n",
            "Iteration 483, loss = 0.44209766\n",
            "Iteration 484, loss = 0.44191744\n",
            "Iteration 485, loss = 0.44172691\n",
            "Iteration 486, loss = 0.44154507\n",
            "Iteration 487, loss = 0.44134536\n",
            "Iteration 488, loss = 0.44117011\n",
            "Iteration 489, loss = 0.44098817\n",
            "Iteration 490, loss = 0.44079285\n",
            "Iteration 491, loss = 0.44059499\n",
            "Iteration 492, loss = 0.44044710\n",
            "Iteration 493, loss = 0.44031209\n",
            "Iteration 494, loss = 0.44009722\n",
            "Iteration 495, loss = 0.43991691\n",
            "Iteration 496, loss = 0.43978070\n",
            "Iteration 497, loss = 0.43960424\n",
            "Iteration 498, loss = 0.43941722\n",
            "Iteration 499, loss = 0.43922913\n",
            "Iteration 500, loss = 0.43905949\n",
            "Iteration 501, loss = 0.43887703\n",
            "Iteration 502, loss = 0.43867791\n",
            "Iteration 503, loss = 0.43851011\n",
            "Iteration 504, loss = 0.43832601\n",
            "Iteration 505, loss = 0.43816442\n",
            "Iteration 506, loss = 0.43800461\n",
            "Iteration 507, loss = 0.43783111\n",
            "Iteration 508, loss = 0.43766865\n",
            "Iteration 509, loss = 0.43751781\n",
            "Iteration 510, loss = 0.43735903\n",
            "Iteration 511, loss = 0.43721597\n",
            "Iteration 512, loss = 0.43704496\n",
            "Iteration 513, loss = 0.43688927\n",
            "Iteration 514, loss = 0.43673235\n",
            "Iteration 515, loss = 0.43658063\n",
            "Iteration 516, loss = 0.43643143\n",
            "Iteration 517, loss = 0.43628549\n",
            "Iteration 518, loss = 0.43613423\n",
            "Iteration 519, loss = 0.43598800\n",
            "Iteration 520, loss = 0.43584972\n",
            "Iteration 521, loss = 0.43572372\n",
            "Iteration 522, loss = 0.43555757\n",
            "Iteration 523, loss = 0.43542714\n",
            "Iteration 524, loss = 0.43526802\n",
            "Iteration 525, loss = 0.43511091\n",
            "Iteration 526, loss = 0.43496973\n",
            "Iteration 527, loss = 0.43483377\n",
            "Iteration 528, loss = 0.43473177\n",
            "Iteration 529, loss = 0.43454024\n",
            "Iteration 530, loss = 0.43441547\n",
            "Iteration 531, loss = 0.43425885\n",
            "Iteration 532, loss = 0.43414656\n",
            "Iteration 533, loss = 0.43398359\n",
            "Iteration 534, loss = 0.43383003\n",
            "Iteration 535, loss = 0.43372624\n",
            "Iteration 536, loss = 0.43357927\n",
            "Iteration 537, loss = 0.43342924\n",
            "Iteration 538, loss = 0.43332470\n",
            "Iteration 539, loss = 0.43319213\n",
            "Iteration 540, loss = 0.43306119\n",
            "Iteration 541, loss = 0.43291027\n",
            "Iteration 542, loss = 0.43277831\n",
            "Iteration 543, loss = 0.43268307\n",
            "Iteration 544, loss = 0.43255706\n",
            "Iteration 545, loss = 0.43241715\n",
            "Iteration 546, loss = 0.43230916\n",
            "Iteration 547, loss = 0.43219962\n",
            "Iteration 548, loss = 0.43207083\n",
            "Iteration 549, loss = 0.43193406\n",
            "Iteration 550, loss = 0.43185347\n",
            "Iteration 551, loss = 0.43169069\n",
            "Iteration 552, loss = 0.43155802\n",
            "Iteration 553, loss = 0.43147693\n",
            "Iteration 554, loss = 0.43138083\n",
            "Iteration 555, loss = 0.43125443\n",
            "Iteration 556, loss = 0.43118379\n",
            "Iteration 557, loss = 0.43101199\n",
            "Iteration 558, loss = 0.43089919\n",
            "Iteration 559, loss = 0.43079035\n",
            "Iteration 560, loss = 0.43067969\n",
            "Iteration 561, loss = 0.43055800\n",
            "Iteration 562, loss = 0.43042766\n",
            "Iteration 563, loss = 0.43030184\n",
            "Iteration 564, loss = 0.43018620\n",
            "Iteration 565, loss = 0.43007447\n",
            "Iteration 566, loss = 0.42993048\n",
            "Iteration 567, loss = 0.42982281\n",
            "Iteration 568, loss = 0.42973070\n",
            "Iteration 569, loss = 0.42960337\n",
            "Iteration 570, loss = 0.42951277\n",
            "Iteration 571, loss = 0.42939349\n",
            "Iteration 572, loss = 0.42928521\n",
            "Iteration 573, loss = 0.42918293\n",
            "Iteration 574, loss = 0.42906680\n",
            "Iteration 575, loss = 0.42896763\n",
            "Iteration 576, loss = 0.42887247\n",
            "Iteration 577, loss = 0.42875797\n",
            "Iteration 578, loss = 0.42863636\n",
            "Iteration 579, loss = 0.42852759\n",
            "Iteration 580, loss = 0.42843441\n",
            "Iteration 581, loss = 0.42832259\n",
            "Iteration 582, loss = 0.42820612\n",
            "Iteration 583, loss = 0.42811479\n",
            "Iteration 584, loss = 0.42799756\n",
            "Iteration 585, loss = 0.42791949\n",
            "Iteration 586, loss = 0.42778690\n",
            "Iteration 587, loss = 0.42769865\n",
            "Iteration 588, loss = 0.42760045\n",
            "Iteration 589, loss = 0.42750701\n",
            "Iteration 590, loss = 0.42743075\n",
            "Iteration 591, loss = 0.42735077\n",
            "Iteration 592, loss = 0.42722763\n",
            "Iteration 593, loss = 0.42713028\n",
            "Iteration 594, loss = 0.42703212\n",
            "Iteration 595, loss = 0.42692019\n",
            "Iteration 596, loss = 0.42681797\n",
            "Iteration 597, loss = 0.42677602\n",
            "Iteration 598, loss = 0.42666566\n",
            "Iteration 599, loss = 0.42654365\n",
            "Iteration 600, loss = 0.42643134\n",
            "Iteration 601, loss = 0.42633436\n",
            "Iteration 602, loss = 0.42627795\n",
            "Iteration 603, loss = 0.42615563\n",
            "Iteration 604, loss = 0.42604697\n",
            "Iteration 605, loss = 0.42596650\n",
            "Iteration 606, loss = 0.42586904\n",
            "Iteration 607, loss = 0.42577262\n",
            "Iteration 608, loss = 0.42568945\n",
            "Iteration 609, loss = 0.42561512\n",
            "Iteration 610, loss = 0.42552525\n",
            "Iteration 611, loss = 0.42545197\n",
            "Iteration 612, loss = 0.42535951\n",
            "Iteration 613, loss = 0.42525912\n",
            "Iteration 614, loss = 0.42517681\n",
            "Iteration 615, loss = 0.42507912\n",
            "Iteration 616, loss = 0.42499082\n",
            "Iteration 617, loss = 0.42491975\n",
            "Iteration 618, loss = 0.42481508\n",
            "Iteration 619, loss = 0.42473183\n",
            "Iteration 620, loss = 0.42464003\n",
            "Iteration 621, loss = 0.42453921\n",
            "Iteration 622, loss = 0.42446697\n",
            "Iteration 623, loss = 0.42438742\n",
            "Iteration 624, loss = 0.42430904\n",
            "Iteration 625, loss = 0.42422278\n",
            "Iteration 626, loss = 0.42415754\n",
            "Iteration 627, loss = 0.42405790\n",
            "Iteration 628, loss = 0.42397513\n",
            "Iteration 629, loss = 0.42391184\n",
            "Iteration 630, loss = 0.42380718\n",
            "Iteration 631, loss = 0.42373291\n",
            "Iteration 632, loss = 0.42367052\n",
            "Iteration 633, loss = 0.42357984\n",
            "Iteration 634, loss = 0.42349838\n",
            "Iteration 635, loss = 0.42342356\n",
            "Iteration 636, loss = 0.42335410\n",
            "Iteration 637, loss = 0.42327123\n",
            "Iteration 638, loss = 0.42320237\n",
            "Iteration 639, loss = 0.42312824\n",
            "Iteration 640, loss = 0.42304576\n",
            "Iteration 641, loss = 0.42297399\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69482954\n",
            "Iteration 2, loss = 0.69433428\n",
            "Iteration 3, loss = 0.69398311\n",
            "Iteration 4, loss = 0.69363138\n",
            "Iteration 5, loss = 0.69327792\n",
            "Iteration 6, loss = 0.69289743\n",
            "Iteration 7, loss = 0.69255484\n",
            "Iteration 8, loss = 0.69220412\n",
            "Iteration 9, loss = 0.69187069\n",
            "Iteration 10, loss = 0.69150950\n",
            "Iteration 11, loss = 0.69118467\n",
            "Iteration 12, loss = 0.69082296\n",
            "Iteration 13, loss = 0.69049080\n",
            "Iteration 14, loss = 0.69014417\n",
            "Iteration 15, loss = 0.68981565\n",
            "Iteration 16, loss = 0.68947951\n",
            "Iteration 17, loss = 0.68910376\n",
            "Iteration 18, loss = 0.68877364\n",
            "Iteration 19, loss = 0.68840925\n",
            "Iteration 20, loss = 0.68805976\n",
            "Iteration 21, loss = 0.68769639\n",
            "Iteration 22, loss = 0.68732074\n",
            "Iteration 23, loss = 0.68694400\n",
            "Iteration 24, loss = 0.68656859\n",
            "Iteration 25, loss = 0.68618066\n",
            "Iteration 26, loss = 0.68579407\n",
            "Iteration 27, loss = 0.68539053\n",
            "Iteration 28, loss = 0.68498210\n",
            "Iteration 29, loss = 0.68459122\n",
            "Iteration 30, loss = 0.68418197\n",
            "Iteration 31, loss = 0.68376534\n",
            "Iteration 32, loss = 0.68334013\n",
            "Iteration 33, loss = 0.68289149\n",
            "Iteration 34, loss = 0.68245337\n",
            "Iteration 35, loss = 0.68199117\n",
            "Iteration 36, loss = 0.68153804\n",
            "Iteration 37, loss = 0.68104637\n",
            "Iteration 38, loss = 0.68055827\n",
            "Iteration 39, loss = 0.68006574\n",
            "Iteration 40, loss = 0.67955077\n",
            "Iteration 41, loss = 0.67901179\n",
            "Iteration 42, loss = 0.67850055\n",
            "Iteration 43, loss = 0.67793674\n",
            "Iteration 44, loss = 0.67739327\n",
            "Iteration 45, loss = 0.67683700\n",
            "Iteration 46, loss = 0.67625474\n",
            "Iteration 47, loss = 0.67566395\n",
            "Iteration 48, loss = 0.67506875\n",
            "Iteration 49, loss = 0.67443858\n",
            "Iteration 50, loss = 0.67381265\n",
            "Iteration 51, loss = 0.67317903\n",
            "Iteration 52, loss = 0.67255576\n",
            "Iteration 53, loss = 0.67189711\n",
            "Iteration 54, loss = 0.67121113\n",
            "Iteration 55, loss = 0.67052600\n",
            "Iteration 56, loss = 0.66982842\n",
            "Iteration 57, loss = 0.66910347\n",
            "Iteration 58, loss = 0.66838751\n",
            "Iteration 59, loss = 0.66764841\n",
            "Iteration 60, loss = 0.66688155\n",
            "Iteration 61, loss = 0.66617010\n",
            "Iteration 62, loss = 0.66536493\n",
            "Iteration 63, loss = 0.66459773\n",
            "Iteration 64, loss = 0.66379836\n",
            "Iteration 65, loss = 0.66299318\n",
            "Iteration 66, loss = 0.66221093\n",
            "Iteration 67, loss = 0.66136302\n",
            "Iteration 68, loss = 0.66050710\n",
            "Iteration 69, loss = 0.65968600\n",
            "Iteration 70, loss = 0.65881858\n",
            "Iteration 71, loss = 0.65798080\n",
            "Iteration 72, loss = 0.65708995\n",
            "Iteration 73, loss = 0.65620545\n",
            "Iteration 74, loss = 0.65530282\n",
            "Iteration 75, loss = 0.65440392\n",
            "Iteration 76, loss = 0.65349219\n",
            "Iteration 77, loss = 0.65259058\n",
            "Iteration 78, loss = 0.65169196\n",
            "Iteration 79, loss = 0.65073616\n",
            "Iteration 80, loss = 0.64980647\n",
            "Iteration 81, loss = 0.64887423\n",
            "Iteration 82, loss = 0.64789913\n",
            "Iteration 83, loss = 0.64697387\n",
            "Iteration 84, loss = 0.64604240\n",
            "Iteration 85, loss = 0.64508520\n",
            "Iteration 86, loss = 0.64411524\n",
            "Iteration 87, loss = 0.64316469\n",
            "Iteration 88, loss = 0.64221461\n",
            "Iteration 89, loss = 0.64124236\n",
            "Iteration 90, loss = 0.64028016\n",
            "Iteration 91, loss = 0.63927766\n",
            "Iteration 92, loss = 0.63833107\n",
            "Iteration 93, loss = 0.63731592\n",
            "Iteration 94, loss = 0.63632016\n",
            "Iteration 95, loss = 0.63535121\n",
            "Iteration 96, loss = 0.63434361\n",
            "Iteration 97, loss = 0.63335750\n",
            "Iteration 98, loss = 0.63236625\n",
            "Iteration 99, loss = 0.63135776\n",
            "Iteration 100, loss = 0.63037030\n",
            "Iteration 101, loss = 0.62932667\n",
            "Iteration 102, loss = 0.62837552\n",
            "Iteration 103, loss = 0.62732766\n",
            "Iteration 104, loss = 0.62629716\n",
            "Iteration 105, loss = 0.62531029\n",
            "Iteration 106, loss = 0.62431164\n",
            "Iteration 107, loss = 0.62334963\n",
            "Iteration 108, loss = 0.62232751\n",
            "Iteration 109, loss = 0.62133090\n",
            "Iteration 110, loss = 0.62036292\n",
            "Iteration 111, loss = 0.61939151\n",
            "Iteration 112, loss = 0.61840099\n",
            "Iteration 113, loss = 0.61742463\n",
            "Iteration 114, loss = 0.61649843\n",
            "Iteration 115, loss = 0.61545674\n",
            "Iteration 116, loss = 0.61448911\n",
            "Iteration 117, loss = 0.61354858\n",
            "Iteration 118, loss = 0.61259677\n",
            "Iteration 119, loss = 0.61162856\n",
            "Iteration 120, loss = 0.61068702\n",
            "Iteration 121, loss = 0.60971098\n",
            "Iteration 122, loss = 0.60879227\n",
            "Iteration 123, loss = 0.60786989\n",
            "Iteration 124, loss = 0.60689888\n",
            "Iteration 125, loss = 0.60592349\n",
            "Iteration 126, loss = 0.60503381\n",
            "Iteration 127, loss = 0.60410332\n",
            "Iteration 128, loss = 0.60314605\n",
            "Iteration 129, loss = 0.60227614\n",
            "Iteration 130, loss = 0.60133745\n",
            "Iteration 131, loss = 0.60041430\n",
            "Iteration 132, loss = 0.59952311\n",
            "Iteration 133, loss = 0.59860913\n",
            "Iteration 134, loss = 0.59770344\n",
            "Iteration 135, loss = 0.59684938\n",
            "Iteration 136, loss = 0.59597312\n",
            "Iteration 137, loss = 0.59509802\n",
            "Iteration 138, loss = 0.59423847\n",
            "Iteration 139, loss = 0.59336561\n",
            "Iteration 140, loss = 0.59246178\n",
            "Iteration 141, loss = 0.59164006\n",
            "Iteration 142, loss = 0.59076004\n",
            "Iteration 143, loss = 0.58989481\n",
            "Iteration 144, loss = 0.58905759\n",
            "Iteration 145, loss = 0.58815373\n",
            "Iteration 146, loss = 0.58732908\n",
            "Iteration 147, loss = 0.58648392\n",
            "Iteration 148, loss = 0.58565094\n",
            "Iteration 149, loss = 0.58481608\n",
            "Iteration 150, loss = 0.58399244\n",
            "Iteration 151, loss = 0.58321154\n",
            "Iteration 152, loss = 0.58238097\n",
            "Iteration 153, loss = 0.58155822\n",
            "Iteration 154, loss = 0.58069612\n",
            "Iteration 155, loss = 0.57993140\n",
            "Iteration 156, loss = 0.57910477\n",
            "Iteration 157, loss = 0.57829582\n",
            "Iteration 158, loss = 0.57751128\n",
            "Iteration 159, loss = 0.57673897\n",
            "Iteration 160, loss = 0.57594306\n",
            "Iteration 161, loss = 0.57513704\n",
            "Iteration 162, loss = 0.57433579\n",
            "Iteration 163, loss = 0.57354533\n",
            "Iteration 164, loss = 0.57277286\n",
            "Iteration 165, loss = 0.57202843\n",
            "Iteration 166, loss = 0.57121542\n",
            "Iteration 167, loss = 0.57048765\n",
            "Iteration 168, loss = 0.56969519\n",
            "Iteration 169, loss = 0.56895138\n",
            "Iteration 170, loss = 0.56818499\n",
            "Iteration 171, loss = 0.56742986\n",
            "Iteration 172, loss = 0.56666960\n",
            "Iteration 173, loss = 0.56594331\n",
            "Iteration 174, loss = 0.56521612\n",
            "Iteration 175, loss = 0.56446967\n",
            "Iteration 176, loss = 0.56373325\n",
            "Iteration 177, loss = 0.56307535\n",
            "Iteration 178, loss = 0.56231904\n",
            "Iteration 179, loss = 0.56160296\n",
            "Iteration 180, loss = 0.56089852\n",
            "Iteration 181, loss = 0.56020217\n",
            "Iteration 182, loss = 0.55948908\n",
            "Iteration 183, loss = 0.55881313\n",
            "Iteration 184, loss = 0.55812106\n",
            "Iteration 185, loss = 0.55746837\n",
            "Iteration 186, loss = 0.55673759\n",
            "Iteration 187, loss = 0.55602822\n",
            "Iteration 188, loss = 0.55535107\n",
            "Iteration 189, loss = 0.55467506\n",
            "Iteration 190, loss = 0.55396174\n",
            "Iteration 191, loss = 0.55328812\n",
            "Iteration 192, loss = 0.55260659\n",
            "Iteration 193, loss = 0.55187889\n",
            "Iteration 194, loss = 0.55122043\n",
            "Iteration 195, loss = 0.55055646\n",
            "Iteration 196, loss = 0.54987308\n",
            "Iteration 197, loss = 0.54920363\n",
            "Iteration 198, loss = 0.54855832\n",
            "Iteration 199, loss = 0.54791892\n",
            "Iteration 200, loss = 0.54724759\n",
            "Iteration 201, loss = 0.54656732\n",
            "Iteration 202, loss = 0.54591369\n",
            "Iteration 203, loss = 0.54531678\n",
            "Iteration 204, loss = 0.54467335\n",
            "Iteration 205, loss = 0.54405649\n",
            "Iteration 206, loss = 0.54342993\n",
            "Iteration 207, loss = 0.54283726\n",
            "Iteration 208, loss = 0.54219349\n",
            "Iteration 209, loss = 0.54157468\n",
            "Iteration 210, loss = 0.54094818\n",
            "Iteration 211, loss = 0.54037821\n",
            "Iteration 212, loss = 0.53973965\n",
            "Iteration 213, loss = 0.53913997\n",
            "Iteration 214, loss = 0.53852096\n",
            "Iteration 215, loss = 0.53791427\n",
            "Iteration 216, loss = 0.53729254\n",
            "Iteration 217, loss = 0.53668768\n",
            "Iteration 218, loss = 0.53609141\n",
            "Iteration 219, loss = 0.53546303\n",
            "Iteration 220, loss = 0.53487740\n",
            "Iteration 221, loss = 0.53430598\n",
            "Iteration 222, loss = 0.53371247\n",
            "Iteration 223, loss = 0.53311053\n",
            "Iteration 224, loss = 0.53253395\n",
            "Iteration 225, loss = 0.53193290\n",
            "Iteration 226, loss = 0.53137363\n",
            "Iteration 227, loss = 0.53075280\n",
            "Iteration 228, loss = 0.53017664\n",
            "Iteration 229, loss = 0.52963596\n",
            "Iteration 230, loss = 0.52903411\n",
            "Iteration 231, loss = 0.52848700\n",
            "Iteration 232, loss = 0.52791180\n",
            "Iteration 233, loss = 0.52736820\n",
            "Iteration 234, loss = 0.52682466\n",
            "Iteration 235, loss = 0.52625438\n",
            "Iteration 236, loss = 0.52569413\n",
            "Iteration 237, loss = 0.52516512\n",
            "Iteration 238, loss = 0.52463353\n",
            "Iteration 239, loss = 0.52408505\n",
            "Iteration 240, loss = 0.52351856\n",
            "Iteration 241, loss = 0.52296911\n",
            "Iteration 242, loss = 0.52242882\n",
            "Iteration 243, loss = 0.52189804\n",
            "Iteration 244, loss = 0.52135590\n",
            "Iteration 245, loss = 0.52079575\n",
            "Iteration 246, loss = 0.52024310\n",
            "Iteration 247, loss = 0.51972068\n",
            "Iteration 248, loss = 0.51922251\n",
            "Iteration 249, loss = 0.51865970\n",
            "Iteration 250, loss = 0.51814862\n",
            "Iteration 251, loss = 0.51760643\n",
            "Iteration 252, loss = 0.51709382\n",
            "Iteration 253, loss = 0.51658865\n",
            "Iteration 254, loss = 0.51602360\n",
            "Iteration 255, loss = 0.51551263\n",
            "Iteration 256, loss = 0.51500716\n",
            "Iteration 257, loss = 0.51447700\n",
            "Iteration 258, loss = 0.51397517\n",
            "Iteration 259, loss = 0.51345140\n",
            "Iteration 260, loss = 0.51293846\n",
            "Iteration 261, loss = 0.51244518\n",
            "Iteration 262, loss = 0.51191819\n",
            "Iteration 263, loss = 0.51143556\n",
            "Iteration 264, loss = 0.51094627\n",
            "Iteration 265, loss = 0.51041196\n",
            "Iteration 266, loss = 0.50991444\n",
            "Iteration 267, loss = 0.50939909\n",
            "Iteration 268, loss = 0.50889143\n",
            "Iteration 269, loss = 0.50843307\n",
            "Iteration 270, loss = 0.50790184\n",
            "Iteration 271, loss = 0.50741165\n",
            "Iteration 272, loss = 0.50692325\n",
            "Iteration 273, loss = 0.50642659\n",
            "Iteration 274, loss = 0.50594482\n",
            "Iteration 275, loss = 0.50550298\n",
            "Iteration 276, loss = 0.50498189\n",
            "Iteration 277, loss = 0.50450650\n",
            "Iteration 278, loss = 0.50403284\n",
            "Iteration 279, loss = 0.50355684\n",
            "Iteration 280, loss = 0.50309060\n",
            "Iteration 281, loss = 0.50259405\n",
            "Iteration 282, loss = 0.50213014\n",
            "Iteration 283, loss = 0.50169791\n",
            "Iteration 284, loss = 0.50119276\n",
            "Iteration 285, loss = 0.50072812\n",
            "Iteration 286, loss = 0.50027575\n",
            "Iteration 287, loss = 0.49977036\n",
            "Iteration 288, loss = 0.49929457\n",
            "Iteration 289, loss = 0.49881899\n",
            "Iteration 290, loss = 0.49838333\n",
            "Iteration 291, loss = 0.49789869\n",
            "Iteration 292, loss = 0.49741922\n",
            "Iteration 293, loss = 0.49698371\n",
            "Iteration 294, loss = 0.49650594\n",
            "Iteration 295, loss = 0.49603487\n",
            "Iteration 296, loss = 0.49556020\n",
            "Iteration 297, loss = 0.49513024\n",
            "Iteration 298, loss = 0.49465142\n",
            "Iteration 299, loss = 0.49422009\n",
            "Iteration 300, loss = 0.49377656\n",
            "Iteration 301, loss = 0.49333237\n",
            "Iteration 302, loss = 0.49288908\n",
            "Iteration 303, loss = 0.49244496\n",
            "Iteration 304, loss = 0.49198250\n",
            "Iteration 305, loss = 0.49157425\n",
            "Iteration 306, loss = 0.49112941\n",
            "Iteration 307, loss = 0.49068625\n",
            "Iteration 308, loss = 0.49026336\n",
            "Iteration 309, loss = 0.48983359\n",
            "Iteration 310, loss = 0.48942504\n",
            "Iteration 311, loss = 0.48900182\n",
            "Iteration 312, loss = 0.48859225\n",
            "Iteration 313, loss = 0.48817589\n",
            "Iteration 314, loss = 0.48776489\n",
            "Iteration 315, loss = 0.48735848\n",
            "Iteration 316, loss = 0.48692704\n",
            "Iteration 317, loss = 0.48651072\n",
            "Iteration 318, loss = 0.48612187\n",
            "Iteration 319, loss = 0.48567600\n",
            "Iteration 320, loss = 0.48527751\n",
            "Iteration 321, loss = 0.48484874\n",
            "Iteration 322, loss = 0.48443554\n",
            "Iteration 323, loss = 0.48402045\n",
            "Iteration 324, loss = 0.48361727\n",
            "Iteration 325, loss = 0.48319181\n",
            "Iteration 326, loss = 0.48278466\n",
            "Iteration 327, loss = 0.48234184\n",
            "Iteration 328, loss = 0.48193953\n",
            "Iteration 329, loss = 0.48151851\n",
            "Iteration 330, loss = 0.48111502\n",
            "Iteration 331, loss = 0.48070767\n",
            "Iteration 332, loss = 0.48028265\n",
            "Iteration 333, loss = 0.47990798\n",
            "Iteration 334, loss = 0.47947749\n",
            "Iteration 335, loss = 0.47909219\n",
            "Iteration 336, loss = 0.47868602\n",
            "Iteration 337, loss = 0.47829145\n",
            "Iteration 338, loss = 0.47789474\n",
            "Iteration 339, loss = 0.47749820\n",
            "Iteration 340, loss = 0.47711032\n",
            "Iteration 341, loss = 0.47673666\n",
            "Iteration 342, loss = 0.47632771\n",
            "Iteration 343, loss = 0.47595733\n",
            "Iteration 344, loss = 0.47556365\n",
            "Iteration 345, loss = 0.47516425\n",
            "Iteration 346, loss = 0.47477651\n",
            "Iteration 347, loss = 0.47443362\n",
            "Iteration 348, loss = 0.47403558\n",
            "Iteration 349, loss = 0.47363107\n",
            "Iteration 350, loss = 0.47327075\n",
            "Iteration 351, loss = 0.47291363\n",
            "Iteration 352, loss = 0.47254581\n",
            "Iteration 353, loss = 0.47219455\n",
            "Iteration 354, loss = 0.47182376\n",
            "Iteration 355, loss = 0.47147308\n",
            "Iteration 356, loss = 0.47109771\n",
            "Iteration 357, loss = 0.47072767\n",
            "Iteration 358, loss = 0.47038129\n",
            "Iteration 359, loss = 0.46999974\n",
            "Iteration 360, loss = 0.46963276\n",
            "Iteration 361, loss = 0.46928786\n",
            "Iteration 362, loss = 0.46895424\n",
            "Iteration 363, loss = 0.46860910\n",
            "Iteration 364, loss = 0.46820847\n",
            "Iteration 365, loss = 0.46786985\n",
            "Iteration 366, loss = 0.46754062\n",
            "Iteration 367, loss = 0.46722407\n",
            "Iteration 368, loss = 0.46687415\n",
            "Iteration 369, loss = 0.46650937\n",
            "Iteration 370, loss = 0.46615344\n",
            "Iteration 371, loss = 0.46581374\n",
            "Iteration 372, loss = 0.46547266\n",
            "Iteration 373, loss = 0.46512696\n",
            "Iteration 374, loss = 0.46479055\n",
            "Iteration 375, loss = 0.46446878\n",
            "Iteration 376, loss = 0.46410462\n",
            "Iteration 377, loss = 0.46381580\n",
            "Iteration 378, loss = 0.46343713\n",
            "Iteration 379, loss = 0.46315349\n",
            "Iteration 380, loss = 0.46281532\n",
            "Iteration 381, loss = 0.46247536\n",
            "Iteration 382, loss = 0.46217106\n",
            "Iteration 383, loss = 0.46186224\n",
            "Iteration 384, loss = 0.46151094\n",
            "Iteration 385, loss = 0.46120252\n",
            "Iteration 386, loss = 0.46088526\n",
            "Iteration 387, loss = 0.46059607\n",
            "Iteration 388, loss = 0.46028726\n",
            "Iteration 389, loss = 0.45998037\n",
            "Iteration 390, loss = 0.45968422\n",
            "Iteration 391, loss = 0.45939676\n",
            "Iteration 392, loss = 0.45908501\n",
            "Iteration 393, loss = 0.45878657\n",
            "Iteration 394, loss = 0.45847841\n",
            "Iteration 395, loss = 0.45818863\n",
            "Iteration 396, loss = 0.45790544\n",
            "Iteration 397, loss = 0.45758075\n",
            "Iteration 398, loss = 0.45728860\n",
            "Iteration 399, loss = 0.45699089\n",
            "Iteration 400, loss = 0.45667255\n",
            "Iteration 401, loss = 0.45640257\n",
            "Iteration 402, loss = 0.45609634\n",
            "Iteration 403, loss = 0.45579223\n",
            "Iteration 404, loss = 0.45550565\n",
            "Iteration 405, loss = 0.45520956\n",
            "Iteration 406, loss = 0.45496053\n",
            "Iteration 407, loss = 0.45463593\n",
            "Iteration 408, loss = 0.45431410\n",
            "Iteration 409, loss = 0.45404367\n",
            "Iteration 410, loss = 0.45376851\n",
            "Iteration 411, loss = 0.45348920\n",
            "Iteration 412, loss = 0.45322775\n",
            "Iteration 413, loss = 0.45292677\n",
            "Iteration 414, loss = 0.45268161\n",
            "Iteration 415, loss = 0.45238911\n",
            "Iteration 416, loss = 0.45209207\n",
            "Iteration 417, loss = 0.45187182\n",
            "Iteration 418, loss = 0.45157844\n",
            "Iteration 419, loss = 0.45132229\n",
            "Iteration 420, loss = 0.45111677\n",
            "Iteration 421, loss = 0.45075526\n",
            "Iteration 422, loss = 0.45050762\n",
            "Iteration 423, loss = 0.45023028\n",
            "Iteration 424, loss = 0.44995877\n",
            "Iteration 425, loss = 0.44971209\n",
            "Iteration 426, loss = 0.44943260\n",
            "Iteration 427, loss = 0.44920060\n",
            "Iteration 428, loss = 0.44893361\n",
            "Iteration 429, loss = 0.44866969\n",
            "Iteration 430, loss = 0.44842353\n",
            "Iteration 431, loss = 0.44818000\n",
            "Iteration 432, loss = 0.44794830\n",
            "Iteration 433, loss = 0.44769743\n",
            "Iteration 434, loss = 0.44744616\n",
            "Iteration 435, loss = 0.44721051\n",
            "Iteration 436, loss = 0.44696504\n",
            "Iteration 437, loss = 0.44670632\n",
            "Iteration 438, loss = 0.44648442\n",
            "Iteration 439, loss = 0.44624114\n",
            "Iteration 440, loss = 0.44601110\n",
            "Iteration 441, loss = 0.44573766\n",
            "Iteration 442, loss = 0.44551695\n",
            "Iteration 443, loss = 0.44531202\n",
            "Iteration 444, loss = 0.44507392\n",
            "Iteration 445, loss = 0.44481202\n",
            "Iteration 446, loss = 0.44458017\n",
            "Iteration 447, loss = 0.44436999\n",
            "Iteration 448, loss = 0.44413332\n",
            "Iteration 449, loss = 0.44392320\n",
            "Iteration 450, loss = 0.44372869\n",
            "Iteration 451, loss = 0.44351094\n",
            "Iteration 452, loss = 0.44324987\n",
            "Iteration 453, loss = 0.44308558\n",
            "Iteration 454, loss = 0.44285209\n",
            "Iteration 455, loss = 0.44257892\n",
            "Iteration 456, loss = 0.44237040\n",
            "Iteration 457, loss = 0.44215924\n",
            "Iteration 458, loss = 0.44193698\n",
            "Iteration 459, loss = 0.44175205\n",
            "Iteration 460, loss = 0.44151819\n",
            "Iteration 461, loss = 0.44131309\n",
            "Iteration 462, loss = 0.44110326\n",
            "Iteration 463, loss = 0.44090087\n",
            "Iteration 464, loss = 0.44072821\n",
            "Iteration 465, loss = 0.44051056\n",
            "Iteration 466, loss = 0.44032283\n",
            "Iteration 467, loss = 0.44012862\n",
            "Iteration 468, loss = 0.43994203\n",
            "Iteration 469, loss = 0.43973510\n",
            "Iteration 470, loss = 0.43954571\n",
            "Iteration 471, loss = 0.43935308\n",
            "Iteration 472, loss = 0.43918359\n",
            "Iteration 473, loss = 0.43898297\n",
            "Iteration 474, loss = 0.43877918\n",
            "Iteration 475, loss = 0.43862247\n",
            "Iteration 476, loss = 0.43842322\n",
            "Iteration 477, loss = 0.43820505\n",
            "Iteration 478, loss = 0.43807199\n",
            "Iteration 479, loss = 0.43784871\n",
            "Iteration 480, loss = 0.43767256\n",
            "Iteration 481, loss = 0.43749455\n",
            "Iteration 482, loss = 0.43730206\n",
            "Iteration 483, loss = 0.43712994\n",
            "Iteration 484, loss = 0.43695375\n",
            "Iteration 485, loss = 0.43677077\n",
            "Iteration 486, loss = 0.43659114\n",
            "Iteration 487, loss = 0.43643137\n",
            "Iteration 488, loss = 0.43625603\n",
            "Iteration 489, loss = 0.43607851\n",
            "Iteration 490, loss = 0.43590231\n",
            "Iteration 491, loss = 0.43570177\n",
            "Iteration 492, loss = 0.43552096\n",
            "Iteration 493, loss = 0.43537768\n",
            "Iteration 494, loss = 0.43521336\n",
            "Iteration 495, loss = 0.43502563\n",
            "Iteration 496, loss = 0.43487686\n",
            "Iteration 497, loss = 0.43470759\n",
            "Iteration 498, loss = 0.43459401\n",
            "Iteration 499, loss = 0.43438380\n",
            "Iteration 500, loss = 0.43419891\n",
            "Iteration 501, loss = 0.43402716\n",
            "Iteration 502, loss = 0.43384076\n",
            "Iteration 503, loss = 0.43368822\n",
            "Iteration 504, loss = 0.43349939\n",
            "Iteration 505, loss = 0.43333490\n",
            "Iteration 506, loss = 0.43316818\n",
            "Iteration 507, loss = 0.43301169\n",
            "Iteration 508, loss = 0.43287822\n",
            "Iteration 509, loss = 0.43269836\n",
            "Iteration 510, loss = 0.43253671\n",
            "Iteration 511, loss = 0.43239399\n",
            "Iteration 512, loss = 0.43223184\n",
            "Iteration 513, loss = 0.43208234\n",
            "Iteration 514, loss = 0.43194480\n",
            "Iteration 515, loss = 0.43178658\n",
            "Iteration 516, loss = 0.43162277\n",
            "Iteration 517, loss = 0.43147258\n",
            "Iteration 518, loss = 0.43136022\n",
            "Iteration 519, loss = 0.43121959\n",
            "Iteration 520, loss = 0.43108210\n",
            "Iteration 521, loss = 0.43095165\n",
            "Iteration 522, loss = 0.43079729\n",
            "Iteration 523, loss = 0.43066315\n",
            "Iteration 524, loss = 0.43053365\n",
            "Iteration 525, loss = 0.43036773\n",
            "Iteration 526, loss = 0.43027784\n",
            "Iteration 527, loss = 0.43008024\n",
            "Iteration 528, loss = 0.43002158\n",
            "Iteration 529, loss = 0.42982394\n",
            "Iteration 530, loss = 0.42967807\n",
            "Iteration 531, loss = 0.42953302\n",
            "Iteration 532, loss = 0.42944222\n",
            "Iteration 533, loss = 0.42927423\n",
            "Iteration 534, loss = 0.42912886\n",
            "Iteration 535, loss = 0.42902124\n",
            "Iteration 536, loss = 0.42887255\n",
            "Iteration 537, loss = 0.42873538\n",
            "Iteration 538, loss = 0.42862226\n",
            "Iteration 539, loss = 0.42850995\n",
            "Iteration 540, loss = 0.42836646\n",
            "Iteration 541, loss = 0.42821896\n",
            "Iteration 542, loss = 0.42808225\n",
            "Iteration 543, loss = 0.42799421\n",
            "Iteration 544, loss = 0.42786151\n",
            "Iteration 545, loss = 0.42774682\n",
            "Iteration 546, loss = 0.42763562\n",
            "Iteration 547, loss = 0.42750667\n",
            "Iteration 548, loss = 0.42738758\n",
            "Iteration 549, loss = 0.42725828\n",
            "Iteration 550, loss = 0.42713745\n",
            "Iteration 551, loss = 0.42702647\n",
            "Iteration 552, loss = 0.42689651\n",
            "Iteration 553, loss = 0.42679182\n",
            "Iteration 554, loss = 0.42671463\n",
            "Iteration 555, loss = 0.42658951\n",
            "Iteration 556, loss = 0.42647283\n",
            "Iteration 557, loss = 0.42638128\n",
            "Iteration 558, loss = 0.42626816\n",
            "Iteration 559, loss = 0.42616355\n",
            "Iteration 560, loss = 0.42604601\n",
            "Iteration 561, loss = 0.42594255\n",
            "Iteration 562, loss = 0.42582581\n",
            "Iteration 563, loss = 0.42571760\n",
            "Iteration 564, loss = 0.42557600\n",
            "Iteration 565, loss = 0.42546756\n",
            "Iteration 566, loss = 0.42534889\n",
            "Iteration 567, loss = 0.42523835\n",
            "Iteration 568, loss = 0.42513738\n",
            "Iteration 569, loss = 0.42501833\n",
            "Iteration 570, loss = 0.42491353\n",
            "Iteration 571, loss = 0.42481690\n",
            "Iteration 572, loss = 0.42470794\n",
            "Iteration 573, loss = 0.42459122\n",
            "Iteration 574, loss = 0.42449714\n",
            "Iteration 575, loss = 0.42438404\n",
            "Iteration 576, loss = 0.42429532\n",
            "Iteration 577, loss = 0.42418095\n",
            "Iteration 578, loss = 0.42407137\n",
            "Iteration 579, loss = 0.42396036\n",
            "Iteration 580, loss = 0.42385844\n",
            "Iteration 581, loss = 0.42375708\n",
            "Iteration 582, loss = 0.42365976\n",
            "Iteration 583, loss = 0.42356366\n",
            "Iteration 584, loss = 0.42343997\n",
            "Iteration 585, loss = 0.42335268\n",
            "Iteration 586, loss = 0.42323244\n",
            "Iteration 587, loss = 0.42313340\n",
            "Iteration 588, loss = 0.42304342\n",
            "Iteration 589, loss = 0.42294359\n",
            "Iteration 590, loss = 0.42284073\n",
            "Iteration 591, loss = 0.42276312\n",
            "Iteration 592, loss = 0.42265348\n",
            "Iteration 593, loss = 0.42255950\n",
            "Iteration 594, loss = 0.42246539\n",
            "Iteration 595, loss = 0.42237670\n",
            "Iteration 596, loss = 0.42226579\n",
            "Iteration 597, loss = 0.42219427\n",
            "Iteration 598, loss = 0.42207142\n",
            "Iteration 599, loss = 0.42197904\n",
            "Iteration 600, loss = 0.42192709\n",
            "Iteration 601, loss = 0.42180814\n",
            "Iteration 602, loss = 0.42170983\n",
            "Iteration 603, loss = 0.42163720\n",
            "Iteration 604, loss = 0.42152062\n",
            "Iteration 605, loss = 0.42147121\n",
            "Iteration 606, loss = 0.42134305\n",
            "Iteration 607, loss = 0.42125585\n",
            "Iteration 608, loss = 0.42115325\n",
            "Iteration 609, loss = 0.42108343\n",
            "Iteration 610, loss = 0.42100209\n",
            "Iteration 611, loss = 0.42092770\n",
            "Iteration 612, loss = 0.42086041\n",
            "Iteration 613, loss = 0.42074013\n",
            "Iteration 614, loss = 0.42065164\n",
            "Iteration 615, loss = 0.42058229\n",
            "Iteration 616, loss = 0.42048061\n",
            "Iteration 617, loss = 0.42040950\n",
            "Iteration 618, loss = 0.42030173\n",
            "Iteration 619, loss = 0.42023448\n",
            "Iteration 620, loss = 0.42015236\n",
            "Iteration 621, loss = 0.42004824\n",
            "Iteration 622, loss = 0.41997691\n",
            "Iteration 623, loss = 0.41989005\n",
            "Iteration 624, loss = 0.41980325\n",
            "Iteration 625, loss = 0.41972312\n",
            "Iteration 626, loss = 0.41968455\n",
            "Iteration 627, loss = 0.41955131\n",
            "Iteration 628, loss = 0.41948919\n",
            "Iteration 629, loss = 0.41943949\n",
            "Iteration 630, loss = 0.41933051\n",
            "Iteration 631, loss = 0.41925310\n",
            "Iteration 632, loss = 0.41921500\n",
            "Iteration 633, loss = 0.41911774\n",
            "Iteration 634, loss = 0.41903778\n",
            "Iteration 635, loss = 0.41896766\n",
            "Iteration 636, loss = 0.41888638\n",
            "Iteration 637, loss = 0.41882001\n",
            "Iteration 638, loss = 0.41876743\n",
            "Iteration 639, loss = 0.41869994\n",
            "Iteration 640, loss = 0.41862516\n",
            "Iteration 641, loss = 0.41852785\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69463349\n",
            "Iteration 2, loss = 0.69420115\n",
            "Iteration 3, loss = 0.69384235\n",
            "Iteration 4, loss = 0.69349440\n",
            "Iteration 5, loss = 0.69315017\n",
            "Iteration 6, loss = 0.69277350\n",
            "Iteration 7, loss = 0.69245117\n",
            "Iteration 8, loss = 0.69208836\n",
            "Iteration 9, loss = 0.69176151\n",
            "Iteration 10, loss = 0.69140902\n",
            "Iteration 11, loss = 0.69108148\n",
            "Iteration 12, loss = 0.69072973\n",
            "Iteration 13, loss = 0.69040573\n",
            "Iteration 14, loss = 0.69006267\n",
            "Iteration 15, loss = 0.68972875\n",
            "Iteration 16, loss = 0.68940663\n",
            "Iteration 17, loss = 0.68901678\n",
            "Iteration 18, loss = 0.68866906\n",
            "Iteration 19, loss = 0.68832999\n",
            "Iteration 20, loss = 0.68795538\n",
            "Iteration 21, loss = 0.68758525\n",
            "Iteration 22, loss = 0.68722159\n",
            "Iteration 23, loss = 0.68683697\n",
            "Iteration 24, loss = 0.68645200\n",
            "Iteration 25, loss = 0.68607788\n",
            "Iteration 26, loss = 0.68569007\n",
            "Iteration 27, loss = 0.68528977\n",
            "Iteration 28, loss = 0.68487346\n",
            "Iteration 29, loss = 0.68447496\n",
            "Iteration 30, loss = 0.68405483\n",
            "Iteration 31, loss = 0.68364795\n",
            "Iteration 32, loss = 0.68319937\n",
            "Iteration 33, loss = 0.68276026\n",
            "Iteration 34, loss = 0.68231375\n",
            "Iteration 35, loss = 0.68184788\n",
            "Iteration 36, loss = 0.68138519\n",
            "Iteration 37, loss = 0.68090183\n",
            "Iteration 38, loss = 0.68042543\n",
            "Iteration 39, loss = 0.67991731\n",
            "Iteration 40, loss = 0.67940822\n",
            "Iteration 41, loss = 0.67886893\n",
            "Iteration 42, loss = 0.67834066\n",
            "Iteration 43, loss = 0.67778847\n",
            "Iteration 44, loss = 0.67723091\n",
            "Iteration 45, loss = 0.67668873\n",
            "Iteration 46, loss = 0.67609751\n",
            "Iteration 47, loss = 0.67550751\n",
            "Iteration 48, loss = 0.67491065\n",
            "Iteration 49, loss = 0.67428194\n",
            "Iteration 50, loss = 0.67366764\n",
            "Iteration 51, loss = 0.67303093\n",
            "Iteration 52, loss = 0.67240469\n",
            "Iteration 53, loss = 0.67174429\n",
            "Iteration 54, loss = 0.67105357\n",
            "Iteration 55, loss = 0.67037728\n",
            "Iteration 56, loss = 0.66967179\n",
            "Iteration 57, loss = 0.66895427\n",
            "Iteration 58, loss = 0.66819759\n",
            "Iteration 59, loss = 0.66749676\n",
            "Iteration 60, loss = 0.66672913\n",
            "Iteration 61, loss = 0.66601591\n",
            "Iteration 62, loss = 0.66523203\n",
            "Iteration 63, loss = 0.66445449\n",
            "Iteration 64, loss = 0.66368457\n",
            "Iteration 65, loss = 0.66289506\n",
            "Iteration 66, loss = 0.66211934\n",
            "Iteration 67, loss = 0.66130385\n",
            "Iteration 68, loss = 0.66045533\n",
            "Iteration 69, loss = 0.65966165\n",
            "Iteration 70, loss = 0.65880859\n",
            "Iteration 71, loss = 0.65800850\n",
            "Iteration 72, loss = 0.65708338\n",
            "Iteration 73, loss = 0.65623210\n",
            "Iteration 74, loss = 0.65533867\n",
            "Iteration 75, loss = 0.65445292\n",
            "Iteration 76, loss = 0.65357035\n",
            "Iteration 77, loss = 0.65265821\n",
            "Iteration 78, loss = 0.65179284\n",
            "Iteration 79, loss = 0.65084774\n",
            "Iteration 80, loss = 0.64993726\n",
            "Iteration 81, loss = 0.64901816\n",
            "Iteration 82, loss = 0.64808254\n",
            "Iteration 83, loss = 0.64716163\n",
            "Iteration 84, loss = 0.64624014\n",
            "Iteration 85, loss = 0.64530760\n",
            "Iteration 86, loss = 0.64435281\n",
            "Iteration 87, loss = 0.64343422\n",
            "Iteration 88, loss = 0.64246220\n",
            "Iteration 89, loss = 0.64150825\n",
            "Iteration 90, loss = 0.64055578\n",
            "Iteration 91, loss = 0.63960143\n",
            "Iteration 92, loss = 0.63865089\n",
            "Iteration 93, loss = 0.63765879\n",
            "Iteration 94, loss = 0.63668255\n",
            "Iteration 95, loss = 0.63574100\n",
            "Iteration 96, loss = 0.63474177\n",
            "Iteration 97, loss = 0.63375377\n",
            "Iteration 98, loss = 0.63277487\n",
            "Iteration 99, loss = 0.63179358\n",
            "Iteration 100, loss = 0.63082798\n",
            "Iteration 101, loss = 0.62979536\n",
            "Iteration 102, loss = 0.62881762\n",
            "Iteration 103, loss = 0.62783310\n",
            "Iteration 104, loss = 0.62680294\n",
            "Iteration 105, loss = 0.62581650\n",
            "Iteration 106, loss = 0.62483884\n",
            "Iteration 107, loss = 0.62388471\n",
            "Iteration 108, loss = 0.62286217\n",
            "Iteration 109, loss = 0.62188060\n",
            "Iteration 110, loss = 0.62091991\n",
            "Iteration 111, loss = 0.61993226\n",
            "Iteration 112, loss = 0.61896562\n",
            "Iteration 113, loss = 0.61799209\n",
            "Iteration 114, loss = 0.61709294\n",
            "Iteration 115, loss = 0.61606738\n",
            "Iteration 116, loss = 0.61514429\n",
            "Iteration 117, loss = 0.61421170\n",
            "Iteration 118, loss = 0.61327073\n",
            "Iteration 119, loss = 0.61234120\n",
            "Iteration 120, loss = 0.61141511\n",
            "Iteration 121, loss = 0.61044112\n",
            "Iteration 122, loss = 0.60955193\n",
            "Iteration 123, loss = 0.60862256\n",
            "Iteration 124, loss = 0.60768655\n",
            "Iteration 125, loss = 0.60672350\n",
            "Iteration 126, loss = 0.60585851\n",
            "Iteration 127, loss = 0.60490085\n",
            "Iteration 128, loss = 0.60396673\n",
            "Iteration 129, loss = 0.60310301\n",
            "Iteration 130, loss = 0.60216528\n",
            "Iteration 131, loss = 0.60125184\n",
            "Iteration 132, loss = 0.60036828\n",
            "Iteration 133, loss = 0.59944915\n",
            "Iteration 134, loss = 0.59855960\n",
            "Iteration 135, loss = 0.59771144\n",
            "Iteration 136, loss = 0.59679976\n",
            "Iteration 137, loss = 0.59595031\n",
            "Iteration 138, loss = 0.59506727\n",
            "Iteration 139, loss = 0.59421237\n",
            "Iteration 140, loss = 0.59332503\n",
            "Iteration 141, loss = 0.59247749\n",
            "Iteration 142, loss = 0.59161262\n",
            "Iteration 143, loss = 0.59074047\n",
            "Iteration 144, loss = 0.58990871\n",
            "Iteration 145, loss = 0.58901071\n",
            "Iteration 146, loss = 0.58821714\n",
            "Iteration 147, loss = 0.58734645\n",
            "Iteration 148, loss = 0.58649740\n",
            "Iteration 149, loss = 0.58566982\n",
            "Iteration 150, loss = 0.58485426\n",
            "Iteration 151, loss = 0.58406009\n",
            "Iteration 152, loss = 0.58321955\n",
            "Iteration 153, loss = 0.58236880\n",
            "Iteration 154, loss = 0.58155277\n",
            "Iteration 155, loss = 0.58075705\n",
            "Iteration 156, loss = 0.57996410\n",
            "Iteration 157, loss = 0.57918104\n",
            "Iteration 158, loss = 0.57839587\n",
            "Iteration 159, loss = 0.57760006\n",
            "Iteration 160, loss = 0.57684973\n",
            "Iteration 161, loss = 0.57603981\n",
            "Iteration 162, loss = 0.57524717\n",
            "Iteration 163, loss = 0.57445388\n",
            "Iteration 164, loss = 0.57370092\n",
            "Iteration 165, loss = 0.57294345\n",
            "Iteration 166, loss = 0.57214488\n",
            "Iteration 167, loss = 0.57141203\n",
            "Iteration 168, loss = 0.57062625\n",
            "Iteration 169, loss = 0.56986609\n",
            "Iteration 170, loss = 0.56913080\n",
            "Iteration 171, loss = 0.56837476\n",
            "Iteration 172, loss = 0.56763310\n",
            "Iteration 173, loss = 0.56690578\n",
            "Iteration 174, loss = 0.56618998\n",
            "Iteration 175, loss = 0.56546308\n",
            "Iteration 176, loss = 0.56471641\n",
            "Iteration 177, loss = 0.56405234\n",
            "Iteration 178, loss = 0.56329111\n",
            "Iteration 179, loss = 0.56258266\n",
            "Iteration 180, loss = 0.56188728\n",
            "Iteration 181, loss = 0.56117061\n",
            "Iteration 182, loss = 0.56048229\n",
            "Iteration 183, loss = 0.55978307\n",
            "Iteration 184, loss = 0.55911730\n",
            "Iteration 185, loss = 0.55847376\n",
            "Iteration 186, loss = 0.55771692\n",
            "Iteration 187, loss = 0.55701730\n",
            "Iteration 188, loss = 0.55636466\n",
            "Iteration 189, loss = 0.55567937\n",
            "Iteration 190, loss = 0.55499064\n",
            "Iteration 191, loss = 0.55431504\n",
            "Iteration 192, loss = 0.55366055\n",
            "Iteration 193, loss = 0.55295249\n",
            "Iteration 194, loss = 0.55231804\n",
            "Iteration 195, loss = 0.55167371\n",
            "Iteration 196, loss = 0.55099034\n",
            "Iteration 197, loss = 0.55032296\n",
            "Iteration 198, loss = 0.54969217\n",
            "Iteration 199, loss = 0.54904481\n",
            "Iteration 200, loss = 0.54839691\n",
            "Iteration 201, loss = 0.54772993\n",
            "Iteration 202, loss = 0.54710074\n",
            "Iteration 203, loss = 0.54650270\n",
            "Iteration 204, loss = 0.54583059\n",
            "Iteration 205, loss = 0.54522830\n",
            "Iteration 206, loss = 0.54457241\n",
            "Iteration 207, loss = 0.54398306\n",
            "Iteration 208, loss = 0.54332109\n",
            "Iteration 209, loss = 0.54272447\n",
            "Iteration 210, loss = 0.54210331\n",
            "Iteration 211, loss = 0.54149281\n",
            "Iteration 212, loss = 0.54089107\n",
            "Iteration 213, loss = 0.54027945\n",
            "Iteration 214, loss = 0.53969230\n",
            "Iteration 215, loss = 0.53909064\n",
            "Iteration 216, loss = 0.53847598\n",
            "Iteration 217, loss = 0.53787574\n",
            "Iteration 218, loss = 0.53727227\n",
            "Iteration 219, loss = 0.53666265\n",
            "Iteration 220, loss = 0.53607938\n",
            "Iteration 221, loss = 0.53551365\n",
            "Iteration 222, loss = 0.53491850\n",
            "Iteration 223, loss = 0.53433237\n",
            "Iteration 224, loss = 0.53376731\n",
            "Iteration 225, loss = 0.53316837\n",
            "Iteration 226, loss = 0.53262710\n",
            "Iteration 227, loss = 0.53203774\n",
            "Iteration 228, loss = 0.53146350\n",
            "Iteration 229, loss = 0.53089236\n",
            "Iteration 230, loss = 0.53030303\n",
            "Iteration 231, loss = 0.52976812\n",
            "Iteration 232, loss = 0.52918405\n",
            "Iteration 233, loss = 0.52866097\n",
            "Iteration 234, loss = 0.52808910\n",
            "Iteration 235, loss = 0.52752486\n",
            "Iteration 236, loss = 0.52697676\n",
            "Iteration 237, loss = 0.52642320\n",
            "Iteration 238, loss = 0.52589079\n",
            "Iteration 239, loss = 0.52535232\n",
            "Iteration 240, loss = 0.52478083\n",
            "Iteration 241, loss = 0.52425090\n",
            "Iteration 242, loss = 0.52372804\n",
            "Iteration 243, loss = 0.52319442\n",
            "Iteration 244, loss = 0.52267174\n",
            "Iteration 245, loss = 0.52212899\n",
            "Iteration 246, loss = 0.52159245\n",
            "Iteration 247, loss = 0.52107856\n",
            "Iteration 248, loss = 0.52057108\n",
            "Iteration 249, loss = 0.52002973\n",
            "Iteration 250, loss = 0.51950934\n",
            "Iteration 251, loss = 0.51899272\n",
            "Iteration 252, loss = 0.51850009\n",
            "Iteration 253, loss = 0.51797949\n",
            "Iteration 254, loss = 0.51743248\n",
            "Iteration 255, loss = 0.51694596\n",
            "Iteration 256, loss = 0.51643040\n",
            "Iteration 257, loss = 0.51591262\n",
            "Iteration 258, loss = 0.51540079\n",
            "Iteration 259, loss = 0.51490170\n",
            "Iteration 260, loss = 0.51438026\n",
            "Iteration 261, loss = 0.51390833\n",
            "Iteration 262, loss = 0.51337167\n",
            "Iteration 263, loss = 0.51288542\n",
            "Iteration 264, loss = 0.51239231\n",
            "Iteration 265, loss = 0.51188088\n",
            "Iteration 266, loss = 0.51139246\n",
            "Iteration 267, loss = 0.51090502\n",
            "Iteration 268, loss = 0.51040181\n",
            "Iteration 269, loss = 0.50993933\n",
            "Iteration 270, loss = 0.50946901\n",
            "Iteration 271, loss = 0.50897358\n",
            "Iteration 272, loss = 0.50847223\n",
            "Iteration 273, loss = 0.50800292\n",
            "Iteration 274, loss = 0.50750581\n",
            "Iteration 275, loss = 0.50707473\n",
            "Iteration 276, loss = 0.50655012\n",
            "Iteration 277, loss = 0.50607890\n",
            "Iteration 278, loss = 0.50559849\n",
            "Iteration 279, loss = 0.50511071\n",
            "Iteration 280, loss = 0.50465823\n",
            "Iteration 281, loss = 0.50416271\n",
            "Iteration 282, loss = 0.50370045\n",
            "Iteration 283, loss = 0.50326478\n",
            "Iteration 284, loss = 0.50277551\n",
            "Iteration 285, loss = 0.50231737\n",
            "Iteration 286, loss = 0.50188745\n",
            "Iteration 287, loss = 0.50141297\n",
            "Iteration 288, loss = 0.50092309\n",
            "Iteration 289, loss = 0.50044781\n",
            "Iteration 290, loss = 0.50006461\n",
            "Iteration 291, loss = 0.49957741\n",
            "Iteration 292, loss = 0.49909807\n",
            "Iteration 293, loss = 0.49866495\n",
            "Iteration 294, loss = 0.49820767\n",
            "Iteration 295, loss = 0.49775165\n",
            "Iteration 296, loss = 0.49729016\n",
            "Iteration 297, loss = 0.49687209\n",
            "Iteration 298, loss = 0.49639757\n",
            "Iteration 299, loss = 0.49598207\n",
            "Iteration 300, loss = 0.49552313\n",
            "Iteration 301, loss = 0.49510265\n",
            "Iteration 302, loss = 0.49465791\n",
            "Iteration 303, loss = 0.49421577\n",
            "Iteration 304, loss = 0.49377480\n",
            "Iteration 305, loss = 0.49334703\n",
            "Iteration 306, loss = 0.49289798\n",
            "Iteration 307, loss = 0.49251113\n",
            "Iteration 308, loss = 0.49207311\n",
            "Iteration 309, loss = 0.49164889\n",
            "Iteration 310, loss = 0.49122445\n",
            "Iteration 311, loss = 0.49080763\n",
            "Iteration 312, loss = 0.49040406\n",
            "Iteration 313, loss = 0.48999438\n",
            "Iteration 314, loss = 0.48956939\n",
            "Iteration 315, loss = 0.48918675\n",
            "Iteration 316, loss = 0.48876543\n",
            "Iteration 317, loss = 0.48834974\n",
            "Iteration 318, loss = 0.48795818\n",
            "Iteration 319, loss = 0.48752087\n",
            "Iteration 320, loss = 0.48712825\n",
            "Iteration 321, loss = 0.48672066\n",
            "Iteration 322, loss = 0.48630049\n",
            "Iteration 323, loss = 0.48591407\n",
            "Iteration 324, loss = 0.48550166\n",
            "Iteration 325, loss = 0.48509149\n",
            "Iteration 326, loss = 0.48465436\n",
            "Iteration 327, loss = 0.48424075\n",
            "Iteration 328, loss = 0.48381909\n",
            "Iteration 329, loss = 0.48339001\n",
            "Iteration 330, loss = 0.48298614\n",
            "Iteration 331, loss = 0.48257036\n",
            "Iteration 332, loss = 0.48215694\n",
            "Iteration 333, loss = 0.48178507\n",
            "Iteration 334, loss = 0.48134143\n",
            "Iteration 335, loss = 0.48097095\n",
            "Iteration 336, loss = 0.48057199\n",
            "Iteration 337, loss = 0.48016398\n",
            "Iteration 338, loss = 0.47977818\n",
            "Iteration 339, loss = 0.47936193\n",
            "Iteration 340, loss = 0.47901194\n",
            "Iteration 341, loss = 0.47861093\n",
            "Iteration 342, loss = 0.47820341\n",
            "Iteration 343, loss = 0.47782684\n",
            "Iteration 344, loss = 0.47743370\n",
            "Iteration 345, loss = 0.47708347\n",
            "Iteration 346, loss = 0.47667990\n",
            "Iteration 347, loss = 0.47631077\n",
            "Iteration 348, loss = 0.47595057\n",
            "Iteration 349, loss = 0.47554663\n",
            "Iteration 350, loss = 0.47518127\n",
            "Iteration 351, loss = 0.47483479\n",
            "Iteration 352, loss = 0.47446611\n",
            "Iteration 353, loss = 0.47409822\n",
            "Iteration 354, loss = 0.47372174\n",
            "Iteration 355, loss = 0.47339497\n",
            "Iteration 356, loss = 0.47301790\n",
            "Iteration 357, loss = 0.47265490\n",
            "Iteration 358, loss = 0.47232376\n",
            "Iteration 359, loss = 0.47195136\n",
            "Iteration 360, loss = 0.47160602\n",
            "Iteration 361, loss = 0.47127632\n",
            "Iteration 362, loss = 0.47093584\n",
            "Iteration 363, loss = 0.47059306\n",
            "Iteration 364, loss = 0.47021331\n",
            "Iteration 365, loss = 0.46989008\n",
            "Iteration 366, loss = 0.46957481\n",
            "Iteration 367, loss = 0.46923025\n",
            "Iteration 368, loss = 0.46893987\n",
            "Iteration 369, loss = 0.46857023\n",
            "Iteration 370, loss = 0.46821159\n",
            "Iteration 371, loss = 0.46787585\n",
            "Iteration 372, loss = 0.46752744\n",
            "Iteration 373, loss = 0.46720882\n",
            "Iteration 374, loss = 0.46685473\n",
            "Iteration 375, loss = 0.46652222\n",
            "Iteration 376, loss = 0.46619039\n",
            "Iteration 377, loss = 0.46584064\n",
            "Iteration 378, loss = 0.46549810\n",
            "Iteration 379, loss = 0.46519691\n",
            "Iteration 380, loss = 0.46486293\n",
            "Iteration 381, loss = 0.46453836\n",
            "Iteration 382, loss = 0.46420149\n",
            "Iteration 383, loss = 0.46388457\n",
            "Iteration 384, loss = 0.46354715\n",
            "Iteration 385, loss = 0.46325023\n",
            "Iteration 386, loss = 0.46291435\n",
            "Iteration 387, loss = 0.46262790\n",
            "Iteration 388, loss = 0.46232071\n",
            "Iteration 389, loss = 0.46199884\n",
            "Iteration 390, loss = 0.46171305\n",
            "Iteration 391, loss = 0.46141405\n",
            "Iteration 392, loss = 0.46109257\n",
            "Iteration 393, loss = 0.46080396\n",
            "Iteration 394, loss = 0.46049330\n",
            "Iteration 395, loss = 0.46019116\n",
            "Iteration 396, loss = 0.45990168\n",
            "Iteration 397, loss = 0.45961518\n",
            "Iteration 398, loss = 0.45932283\n",
            "Iteration 399, loss = 0.45902593\n",
            "Iteration 400, loss = 0.45871124\n",
            "Iteration 401, loss = 0.45842124\n",
            "Iteration 402, loss = 0.45812537\n",
            "Iteration 403, loss = 0.45781785\n",
            "Iteration 404, loss = 0.45752312\n",
            "Iteration 405, loss = 0.45724864\n",
            "Iteration 406, loss = 0.45696585\n",
            "Iteration 407, loss = 0.45666715\n",
            "Iteration 408, loss = 0.45635643\n",
            "Iteration 409, loss = 0.45607395\n",
            "Iteration 410, loss = 0.45581548\n",
            "Iteration 411, loss = 0.45550847\n",
            "Iteration 412, loss = 0.45525968\n",
            "Iteration 413, loss = 0.45496883\n",
            "Iteration 414, loss = 0.45471149\n",
            "Iteration 415, loss = 0.45443161\n",
            "Iteration 416, loss = 0.45412685\n",
            "Iteration 417, loss = 0.45394610\n",
            "Iteration 418, loss = 0.45363093\n",
            "Iteration 419, loss = 0.45337372\n",
            "Iteration 420, loss = 0.45314924\n",
            "Iteration 421, loss = 0.45281483\n",
            "Iteration 422, loss = 0.45255873\n",
            "Iteration 423, loss = 0.45227034\n",
            "Iteration 424, loss = 0.45199747\n",
            "Iteration 425, loss = 0.45174457\n",
            "Iteration 426, loss = 0.45148697\n",
            "Iteration 427, loss = 0.45123669\n",
            "Iteration 428, loss = 0.45097751\n",
            "Iteration 429, loss = 0.45070874\n",
            "Iteration 430, loss = 0.45046015\n",
            "Iteration 431, loss = 0.45020732\n",
            "Iteration 432, loss = 0.44999994\n",
            "Iteration 433, loss = 0.44973500\n",
            "Iteration 434, loss = 0.44948164\n",
            "Iteration 435, loss = 0.44923331\n",
            "Iteration 436, loss = 0.44901219\n",
            "Iteration 437, loss = 0.44875537\n",
            "Iteration 438, loss = 0.44851813\n",
            "Iteration 439, loss = 0.44827830\n",
            "Iteration 440, loss = 0.44803977\n",
            "Iteration 441, loss = 0.44778328\n",
            "Iteration 442, loss = 0.44755645\n",
            "Iteration 443, loss = 0.44733475\n",
            "Iteration 444, loss = 0.44711239\n",
            "Iteration 445, loss = 0.44686431\n",
            "Iteration 446, loss = 0.44663751\n",
            "Iteration 447, loss = 0.44643628\n",
            "Iteration 448, loss = 0.44618176\n",
            "Iteration 449, loss = 0.44597788\n",
            "Iteration 450, loss = 0.44576674\n",
            "Iteration 451, loss = 0.44551666\n",
            "Iteration 452, loss = 0.44531098\n",
            "Iteration 453, loss = 0.44519005\n",
            "Iteration 454, loss = 0.44493451\n",
            "Iteration 455, loss = 0.44468308\n",
            "Iteration 456, loss = 0.44445600\n",
            "Iteration 457, loss = 0.44420725\n",
            "Iteration 458, loss = 0.44400483\n",
            "Iteration 459, loss = 0.44379859\n",
            "Iteration 460, loss = 0.44356145\n",
            "Iteration 461, loss = 0.44335752\n",
            "Iteration 462, loss = 0.44316352\n",
            "Iteration 463, loss = 0.44294461\n",
            "Iteration 464, loss = 0.44277276\n",
            "Iteration 465, loss = 0.44255868\n",
            "Iteration 466, loss = 0.44234618\n",
            "Iteration 467, loss = 0.44215954\n",
            "Iteration 468, loss = 0.44195763\n",
            "Iteration 469, loss = 0.44176044\n",
            "Iteration 470, loss = 0.44154945\n",
            "Iteration 471, loss = 0.44134286\n",
            "Iteration 472, loss = 0.44117267\n",
            "Iteration 473, loss = 0.44098554\n",
            "Iteration 474, loss = 0.44074972\n",
            "Iteration 475, loss = 0.44062728\n",
            "Iteration 476, loss = 0.44039360\n",
            "Iteration 477, loss = 0.44017778\n",
            "Iteration 478, loss = 0.44002961\n",
            "Iteration 479, loss = 0.43981976\n",
            "Iteration 480, loss = 0.43963015\n",
            "Iteration 481, loss = 0.43944297\n",
            "Iteration 482, loss = 0.43925825\n",
            "Iteration 483, loss = 0.43906193\n",
            "Iteration 484, loss = 0.43889994\n",
            "Iteration 485, loss = 0.43871857\n",
            "Iteration 486, loss = 0.43853713\n",
            "Iteration 487, loss = 0.43834950\n",
            "Iteration 488, loss = 0.43816309\n",
            "Iteration 489, loss = 0.43801731\n",
            "Iteration 490, loss = 0.43785237\n",
            "Iteration 491, loss = 0.43763099\n",
            "Iteration 492, loss = 0.43744342\n",
            "Iteration 493, loss = 0.43726732\n",
            "Iteration 494, loss = 0.43711307\n",
            "Iteration 495, loss = 0.43691332\n",
            "Iteration 496, loss = 0.43678386\n",
            "Iteration 497, loss = 0.43657476\n",
            "Iteration 498, loss = 0.43642179\n",
            "Iteration 499, loss = 0.43622333\n",
            "Iteration 500, loss = 0.43605038\n",
            "Iteration 501, loss = 0.43587527\n",
            "Iteration 502, loss = 0.43569637\n",
            "Iteration 503, loss = 0.43552502\n",
            "Iteration 504, loss = 0.43534380\n",
            "Iteration 505, loss = 0.43517780\n",
            "Iteration 506, loss = 0.43502564\n",
            "Iteration 507, loss = 0.43485053\n",
            "Iteration 508, loss = 0.43470779\n",
            "Iteration 509, loss = 0.43453829\n",
            "Iteration 510, loss = 0.43436475\n",
            "Iteration 511, loss = 0.43421378\n",
            "Iteration 512, loss = 0.43404670\n",
            "Iteration 513, loss = 0.43389761\n",
            "Iteration 514, loss = 0.43376293\n",
            "Iteration 515, loss = 0.43358035\n",
            "Iteration 516, loss = 0.43343863\n",
            "Iteration 517, loss = 0.43329274\n",
            "Iteration 518, loss = 0.43316892\n",
            "Iteration 519, loss = 0.43300995\n",
            "Iteration 520, loss = 0.43287639\n",
            "Iteration 521, loss = 0.43273872\n",
            "Iteration 522, loss = 0.43258873\n",
            "Iteration 523, loss = 0.43243918\n",
            "Iteration 524, loss = 0.43229323\n",
            "Iteration 525, loss = 0.43216354\n",
            "Iteration 526, loss = 0.43201181\n",
            "Iteration 527, loss = 0.43184206\n",
            "Iteration 528, loss = 0.43181067\n",
            "Iteration 529, loss = 0.43161515\n",
            "Iteration 530, loss = 0.43146717\n",
            "Iteration 531, loss = 0.43131686\n",
            "Iteration 532, loss = 0.43123147\n",
            "Iteration 533, loss = 0.43104156\n",
            "Iteration 534, loss = 0.43090304\n",
            "Iteration 535, loss = 0.43077891\n",
            "Iteration 536, loss = 0.43065108\n",
            "Iteration 537, loss = 0.43050714\n",
            "Iteration 538, loss = 0.43037080\n",
            "Iteration 539, loss = 0.43025677\n",
            "Iteration 540, loss = 0.43013143\n",
            "Iteration 541, loss = 0.42997265\n",
            "Iteration 542, loss = 0.42984860\n",
            "Iteration 543, loss = 0.42978172\n",
            "Iteration 544, loss = 0.42965349\n",
            "Iteration 545, loss = 0.42951495\n",
            "Iteration 546, loss = 0.42941085\n",
            "Iteration 547, loss = 0.42926620\n",
            "Iteration 548, loss = 0.42913324\n",
            "Iteration 549, loss = 0.42899741\n",
            "Iteration 550, loss = 0.42887814\n",
            "Iteration 551, loss = 0.42876049\n",
            "Iteration 552, loss = 0.42862303\n",
            "Iteration 553, loss = 0.42853379\n",
            "Iteration 554, loss = 0.42844020\n",
            "Iteration 555, loss = 0.42831182\n",
            "Iteration 556, loss = 0.42821614\n",
            "Iteration 557, loss = 0.42807665\n",
            "Iteration 558, loss = 0.42796032\n",
            "Iteration 559, loss = 0.42784000\n",
            "Iteration 560, loss = 0.42774701\n",
            "Iteration 561, loss = 0.42760786\n",
            "Iteration 562, loss = 0.42750037\n",
            "Iteration 563, loss = 0.42740939\n",
            "Iteration 564, loss = 0.42722512\n",
            "Iteration 565, loss = 0.42712322\n",
            "Iteration 566, loss = 0.42700613\n",
            "Iteration 567, loss = 0.42688854\n",
            "Iteration 568, loss = 0.42677860\n",
            "Iteration 569, loss = 0.42665639\n",
            "Iteration 570, loss = 0.42653912\n",
            "Iteration 571, loss = 0.42643924\n",
            "Iteration 572, loss = 0.42635754\n",
            "Iteration 573, loss = 0.42622349\n",
            "Iteration 574, loss = 0.42611448\n",
            "Iteration 575, loss = 0.42601371\n",
            "Iteration 576, loss = 0.42589508\n",
            "Iteration 577, loss = 0.42582322\n",
            "Iteration 578, loss = 0.42569036\n",
            "Iteration 579, loss = 0.42556541\n",
            "Iteration 580, loss = 0.42546087\n",
            "Iteration 581, loss = 0.42536759\n",
            "Iteration 582, loss = 0.42524855\n",
            "Iteration 583, loss = 0.42513636\n",
            "Iteration 584, loss = 0.42501238\n",
            "Iteration 585, loss = 0.42492313\n",
            "Iteration 586, loss = 0.42481377\n",
            "Iteration 587, loss = 0.42470984\n",
            "Iteration 588, loss = 0.42461225\n",
            "Iteration 589, loss = 0.42449669\n",
            "Iteration 590, loss = 0.42439852\n",
            "Iteration 591, loss = 0.42430407\n",
            "Iteration 592, loss = 0.42421087\n",
            "Iteration 593, loss = 0.42413457\n",
            "Iteration 594, loss = 0.42402227\n",
            "Iteration 595, loss = 0.42393664\n",
            "Iteration 596, loss = 0.42382726\n",
            "Iteration 597, loss = 0.42372240\n",
            "Iteration 598, loss = 0.42363438\n",
            "Iteration 599, loss = 0.42351765\n",
            "Iteration 600, loss = 0.42341889\n",
            "Iteration 601, loss = 0.42333307\n",
            "Iteration 602, loss = 0.42324471\n",
            "Iteration 603, loss = 0.42317119\n",
            "Iteration 604, loss = 0.42304021\n",
            "Iteration 605, loss = 0.42299181\n",
            "Iteration 606, loss = 0.42286816\n",
            "Iteration 607, loss = 0.42280599\n",
            "Iteration 608, loss = 0.42265524\n",
            "Iteration 609, loss = 0.42258156\n",
            "Iteration 610, loss = 0.42250006\n",
            "Iteration 611, loss = 0.42242155\n",
            "Iteration 612, loss = 0.42231577\n",
            "Iteration 613, loss = 0.42222081\n",
            "Iteration 614, loss = 0.42213276\n",
            "Iteration 615, loss = 0.42204971\n",
            "Iteration 616, loss = 0.42196967\n",
            "Iteration 617, loss = 0.42191933\n",
            "Iteration 618, loss = 0.42180868\n",
            "Iteration 619, loss = 0.42172414\n",
            "Iteration 620, loss = 0.42161899\n",
            "Iteration 621, loss = 0.42152299\n",
            "Iteration 622, loss = 0.42142905\n",
            "Iteration 623, loss = 0.42133564\n",
            "Iteration 624, loss = 0.42124475\n",
            "Iteration 625, loss = 0.42116043\n",
            "Iteration 626, loss = 0.42111716\n",
            "Iteration 627, loss = 0.42096859\n",
            "Iteration 628, loss = 0.42089639\n",
            "Iteration 629, loss = 0.42082368\n",
            "Iteration 630, loss = 0.42073472\n",
            "Iteration 631, loss = 0.42064922\n",
            "Iteration 632, loss = 0.42057787\n",
            "Iteration 633, loss = 0.42050236\n",
            "Iteration 634, loss = 0.42040756\n",
            "Iteration 635, loss = 0.42032903\n",
            "Iteration 636, loss = 0.42025997\n",
            "Iteration 637, loss = 0.42017210\n",
            "Iteration 638, loss = 0.42014133\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69460286\n",
            "Iteration 2, loss = 0.69417046\n",
            "Iteration 3, loss = 0.69380062\n",
            "Iteration 4, loss = 0.69346539\n",
            "Iteration 5, loss = 0.69311080\n",
            "Iteration 6, loss = 0.69274491\n",
            "Iteration 7, loss = 0.69241642\n",
            "Iteration 8, loss = 0.69206563\n",
            "Iteration 9, loss = 0.69174727\n",
            "Iteration 10, loss = 0.69139108\n",
            "Iteration 11, loss = 0.69104824\n",
            "Iteration 12, loss = 0.69070188\n",
            "Iteration 13, loss = 0.69037090\n",
            "Iteration 14, loss = 0.69002582\n",
            "Iteration 15, loss = 0.68969257\n",
            "Iteration 16, loss = 0.68935154\n",
            "Iteration 17, loss = 0.68895887\n",
            "Iteration 18, loss = 0.68861855\n",
            "Iteration 19, loss = 0.68825923\n",
            "Iteration 20, loss = 0.68788872\n",
            "Iteration 21, loss = 0.68752430\n",
            "Iteration 22, loss = 0.68713388\n",
            "Iteration 23, loss = 0.68674225\n",
            "Iteration 24, loss = 0.68636576\n",
            "Iteration 25, loss = 0.68596595\n",
            "Iteration 26, loss = 0.68558030\n",
            "Iteration 27, loss = 0.68516977\n",
            "Iteration 28, loss = 0.68473692\n",
            "Iteration 29, loss = 0.68433153\n",
            "Iteration 30, loss = 0.68389337\n",
            "Iteration 31, loss = 0.68348304\n",
            "Iteration 32, loss = 0.68302292\n",
            "Iteration 33, loss = 0.68256402\n",
            "Iteration 34, loss = 0.68211021\n",
            "Iteration 35, loss = 0.68162273\n",
            "Iteration 36, loss = 0.68115766\n",
            "Iteration 37, loss = 0.68065063\n",
            "Iteration 38, loss = 0.68015781\n",
            "Iteration 39, loss = 0.67964681\n",
            "Iteration 40, loss = 0.67910720\n",
            "Iteration 41, loss = 0.67856351\n",
            "Iteration 42, loss = 0.67802462\n",
            "Iteration 43, loss = 0.67744616\n",
            "Iteration 44, loss = 0.67688461\n",
            "Iteration 45, loss = 0.67634439\n",
            "Iteration 46, loss = 0.67571719\n",
            "Iteration 47, loss = 0.67512342\n",
            "Iteration 48, loss = 0.67451958\n",
            "Iteration 49, loss = 0.67386799\n",
            "Iteration 50, loss = 0.67323227\n",
            "Iteration 51, loss = 0.67257995\n",
            "Iteration 52, loss = 0.67193157\n",
            "Iteration 53, loss = 0.67128020\n",
            "Iteration 54, loss = 0.67057034\n",
            "Iteration 55, loss = 0.66986245\n",
            "Iteration 56, loss = 0.66916424\n",
            "Iteration 57, loss = 0.66842540\n",
            "Iteration 58, loss = 0.66766609\n",
            "Iteration 59, loss = 0.66695896\n",
            "Iteration 60, loss = 0.66618864\n",
            "Iteration 61, loss = 0.66547190\n",
            "Iteration 62, loss = 0.66467942\n",
            "Iteration 63, loss = 0.66389049\n",
            "Iteration 64, loss = 0.66308145\n",
            "Iteration 65, loss = 0.66231167\n",
            "Iteration 66, loss = 0.66150491\n",
            "Iteration 67, loss = 0.66067409\n",
            "Iteration 68, loss = 0.65986237\n",
            "Iteration 69, loss = 0.65899872\n",
            "Iteration 70, loss = 0.65814434\n",
            "Iteration 71, loss = 0.65733763\n",
            "Iteration 72, loss = 0.65641884\n",
            "Iteration 73, loss = 0.65554531\n",
            "Iteration 74, loss = 0.65465491\n",
            "Iteration 75, loss = 0.65375908\n",
            "Iteration 76, loss = 0.65287412\n",
            "Iteration 77, loss = 0.65193865\n",
            "Iteration 78, loss = 0.65103759\n",
            "Iteration 79, loss = 0.65009324\n",
            "Iteration 80, loss = 0.64917737\n",
            "Iteration 81, loss = 0.64822382\n",
            "Iteration 82, loss = 0.64728269\n",
            "Iteration 83, loss = 0.64633857\n",
            "Iteration 84, loss = 0.64540643\n",
            "Iteration 85, loss = 0.64444876\n",
            "Iteration 86, loss = 0.64350346\n",
            "Iteration 87, loss = 0.64255560\n",
            "Iteration 88, loss = 0.64158495\n",
            "Iteration 89, loss = 0.64063570\n",
            "Iteration 90, loss = 0.63966300\n",
            "Iteration 91, loss = 0.63869117\n",
            "Iteration 92, loss = 0.63771448\n",
            "Iteration 93, loss = 0.63673134\n",
            "Iteration 94, loss = 0.63573405\n",
            "Iteration 95, loss = 0.63476043\n",
            "Iteration 96, loss = 0.63378381\n",
            "Iteration 97, loss = 0.63275027\n",
            "Iteration 98, loss = 0.63176779\n",
            "Iteration 99, loss = 0.63078540\n",
            "Iteration 100, loss = 0.62977960\n",
            "Iteration 101, loss = 0.62873162\n",
            "Iteration 102, loss = 0.62773634\n",
            "Iteration 103, loss = 0.62675570\n",
            "Iteration 104, loss = 0.62571608\n",
            "Iteration 105, loss = 0.62471746\n",
            "Iteration 106, loss = 0.62372866\n",
            "Iteration 107, loss = 0.62276690\n",
            "Iteration 108, loss = 0.62174573\n",
            "Iteration 109, loss = 0.62078594\n",
            "Iteration 110, loss = 0.61979873\n",
            "Iteration 111, loss = 0.61881579\n",
            "Iteration 112, loss = 0.61785218\n",
            "Iteration 113, loss = 0.61686713\n",
            "Iteration 114, loss = 0.61594649\n",
            "Iteration 115, loss = 0.61492899\n",
            "Iteration 116, loss = 0.61399672\n",
            "Iteration 117, loss = 0.61302581\n",
            "Iteration 118, loss = 0.61210141\n",
            "Iteration 119, loss = 0.61113347\n",
            "Iteration 120, loss = 0.61022314\n",
            "Iteration 121, loss = 0.60925657\n",
            "Iteration 122, loss = 0.60832821\n",
            "Iteration 123, loss = 0.60741398\n",
            "Iteration 124, loss = 0.60645256\n",
            "Iteration 125, loss = 0.60551703\n",
            "Iteration 126, loss = 0.60461528\n",
            "Iteration 127, loss = 0.60366655\n",
            "Iteration 128, loss = 0.60272673\n",
            "Iteration 129, loss = 0.60187752\n",
            "Iteration 130, loss = 0.60089510\n",
            "Iteration 131, loss = 0.60002289\n",
            "Iteration 132, loss = 0.59911391\n",
            "Iteration 133, loss = 0.59818830\n",
            "Iteration 134, loss = 0.59730472\n",
            "Iteration 135, loss = 0.59644947\n",
            "Iteration 136, loss = 0.59555105\n",
            "Iteration 137, loss = 0.59469346\n",
            "Iteration 138, loss = 0.59380925\n",
            "Iteration 139, loss = 0.59296749\n",
            "Iteration 140, loss = 0.59208020\n",
            "Iteration 141, loss = 0.59124575\n",
            "Iteration 142, loss = 0.59038947\n",
            "Iteration 143, loss = 0.58952735\n",
            "Iteration 144, loss = 0.58870557\n",
            "Iteration 145, loss = 0.58784623\n",
            "Iteration 146, loss = 0.58703964\n",
            "Iteration 147, loss = 0.58618563\n",
            "Iteration 148, loss = 0.58533673\n",
            "Iteration 149, loss = 0.58453489\n",
            "Iteration 150, loss = 0.58370938\n",
            "Iteration 151, loss = 0.58295197\n",
            "Iteration 152, loss = 0.58210139\n",
            "Iteration 153, loss = 0.58127630\n",
            "Iteration 154, loss = 0.58047266\n",
            "Iteration 155, loss = 0.57969464\n",
            "Iteration 156, loss = 0.57890732\n",
            "Iteration 157, loss = 0.57813937\n",
            "Iteration 158, loss = 0.57738341\n",
            "Iteration 159, loss = 0.57656679\n",
            "Iteration 160, loss = 0.57585423\n",
            "Iteration 161, loss = 0.57505054\n",
            "Iteration 162, loss = 0.57427107\n",
            "Iteration 163, loss = 0.57348475\n",
            "Iteration 164, loss = 0.57274612\n",
            "Iteration 165, loss = 0.57199418\n",
            "Iteration 166, loss = 0.57121430\n",
            "Iteration 167, loss = 0.57050041\n",
            "Iteration 168, loss = 0.56971717\n",
            "Iteration 169, loss = 0.56898968\n",
            "Iteration 170, loss = 0.56825724\n",
            "Iteration 171, loss = 0.56751697\n",
            "Iteration 172, loss = 0.56681304\n",
            "Iteration 173, loss = 0.56609631\n",
            "Iteration 174, loss = 0.56537182\n",
            "Iteration 175, loss = 0.56465226\n",
            "Iteration 176, loss = 0.56395053\n",
            "Iteration 177, loss = 0.56331659\n",
            "Iteration 178, loss = 0.56254647\n",
            "Iteration 179, loss = 0.56185446\n",
            "Iteration 180, loss = 0.56116448\n",
            "Iteration 181, loss = 0.56048405\n",
            "Iteration 182, loss = 0.55979278\n",
            "Iteration 183, loss = 0.55913441\n",
            "Iteration 184, loss = 0.55847179\n",
            "Iteration 185, loss = 0.55786116\n",
            "Iteration 186, loss = 0.55713895\n",
            "Iteration 187, loss = 0.55646755\n",
            "Iteration 188, loss = 0.55582851\n",
            "Iteration 189, loss = 0.55516728\n",
            "Iteration 190, loss = 0.55450765\n",
            "Iteration 191, loss = 0.55385248\n",
            "Iteration 192, loss = 0.55322618\n",
            "Iteration 193, loss = 0.55253829\n",
            "Iteration 194, loss = 0.55195431\n",
            "Iteration 195, loss = 0.55133804\n",
            "Iteration 196, loss = 0.55066908\n",
            "Iteration 197, loss = 0.55004594\n",
            "Iteration 198, loss = 0.54944257\n",
            "Iteration 199, loss = 0.54883915\n",
            "Iteration 200, loss = 0.54821704\n",
            "Iteration 201, loss = 0.54757502\n",
            "Iteration 202, loss = 0.54697225\n",
            "Iteration 203, loss = 0.54636692\n",
            "Iteration 204, loss = 0.54573730\n",
            "Iteration 205, loss = 0.54514118\n",
            "Iteration 206, loss = 0.54450854\n",
            "Iteration 207, loss = 0.54392531\n",
            "Iteration 208, loss = 0.54331382\n",
            "Iteration 209, loss = 0.54271477\n",
            "Iteration 210, loss = 0.54210797\n",
            "Iteration 211, loss = 0.54151816\n",
            "Iteration 212, loss = 0.54092118\n",
            "Iteration 213, loss = 0.54034474\n",
            "Iteration 214, loss = 0.53980041\n",
            "Iteration 215, loss = 0.53922467\n",
            "Iteration 216, loss = 0.53862787\n",
            "Iteration 217, loss = 0.53804042\n",
            "Iteration 218, loss = 0.53747667\n",
            "Iteration 219, loss = 0.53687824\n",
            "Iteration 220, loss = 0.53632457\n",
            "Iteration 221, loss = 0.53577309\n",
            "Iteration 222, loss = 0.53521636\n",
            "Iteration 223, loss = 0.53465486\n",
            "Iteration 224, loss = 0.53411163\n",
            "Iteration 225, loss = 0.53351449\n",
            "Iteration 226, loss = 0.53299160\n",
            "Iteration 227, loss = 0.53239992\n",
            "Iteration 228, loss = 0.53187449\n",
            "Iteration 229, loss = 0.53128995\n",
            "Iteration 230, loss = 0.53073820\n",
            "Iteration 231, loss = 0.53020262\n",
            "Iteration 232, loss = 0.52963611\n",
            "Iteration 233, loss = 0.52912890\n",
            "Iteration 234, loss = 0.52859243\n",
            "Iteration 235, loss = 0.52804286\n",
            "Iteration 236, loss = 0.52750133\n",
            "Iteration 237, loss = 0.52696438\n",
            "Iteration 238, loss = 0.52644287\n",
            "Iteration 239, loss = 0.52590624\n",
            "Iteration 240, loss = 0.52536038\n",
            "Iteration 241, loss = 0.52483698\n",
            "Iteration 242, loss = 0.52429677\n",
            "Iteration 243, loss = 0.52381569\n",
            "Iteration 244, loss = 0.52327268\n",
            "Iteration 245, loss = 0.52273678\n",
            "Iteration 246, loss = 0.52221767\n",
            "Iteration 247, loss = 0.52170265\n",
            "Iteration 248, loss = 0.52120170\n",
            "Iteration 249, loss = 0.52068390\n",
            "Iteration 250, loss = 0.52016952\n",
            "Iteration 251, loss = 0.51967128\n",
            "Iteration 252, loss = 0.51917071\n",
            "Iteration 253, loss = 0.51867112\n",
            "Iteration 254, loss = 0.51812883\n",
            "Iteration 255, loss = 0.51764793\n",
            "Iteration 256, loss = 0.51714474\n",
            "Iteration 257, loss = 0.51663291\n",
            "Iteration 258, loss = 0.51614296\n",
            "Iteration 259, loss = 0.51563438\n",
            "Iteration 260, loss = 0.51512211\n",
            "Iteration 261, loss = 0.51464390\n",
            "Iteration 262, loss = 0.51412676\n",
            "Iteration 263, loss = 0.51363661\n",
            "Iteration 264, loss = 0.51314774\n",
            "Iteration 265, loss = 0.51263308\n",
            "Iteration 266, loss = 0.51213775\n",
            "Iteration 267, loss = 0.51165770\n",
            "Iteration 268, loss = 0.51116149\n",
            "Iteration 269, loss = 0.51067653\n",
            "Iteration 270, loss = 0.51023554\n",
            "Iteration 271, loss = 0.50971882\n",
            "Iteration 272, loss = 0.50922958\n",
            "Iteration 273, loss = 0.50873798\n",
            "Iteration 274, loss = 0.50824141\n",
            "Iteration 275, loss = 0.50778500\n",
            "Iteration 276, loss = 0.50728361\n",
            "Iteration 277, loss = 0.50681014\n",
            "Iteration 278, loss = 0.50634204\n",
            "Iteration 279, loss = 0.50583523\n",
            "Iteration 280, loss = 0.50538068\n",
            "Iteration 281, loss = 0.50487495\n",
            "Iteration 282, loss = 0.50442023\n",
            "Iteration 283, loss = 0.50399922\n",
            "Iteration 284, loss = 0.50351726\n",
            "Iteration 285, loss = 0.50305310\n",
            "Iteration 286, loss = 0.50262971\n",
            "Iteration 287, loss = 0.50216696\n",
            "Iteration 288, loss = 0.50168785\n",
            "Iteration 289, loss = 0.50122427\n",
            "Iteration 290, loss = 0.50082033\n",
            "Iteration 291, loss = 0.50035504\n",
            "Iteration 292, loss = 0.49987570\n",
            "Iteration 293, loss = 0.49944782\n",
            "Iteration 294, loss = 0.49898665\n",
            "Iteration 295, loss = 0.49851859\n",
            "Iteration 296, loss = 0.49806013\n",
            "Iteration 297, loss = 0.49763643\n",
            "Iteration 298, loss = 0.49715142\n",
            "Iteration 299, loss = 0.49672802\n",
            "Iteration 300, loss = 0.49625999\n",
            "Iteration 301, loss = 0.49583893\n",
            "Iteration 302, loss = 0.49539052\n",
            "Iteration 303, loss = 0.49493765\n",
            "Iteration 304, loss = 0.49449562\n",
            "Iteration 305, loss = 0.49407007\n",
            "Iteration 306, loss = 0.49360981\n",
            "Iteration 307, loss = 0.49319262\n",
            "Iteration 308, loss = 0.49277073\n",
            "Iteration 309, loss = 0.49233765\n",
            "Iteration 310, loss = 0.49189732\n",
            "Iteration 311, loss = 0.49146997\n",
            "Iteration 312, loss = 0.49105991\n",
            "Iteration 313, loss = 0.49064736\n",
            "Iteration 314, loss = 0.49021222\n",
            "Iteration 315, loss = 0.48980245\n",
            "Iteration 316, loss = 0.48938944\n",
            "Iteration 317, loss = 0.48895204\n",
            "Iteration 318, loss = 0.48856881\n",
            "Iteration 319, loss = 0.48811961\n",
            "Iteration 320, loss = 0.48770818\n",
            "Iteration 321, loss = 0.48729810\n",
            "Iteration 322, loss = 0.48685774\n",
            "Iteration 323, loss = 0.48644965\n",
            "Iteration 324, loss = 0.48604833\n",
            "Iteration 325, loss = 0.48562678\n",
            "Iteration 326, loss = 0.48520126\n",
            "Iteration 327, loss = 0.48478163\n",
            "Iteration 328, loss = 0.48436111\n",
            "Iteration 329, loss = 0.48393461\n",
            "Iteration 330, loss = 0.48352214\n",
            "Iteration 331, loss = 0.48311582\n",
            "Iteration 332, loss = 0.48272221\n",
            "Iteration 333, loss = 0.48232153\n",
            "Iteration 334, loss = 0.48187460\n",
            "Iteration 335, loss = 0.48148733\n",
            "Iteration 336, loss = 0.48108498\n",
            "Iteration 337, loss = 0.48065985\n",
            "Iteration 338, loss = 0.48027884\n",
            "Iteration 339, loss = 0.47985518\n",
            "Iteration 340, loss = 0.47951369\n",
            "Iteration 341, loss = 0.47908653\n",
            "Iteration 342, loss = 0.47869199\n",
            "Iteration 343, loss = 0.47831319\n",
            "Iteration 344, loss = 0.47790407\n",
            "Iteration 345, loss = 0.47757969\n",
            "Iteration 346, loss = 0.47717181\n",
            "Iteration 347, loss = 0.47679338\n",
            "Iteration 348, loss = 0.47643603\n",
            "Iteration 349, loss = 0.47604221\n",
            "Iteration 350, loss = 0.47567022\n",
            "Iteration 351, loss = 0.47532653\n",
            "Iteration 352, loss = 0.47494850\n",
            "Iteration 353, loss = 0.47457602\n",
            "Iteration 354, loss = 0.47417968\n",
            "Iteration 355, loss = 0.47386692\n",
            "Iteration 356, loss = 0.47344912\n",
            "Iteration 357, loss = 0.47308479\n",
            "Iteration 358, loss = 0.47274212\n",
            "Iteration 359, loss = 0.47235462\n",
            "Iteration 360, loss = 0.47198729\n",
            "Iteration 361, loss = 0.47165648\n",
            "Iteration 362, loss = 0.47128924\n",
            "Iteration 363, loss = 0.47093151\n",
            "Iteration 364, loss = 0.47057073\n",
            "Iteration 365, loss = 0.47024100\n",
            "Iteration 366, loss = 0.46991855\n",
            "Iteration 367, loss = 0.46957121\n",
            "Iteration 368, loss = 0.46925207\n",
            "Iteration 369, loss = 0.46888486\n",
            "Iteration 370, loss = 0.46853792\n",
            "Iteration 371, loss = 0.46819759\n",
            "Iteration 372, loss = 0.46785444\n",
            "Iteration 373, loss = 0.46751661\n",
            "Iteration 374, loss = 0.46717345\n",
            "Iteration 375, loss = 0.46683300\n",
            "Iteration 376, loss = 0.46649471\n",
            "Iteration 377, loss = 0.46614542\n",
            "Iteration 378, loss = 0.46581078\n",
            "Iteration 379, loss = 0.46550518\n",
            "Iteration 380, loss = 0.46513717\n",
            "Iteration 381, loss = 0.46482608\n",
            "Iteration 382, loss = 0.46449193\n",
            "Iteration 383, loss = 0.46414902\n",
            "Iteration 384, loss = 0.46380309\n",
            "Iteration 385, loss = 0.46351529\n",
            "Iteration 386, loss = 0.46317221\n",
            "Iteration 387, loss = 0.46288442\n",
            "Iteration 388, loss = 0.46256811\n",
            "Iteration 389, loss = 0.46225069\n",
            "Iteration 390, loss = 0.46194767\n",
            "Iteration 391, loss = 0.46163468\n",
            "Iteration 392, loss = 0.46131165\n",
            "Iteration 393, loss = 0.46100934\n",
            "Iteration 394, loss = 0.46070429\n",
            "Iteration 395, loss = 0.46039577\n",
            "Iteration 396, loss = 0.46010937\n",
            "Iteration 397, loss = 0.45982203\n",
            "Iteration 398, loss = 0.45950484\n",
            "Iteration 399, loss = 0.45921855\n",
            "Iteration 400, loss = 0.45888860\n",
            "Iteration 401, loss = 0.45859985\n",
            "Iteration 402, loss = 0.45829538\n",
            "Iteration 403, loss = 0.45798958\n",
            "Iteration 404, loss = 0.45770590\n",
            "Iteration 405, loss = 0.45739704\n",
            "Iteration 406, loss = 0.45714505\n",
            "Iteration 407, loss = 0.45683645\n",
            "Iteration 408, loss = 0.45652168\n",
            "Iteration 409, loss = 0.45622917\n",
            "Iteration 410, loss = 0.45595542\n",
            "Iteration 411, loss = 0.45566305\n",
            "Iteration 412, loss = 0.45543162\n",
            "Iteration 413, loss = 0.45510022\n",
            "Iteration 414, loss = 0.45483898\n",
            "Iteration 415, loss = 0.45457605\n",
            "Iteration 416, loss = 0.45427058\n",
            "Iteration 417, loss = 0.45407559\n",
            "Iteration 418, loss = 0.45375023\n",
            "Iteration 419, loss = 0.45348988\n",
            "Iteration 420, loss = 0.45326253\n",
            "Iteration 421, loss = 0.45294183\n",
            "Iteration 422, loss = 0.45269426\n",
            "Iteration 423, loss = 0.45239214\n",
            "Iteration 424, loss = 0.45213239\n",
            "Iteration 425, loss = 0.45187887\n",
            "Iteration 426, loss = 0.45161941\n",
            "Iteration 427, loss = 0.45137703\n",
            "Iteration 428, loss = 0.45110488\n",
            "Iteration 429, loss = 0.45084043\n",
            "Iteration 430, loss = 0.45060234\n",
            "Iteration 431, loss = 0.45034632\n",
            "Iteration 432, loss = 0.45011383\n",
            "Iteration 433, loss = 0.44987360\n",
            "Iteration 434, loss = 0.44962179\n",
            "Iteration 435, loss = 0.44936890\n",
            "Iteration 436, loss = 0.44914188\n",
            "Iteration 437, loss = 0.44888341\n",
            "Iteration 438, loss = 0.44863804\n",
            "Iteration 439, loss = 0.44840357\n",
            "Iteration 440, loss = 0.44816199\n",
            "Iteration 441, loss = 0.44791074\n",
            "Iteration 442, loss = 0.44766504\n",
            "Iteration 443, loss = 0.44744893\n",
            "Iteration 444, loss = 0.44722511\n",
            "Iteration 445, loss = 0.44697559\n",
            "Iteration 446, loss = 0.44673577\n",
            "Iteration 447, loss = 0.44652211\n",
            "Iteration 448, loss = 0.44628636\n",
            "Iteration 449, loss = 0.44607552\n",
            "Iteration 450, loss = 0.44584714\n",
            "Iteration 451, loss = 0.44561659\n",
            "Iteration 452, loss = 0.44541296\n",
            "Iteration 453, loss = 0.44525034\n",
            "Iteration 454, loss = 0.44500804\n",
            "Iteration 455, loss = 0.44478279\n",
            "Iteration 456, loss = 0.44455257\n",
            "Iteration 457, loss = 0.44431197\n",
            "Iteration 458, loss = 0.44410968\n",
            "Iteration 459, loss = 0.44390440\n",
            "Iteration 460, loss = 0.44367820\n",
            "Iteration 461, loss = 0.44347984\n",
            "Iteration 462, loss = 0.44329406\n",
            "Iteration 463, loss = 0.44306889\n",
            "Iteration 464, loss = 0.44290347\n",
            "Iteration 465, loss = 0.44267569\n",
            "Iteration 466, loss = 0.44246675\n",
            "Iteration 467, loss = 0.44227405\n",
            "Iteration 468, loss = 0.44207902\n",
            "Iteration 469, loss = 0.44188376\n",
            "Iteration 470, loss = 0.44168223\n",
            "Iteration 471, loss = 0.44147066\n",
            "Iteration 472, loss = 0.44130072\n",
            "Iteration 473, loss = 0.44110528\n",
            "Iteration 474, loss = 0.44089362\n",
            "Iteration 475, loss = 0.44075246\n",
            "Iteration 476, loss = 0.44051482\n",
            "Iteration 477, loss = 0.44032129\n",
            "Iteration 478, loss = 0.44015624\n",
            "Iteration 479, loss = 0.43992927\n",
            "Iteration 480, loss = 0.43975887\n",
            "Iteration 481, loss = 0.43957362\n",
            "Iteration 482, loss = 0.43937592\n",
            "Iteration 483, loss = 0.43919023\n",
            "Iteration 484, loss = 0.43904098\n",
            "Iteration 485, loss = 0.43885329\n",
            "Iteration 486, loss = 0.43868347\n",
            "Iteration 487, loss = 0.43849697\n",
            "Iteration 488, loss = 0.43832325\n",
            "Iteration 489, loss = 0.43814926\n",
            "Iteration 490, loss = 0.43799012\n",
            "Iteration 491, loss = 0.43778679\n",
            "Iteration 492, loss = 0.43758610\n",
            "Iteration 493, loss = 0.43743813\n",
            "Iteration 494, loss = 0.43727787\n",
            "Iteration 495, loss = 0.43708795\n",
            "Iteration 496, loss = 0.43694297\n",
            "Iteration 497, loss = 0.43673498\n",
            "Iteration 498, loss = 0.43660228\n",
            "Iteration 499, loss = 0.43640004\n",
            "Iteration 500, loss = 0.43624166\n",
            "Iteration 501, loss = 0.43606334\n",
            "Iteration 502, loss = 0.43589106\n",
            "Iteration 503, loss = 0.43571356\n",
            "Iteration 504, loss = 0.43552926\n",
            "Iteration 505, loss = 0.43535748\n",
            "Iteration 506, loss = 0.43522074\n",
            "Iteration 507, loss = 0.43504328\n",
            "Iteration 508, loss = 0.43489568\n",
            "Iteration 509, loss = 0.43473928\n",
            "Iteration 510, loss = 0.43458801\n",
            "Iteration 511, loss = 0.43443644\n",
            "Iteration 512, loss = 0.43427559\n",
            "Iteration 513, loss = 0.43412560\n",
            "Iteration 514, loss = 0.43395510\n",
            "Iteration 515, loss = 0.43381161\n",
            "Iteration 516, loss = 0.43368752\n",
            "Iteration 517, loss = 0.43354049\n",
            "Iteration 518, loss = 0.43339832\n",
            "Iteration 519, loss = 0.43325380\n",
            "Iteration 520, loss = 0.43312398\n",
            "Iteration 521, loss = 0.43298886\n",
            "Iteration 522, loss = 0.43283975\n",
            "Iteration 523, loss = 0.43269870\n",
            "Iteration 524, loss = 0.43256047\n",
            "Iteration 525, loss = 0.43242486\n",
            "Iteration 526, loss = 0.43228188\n",
            "Iteration 527, loss = 0.43212124\n",
            "Iteration 528, loss = 0.43206056\n",
            "Iteration 529, loss = 0.43187774\n",
            "Iteration 530, loss = 0.43172580\n",
            "Iteration 531, loss = 0.43157961\n",
            "Iteration 532, loss = 0.43148840\n",
            "Iteration 533, loss = 0.43131001\n",
            "Iteration 534, loss = 0.43117527\n",
            "Iteration 535, loss = 0.43104517\n",
            "Iteration 536, loss = 0.43092366\n",
            "Iteration 537, loss = 0.43078455\n",
            "Iteration 538, loss = 0.43064375\n",
            "Iteration 539, loss = 0.43054595\n",
            "Iteration 540, loss = 0.43039506\n",
            "Iteration 541, loss = 0.43027279\n",
            "Iteration 542, loss = 0.43014732\n",
            "Iteration 543, loss = 0.43006700\n",
            "Iteration 544, loss = 0.42994787\n",
            "Iteration 545, loss = 0.42980685\n",
            "Iteration 546, loss = 0.42969599\n",
            "Iteration 547, loss = 0.42956461\n",
            "Iteration 548, loss = 0.42945127\n",
            "Iteration 549, loss = 0.42930935\n",
            "Iteration 550, loss = 0.42919971\n",
            "Iteration 551, loss = 0.42907007\n",
            "Iteration 552, loss = 0.42895041\n",
            "Iteration 553, loss = 0.42885823\n",
            "Iteration 554, loss = 0.42876325\n",
            "Iteration 555, loss = 0.42865220\n",
            "Iteration 556, loss = 0.42856767\n",
            "Iteration 557, loss = 0.42841575\n",
            "Iteration 558, loss = 0.42830008\n",
            "Iteration 559, loss = 0.42818280\n",
            "Iteration 560, loss = 0.42809212\n",
            "Iteration 561, loss = 0.42795477\n",
            "Iteration 562, loss = 0.42786994\n",
            "Iteration 563, loss = 0.42776057\n",
            "Iteration 564, loss = 0.42761193\n",
            "Iteration 565, loss = 0.42751353\n",
            "Iteration 566, loss = 0.42740684\n",
            "Iteration 567, loss = 0.42730584\n",
            "Iteration 568, loss = 0.42718998\n",
            "Iteration 569, loss = 0.42706524\n",
            "Iteration 570, loss = 0.42696149\n",
            "Iteration 571, loss = 0.42685234\n",
            "Iteration 572, loss = 0.42681001\n",
            "Iteration 573, loss = 0.42665629\n",
            "Iteration 574, loss = 0.42656210\n",
            "Iteration 575, loss = 0.42647794\n",
            "Iteration 576, loss = 0.42635807\n",
            "Iteration 577, loss = 0.42628369\n",
            "Iteration 578, loss = 0.42615154\n",
            "Iteration 579, loss = 0.42603688\n",
            "Iteration 580, loss = 0.42593964\n",
            "Iteration 581, loss = 0.42584995\n",
            "Iteration 582, loss = 0.42572364\n",
            "Iteration 583, loss = 0.42561954\n",
            "Iteration 584, loss = 0.42550353\n",
            "Iteration 585, loss = 0.42542488\n",
            "Iteration 586, loss = 0.42531108\n",
            "Iteration 587, loss = 0.42520988\n",
            "Iteration 588, loss = 0.42512226\n",
            "Iteration 589, loss = 0.42501322\n",
            "Iteration 590, loss = 0.42492975\n",
            "Iteration 591, loss = 0.42483885\n",
            "Iteration 592, loss = 0.42476519\n",
            "Iteration 593, loss = 0.42466311\n",
            "Iteration 594, loss = 0.42456968\n",
            "Iteration 595, loss = 0.42448492\n",
            "Iteration 596, loss = 0.42437852\n",
            "Iteration 597, loss = 0.42428378\n",
            "Iteration 598, loss = 0.42418717\n",
            "Iteration 599, loss = 0.42408898\n",
            "Iteration 600, loss = 0.42400036\n",
            "Iteration 601, loss = 0.42392360\n",
            "Iteration 602, loss = 0.42383057\n",
            "Iteration 603, loss = 0.42376695\n",
            "Iteration 604, loss = 0.42366299\n",
            "Iteration 605, loss = 0.42359366\n",
            "Iteration 606, loss = 0.42348717\n",
            "Iteration 607, loss = 0.42340627\n",
            "Iteration 608, loss = 0.42328364\n",
            "Iteration 609, loss = 0.42321774\n",
            "Iteration 610, loss = 0.42313618\n",
            "Iteration 611, loss = 0.42306041\n",
            "Iteration 612, loss = 0.42296949\n",
            "Iteration 613, loss = 0.42287900\n",
            "Iteration 614, loss = 0.42279468\n",
            "Iteration 615, loss = 0.42276330\n",
            "Iteration 616, loss = 0.42265188\n",
            "Iteration 617, loss = 0.42259431\n",
            "Iteration 618, loss = 0.42247341\n",
            "Iteration 619, loss = 0.42241371\n",
            "Iteration 620, loss = 0.42231255\n",
            "Iteration 621, loss = 0.42222932\n",
            "Iteration 622, loss = 0.42213098\n",
            "Iteration 623, loss = 0.42205453\n",
            "Iteration 624, loss = 0.42195934\n",
            "Iteration 625, loss = 0.42189576\n",
            "Iteration 626, loss = 0.42185305\n",
            "Iteration 627, loss = 0.42171412\n",
            "Iteration 628, loss = 0.42164897\n",
            "Iteration 629, loss = 0.42157224\n",
            "Iteration 630, loss = 0.42149487\n",
            "Iteration 631, loss = 0.42141662\n",
            "Iteration 632, loss = 0.42134780\n",
            "Iteration 633, loss = 0.42128449\n",
            "Iteration 634, loss = 0.42119437\n",
            "Iteration 635, loss = 0.42112654\n",
            "Iteration 636, loss = 0.42106105\n",
            "Iteration 637, loss = 0.42097214\n",
            "Iteration 638, loss = 0.42096644\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69471213\n",
            "Iteration 2, loss = 0.69428274\n",
            "Iteration 3, loss = 0.69390825\n",
            "Iteration 4, loss = 0.69355857\n",
            "Iteration 5, loss = 0.69319836\n",
            "Iteration 6, loss = 0.69283788\n",
            "Iteration 7, loss = 0.69250190\n",
            "Iteration 8, loss = 0.69215479\n",
            "Iteration 9, loss = 0.69182963\n",
            "Iteration 10, loss = 0.69147758\n",
            "Iteration 11, loss = 0.69112667\n",
            "Iteration 12, loss = 0.69078330\n",
            "Iteration 13, loss = 0.69043829\n",
            "Iteration 14, loss = 0.69009936\n",
            "Iteration 15, loss = 0.68976192\n",
            "Iteration 16, loss = 0.68944878\n",
            "Iteration 17, loss = 0.68903412\n",
            "Iteration 18, loss = 0.68869665\n",
            "Iteration 19, loss = 0.68833492\n",
            "Iteration 20, loss = 0.68797065\n",
            "Iteration 21, loss = 0.68761351\n",
            "Iteration 22, loss = 0.68722632\n",
            "Iteration 23, loss = 0.68686763\n",
            "Iteration 24, loss = 0.68647356\n",
            "Iteration 25, loss = 0.68609328\n",
            "Iteration 26, loss = 0.68570697\n",
            "Iteration 27, loss = 0.68529862\n",
            "Iteration 28, loss = 0.68488406\n",
            "Iteration 29, loss = 0.68449104\n",
            "Iteration 30, loss = 0.68408140\n",
            "Iteration 31, loss = 0.68366581\n",
            "Iteration 32, loss = 0.68323099\n",
            "Iteration 33, loss = 0.68278470\n",
            "Iteration 34, loss = 0.68232925\n",
            "Iteration 35, loss = 0.68186818\n",
            "Iteration 36, loss = 0.68141826\n",
            "Iteration 37, loss = 0.68091915\n",
            "Iteration 38, loss = 0.68044836\n",
            "Iteration 39, loss = 0.67993069\n",
            "Iteration 40, loss = 0.67941337\n",
            "Iteration 41, loss = 0.67887746\n",
            "Iteration 42, loss = 0.67835415\n",
            "Iteration 43, loss = 0.67779459\n",
            "Iteration 44, loss = 0.67724861\n",
            "Iteration 45, loss = 0.67670475\n",
            "Iteration 46, loss = 0.67610617\n",
            "Iteration 47, loss = 0.67551735\n",
            "Iteration 48, loss = 0.67492067\n",
            "Iteration 49, loss = 0.67430570\n",
            "Iteration 50, loss = 0.67366281\n",
            "Iteration 51, loss = 0.67301986\n",
            "Iteration 52, loss = 0.67237009\n",
            "Iteration 53, loss = 0.67176353\n",
            "Iteration 54, loss = 0.67105659\n",
            "Iteration 55, loss = 0.67034792\n",
            "Iteration 56, loss = 0.66967235\n",
            "Iteration 57, loss = 0.66895157\n",
            "Iteration 58, loss = 0.66822483\n",
            "Iteration 59, loss = 0.66751527\n",
            "Iteration 60, loss = 0.66677386\n",
            "Iteration 61, loss = 0.66605613\n",
            "Iteration 62, loss = 0.66531475\n",
            "Iteration 63, loss = 0.66455440\n",
            "Iteration 64, loss = 0.66374831\n",
            "Iteration 65, loss = 0.66298044\n",
            "Iteration 66, loss = 0.66221344\n",
            "Iteration 67, loss = 0.66142853\n",
            "Iteration 68, loss = 0.66062259\n",
            "Iteration 69, loss = 0.65978498\n",
            "Iteration 70, loss = 0.65896575\n",
            "Iteration 71, loss = 0.65817068\n",
            "Iteration 72, loss = 0.65729884\n",
            "Iteration 73, loss = 0.65644458\n",
            "Iteration 74, loss = 0.65561648\n",
            "Iteration 75, loss = 0.65473092\n",
            "Iteration 76, loss = 0.65387344\n",
            "Iteration 77, loss = 0.65297348\n",
            "Iteration 78, loss = 0.65208824\n",
            "Iteration 79, loss = 0.65119184\n",
            "Iteration 80, loss = 0.65031114\n",
            "Iteration 81, loss = 0.64939889\n",
            "Iteration 82, loss = 0.64846493\n",
            "Iteration 83, loss = 0.64753927\n",
            "Iteration 84, loss = 0.64663381\n",
            "Iteration 85, loss = 0.64569492\n",
            "Iteration 86, loss = 0.64474350\n",
            "Iteration 87, loss = 0.64384292\n",
            "Iteration 88, loss = 0.64288286\n",
            "Iteration 89, loss = 0.64191998\n",
            "Iteration 90, loss = 0.64099098\n",
            "Iteration 91, loss = 0.64001203\n",
            "Iteration 92, loss = 0.63904770\n",
            "Iteration 93, loss = 0.63809918\n",
            "Iteration 94, loss = 0.63711597\n",
            "Iteration 95, loss = 0.63613762\n",
            "Iteration 96, loss = 0.63517058\n",
            "Iteration 97, loss = 0.63417950\n",
            "Iteration 98, loss = 0.63319876\n",
            "Iteration 99, loss = 0.63222413\n",
            "Iteration 100, loss = 0.63123862\n",
            "Iteration 101, loss = 0.63020268\n",
            "Iteration 102, loss = 0.62923546\n",
            "Iteration 103, loss = 0.62826411\n",
            "Iteration 104, loss = 0.62724813\n",
            "Iteration 105, loss = 0.62628395\n",
            "Iteration 106, loss = 0.62532715\n",
            "Iteration 107, loss = 0.62435946\n",
            "Iteration 108, loss = 0.62338204\n",
            "Iteration 109, loss = 0.62241591\n",
            "Iteration 110, loss = 0.62146361\n",
            "Iteration 111, loss = 0.62046874\n",
            "Iteration 112, loss = 0.61952285\n",
            "Iteration 113, loss = 0.61855341\n",
            "Iteration 114, loss = 0.61760429\n",
            "Iteration 115, loss = 0.61662822\n",
            "Iteration 116, loss = 0.61567276\n",
            "Iteration 117, loss = 0.61470243\n",
            "Iteration 118, loss = 0.61378360\n",
            "Iteration 119, loss = 0.61282276\n",
            "Iteration 120, loss = 0.61190514\n",
            "Iteration 121, loss = 0.61094764\n",
            "Iteration 122, loss = 0.61003614\n",
            "Iteration 123, loss = 0.60915291\n",
            "Iteration 124, loss = 0.60817663\n",
            "Iteration 125, loss = 0.60727571\n",
            "Iteration 126, loss = 0.60636760\n",
            "Iteration 127, loss = 0.60542114\n",
            "Iteration 128, loss = 0.60449520\n",
            "Iteration 129, loss = 0.60362803\n",
            "Iteration 130, loss = 0.60264342\n",
            "Iteration 131, loss = 0.60178346\n",
            "Iteration 132, loss = 0.60084745\n",
            "Iteration 133, loss = 0.59990935\n",
            "Iteration 134, loss = 0.59902441\n",
            "Iteration 135, loss = 0.59817414\n",
            "Iteration 136, loss = 0.59728178\n",
            "Iteration 137, loss = 0.59639090\n",
            "Iteration 138, loss = 0.59551738\n",
            "Iteration 139, loss = 0.59465679\n",
            "Iteration 140, loss = 0.59377293\n",
            "Iteration 141, loss = 0.59297020\n",
            "Iteration 142, loss = 0.59207147\n",
            "Iteration 143, loss = 0.59121185\n",
            "Iteration 144, loss = 0.59037307\n",
            "Iteration 145, loss = 0.58953931\n",
            "Iteration 146, loss = 0.58867369\n",
            "Iteration 147, loss = 0.58785867\n",
            "Iteration 148, loss = 0.58701344\n",
            "Iteration 149, loss = 0.58619167\n",
            "Iteration 150, loss = 0.58534703\n",
            "Iteration 151, loss = 0.58456365\n",
            "Iteration 152, loss = 0.58373231\n",
            "Iteration 153, loss = 0.58290174\n",
            "Iteration 154, loss = 0.58208371\n",
            "Iteration 155, loss = 0.58128915\n",
            "Iteration 156, loss = 0.58049292\n",
            "Iteration 157, loss = 0.57971006\n",
            "Iteration 158, loss = 0.57896051\n",
            "Iteration 159, loss = 0.57817800\n",
            "Iteration 160, loss = 0.57743545\n",
            "Iteration 161, loss = 0.57664805\n",
            "Iteration 162, loss = 0.57585865\n",
            "Iteration 163, loss = 0.57507305\n",
            "Iteration 164, loss = 0.57432575\n",
            "Iteration 165, loss = 0.57357778\n",
            "Iteration 166, loss = 0.57278340\n",
            "Iteration 167, loss = 0.57207541\n",
            "Iteration 168, loss = 0.57127681\n",
            "Iteration 169, loss = 0.57054357\n",
            "Iteration 170, loss = 0.56980558\n",
            "Iteration 171, loss = 0.56905337\n",
            "Iteration 172, loss = 0.56834986\n",
            "Iteration 173, loss = 0.56762638\n",
            "Iteration 174, loss = 0.56692583\n",
            "Iteration 175, loss = 0.56619122\n",
            "Iteration 176, loss = 0.56549186\n",
            "Iteration 177, loss = 0.56484954\n",
            "Iteration 178, loss = 0.56406428\n",
            "Iteration 179, loss = 0.56337512\n",
            "Iteration 180, loss = 0.56267245\n",
            "Iteration 181, loss = 0.56199547\n",
            "Iteration 182, loss = 0.56126741\n",
            "Iteration 183, loss = 0.56058954\n",
            "Iteration 184, loss = 0.55991716\n",
            "Iteration 185, loss = 0.55931198\n",
            "Iteration 186, loss = 0.55857788\n",
            "Iteration 187, loss = 0.55788941\n",
            "Iteration 188, loss = 0.55723863\n",
            "Iteration 189, loss = 0.55656306\n",
            "Iteration 190, loss = 0.55588247\n",
            "Iteration 191, loss = 0.55522942\n",
            "Iteration 192, loss = 0.55458489\n",
            "Iteration 193, loss = 0.55389137\n",
            "Iteration 194, loss = 0.55327793\n",
            "Iteration 195, loss = 0.55264741\n",
            "Iteration 196, loss = 0.55197998\n",
            "Iteration 197, loss = 0.55135716\n",
            "Iteration 198, loss = 0.55073338\n",
            "Iteration 199, loss = 0.55011924\n",
            "Iteration 200, loss = 0.54950676\n",
            "Iteration 201, loss = 0.54886098\n",
            "Iteration 202, loss = 0.54824084\n",
            "Iteration 203, loss = 0.54762323\n",
            "Iteration 204, loss = 0.54699436\n",
            "Iteration 205, loss = 0.54639812\n",
            "Iteration 206, loss = 0.54574096\n",
            "Iteration 207, loss = 0.54517320\n",
            "Iteration 208, loss = 0.54452678\n",
            "Iteration 209, loss = 0.54393809\n",
            "Iteration 210, loss = 0.54333058\n",
            "Iteration 211, loss = 0.54274313\n",
            "Iteration 212, loss = 0.54216266\n",
            "Iteration 213, loss = 0.54155489\n",
            "Iteration 214, loss = 0.54099307\n",
            "Iteration 215, loss = 0.54040282\n",
            "Iteration 216, loss = 0.53980128\n",
            "Iteration 217, loss = 0.53921090\n",
            "Iteration 218, loss = 0.53864824\n",
            "Iteration 219, loss = 0.53803216\n",
            "Iteration 220, loss = 0.53747590\n",
            "Iteration 221, loss = 0.53691295\n",
            "Iteration 222, loss = 0.53634585\n",
            "Iteration 223, loss = 0.53578651\n",
            "Iteration 224, loss = 0.53524360\n",
            "Iteration 225, loss = 0.53463352\n",
            "Iteration 226, loss = 0.53409916\n",
            "Iteration 227, loss = 0.53350971\n",
            "Iteration 228, loss = 0.53298483\n",
            "Iteration 229, loss = 0.53240908\n",
            "Iteration 230, loss = 0.53185277\n",
            "Iteration 231, loss = 0.53130837\n",
            "Iteration 232, loss = 0.53075284\n",
            "Iteration 233, loss = 0.53024299\n",
            "Iteration 234, loss = 0.52968729\n",
            "Iteration 235, loss = 0.52916842\n",
            "Iteration 236, loss = 0.52859966\n",
            "Iteration 237, loss = 0.52807161\n",
            "Iteration 238, loss = 0.52755112\n",
            "Iteration 239, loss = 0.52701933\n",
            "Iteration 240, loss = 0.52647947\n",
            "Iteration 241, loss = 0.52595584\n",
            "Iteration 242, loss = 0.52541280\n",
            "Iteration 243, loss = 0.52491728\n",
            "Iteration 244, loss = 0.52439661\n",
            "Iteration 245, loss = 0.52385819\n",
            "Iteration 246, loss = 0.52333582\n",
            "Iteration 247, loss = 0.52281695\n",
            "Iteration 248, loss = 0.52233470\n",
            "Iteration 249, loss = 0.52180039\n",
            "Iteration 250, loss = 0.52129596\n",
            "Iteration 251, loss = 0.52080974\n",
            "Iteration 252, loss = 0.52031262\n",
            "Iteration 253, loss = 0.51979610\n",
            "Iteration 254, loss = 0.51927211\n",
            "Iteration 255, loss = 0.51878548\n",
            "Iteration 256, loss = 0.51828014\n",
            "Iteration 257, loss = 0.51777912\n",
            "Iteration 258, loss = 0.51729127\n",
            "Iteration 259, loss = 0.51677769\n",
            "Iteration 260, loss = 0.51627591\n",
            "Iteration 261, loss = 0.51580226\n",
            "Iteration 262, loss = 0.51528999\n",
            "Iteration 263, loss = 0.51480664\n",
            "Iteration 264, loss = 0.51431584\n",
            "Iteration 265, loss = 0.51382973\n",
            "Iteration 266, loss = 0.51332273\n",
            "Iteration 267, loss = 0.51285061\n",
            "Iteration 268, loss = 0.51236814\n",
            "Iteration 269, loss = 0.51191494\n",
            "Iteration 270, loss = 0.51144796\n",
            "Iteration 271, loss = 0.51097151\n",
            "Iteration 272, loss = 0.51049150\n",
            "Iteration 273, loss = 0.51000889\n",
            "Iteration 274, loss = 0.50953531\n",
            "Iteration 275, loss = 0.50907904\n",
            "Iteration 276, loss = 0.50857886\n",
            "Iteration 277, loss = 0.50811267\n",
            "Iteration 278, loss = 0.50766909\n",
            "Iteration 279, loss = 0.50716643\n",
            "Iteration 280, loss = 0.50671887\n",
            "Iteration 281, loss = 0.50623230\n",
            "Iteration 282, loss = 0.50577682\n",
            "Iteration 283, loss = 0.50539175\n",
            "Iteration 284, loss = 0.50490988\n",
            "Iteration 285, loss = 0.50445754\n",
            "Iteration 286, loss = 0.50404088\n",
            "Iteration 287, loss = 0.50357955\n",
            "Iteration 288, loss = 0.50313206\n",
            "Iteration 289, loss = 0.50267481\n",
            "Iteration 290, loss = 0.50226112\n",
            "Iteration 291, loss = 0.50182916\n",
            "Iteration 292, loss = 0.50134541\n",
            "Iteration 293, loss = 0.50091526\n",
            "Iteration 294, loss = 0.50047297\n",
            "Iteration 295, loss = 0.50001416\n",
            "Iteration 296, loss = 0.49956367\n",
            "Iteration 297, loss = 0.49915308\n",
            "Iteration 298, loss = 0.49869575\n",
            "Iteration 299, loss = 0.49827732\n",
            "Iteration 300, loss = 0.49783240\n",
            "Iteration 301, loss = 0.49740903\n",
            "Iteration 302, loss = 0.49698052\n",
            "Iteration 303, loss = 0.49656375\n",
            "Iteration 304, loss = 0.49613096\n",
            "Iteration 305, loss = 0.49570222\n",
            "Iteration 306, loss = 0.49527169\n",
            "Iteration 307, loss = 0.49484952\n",
            "Iteration 308, loss = 0.49443578\n",
            "Iteration 309, loss = 0.49402335\n",
            "Iteration 310, loss = 0.49359221\n",
            "Iteration 311, loss = 0.49318217\n",
            "Iteration 312, loss = 0.49277454\n",
            "Iteration 313, loss = 0.49236449\n",
            "Iteration 314, loss = 0.49197042\n",
            "Iteration 315, loss = 0.49155657\n",
            "Iteration 316, loss = 0.49116274\n",
            "Iteration 317, loss = 0.49073969\n",
            "Iteration 318, loss = 0.49036634\n",
            "Iteration 319, loss = 0.48994295\n",
            "Iteration 320, loss = 0.48954300\n",
            "Iteration 321, loss = 0.48913494\n",
            "Iteration 322, loss = 0.48871733\n",
            "Iteration 323, loss = 0.48832346\n",
            "Iteration 324, loss = 0.48793458\n",
            "Iteration 325, loss = 0.48753314\n",
            "Iteration 326, loss = 0.48710609\n",
            "Iteration 327, loss = 0.48671236\n",
            "Iteration 328, loss = 0.48631492\n",
            "Iteration 329, loss = 0.48589275\n",
            "Iteration 330, loss = 0.48550036\n",
            "Iteration 331, loss = 0.48511310\n",
            "Iteration 332, loss = 0.48471628\n",
            "Iteration 333, loss = 0.48434843\n",
            "Iteration 334, loss = 0.48391536\n",
            "Iteration 335, loss = 0.48353061\n",
            "Iteration 336, loss = 0.48314370\n",
            "Iteration 337, loss = 0.48276607\n",
            "Iteration 338, loss = 0.48238445\n",
            "Iteration 339, loss = 0.48199520\n",
            "Iteration 340, loss = 0.48162730\n",
            "Iteration 341, loss = 0.48126798\n",
            "Iteration 342, loss = 0.48086958\n",
            "Iteration 343, loss = 0.48049973\n",
            "Iteration 344, loss = 0.48011624\n",
            "Iteration 345, loss = 0.47975895\n",
            "Iteration 346, loss = 0.47940662\n",
            "Iteration 347, loss = 0.47903757\n",
            "Iteration 348, loss = 0.47870104\n",
            "Iteration 349, loss = 0.47830671\n",
            "Iteration 350, loss = 0.47794752\n",
            "Iteration 351, loss = 0.47761988\n",
            "Iteration 352, loss = 0.47725316\n",
            "Iteration 353, loss = 0.47688153\n",
            "Iteration 354, loss = 0.47651379\n",
            "Iteration 355, loss = 0.47618593\n",
            "Iteration 356, loss = 0.47579125\n",
            "Iteration 357, loss = 0.47543345\n",
            "Iteration 358, loss = 0.47510111\n",
            "Iteration 359, loss = 0.47473447\n",
            "Iteration 360, loss = 0.47439293\n",
            "Iteration 361, loss = 0.47404199\n",
            "Iteration 362, loss = 0.47370741\n",
            "Iteration 363, loss = 0.47335894\n",
            "Iteration 364, loss = 0.47303221\n",
            "Iteration 365, loss = 0.47268823\n",
            "Iteration 366, loss = 0.47238914\n",
            "Iteration 367, loss = 0.47206096\n",
            "Iteration 368, loss = 0.47175504\n",
            "Iteration 369, loss = 0.47139649\n",
            "Iteration 370, loss = 0.47104006\n",
            "Iteration 371, loss = 0.47073788\n",
            "Iteration 372, loss = 0.47037842\n",
            "Iteration 373, loss = 0.47004878\n",
            "Iteration 374, loss = 0.46973164\n",
            "Iteration 375, loss = 0.46938510\n",
            "Iteration 376, loss = 0.46906388\n",
            "Iteration 377, loss = 0.46873323\n",
            "Iteration 378, loss = 0.46839394\n",
            "Iteration 379, loss = 0.46809582\n",
            "Iteration 380, loss = 0.46776503\n",
            "Iteration 381, loss = 0.46743156\n",
            "Iteration 382, loss = 0.46712569\n",
            "Iteration 383, loss = 0.46680700\n",
            "Iteration 384, loss = 0.46647157\n",
            "Iteration 385, loss = 0.46619451\n",
            "Iteration 386, loss = 0.46587258\n",
            "Iteration 387, loss = 0.46558798\n",
            "Iteration 388, loss = 0.46528340\n",
            "Iteration 389, loss = 0.46497758\n",
            "Iteration 390, loss = 0.46470209\n",
            "Iteration 391, loss = 0.46439887\n",
            "Iteration 392, loss = 0.46410085\n",
            "Iteration 393, loss = 0.46380645\n",
            "Iteration 394, loss = 0.46351668\n",
            "Iteration 395, loss = 0.46323253\n",
            "Iteration 396, loss = 0.46296579\n",
            "Iteration 397, loss = 0.46271846\n",
            "Iteration 398, loss = 0.46241504\n",
            "Iteration 399, loss = 0.46213019\n",
            "Iteration 400, loss = 0.46181187\n",
            "Iteration 401, loss = 0.46154547\n",
            "Iteration 402, loss = 0.46125209\n",
            "Iteration 403, loss = 0.46095919\n",
            "Iteration 404, loss = 0.46067998\n",
            "Iteration 405, loss = 0.46038277\n",
            "Iteration 406, loss = 0.46014372\n",
            "Iteration 407, loss = 0.45983112\n",
            "Iteration 408, loss = 0.45952814\n",
            "Iteration 409, loss = 0.45925651\n",
            "Iteration 410, loss = 0.45898936\n",
            "Iteration 411, loss = 0.45869272\n",
            "Iteration 412, loss = 0.45848868\n",
            "Iteration 413, loss = 0.45815801\n",
            "Iteration 414, loss = 0.45791499\n",
            "Iteration 415, loss = 0.45764238\n",
            "Iteration 416, loss = 0.45735126\n",
            "Iteration 417, loss = 0.45715430\n",
            "Iteration 418, loss = 0.45684317\n",
            "Iteration 419, loss = 0.45658701\n",
            "Iteration 420, loss = 0.45635756\n",
            "Iteration 421, loss = 0.45605164\n",
            "Iteration 422, loss = 0.45582397\n",
            "Iteration 423, loss = 0.45552553\n",
            "Iteration 424, loss = 0.45528213\n",
            "Iteration 425, loss = 0.45501653\n",
            "Iteration 426, loss = 0.45477258\n",
            "Iteration 427, loss = 0.45453317\n",
            "Iteration 428, loss = 0.45427982\n",
            "Iteration 429, loss = 0.45403266\n",
            "Iteration 430, loss = 0.45379122\n",
            "Iteration 431, loss = 0.45354883\n",
            "Iteration 432, loss = 0.45330572\n",
            "Iteration 433, loss = 0.45308796\n",
            "Iteration 434, loss = 0.45284903\n",
            "Iteration 435, loss = 0.45261813\n",
            "Iteration 436, loss = 0.45241450\n",
            "Iteration 437, loss = 0.45215007\n",
            "Iteration 438, loss = 0.45193207\n",
            "Iteration 439, loss = 0.45170912\n",
            "Iteration 440, loss = 0.45148344\n",
            "Iteration 441, loss = 0.45125153\n",
            "Iteration 442, loss = 0.45102459\n",
            "Iteration 443, loss = 0.45079962\n",
            "Iteration 444, loss = 0.45060243\n",
            "Iteration 445, loss = 0.45033866\n",
            "Iteration 446, loss = 0.45012244\n",
            "Iteration 447, loss = 0.44991531\n",
            "Iteration 448, loss = 0.44971764\n",
            "Iteration 449, loss = 0.44947606\n",
            "Iteration 450, loss = 0.44927460\n",
            "Iteration 451, loss = 0.44904520\n",
            "Iteration 452, loss = 0.44887473\n",
            "Iteration 453, loss = 0.44872568\n",
            "Iteration 454, loss = 0.44846314\n",
            "Iteration 455, loss = 0.44825558\n",
            "Iteration 456, loss = 0.44805817\n",
            "Iteration 457, loss = 0.44781888\n",
            "Iteration 458, loss = 0.44762849\n",
            "Iteration 459, loss = 0.44741728\n",
            "Iteration 460, loss = 0.44719707\n",
            "Iteration 461, loss = 0.44700430\n",
            "Iteration 462, loss = 0.44681966\n",
            "Iteration 463, loss = 0.44660081\n",
            "Iteration 464, loss = 0.44642694\n",
            "Iteration 465, loss = 0.44621000\n",
            "Iteration 466, loss = 0.44603799\n",
            "Iteration 467, loss = 0.44584127\n",
            "Iteration 468, loss = 0.44566145\n",
            "Iteration 469, loss = 0.44546083\n",
            "Iteration 470, loss = 0.44527226\n",
            "Iteration 471, loss = 0.44506546\n",
            "Iteration 472, loss = 0.44490192\n",
            "Iteration 473, loss = 0.44470301\n",
            "Iteration 474, loss = 0.44451268\n",
            "Iteration 475, loss = 0.44438072\n",
            "Iteration 476, loss = 0.44414904\n",
            "Iteration 477, loss = 0.44396910\n",
            "Iteration 478, loss = 0.44379944\n",
            "Iteration 479, loss = 0.44359339\n",
            "Iteration 480, loss = 0.44342083\n",
            "Iteration 481, loss = 0.44324586\n",
            "Iteration 482, loss = 0.44306469\n",
            "Iteration 483, loss = 0.44286762\n",
            "Iteration 484, loss = 0.44273657\n",
            "Iteration 485, loss = 0.44254999\n",
            "Iteration 486, loss = 0.44238563\n",
            "Iteration 487, loss = 0.44220499\n",
            "Iteration 488, loss = 0.44204295\n",
            "Iteration 489, loss = 0.44186602\n",
            "Iteration 490, loss = 0.44171595\n",
            "Iteration 491, loss = 0.44152634\n",
            "Iteration 492, loss = 0.44133313\n",
            "Iteration 493, loss = 0.44116221\n",
            "Iteration 494, loss = 0.44101428\n",
            "Iteration 495, loss = 0.44085047\n",
            "Iteration 496, loss = 0.44073944\n",
            "Iteration 497, loss = 0.44051449\n",
            "Iteration 498, loss = 0.44039852\n",
            "Iteration 499, loss = 0.44021541\n",
            "Iteration 500, loss = 0.44007236\n",
            "Iteration 501, loss = 0.43990715\n",
            "Iteration 502, loss = 0.43975792\n",
            "Iteration 503, loss = 0.43957892\n",
            "Iteration 504, loss = 0.43940947\n",
            "Iteration 505, loss = 0.43926090\n",
            "Iteration 506, loss = 0.43911100\n",
            "Iteration 507, loss = 0.43893895\n",
            "Iteration 508, loss = 0.43881583\n",
            "Iteration 509, loss = 0.43866359\n",
            "Iteration 510, loss = 0.43853224\n",
            "Iteration 511, loss = 0.43837092\n",
            "Iteration 512, loss = 0.43821105\n",
            "Iteration 513, loss = 0.43806915\n",
            "Iteration 514, loss = 0.43791610\n",
            "Iteration 515, loss = 0.43778348\n",
            "Iteration 516, loss = 0.43762900\n",
            "Iteration 517, loss = 0.43750436\n",
            "Iteration 518, loss = 0.43735560\n",
            "Iteration 519, loss = 0.43722263\n",
            "Iteration 520, loss = 0.43708610\n",
            "Iteration 521, loss = 0.43698129\n",
            "Iteration 522, loss = 0.43683650\n",
            "Iteration 523, loss = 0.43671053\n",
            "Iteration 524, loss = 0.43656694\n",
            "Iteration 525, loss = 0.43643311\n",
            "Iteration 526, loss = 0.43630515\n",
            "Iteration 527, loss = 0.43614197\n",
            "Iteration 528, loss = 0.43607078\n",
            "Iteration 529, loss = 0.43590441\n",
            "Iteration 530, loss = 0.43576292\n",
            "Iteration 531, loss = 0.43564483\n",
            "Iteration 532, loss = 0.43553696\n",
            "Iteration 533, loss = 0.43537180\n",
            "Iteration 534, loss = 0.43524917\n",
            "Iteration 535, loss = 0.43512539\n",
            "Iteration 536, loss = 0.43500802\n",
            "Iteration 537, loss = 0.43489119\n",
            "Iteration 538, loss = 0.43475852\n",
            "Iteration 539, loss = 0.43466110\n",
            "Iteration 540, loss = 0.43451944\n",
            "Iteration 541, loss = 0.43440979\n",
            "Iteration 542, loss = 0.43430225\n",
            "Iteration 543, loss = 0.43423960\n",
            "Iteration 544, loss = 0.43410142\n",
            "Iteration 545, loss = 0.43397662\n",
            "Iteration 546, loss = 0.43384758\n",
            "Iteration 547, loss = 0.43372399\n",
            "Iteration 548, loss = 0.43360470\n",
            "Iteration 549, loss = 0.43348333\n",
            "Iteration 550, loss = 0.43337635\n",
            "Iteration 551, loss = 0.43326151\n",
            "Iteration 552, loss = 0.43314901\n",
            "Iteration 553, loss = 0.43307048\n",
            "Iteration 554, loss = 0.43296495\n",
            "Iteration 555, loss = 0.43290586\n",
            "Iteration 556, loss = 0.43278170\n",
            "Iteration 557, loss = 0.43266174\n",
            "Iteration 558, loss = 0.43254788\n",
            "Iteration 559, loss = 0.43244385\n",
            "Iteration 560, loss = 0.43235408\n",
            "Iteration 561, loss = 0.43222472\n",
            "Iteration 562, loss = 0.43212948\n",
            "Iteration 563, loss = 0.43205569\n",
            "Iteration 564, loss = 0.43190347\n",
            "Iteration 565, loss = 0.43181133\n",
            "Iteration 566, loss = 0.43171034\n",
            "Iteration 567, loss = 0.43161746\n",
            "Iteration 568, loss = 0.43151165\n",
            "Iteration 569, loss = 0.43139931\n",
            "Iteration 570, loss = 0.43131935\n",
            "Iteration 571, loss = 0.43120045\n",
            "Iteration 572, loss = 0.43123410\n",
            "Iteration 573, loss = 0.43103414\n",
            "Iteration 574, loss = 0.43093996\n",
            "Iteration 575, loss = 0.43084913\n",
            "Iteration 576, loss = 0.43073408\n",
            "Iteration 577, loss = 0.43068476\n",
            "Iteration 578, loss = 0.43053689\n",
            "Iteration 579, loss = 0.43043330\n",
            "Iteration 580, loss = 0.43035673\n",
            "Iteration 581, loss = 0.43024875\n",
            "Iteration 582, loss = 0.43015672\n",
            "Iteration 583, loss = 0.43005346\n",
            "Iteration 584, loss = 0.42995679\n",
            "Iteration 585, loss = 0.42987518\n",
            "Iteration 586, loss = 0.42976814\n",
            "Iteration 587, loss = 0.42967104\n",
            "Iteration 588, loss = 0.42958645\n",
            "Iteration 589, loss = 0.42949286\n",
            "Iteration 590, loss = 0.42942194\n",
            "Iteration 591, loss = 0.42932180\n",
            "Iteration 592, loss = 0.42924445\n",
            "Iteration 593, loss = 0.42915407\n",
            "Iteration 594, loss = 0.42906207\n",
            "Iteration 595, loss = 0.42898739\n",
            "Iteration 596, loss = 0.42889131\n",
            "Iteration 597, loss = 0.42882171\n",
            "Iteration 598, loss = 0.42871882\n",
            "Iteration 599, loss = 0.42864032\n",
            "Iteration 600, loss = 0.42854479\n",
            "Iteration 601, loss = 0.42845823\n",
            "Iteration 602, loss = 0.42839558\n",
            "Iteration 603, loss = 0.42830848\n",
            "Iteration 604, loss = 0.42823394\n",
            "Iteration 605, loss = 0.42817003\n",
            "Iteration 606, loss = 0.42807476\n",
            "Iteration 607, loss = 0.42801125\n",
            "Iteration 608, loss = 0.42789118\n",
            "Iteration 609, loss = 0.42782609\n",
            "Iteration 610, loss = 0.42775166\n",
            "Iteration 611, loss = 0.42769444\n",
            "Iteration 612, loss = 0.42763710\n",
            "Iteration 613, loss = 0.42750245\n",
            "Iteration 614, loss = 0.42741879\n",
            "Iteration 615, loss = 0.42737242\n",
            "Iteration 616, loss = 0.42727790\n",
            "Iteration 617, loss = 0.42723150\n",
            "Iteration 618, loss = 0.42711016\n",
            "Iteration 619, loss = 0.42704812\n",
            "Iteration 620, loss = 0.42696511\n",
            "Iteration 621, loss = 0.42689278\n",
            "Iteration 622, loss = 0.42680647\n",
            "Iteration 623, loss = 0.42673115\n",
            "Iteration 624, loss = 0.42665097\n",
            "Iteration 625, loss = 0.42658437\n",
            "Iteration 626, loss = 0.42656888\n",
            "Iteration 627, loss = 0.42642926\n",
            "Iteration 628, loss = 0.42636557\n",
            "Iteration 629, loss = 0.42629909\n",
            "Iteration 630, loss = 0.42620579\n",
            "Iteration 631, loss = 0.42614177\n",
            "Iteration 632, loss = 0.42608030\n",
            "Iteration 633, loss = 0.42601784\n",
            "Iteration 634, loss = 0.42594305\n",
            "Iteration 635, loss = 0.42586547\n",
            "Iteration 636, loss = 0.42581255\n",
            "Iteration 637, loss = 0.42572128\n",
            "Iteration 638, loss = 0.42570188\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69481829\n",
            "Iteration 2, loss = 0.69444909\n",
            "Iteration 3, loss = 0.69407690\n",
            "Iteration 4, loss = 0.69371192\n",
            "Iteration 5, loss = 0.69333479\n",
            "Iteration 6, loss = 0.69298723\n",
            "Iteration 7, loss = 0.69266712\n",
            "Iteration 8, loss = 0.69231240\n",
            "Iteration 9, loss = 0.69200480\n",
            "Iteration 10, loss = 0.69167765\n",
            "Iteration 11, loss = 0.69131884\n",
            "Iteration 12, loss = 0.69098264\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69472309\n",
            "Iteration 2, loss = 0.69434367\n",
            "Iteration 3, loss = 0.69397299\n",
            "Iteration 4, loss = 0.69361484\n",
            "Iteration 5, loss = 0.69322995\n",
            "Iteration 6, loss = 0.69289390\n",
            "Iteration 7, loss = 0.69255480\n",
            "Iteration 8, loss = 0.69221433\n",
            "Iteration 9, loss = 0.69190139\n",
            "Iteration 10, loss = 0.69155964\n",
            "Iteration 11, loss = 0.69120553\n",
            "Iteration 12, loss = 0.69086993\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69459895\n",
            "Iteration 2, loss = 0.69418306\n",
            "Iteration 3, loss = 0.69381772\n",
            "Iteration 4, loss = 0.69344965\n",
            "Iteration 5, loss = 0.69307784\n",
            "Iteration 6, loss = 0.69274224\n",
            "Iteration 7, loss = 0.69240447\n",
            "Iteration 8, loss = 0.69207223\n",
            "Iteration 9, loss = 0.69174678\n",
            "Iteration 10, loss = 0.69140644\n",
            "Iteration 11, loss = 0.69106454\n",
            "Iteration 12, loss = 0.69072542\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69470200\n",
            "Iteration 2, loss = 0.69428016\n",
            "Iteration 3, loss = 0.69389849\n",
            "Iteration 4, loss = 0.69353959\n",
            "Iteration 5, loss = 0.69316858\n",
            "Iteration 6, loss = 0.69279835\n",
            "Iteration 7, loss = 0.69246205\n",
            "Iteration 8, loss = 0.69212603\n",
            "Iteration 9, loss = 0.69180658\n",
            "Iteration 10, loss = 0.69145865\n",
            "Iteration 11, loss = 0.69111892\n",
            "Iteration 12, loss = 0.69077270\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69486222\n",
            "Iteration 2, loss = 0.69440173\n",
            "Iteration 3, loss = 0.69405436\n",
            "Iteration 4, loss = 0.69370789\n",
            "Iteration 5, loss = 0.69334119\n",
            "Iteration 6, loss = 0.69298727\n",
            "Iteration 7, loss = 0.69265666\n",
            "Iteration 8, loss = 0.69231392\n",
            "Iteration 9, loss = 0.69199990\n",
            "Iteration 10, loss = 0.69163630\n",
            "Iteration 11, loss = 0.69130766\n",
            "Iteration 12, loss = 0.69097020\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69456510\n",
            "Iteration 2, loss = 0.69413215\n",
            "Iteration 3, loss = 0.69375891\n",
            "Iteration 4, loss = 0.69343161\n",
            "Iteration 5, loss = 0.69305278\n",
            "Iteration 6, loss = 0.69270355\n",
            "Iteration 7, loss = 0.69240020\n",
            "Iteration 8, loss = 0.69205971\n",
            "Iteration 9, loss = 0.69173530\n",
            "Iteration 10, loss = 0.69139766\n",
            "Iteration 11, loss = 0.69107223\n",
            "Iteration 12, loss = 0.69073003\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69482954\n",
            "Iteration 2, loss = 0.69433428\n",
            "Iteration 3, loss = 0.69398311\n",
            "Iteration 4, loss = 0.69363138\n",
            "Iteration 5, loss = 0.69327792\n",
            "Iteration 6, loss = 0.69289743\n",
            "Iteration 7, loss = 0.69255484\n",
            "Iteration 8, loss = 0.69220412\n",
            "Iteration 9, loss = 0.69187069\n",
            "Iteration 10, loss = 0.69150950\n",
            "Iteration 11, loss = 0.69118467\n",
            "Iteration 12, loss = 0.69082296\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69463349\n",
            "Iteration 2, loss = 0.69420115\n",
            "Iteration 3, loss = 0.69384235\n",
            "Iteration 4, loss = 0.69349440\n",
            "Iteration 5, loss = 0.69315017\n",
            "Iteration 6, loss = 0.69277350\n",
            "Iteration 7, loss = 0.69245117\n",
            "Iteration 8, loss = 0.69208836\n",
            "Iteration 9, loss = 0.69176151\n",
            "Iteration 10, loss = 0.69140902\n",
            "Iteration 11, loss = 0.69108148\n",
            "Iteration 12, loss = 0.69072973\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69460286\n",
            "Iteration 2, loss = 0.69417046\n",
            "Iteration 3, loss = 0.69380062\n",
            "Iteration 4, loss = 0.69346539\n",
            "Iteration 5, loss = 0.69311080\n",
            "Iteration 6, loss = 0.69274491\n",
            "Iteration 7, loss = 0.69241642\n",
            "Iteration 8, loss = 0.69206563\n",
            "Iteration 9, loss = 0.69174727\n",
            "Iteration 10, loss = 0.69139108\n",
            "Iteration 11, loss = 0.69104824\n",
            "Iteration 12, loss = 0.69070188\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69471213\n",
            "Iteration 2, loss = 0.69428274\n",
            "Iteration 3, loss = 0.69390825\n",
            "Iteration 4, loss = 0.69355857\n",
            "Iteration 5, loss = 0.69319836\n",
            "Iteration 6, loss = 0.69283788\n",
            "Iteration 7, loss = 0.69250190\n",
            "Iteration 8, loss = 0.69215479\n",
            "Iteration 9, loss = 0.69182963\n",
            "Iteration 10, loss = 0.69147758\n",
            "Iteration 11, loss = 0.69112667\n",
            "Iteration 12, loss = 0.69078330\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.70726748\n",
            "Iteration 2, loss = 0.70474285\n",
            "Iteration 3, loss = 0.70214483\n",
            "Iteration 4, loss = 0.69993801\n",
            "Iteration 5, loss = 0.69760512\n",
            "Iteration 6, loss = 0.69523673\n",
            "Iteration 7, loss = 0.69302907\n",
            "Iteration 8, loss = 0.69069492\n",
            "Iteration 9, loss = 0.68855522\n",
            "Iteration 10, loss = 0.68650624\n",
            "Iteration 11, loss = 0.68469661\n",
            "Iteration 12, loss = 0.68294287\n",
            "Iteration 13, loss = 0.68105488\n",
            "Iteration 14, loss = 0.67938544\n",
            "Iteration 15, loss = 0.67774353\n",
            "Iteration 16, loss = 0.67638207\n",
            "Iteration 17, loss = 0.67488343\n",
            "Iteration 18, loss = 0.67357352\n",
            "Iteration 19, loss = 0.67219227\n",
            "Iteration 20, loss = 0.67087926\n",
            "Iteration 21, loss = 0.66961376\n",
            "Iteration 22, loss = 0.66830656\n",
            "Iteration 23, loss = 0.66703262\n",
            "Iteration 24, loss = 0.66572444\n",
            "Iteration 25, loss = 0.66452435\n",
            "Iteration 26, loss = 0.66325318\n",
            "Iteration 27, loss = 0.66196006\n",
            "Iteration 28, loss = 0.66063807\n",
            "Iteration 29, loss = 0.65933878\n",
            "Iteration 30, loss = 0.65804512\n",
            "Iteration 31, loss = 0.65674049\n",
            "Iteration 32, loss = 0.65539863\n",
            "Iteration 33, loss = 0.65408655\n",
            "Iteration 34, loss = 0.65279113\n",
            "Iteration 35, loss = 0.65145861\n",
            "Iteration 36, loss = 0.65014005\n",
            "Iteration 37, loss = 0.64886977\n",
            "Iteration 38, loss = 0.64745512\n",
            "Iteration 39, loss = 0.64608045\n",
            "Iteration 40, loss = 0.64479803\n",
            "Iteration 41, loss = 0.64332743\n",
            "Iteration 42, loss = 0.64191602\n",
            "Iteration 43, loss = 0.64049443\n",
            "Iteration 44, loss = 0.63910440\n",
            "Iteration 45, loss = 0.63775095\n",
            "Iteration 46, loss = 0.63643092\n",
            "Iteration 47, loss = 0.63502632\n",
            "Iteration 48, loss = 0.63361504\n",
            "Iteration 49, loss = 0.63221307\n",
            "Iteration 50, loss = 0.63081191\n",
            "Iteration 51, loss = 0.62943018\n",
            "Iteration 52, loss = 0.62802355\n",
            "Iteration 53, loss = 0.62660563\n",
            "Iteration 54, loss = 0.62504413\n",
            "Iteration 55, loss = 0.62357597\n",
            "Iteration 56, loss = 0.62203861\n",
            "Iteration 57, loss = 0.62056461\n",
            "Iteration 58, loss = 0.61900895\n",
            "Iteration 59, loss = 0.61748222\n",
            "Iteration 60, loss = 0.61595047\n",
            "Iteration 61, loss = 0.61441222\n",
            "Iteration 62, loss = 0.61279187\n",
            "Iteration 63, loss = 0.61133758\n",
            "Iteration 64, loss = 0.60975173\n",
            "Iteration 65, loss = 0.60826608\n",
            "Iteration 66, loss = 0.60665775\n",
            "Iteration 67, loss = 0.60519153\n",
            "Iteration 68, loss = 0.60354429\n",
            "Iteration 69, loss = 0.60193952\n",
            "Iteration 70, loss = 0.60038454\n",
            "Iteration 71, loss = 0.59875342\n",
            "Iteration 72, loss = 0.59724172\n",
            "Iteration 73, loss = 0.59568940\n",
            "Iteration 74, loss = 0.59411755\n",
            "Iteration 75, loss = 0.59247741\n",
            "Iteration 76, loss = 0.59099312\n",
            "Iteration 77, loss = 0.58950039\n",
            "Iteration 78, loss = 0.58798962\n",
            "Iteration 79, loss = 0.58644068\n",
            "Iteration 80, loss = 0.58490456\n",
            "Iteration 81, loss = 0.58334570\n",
            "Iteration 82, loss = 0.58185184\n",
            "Iteration 83, loss = 0.58034850\n",
            "Iteration 84, loss = 0.57873498\n",
            "Iteration 85, loss = 0.57727232\n",
            "Iteration 86, loss = 0.57572464\n",
            "Iteration 87, loss = 0.57419009\n",
            "Iteration 88, loss = 0.57266928\n",
            "Iteration 89, loss = 0.57127440\n",
            "Iteration 90, loss = 0.56981067\n",
            "Iteration 91, loss = 0.56833181\n",
            "Iteration 92, loss = 0.56689415\n",
            "Iteration 93, loss = 0.56547485\n",
            "Iteration 94, loss = 0.56405420\n",
            "Iteration 95, loss = 0.56269036\n",
            "Iteration 96, loss = 0.56130434\n",
            "Iteration 97, loss = 0.55994609\n",
            "Iteration 98, loss = 0.55847735\n",
            "Iteration 99, loss = 0.55712999\n",
            "Iteration 100, loss = 0.55588485\n",
            "Iteration 101, loss = 0.55444629\n",
            "Iteration 102, loss = 0.55305484\n",
            "Iteration 103, loss = 0.55173634\n",
            "Iteration 104, loss = 0.55040718\n",
            "Iteration 105, loss = 0.54908623\n",
            "Iteration 106, loss = 0.54779022\n",
            "Iteration 107, loss = 0.54637598\n",
            "Iteration 108, loss = 0.54499591\n",
            "Iteration 109, loss = 0.54365887\n",
            "Iteration 110, loss = 0.54219262\n",
            "Iteration 111, loss = 0.54080409\n",
            "Iteration 112, loss = 0.53941023\n",
            "Iteration 113, loss = 0.53802546\n",
            "Iteration 114, loss = 0.53669375\n",
            "Iteration 115, loss = 0.53530705\n",
            "Iteration 116, loss = 0.53384714\n",
            "Iteration 117, loss = 0.53250136\n",
            "Iteration 118, loss = 0.53108438\n",
            "Iteration 119, loss = 0.52969609\n",
            "Iteration 120, loss = 0.52829965\n",
            "Iteration 121, loss = 0.52702573\n",
            "Iteration 122, loss = 0.52558707\n",
            "Iteration 123, loss = 0.52422791\n",
            "Iteration 124, loss = 0.52287870\n",
            "Iteration 125, loss = 0.52165300\n",
            "Iteration 126, loss = 0.52033111\n",
            "Iteration 127, loss = 0.51901550\n",
            "Iteration 128, loss = 0.51777467\n",
            "Iteration 129, loss = 0.51646278\n",
            "Iteration 130, loss = 0.51522600\n",
            "Iteration 131, loss = 0.51391277\n",
            "Iteration 132, loss = 0.51264701\n",
            "Iteration 133, loss = 0.51138166\n",
            "Iteration 134, loss = 0.51012193\n",
            "Iteration 135, loss = 0.50882427\n",
            "Iteration 136, loss = 0.50756554\n",
            "Iteration 137, loss = 0.50630796\n",
            "Iteration 138, loss = 0.50509099\n",
            "Iteration 139, loss = 0.50381698\n",
            "Iteration 140, loss = 0.50266805\n",
            "Iteration 141, loss = 0.50139717\n",
            "Iteration 142, loss = 0.50007585\n",
            "Iteration 143, loss = 0.49894433\n",
            "Iteration 144, loss = 0.49771623\n",
            "Iteration 145, loss = 0.49660244\n",
            "Iteration 146, loss = 0.49541280\n",
            "Iteration 147, loss = 0.49424033\n",
            "Iteration 148, loss = 0.49310363\n",
            "Iteration 149, loss = 0.49195481\n",
            "Iteration 150, loss = 0.49085094\n",
            "Iteration 151, loss = 0.48986791\n",
            "Iteration 152, loss = 0.48863106\n",
            "Iteration 153, loss = 0.48755010\n",
            "Iteration 154, loss = 0.48646138\n",
            "Iteration 155, loss = 0.48535707\n",
            "Iteration 156, loss = 0.48428317\n",
            "Iteration 157, loss = 0.48321350\n",
            "Iteration 158, loss = 0.48216948\n",
            "Iteration 159, loss = 0.48110691\n",
            "Iteration 160, loss = 0.48008091\n",
            "Iteration 161, loss = 0.47901442\n",
            "Iteration 162, loss = 0.47801466\n",
            "Iteration 163, loss = 0.47707978\n",
            "Iteration 164, loss = 0.47597242\n",
            "Iteration 165, loss = 0.47503223\n",
            "Iteration 166, loss = 0.47404161\n",
            "Iteration 167, loss = 0.47305255\n",
            "Iteration 168, loss = 0.47216254\n",
            "Iteration 169, loss = 0.47118064\n",
            "Iteration 170, loss = 0.47025015\n",
            "Iteration 171, loss = 0.46932089\n",
            "Iteration 172, loss = 0.46841021\n",
            "Iteration 173, loss = 0.46742683\n",
            "Iteration 174, loss = 0.46650759\n",
            "Iteration 175, loss = 0.46563740\n",
            "Iteration 176, loss = 0.46473193\n",
            "Iteration 177, loss = 0.46388042\n",
            "Iteration 178, loss = 0.46299942\n",
            "Iteration 179, loss = 0.46216704\n",
            "Iteration 180, loss = 0.46132760\n",
            "Iteration 181, loss = 0.46041923\n",
            "Iteration 182, loss = 0.45959316\n",
            "Iteration 183, loss = 0.45882760\n",
            "Iteration 184, loss = 0.45796585\n",
            "Iteration 185, loss = 0.45717177\n",
            "Iteration 186, loss = 0.45627386\n",
            "Iteration 187, loss = 0.45555115\n",
            "Iteration 188, loss = 0.45471792\n",
            "Iteration 189, loss = 0.45397741\n",
            "Iteration 190, loss = 0.45325848\n",
            "Iteration 191, loss = 0.45249158\n",
            "Iteration 192, loss = 0.45172759\n",
            "Iteration 193, loss = 0.45097589\n",
            "Iteration 194, loss = 0.45028413\n",
            "Iteration 195, loss = 0.44957016\n",
            "Iteration 196, loss = 0.44887277\n",
            "Iteration 197, loss = 0.44817100\n",
            "Iteration 198, loss = 0.44747904\n",
            "Iteration 199, loss = 0.44683509\n",
            "Iteration 200, loss = 0.44619128\n",
            "Iteration 201, loss = 0.44554872\n",
            "Iteration 202, loss = 0.44491219\n",
            "Iteration 203, loss = 0.44429224\n",
            "Iteration 204, loss = 0.44360764\n",
            "Iteration 205, loss = 0.44306632\n",
            "Iteration 206, loss = 0.44244926\n",
            "Iteration 207, loss = 0.44186095\n",
            "Iteration 208, loss = 0.44129836\n",
            "Iteration 209, loss = 0.44077840\n",
            "Iteration 210, loss = 0.44014398\n",
            "Iteration 211, loss = 0.43957975\n",
            "Iteration 212, loss = 0.43899426\n",
            "Iteration 213, loss = 0.43846406\n",
            "Iteration 214, loss = 0.43798486\n",
            "Iteration 215, loss = 0.43742513\n",
            "Iteration 216, loss = 0.43690507\n",
            "Iteration 217, loss = 0.43644291\n",
            "Iteration 218, loss = 0.43592701\n",
            "Iteration 219, loss = 0.43544055\n",
            "Iteration 220, loss = 0.43493354\n",
            "Iteration 221, loss = 0.43448900\n",
            "Iteration 222, loss = 0.43403955\n",
            "Iteration 223, loss = 0.43349304\n",
            "Iteration 224, loss = 0.43313868\n",
            "Iteration 225, loss = 0.43267771\n",
            "Iteration 226, loss = 0.43226457\n",
            "Iteration 227, loss = 0.43190560\n",
            "Iteration 228, loss = 0.43140698\n",
            "Iteration 229, loss = 0.43102583\n",
            "Iteration 230, loss = 0.43058993\n",
            "Iteration 231, loss = 0.43027971\n",
            "Iteration 232, loss = 0.42982543\n",
            "Iteration 233, loss = 0.42949565\n",
            "Iteration 234, loss = 0.42907401\n",
            "Iteration 235, loss = 0.42873265\n",
            "Iteration 236, loss = 0.42843124\n",
            "Iteration 237, loss = 0.42799594\n",
            "Iteration 238, loss = 0.42767539\n",
            "Iteration 239, loss = 0.42729510\n",
            "Iteration 240, loss = 0.42698767\n",
            "Iteration 241, loss = 0.42664784\n",
            "Iteration 242, loss = 0.42633936\n",
            "Iteration 243, loss = 0.42599563\n",
            "Iteration 244, loss = 0.42567545\n",
            "Iteration 245, loss = 0.42537337\n",
            "Iteration 246, loss = 0.42504698\n",
            "Iteration 247, loss = 0.42472648\n",
            "Iteration 248, loss = 0.42444412\n",
            "Iteration 249, loss = 0.42418261\n",
            "Iteration 250, loss = 0.42384238\n",
            "Iteration 251, loss = 0.42355618\n",
            "Iteration 252, loss = 0.42324035\n",
            "Iteration 253, loss = 0.42296336\n",
            "Iteration 254, loss = 0.42273104\n",
            "Iteration 255, loss = 0.42239509\n",
            "Iteration 256, loss = 0.42219839\n",
            "Iteration 257, loss = 0.42191599\n",
            "Iteration 258, loss = 0.42161044\n",
            "Iteration 259, loss = 0.42139319\n",
            "Iteration 260, loss = 0.42120867\n",
            "Iteration 261, loss = 0.42088843\n",
            "Iteration 262, loss = 0.42062382\n",
            "Iteration 263, loss = 0.42044495\n",
            "Iteration 264, loss = 0.42016865\n",
            "Iteration 265, loss = 0.41996423\n",
            "Iteration 266, loss = 0.41966387\n",
            "Iteration 267, loss = 0.41942695\n",
            "Iteration 268, loss = 0.41924000\n",
            "Iteration 269, loss = 0.41905326\n",
            "Iteration 270, loss = 0.41879869\n",
            "Iteration 271, loss = 0.41854889\n",
            "Iteration 272, loss = 0.41837171\n",
            "Iteration 273, loss = 0.41812696\n",
            "Iteration 274, loss = 0.41792099\n",
            "Iteration 275, loss = 0.41773810\n",
            "Iteration 276, loss = 0.41753250\n",
            "Iteration 277, loss = 0.41733221\n",
            "Iteration 278, loss = 0.41717911\n",
            "Iteration 279, loss = 0.41695621\n",
            "Iteration 280, loss = 0.41677905\n",
            "Iteration 281, loss = 0.41664709\n",
            "Iteration 282, loss = 0.41643229\n",
            "Iteration 283, loss = 0.41627530\n",
            "Iteration 284, loss = 0.41610869\n",
            "Iteration 285, loss = 0.41592374\n",
            "Iteration 286, loss = 0.41580545\n",
            "Iteration 287, loss = 0.41558745\n",
            "Iteration 288, loss = 0.41544424\n",
            "Iteration 289, loss = 0.41529035\n",
            "Iteration 290, loss = 0.41518089\n",
            "Iteration 291, loss = 0.41512465\n",
            "Iteration 292, loss = 0.41481667\n",
            "Iteration 293, loss = 0.41467708\n",
            "Iteration 294, loss = 0.41453556\n",
            "Iteration 295, loss = 0.41442410\n",
            "Iteration 296, loss = 0.41429475\n",
            "Iteration 297, loss = 0.41415380\n",
            "Iteration 298, loss = 0.41405185\n",
            "Iteration 299, loss = 0.41389750\n",
            "Iteration 300, loss = 0.41380732\n",
            "Iteration 301, loss = 0.41365328\n",
            "Iteration 302, loss = 0.41366093\n",
            "Iteration 303, loss = 0.41347150\n",
            "Iteration 304, loss = 0.41334837\n",
            "Iteration 305, loss = 0.41327888\n",
            "Iteration 306, loss = 0.41309255\n",
            "Iteration 307, loss = 0.41298656\n",
            "Iteration 308, loss = 0.41283383\n",
            "Iteration 309, loss = 0.41273841\n",
            "Iteration 310, loss = 0.41263576\n",
            "Iteration 311, loss = 0.41252926\n",
            "Iteration 312, loss = 0.41239005\n",
            "Iteration 313, loss = 0.41239868\n",
            "Iteration 314, loss = 0.41219773\n",
            "Iteration 315, loss = 0.41210210\n",
            "Iteration 316, loss = 0.41197572\n",
            "Iteration 317, loss = 0.41194720\n",
            "Iteration 318, loss = 0.41183972\n",
            "Iteration 319, loss = 0.41177534\n",
            "Iteration 320, loss = 0.41165467\n",
            "Iteration 321, loss = 0.41153623\n",
            "Iteration 322, loss = 0.41144290\n",
            "Iteration 323, loss = 0.41136278\n",
            "Iteration 324, loss = 0.41123441\n",
            "Iteration 325, loss = 0.41115830\n",
            "Iteration 326, loss = 0.41106202\n",
            "Iteration 327, loss = 0.41102501\n",
            "Iteration 328, loss = 0.41089273\n",
            "Iteration 329, loss = 0.41082226\n",
            "Iteration 330, loss = 0.41071455\n",
            "Iteration 331, loss = 0.41068555\n",
            "Iteration 332, loss = 0.41064065\n",
            "Iteration 333, loss = 0.41058815\n",
            "Iteration 334, loss = 0.41042460\n",
            "Iteration 335, loss = 0.41036663\n",
            "Iteration 336, loss = 0.41026318\n",
            "Iteration 337, loss = 0.41020290\n",
            "Iteration 338, loss = 0.41010134\n",
            "Iteration 339, loss = 0.41005362\n",
            "Iteration 340, loss = 0.40994747\n",
            "Iteration 341, loss = 0.40991571\n",
            "Iteration 342, loss = 0.40979443\n",
            "Iteration 343, loss = 0.40976327\n",
            "Iteration 344, loss = 0.40968543\n",
            "Iteration 345, loss = 0.40960111\n",
            "Iteration 346, loss = 0.40957369\n",
            "Iteration 347, loss = 0.40953309\n",
            "Iteration 348, loss = 0.40943039\n",
            "Iteration 349, loss = 0.40937629\n",
            "Iteration 350, loss = 0.40934556\n",
            "Iteration 351, loss = 0.40922981\n",
            "Iteration 352, loss = 0.40923577\n",
            "Iteration 353, loss = 0.40912045\n",
            "Iteration 354, loss = 0.40906869\n",
            "Iteration 355, loss = 0.40901704\n",
            "Iteration 356, loss = 0.40898123\n",
            "Iteration 357, loss = 0.40890485\n",
            "Iteration 358, loss = 0.40885915\n",
            "Iteration 359, loss = 0.40877587\n",
            "Iteration 360, loss = 0.40878808\n",
            "Iteration 361, loss = 0.40868710\n",
            "Iteration 362, loss = 0.40864186\n",
            "Iteration 363, loss = 0.40860231\n",
            "Iteration 364, loss = 0.40854525\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Melhores hiperparâmetros encontrados através do Grid Search:\n",
            "{'activation': 'relu', 'max_iter': 1000, 'solver': 'adam', 'tol': 0.0001}\n",
            "Melhor pontuação (acurácia) encontrada através do Grid Search: 0.8144736842105263\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ajuste do modelo MLP aos dados de treinamento\n",
        "rna_best_grid = MLPClassifier(**best_params_grid)\n",
        "rna_best_grid.fit(X_train_oversampled, y_train_oversampled)\n",
        "\n",
        "# Predições nos dados de teste usando o modelo com melhores hiperparâmetros encontrados pelo Grid Search\n",
        "pred_grid = rna_best_grid.predict(X_test)"
      ],
      "metadata": {
        "id": "lecB0Te4Pwlj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cálculo e impressão da acurácia nos dados de teste\n",
        "accuracy_grid = accuracy_score(y_test, pred_grid)\n",
        "print(\"Acurácia do modelo MLP com melhores hiperparâmetros pelo Grid Search:\", accuracy_grid)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GCK9EVTLP1cI",
        "outputId": "3369356d-859d-479c-962e-75874a5dbffd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acurácia do modelo MLP com melhores hiperparâmetros pelo Grid Search: 0.7649253731343284\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Matriz de confusão para o modelo com melhores hiperparâmetros pelo Grid Search\n",
        "print(\"Matriz de Confusão - Grid Search\")\n",
        "cm_rna_grid = confusion_matrix(y_test, pred_grid)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9UMAmSrcQE57",
        "outputId": "39285119-bbfb-4953-ea4c-940f837804d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matriz de Confusão - Grid Search\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtém a média das acurácias (10 folds) referente ao conjunto treino\n",
        "rna1_grid = g_results.loc[g_search.best_index_,'mean_test_score']"
      ],
      "metadata": {
        "id": "ovW2OrzRRu-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RandomSearch"
      ],
      "metadata": {
        "id": "X7tudlp4PdWh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Random Search\n",
        "param_dist = {\n",
        "    'activation': ['relu', 'tanh', 'logistic'],\n",
        "    'solver': ['sgd', 'adam'],\n",
        "    'max_iter': [1000, 2000],\n",
        "    'tol': [0.0001, 0.001],\n",
        "}\n",
        "\n",
        "r_search = RandomizedSearchCV(estimator=rna, param_distributions=param_dist, cv=10)\n",
        "r_search.fit(X_train_oversampled, y_train_oversampled)\n",
        "\n",
        "best_params_random = r_search.best_params_\n",
        "best_score_random = r_search.best_score_\n",
        "\n",
        "print(\"Melhores hiperparâmetros encontrados através do Random Search:\")\n",
        "print(best_params_random)\n",
        "print(\"Melhor pontuação (acurácia) encontrada através do Random Search:\", best_score_random)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DkjfVqD8Kke0",
        "outputId": "08365b53-a030-4723-b786-75c9a8b0f470"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mA saída de streaming foi truncada nas últimas 5000 linhas.\u001b[0m\n",
            "Iteration 283, loss = 0.59306934\n",
            "Iteration 284, loss = 0.59256638\n",
            "Iteration 285, loss = 0.59208252\n",
            "Iteration 286, loss = 0.59158224\n",
            "Iteration 287, loss = 0.59109537\n",
            "Iteration 288, loss = 0.59057768\n",
            "Iteration 289, loss = 0.59009046\n",
            "Iteration 290, loss = 0.58958766\n",
            "Iteration 291, loss = 0.58908327\n",
            "Iteration 292, loss = 0.58857236\n",
            "Iteration 293, loss = 0.58808006\n",
            "Iteration 294, loss = 0.58755871\n",
            "Iteration 295, loss = 0.58707761\n",
            "Iteration 296, loss = 0.58657174\n",
            "Iteration 297, loss = 0.58608529\n",
            "Iteration 298, loss = 0.58556143\n",
            "Iteration 299, loss = 0.58508170\n",
            "Iteration 300, loss = 0.58456542\n",
            "Iteration 301, loss = 0.58405704\n",
            "Iteration 302, loss = 0.58357538\n",
            "Iteration 303, loss = 0.58305688\n",
            "Iteration 304, loss = 0.58254906\n",
            "Iteration 305, loss = 0.58206087\n",
            "Iteration 306, loss = 0.58154784\n",
            "Iteration 307, loss = 0.58105251\n",
            "Iteration 308, loss = 0.58056094\n",
            "Iteration 309, loss = 0.58007386\n",
            "Iteration 310, loss = 0.57956306\n",
            "Iteration 311, loss = 0.57907900\n",
            "Iteration 312, loss = 0.57857697\n",
            "Iteration 313, loss = 0.57809355\n",
            "Iteration 314, loss = 0.57759195\n",
            "Iteration 315, loss = 0.57709229\n",
            "Iteration 316, loss = 0.57658502\n",
            "Iteration 317, loss = 0.57609356\n",
            "Iteration 318, loss = 0.57558498\n",
            "Iteration 319, loss = 0.57509074\n",
            "Iteration 320, loss = 0.57458099\n",
            "Iteration 321, loss = 0.57406349\n",
            "Iteration 322, loss = 0.57356092\n",
            "Iteration 323, loss = 0.57303645\n",
            "Iteration 324, loss = 0.57253657\n",
            "Iteration 325, loss = 0.57201630\n",
            "Iteration 326, loss = 0.57151607\n",
            "Iteration 327, loss = 0.57100876\n",
            "Iteration 328, loss = 0.57049315\n",
            "Iteration 329, loss = 0.56997916\n",
            "Iteration 330, loss = 0.56947159\n",
            "Iteration 331, loss = 0.56894914\n",
            "Iteration 332, loss = 0.56845010\n",
            "Iteration 333, loss = 0.56792941\n",
            "Iteration 334, loss = 0.56743026\n",
            "Iteration 335, loss = 0.56693296\n",
            "Iteration 336, loss = 0.56641890\n",
            "Iteration 337, loss = 0.56593234\n",
            "Iteration 338, loss = 0.56542040\n",
            "Iteration 339, loss = 0.56489904\n",
            "Iteration 340, loss = 0.56441787\n",
            "Iteration 341, loss = 0.56390749\n",
            "Iteration 342, loss = 0.56337964\n",
            "Iteration 343, loss = 0.56289275\n",
            "Iteration 344, loss = 0.56239597\n",
            "Iteration 345, loss = 0.56186972\n",
            "Iteration 346, loss = 0.56135767\n",
            "Iteration 347, loss = 0.56086590\n",
            "Iteration 348, loss = 0.56035156\n",
            "Iteration 349, loss = 0.55984878\n",
            "Iteration 350, loss = 0.55935093\n",
            "Iteration 351, loss = 0.55885184\n",
            "Iteration 352, loss = 0.55836372\n",
            "Iteration 353, loss = 0.55788930\n",
            "Iteration 354, loss = 0.55739155\n",
            "Iteration 355, loss = 0.55689925\n",
            "Iteration 356, loss = 0.55641482\n",
            "Iteration 357, loss = 0.55592342\n",
            "Iteration 358, loss = 0.55542826\n",
            "Iteration 359, loss = 0.55492423\n",
            "Iteration 360, loss = 0.55442673\n",
            "Iteration 361, loss = 0.55396063\n",
            "Iteration 362, loss = 0.55345504\n",
            "Iteration 363, loss = 0.55297252\n",
            "Iteration 364, loss = 0.55246349\n",
            "Iteration 365, loss = 0.55198263\n",
            "Iteration 366, loss = 0.55148531\n",
            "Iteration 367, loss = 0.55098493\n",
            "Iteration 368, loss = 0.55048313\n",
            "Iteration 369, loss = 0.54999883\n",
            "Iteration 370, loss = 0.54950777\n",
            "Iteration 371, loss = 0.54901939\n",
            "Iteration 372, loss = 0.54852748\n",
            "Iteration 373, loss = 0.54805060\n",
            "Iteration 374, loss = 0.54757018\n",
            "Iteration 375, loss = 0.54708040\n",
            "Iteration 376, loss = 0.54660493\n",
            "Iteration 377, loss = 0.54611734\n",
            "Iteration 378, loss = 0.54564096\n",
            "Iteration 379, loss = 0.54513749\n",
            "Iteration 380, loss = 0.54467389\n",
            "Iteration 381, loss = 0.54418122\n",
            "Iteration 382, loss = 0.54367699\n",
            "Iteration 383, loss = 0.54322900\n",
            "Iteration 384, loss = 0.54274577\n",
            "Iteration 385, loss = 0.54228869\n",
            "Iteration 386, loss = 0.54178785\n",
            "Iteration 387, loss = 0.54132990\n",
            "Iteration 388, loss = 0.54087044\n",
            "Iteration 389, loss = 0.54041093\n",
            "Iteration 390, loss = 0.53994484\n",
            "Iteration 391, loss = 0.53946900\n",
            "Iteration 392, loss = 0.53901792\n",
            "Iteration 393, loss = 0.53854504\n",
            "Iteration 394, loss = 0.53808095\n",
            "Iteration 395, loss = 0.53762227\n",
            "Iteration 396, loss = 0.53715319\n",
            "Iteration 397, loss = 0.53670150\n",
            "Iteration 398, loss = 0.53623375\n",
            "Iteration 399, loss = 0.53576305\n",
            "Iteration 400, loss = 0.53529435\n",
            "Iteration 401, loss = 0.53484361\n",
            "Iteration 402, loss = 0.53438741\n",
            "Iteration 403, loss = 0.53391785\n",
            "Iteration 404, loss = 0.53347622\n",
            "Iteration 405, loss = 0.53302268\n",
            "Iteration 406, loss = 0.53257681\n",
            "Iteration 407, loss = 0.53212550\n",
            "Iteration 408, loss = 0.53167135\n",
            "Iteration 409, loss = 0.53123816\n",
            "Iteration 410, loss = 0.53079168\n",
            "Iteration 411, loss = 0.53036518\n",
            "Iteration 412, loss = 0.52991755\n",
            "Iteration 413, loss = 0.52947641\n",
            "Iteration 414, loss = 0.52906333\n",
            "Iteration 415, loss = 0.52863914\n",
            "Iteration 416, loss = 0.52818905\n",
            "Iteration 417, loss = 0.52777335\n",
            "Iteration 418, loss = 0.52734350\n",
            "Iteration 419, loss = 0.52688574\n",
            "Iteration 420, loss = 0.52648004\n",
            "Iteration 421, loss = 0.52601258\n",
            "Iteration 422, loss = 0.52558331\n",
            "Iteration 423, loss = 0.52515425\n",
            "Iteration 424, loss = 0.52471617\n",
            "Iteration 425, loss = 0.52429371\n",
            "Iteration 426, loss = 0.52387572\n",
            "Iteration 427, loss = 0.52344139\n",
            "Iteration 428, loss = 0.52304053\n",
            "Iteration 429, loss = 0.52259637\n",
            "Iteration 430, loss = 0.52219204\n",
            "Iteration 431, loss = 0.52177195\n",
            "Iteration 432, loss = 0.52135804\n",
            "Iteration 433, loss = 0.52096787\n",
            "Iteration 434, loss = 0.52052889\n",
            "Iteration 435, loss = 0.52012565\n",
            "Iteration 436, loss = 0.51973210\n",
            "Iteration 437, loss = 0.51931130\n",
            "Iteration 438, loss = 0.51889569\n",
            "Iteration 439, loss = 0.51849148\n",
            "Iteration 440, loss = 0.51808235\n",
            "Iteration 441, loss = 0.51766592\n",
            "Iteration 442, loss = 0.51725925\n",
            "Iteration 443, loss = 0.51685373\n",
            "Iteration 444, loss = 0.51646080\n",
            "Iteration 445, loss = 0.51604227\n",
            "Iteration 446, loss = 0.51564871\n",
            "Iteration 447, loss = 0.51525768\n",
            "Iteration 448, loss = 0.51485281\n",
            "Iteration 449, loss = 0.51447277\n",
            "Iteration 450, loss = 0.51409787\n",
            "Iteration 451, loss = 0.51368884\n",
            "Iteration 452, loss = 0.51330336\n",
            "Iteration 453, loss = 0.51292055\n",
            "Iteration 454, loss = 0.51250809\n",
            "Iteration 455, loss = 0.51212950\n",
            "Iteration 456, loss = 0.51174233\n",
            "Iteration 457, loss = 0.51134502\n",
            "Iteration 458, loss = 0.51097861\n",
            "Iteration 459, loss = 0.51059367\n",
            "Iteration 460, loss = 0.51020297\n",
            "Iteration 461, loss = 0.50983864\n",
            "Iteration 462, loss = 0.50946332\n",
            "Iteration 463, loss = 0.50907606\n",
            "Iteration 464, loss = 0.50871832\n",
            "Iteration 465, loss = 0.50835218\n",
            "Iteration 466, loss = 0.50798093\n",
            "Iteration 467, loss = 0.50761626\n",
            "Iteration 468, loss = 0.50725418\n",
            "Iteration 469, loss = 0.50688455\n",
            "Iteration 470, loss = 0.50650372\n",
            "Iteration 471, loss = 0.50614404\n",
            "Iteration 472, loss = 0.50579373\n",
            "Iteration 473, loss = 0.50543142\n",
            "Iteration 474, loss = 0.50507203\n",
            "Iteration 475, loss = 0.50471489\n",
            "Iteration 476, loss = 0.50436385\n",
            "Iteration 477, loss = 0.50400555\n",
            "Iteration 478, loss = 0.50367258\n",
            "Iteration 479, loss = 0.50331663\n",
            "Iteration 480, loss = 0.50298289\n",
            "Iteration 481, loss = 0.50263683\n",
            "Iteration 482, loss = 0.50231917\n",
            "Iteration 483, loss = 0.50195560\n",
            "Iteration 484, loss = 0.50162208\n",
            "Iteration 485, loss = 0.50127929\n",
            "Iteration 486, loss = 0.50095214\n",
            "Iteration 487, loss = 0.50061964\n",
            "Iteration 488, loss = 0.50028185\n",
            "Iteration 489, loss = 0.49994497\n",
            "Iteration 490, loss = 0.49961149\n",
            "Iteration 491, loss = 0.49924971\n",
            "Iteration 492, loss = 0.49894165\n",
            "Iteration 493, loss = 0.49859439\n",
            "Iteration 494, loss = 0.49825574\n",
            "Iteration 495, loss = 0.49790881\n",
            "Iteration 496, loss = 0.49759648\n",
            "Iteration 497, loss = 0.49725156\n",
            "Iteration 498, loss = 0.49692435\n",
            "Iteration 499, loss = 0.49659455\n",
            "Iteration 500, loss = 0.49627795\n",
            "Iteration 501, loss = 0.49594731\n",
            "Iteration 502, loss = 0.49562918\n",
            "Iteration 503, loss = 0.49529525\n",
            "Iteration 504, loss = 0.49497492\n",
            "Iteration 505, loss = 0.49465280\n",
            "Iteration 506, loss = 0.49433840\n",
            "Iteration 507, loss = 0.49402335\n",
            "Iteration 508, loss = 0.49370531\n",
            "Iteration 509, loss = 0.49340075\n",
            "Iteration 510, loss = 0.49307968\n",
            "Iteration 511, loss = 0.49279084\n",
            "Iteration 512, loss = 0.49247283\n",
            "Iteration 513, loss = 0.49217412\n",
            "Iteration 514, loss = 0.49186565\n",
            "Iteration 515, loss = 0.49156956\n",
            "Iteration 516, loss = 0.49127717\n",
            "Iteration 517, loss = 0.49097377\n",
            "Iteration 518, loss = 0.49068430\n",
            "Iteration 519, loss = 0.49037000\n",
            "Iteration 520, loss = 0.49008132\n",
            "Iteration 521, loss = 0.48979427\n",
            "Iteration 522, loss = 0.48950150\n",
            "Iteration 523, loss = 0.48921555\n",
            "Iteration 524, loss = 0.48891751\n",
            "Iteration 525, loss = 0.48862826\n",
            "Iteration 526, loss = 0.48833751\n",
            "Iteration 527, loss = 0.48805337\n",
            "Iteration 528, loss = 0.48777331\n",
            "Iteration 529, loss = 0.48747499\n",
            "Iteration 530, loss = 0.48719913\n",
            "Iteration 531, loss = 0.48691584\n",
            "Iteration 532, loss = 0.48664055\n",
            "Iteration 533, loss = 0.48635802\n",
            "Iteration 534, loss = 0.48608802\n",
            "Iteration 535, loss = 0.48581837\n",
            "Iteration 536, loss = 0.48554595\n",
            "Iteration 537, loss = 0.48527705\n",
            "Iteration 538, loss = 0.48499908\n",
            "Iteration 539, loss = 0.48474471\n",
            "Iteration 540, loss = 0.48448254\n",
            "Iteration 541, loss = 0.48420847\n",
            "Iteration 542, loss = 0.48395342\n",
            "Iteration 543, loss = 0.48370120\n",
            "Iteration 544, loss = 0.48344478\n",
            "Iteration 545, loss = 0.48317612\n",
            "Iteration 546, loss = 0.48292875\n",
            "Iteration 547, loss = 0.48267329\n",
            "Iteration 548, loss = 0.48241806\n",
            "Iteration 549, loss = 0.48214874\n",
            "Iteration 550, loss = 0.48189765\n",
            "Iteration 551, loss = 0.48164077\n",
            "Iteration 552, loss = 0.48138453\n",
            "Iteration 553, loss = 0.48114340\n",
            "Iteration 554, loss = 0.48088167\n",
            "Iteration 555, loss = 0.48064976\n",
            "Iteration 556, loss = 0.48040668\n",
            "Iteration 557, loss = 0.48015726\n",
            "Iteration 558, loss = 0.47992198\n",
            "Iteration 559, loss = 0.47966363\n",
            "Iteration 560, loss = 0.47944670\n",
            "Iteration 561, loss = 0.47919999\n",
            "Iteration 562, loss = 0.47896525\n",
            "Iteration 563, loss = 0.47872734\n",
            "Iteration 564, loss = 0.47848100\n",
            "Iteration 565, loss = 0.47828128\n",
            "Iteration 566, loss = 0.47800329\n",
            "Iteration 567, loss = 0.47779256\n",
            "Iteration 568, loss = 0.47755495\n",
            "Iteration 569, loss = 0.47733229\n",
            "Iteration 570, loss = 0.47711054\n",
            "Iteration 571, loss = 0.47688650\n",
            "Iteration 572, loss = 0.47665233\n",
            "Iteration 573, loss = 0.47641682\n",
            "Iteration 574, loss = 0.47619253\n",
            "Iteration 575, loss = 0.47595723\n",
            "Iteration 576, loss = 0.47573700\n",
            "Iteration 577, loss = 0.47551083\n",
            "Iteration 578, loss = 0.47527629\n",
            "Iteration 579, loss = 0.47504964\n",
            "Iteration 580, loss = 0.47483631\n",
            "Iteration 581, loss = 0.47461042\n",
            "Iteration 582, loss = 0.47439095\n",
            "Iteration 583, loss = 0.47417711\n",
            "Iteration 584, loss = 0.47395825\n",
            "Iteration 585, loss = 0.47376306\n",
            "Iteration 586, loss = 0.47353970\n",
            "Iteration 587, loss = 0.47332630\n",
            "Iteration 588, loss = 0.47312710\n",
            "Iteration 589, loss = 0.47290825\n",
            "Iteration 590, loss = 0.47270886\n",
            "Iteration 591, loss = 0.47251691\n",
            "Iteration 592, loss = 0.47228976\n",
            "Iteration 593, loss = 0.47209253\n",
            "Iteration 594, loss = 0.47187913\n",
            "Iteration 595, loss = 0.47168395\n",
            "Iteration 596, loss = 0.47147855\n",
            "Iteration 597, loss = 0.47127349\n",
            "Iteration 598, loss = 0.47107115\n",
            "Iteration 599, loss = 0.47087060\n",
            "Iteration 600, loss = 0.47067741\n",
            "Iteration 601, loss = 0.47046925\n",
            "Iteration 602, loss = 0.47028665\n",
            "Iteration 603, loss = 0.47009069\n",
            "Iteration 604, loss = 0.46990265\n",
            "Iteration 605, loss = 0.46972928\n",
            "Iteration 606, loss = 0.46952166\n",
            "Iteration 607, loss = 0.46933663\n",
            "Iteration 608, loss = 0.46915084\n",
            "Iteration 609, loss = 0.46897875\n",
            "Iteration 610, loss = 0.46878498\n",
            "Iteration 611, loss = 0.46859995\n",
            "Iteration 612, loss = 0.46841605\n",
            "Iteration 613, loss = 0.46821696\n",
            "Iteration 614, loss = 0.46803712\n",
            "Iteration 615, loss = 0.46785101\n",
            "Iteration 616, loss = 0.46766616\n",
            "Iteration 617, loss = 0.46749437\n",
            "Iteration 618, loss = 0.46730045\n",
            "Iteration 619, loss = 0.46711577\n",
            "Iteration 620, loss = 0.46693265\n",
            "Iteration 621, loss = 0.46676332\n",
            "Iteration 622, loss = 0.46659124\n",
            "Iteration 623, loss = 0.46641748\n",
            "Iteration 624, loss = 0.46624764\n",
            "Iteration 625, loss = 0.46609001\n",
            "Iteration 626, loss = 0.46592317\n",
            "Iteration 627, loss = 0.46574485\n",
            "Iteration 628, loss = 0.46557802\n",
            "Iteration 629, loss = 0.46541129\n",
            "Iteration 630, loss = 0.46525449\n",
            "Iteration 631, loss = 0.46508587\n",
            "Iteration 632, loss = 0.46492505\n",
            "Iteration 633, loss = 0.46476401\n",
            "Iteration 634, loss = 0.46460847\n",
            "Iteration 635, loss = 0.46444718\n",
            "Iteration 636, loss = 0.46429656\n",
            "Iteration 637, loss = 0.46414006\n",
            "Iteration 638, loss = 0.46398278\n",
            "Iteration 639, loss = 0.46382964\n",
            "Iteration 640, loss = 0.46367324\n",
            "Iteration 641, loss = 0.46352198\n",
            "Iteration 642, loss = 0.46336552\n",
            "Iteration 643, loss = 0.46320835\n",
            "Iteration 644, loss = 0.46305511\n",
            "Iteration 645, loss = 0.46292191\n",
            "Iteration 646, loss = 0.46275931\n",
            "Iteration 647, loss = 0.46260246\n",
            "Iteration 648, loss = 0.46244907\n",
            "Iteration 649, loss = 0.46230520\n",
            "Iteration 650, loss = 0.46215727\n",
            "Iteration 651, loss = 0.46202100\n",
            "Iteration 652, loss = 0.46186404\n",
            "Iteration 653, loss = 0.46172868\n",
            "Iteration 654, loss = 0.46159599\n",
            "Iteration 655, loss = 0.46144323\n",
            "Iteration 656, loss = 0.46130149\n",
            "Iteration 657, loss = 0.46115894\n",
            "Iteration 658, loss = 0.46103163\n",
            "Iteration 659, loss = 0.46088975\n",
            "Iteration 660, loss = 0.46074445\n",
            "Iteration 661, loss = 0.46060121\n",
            "Iteration 662, loss = 0.46047201\n",
            "Iteration 663, loss = 0.46031189\n",
            "Iteration 664, loss = 0.46017758\n",
            "Iteration 665, loss = 0.46004048\n",
            "Iteration 666, loss = 0.45989758\n",
            "Iteration 667, loss = 0.45975639\n",
            "Iteration 668, loss = 0.45961562\n",
            "Iteration 669, loss = 0.45947380\n",
            "Iteration 670, loss = 0.45934298\n",
            "Iteration 671, loss = 0.45920554\n",
            "Iteration 672, loss = 0.45906985\n",
            "Iteration 673, loss = 0.45892841\n",
            "Iteration 674, loss = 0.45880474\n",
            "Iteration 675, loss = 0.45867355\n",
            "Iteration 676, loss = 0.45854726\n",
            "Iteration 677, loss = 0.45840966\n",
            "Iteration 678, loss = 0.45828794\n",
            "Iteration 679, loss = 0.45817047\n",
            "Iteration 680, loss = 0.45802984\n",
            "Iteration 681, loss = 0.45792552\n",
            "Iteration 682, loss = 0.45778141\n",
            "Iteration 683, loss = 0.45765253\n",
            "Iteration 684, loss = 0.45753480\n",
            "Iteration 685, loss = 0.45741519\n",
            "Iteration 686, loss = 0.45730307\n",
            "Iteration 687, loss = 0.45718568\n",
            "Iteration 688, loss = 0.45705736\n",
            "Iteration 689, loss = 0.45695103\n",
            "Iteration 690, loss = 0.45683133\n",
            "Iteration 691, loss = 0.45670581\n",
            "Iteration 692, loss = 0.45658217\n",
            "Iteration 693, loss = 0.45646849\n",
            "Iteration 694, loss = 0.45636192\n",
            "Iteration 695, loss = 0.45624732\n",
            "Iteration 696, loss = 0.45613256\n",
            "Iteration 697, loss = 0.45601364\n",
            "Iteration 698, loss = 0.45590321\n",
            "Iteration 699, loss = 0.45578803\n",
            "Iteration 700, loss = 0.45568193\n",
            "Iteration 701, loss = 0.45556524\n",
            "Iteration 702, loss = 0.45545391\n",
            "Iteration 703, loss = 0.45534912\n",
            "Iteration 704, loss = 0.45523777\n",
            "Iteration 705, loss = 0.45513274\n",
            "Iteration 706, loss = 0.45502170\n",
            "Iteration 707, loss = 0.45490676\n",
            "Iteration 708, loss = 0.45480576\n",
            "Iteration 709, loss = 0.45469202\n",
            "Iteration 710, loss = 0.45460636\n",
            "Iteration 711, loss = 0.45449025\n",
            "Iteration 712, loss = 0.45438896\n",
            "Iteration 713, loss = 0.45431200\n",
            "Iteration 714, loss = 0.45418119\n",
            "Iteration 715, loss = 0.45407653\n",
            "Iteration 716, loss = 0.45397490\n",
            "Iteration 717, loss = 0.45387133\n",
            "Iteration 718, loss = 0.45376407\n",
            "Iteration 719, loss = 0.45366457\n",
            "Iteration 720, loss = 0.45355404\n",
            "Iteration 721, loss = 0.45345041\n",
            "Iteration 722, loss = 0.45335384\n",
            "Iteration 723, loss = 0.45327213\n",
            "Iteration 724, loss = 0.45315966\n",
            "Iteration 725, loss = 0.45307888\n",
            "Iteration 726, loss = 0.45297017\n",
            "Iteration 727, loss = 0.45288909\n",
            "Iteration 728, loss = 0.45279176\n",
            "Iteration 729, loss = 0.45267746\n",
            "Iteration 730, loss = 0.45258597\n",
            "Iteration 731, loss = 0.45248764\n",
            "Iteration 732, loss = 0.45240118\n",
            "Iteration 733, loss = 0.45230363\n",
            "Iteration 734, loss = 0.45220536\n",
            "Iteration 735, loss = 0.45211874\n",
            "Iteration 736, loss = 0.45203030\n",
            "Iteration 737, loss = 0.45194593\n",
            "Iteration 738, loss = 0.45185578\n",
            "Iteration 739, loss = 0.45174816\n",
            "Iteration 740, loss = 0.45167044\n",
            "Iteration 741, loss = 0.45155940\n",
            "Iteration 742, loss = 0.45147632\n",
            "Iteration 743, loss = 0.45137712\n",
            "Iteration 744, loss = 0.45129033\n",
            "Iteration 745, loss = 0.45120298\n",
            "Iteration 746, loss = 0.45112370\n",
            "Iteration 747, loss = 0.45102616\n",
            "Iteration 748, loss = 0.45093510\n",
            "Iteration 749, loss = 0.45085085\n",
            "Iteration 750, loss = 0.45076300\n",
            "Iteration 751, loss = 0.45067648\n",
            "Iteration 752, loss = 0.45058195\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.71403990\n",
            "Iteration 2, loss = 0.71371261\n",
            "Iteration 3, loss = 0.71321790\n",
            "Iteration 4, loss = 0.71264655\n",
            "Iteration 5, loss = 0.71197629\n",
            "Iteration 6, loss = 0.71127822\n",
            "Iteration 7, loss = 0.71056045\n",
            "Iteration 8, loss = 0.70988870\n",
            "Iteration 9, loss = 0.70918572\n",
            "Iteration 10, loss = 0.70848001\n",
            "Iteration 11, loss = 0.70776767\n",
            "Iteration 12, loss = 0.70705868\n",
            "Iteration 13, loss = 0.70636818\n",
            "Iteration 14, loss = 0.70562503\n",
            "Iteration 15, loss = 0.70502670\n",
            "Iteration 16, loss = 0.70429623\n",
            "Iteration 17, loss = 0.70361435\n",
            "Iteration 18, loss = 0.70299550\n",
            "Iteration 19, loss = 0.70235960\n",
            "Iteration 20, loss = 0.70173638\n",
            "Iteration 21, loss = 0.70113845\n",
            "Iteration 22, loss = 0.70052588\n",
            "Iteration 23, loss = 0.69994199\n",
            "Iteration 24, loss = 0.69934792\n",
            "Iteration 25, loss = 0.69878252\n",
            "Iteration 26, loss = 0.69823460\n",
            "Iteration 27, loss = 0.69765111\n",
            "Iteration 28, loss = 0.69711244\n",
            "Iteration 29, loss = 0.69657663\n",
            "Iteration 30, loss = 0.69604337\n",
            "Iteration 31, loss = 0.69554206\n",
            "Iteration 32, loss = 0.69501247\n",
            "Iteration 33, loss = 0.69450691\n",
            "Iteration 34, loss = 0.69400618\n",
            "Iteration 35, loss = 0.69352634\n",
            "Iteration 36, loss = 0.69302693\n",
            "Iteration 37, loss = 0.69254010\n",
            "Iteration 38, loss = 0.69205537\n",
            "Iteration 39, loss = 0.69157381\n",
            "Iteration 40, loss = 0.69111322\n",
            "Iteration 41, loss = 0.69064487\n",
            "Iteration 42, loss = 0.69020829\n",
            "Iteration 43, loss = 0.68974692\n",
            "Iteration 44, loss = 0.68931969\n",
            "Iteration 45, loss = 0.68887479\n",
            "Iteration 46, loss = 0.68845097\n",
            "Iteration 47, loss = 0.68803049\n",
            "Iteration 48, loss = 0.68757845\n",
            "Iteration 49, loss = 0.68715405\n",
            "Iteration 50, loss = 0.68674565\n",
            "Iteration 51, loss = 0.68631740\n",
            "Iteration 52, loss = 0.68590908\n",
            "Iteration 53, loss = 0.68550996\n",
            "Iteration 54, loss = 0.68508468\n",
            "Iteration 55, loss = 0.68469734\n",
            "Iteration 56, loss = 0.68428829\n",
            "Iteration 57, loss = 0.68388629\n",
            "Iteration 58, loss = 0.68349510\n",
            "Iteration 59, loss = 0.68311311\n",
            "Iteration 60, loss = 0.68274298\n",
            "Iteration 61, loss = 0.68234581\n",
            "Iteration 62, loss = 0.68198420\n",
            "Iteration 63, loss = 0.68160943\n",
            "Iteration 64, loss = 0.68122432\n",
            "Iteration 65, loss = 0.68085087\n",
            "Iteration 66, loss = 0.68049722\n",
            "Iteration 67, loss = 0.68012493\n",
            "Iteration 68, loss = 0.67976096\n",
            "Iteration 69, loss = 0.67938708\n",
            "Iteration 70, loss = 0.67903550\n",
            "Iteration 71, loss = 0.67865917\n",
            "Iteration 72, loss = 0.67830396\n",
            "Iteration 73, loss = 0.67795166\n",
            "Iteration 74, loss = 0.67758051\n",
            "Iteration 75, loss = 0.67723506\n",
            "Iteration 76, loss = 0.67688459\n",
            "Iteration 77, loss = 0.67652732\n",
            "Iteration 78, loss = 0.67617457\n",
            "Iteration 79, loss = 0.67582465\n",
            "Iteration 80, loss = 0.67545092\n",
            "Iteration 81, loss = 0.67511203\n",
            "Iteration 82, loss = 0.67474877\n",
            "Iteration 83, loss = 0.67438624\n",
            "Iteration 84, loss = 0.67403975\n",
            "Iteration 85, loss = 0.67368273\n",
            "Iteration 86, loss = 0.67332592\n",
            "Iteration 87, loss = 0.67297579\n",
            "Iteration 88, loss = 0.67263911\n",
            "Iteration 89, loss = 0.67230034\n",
            "Iteration 90, loss = 0.67193456\n",
            "Iteration 91, loss = 0.67159124\n",
            "Iteration 92, loss = 0.67124411\n",
            "Iteration 93, loss = 0.67089573\n",
            "Iteration 94, loss = 0.67056128\n",
            "Iteration 95, loss = 0.67019445\n",
            "Iteration 96, loss = 0.66986143\n",
            "Iteration 97, loss = 0.66951438\n",
            "Iteration 98, loss = 0.66917011\n",
            "Iteration 99, loss = 0.66882395\n",
            "Iteration 100, loss = 0.66847541\n",
            "Iteration 101, loss = 0.66812367\n",
            "Iteration 102, loss = 0.66777187\n",
            "Iteration 103, loss = 0.66743211\n",
            "Iteration 104, loss = 0.66707042\n",
            "Iteration 105, loss = 0.66672716\n",
            "Iteration 106, loss = 0.66638531\n",
            "Iteration 107, loss = 0.66604281\n",
            "Iteration 108, loss = 0.66569412\n",
            "Iteration 109, loss = 0.66534557\n",
            "Iteration 110, loss = 0.66500054\n",
            "Iteration 111, loss = 0.66466729\n",
            "Iteration 112, loss = 0.66432454\n",
            "Iteration 113, loss = 0.66397666\n",
            "Iteration 114, loss = 0.66365025\n",
            "Iteration 115, loss = 0.66329773\n",
            "Iteration 116, loss = 0.66294160\n",
            "Iteration 117, loss = 0.66261148\n",
            "Iteration 118, loss = 0.66227139\n",
            "Iteration 119, loss = 0.66192978\n",
            "Iteration 120, loss = 0.66158700\n",
            "Iteration 121, loss = 0.66123646\n",
            "Iteration 122, loss = 0.66089239\n",
            "Iteration 123, loss = 0.66054966\n",
            "Iteration 124, loss = 0.66020015\n",
            "Iteration 125, loss = 0.65984417\n",
            "Iteration 126, loss = 0.65951026\n",
            "Iteration 127, loss = 0.65915720\n",
            "Iteration 128, loss = 0.65881055\n",
            "Iteration 129, loss = 0.65847728\n",
            "Iteration 130, loss = 0.65812388\n",
            "Iteration 131, loss = 0.65777253\n",
            "Iteration 132, loss = 0.65742484\n",
            "Iteration 133, loss = 0.65707614\n",
            "Iteration 134, loss = 0.65671967\n",
            "Iteration 135, loss = 0.65638317\n",
            "Iteration 136, loss = 0.65602099\n",
            "Iteration 137, loss = 0.65567433\n",
            "Iteration 138, loss = 0.65532718\n",
            "Iteration 139, loss = 0.65497582\n",
            "Iteration 140, loss = 0.65461221\n",
            "Iteration 141, loss = 0.65427249\n",
            "Iteration 142, loss = 0.65390597\n",
            "Iteration 143, loss = 0.65354956\n",
            "Iteration 144, loss = 0.65319617\n",
            "Iteration 145, loss = 0.65281022\n",
            "Iteration 146, loss = 0.65247338\n",
            "Iteration 147, loss = 0.65209949\n",
            "Iteration 148, loss = 0.65173394\n",
            "Iteration 149, loss = 0.65137954\n",
            "Iteration 150, loss = 0.65101471\n",
            "Iteration 151, loss = 0.65064756\n",
            "Iteration 152, loss = 0.65030275\n",
            "Iteration 153, loss = 0.64991543\n",
            "Iteration 154, loss = 0.64954409\n",
            "Iteration 155, loss = 0.64919032\n",
            "Iteration 156, loss = 0.64882232\n",
            "Iteration 157, loss = 0.64846117\n",
            "Iteration 158, loss = 0.64808996\n",
            "Iteration 159, loss = 0.64772586\n",
            "Iteration 160, loss = 0.64735828\n",
            "Iteration 161, loss = 0.64697444\n",
            "Iteration 162, loss = 0.64660629\n",
            "Iteration 163, loss = 0.64622290\n",
            "Iteration 164, loss = 0.64584015\n",
            "Iteration 165, loss = 0.64547199\n",
            "Iteration 166, loss = 0.64508321\n",
            "Iteration 167, loss = 0.64471232\n",
            "Iteration 168, loss = 0.64431431\n",
            "Iteration 169, loss = 0.64393770\n",
            "Iteration 170, loss = 0.64355359\n",
            "Iteration 171, loss = 0.64316402\n",
            "Iteration 172, loss = 0.64278318\n",
            "Iteration 173, loss = 0.64240145\n",
            "Iteration 174, loss = 0.64201975\n",
            "Iteration 175, loss = 0.64162162\n",
            "Iteration 176, loss = 0.64123138\n",
            "Iteration 177, loss = 0.64085976\n",
            "Iteration 178, loss = 0.64046116\n",
            "Iteration 179, loss = 0.64006587\n",
            "Iteration 180, loss = 0.63967554\n",
            "Iteration 181, loss = 0.63928073\n",
            "Iteration 182, loss = 0.63888697\n",
            "Iteration 183, loss = 0.63850163\n",
            "Iteration 184, loss = 0.63809683\n",
            "Iteration 185, loss = 0.63771534\n",
            "Iteration 186, loss = 0.63730544\n",
            "Iteration 187, loss = 0.63688817\n",
            "Iteration 188, loss = 0.63648900\n",
            "Iteration 189, loss = 0.63607564\n",
            "Iteration 190, loss = 0.63567407\n",
            "Iteration 191, loss = 0.63526460\n",
            "Iteration 192, loss = 0.63484399\n",
            "Iteration 193, loss = 0.63443535\n",
            "Iteration 194, loss = 0.63402429\n",
            "Iteration 195, loss = 0.63359761\n",
            "Iteration 196, loss = 0.63317282\n",
            "Iteration 197, loss = 0.63275341\n",
            "Iteration 198, loss = 0.63234643\n",
            "Iteration 199, loss = 0.63192305\n",
            "Iteration 200, loss = 0.63149716\n",
            "Iteration 201, loss = 0.63106762\n",
            "Iteration 202, loss = 0.63067181\n",
            "Iteration 203, loss = 0.63026071\n",
            "Iteration 204, loss = 0.62983750\n",
            "Iteration 205, loss = 0.62942629\n",
            "Iteration 206, loss = 0.62900994\n",
            "Iteration 207, loss = 0.62860241\n",
            "Iteration 208, loss = 0.62816176\n",
            "Iteration 209, loss = 0.62774628\n",
            "Iteration 210, loss = 0.62732218\n",
            "Iteration 211, loss = 0.62689771\n",
            "Iteration 212, loss = 0.62646471\n",
            "Iteration 213, loss = 0.62604256\n",
            "Iteration 214, loss = 0.62560837\n",
            "Iteration 215, loss = 0.62517482\n",
            "Iteration 216, loss = 0.62472289\n",
            "Iteration 217, loss = 0.62427996\n",
            "Iteration 218, loss = 0.62385239\n",
            "Iteration 219, loss = 0.62340089\n",
            "Iteration 220, loss = 0.62295802\n",
            "Iteration 221, loss = 0.62251763\n",
            "Iteration 222, loss = 0.62206881\n",
            "Iteration 223, loss = 0.62163170\n",
            "Iteration 224, loss = 0.62118791\n",
            "Iteration 225, loss = 0.62073227\n",
            "Iteration 226, loss = 0.62029374\n",
            "Iteration 227, loss = 0.61982704\n",
            "Iteration 228, loss = 0.61938027\n",
            "Iteration 229, loss = 0.61892717\n",
            "Iteration 230, loss = 0.61846845\n",
            "Iteration 231, loss = 0.61802659\n",
            "Iteration 232, loss = 0.61755683\n",
            "Iteration 233, loss = 0.61711254\n",
            "Iteration 234, loss = 0.61664034\n",
            "Iteration 235, loss = 0.61619429\n",
            "Iteration 236, loss = 0.61572561\n",
            "Iteration 237, loss = 0.61527531\n",
            "Iteration 238, loss = 0.61480069\n",
            "Iteration 239, loss = 0.61435906\n",
            "Iteration 240, loss = 0.61387359\n",
            "Iteration 241, loss = 0.61341591\n",
            "Iteration 242, loss = 0.61294468\n",
            "Iteration 243, loss = 0.61248113\n",
            "Iteration 244, loss = 0.61200648\n",
            "Iteration 245, loss = 0.61153303\n",
            "Iteration 246, loss = 0.61105985\n",
            "Iteration 247, loss = 0.61059035\n",
            "Iteration 248, loss = 0.61013738\n",
            "Iteration 249, loss = 0.60963578\n",
            "Iteration 250, loss = 0.60917909\n",
            "Iteration 251, loss = 0.60869106\n",
            "Iteration 252, loss = 0.60822401\n",
            "Iteration 253, loss = 0.60775445\n",
            "Iteration 254, loss = 0.60725946\n",
            "Iteration 255, loss = 0.60676853\n",
            "Iteration 256, loss = 0.60630474\n",
            "Iteration 257, loss = 0.60581154\n",
            "Iteration 258, loss = 0.60534392\n",
            "Iteration 259, loss = 0.60484599\n",
            "Iteration 260, loss = 0.60436765\n",
            "Iteration 261, loss = 0.60389150\n",
            "Iteration 262, loss = 0.60339909\n",
            "Iteration 263, loss = 0.60290963\n",
            "Iteration 264, loss = 0.60242039\n",
            "Iteration 265, loss = 0.60192870\n",
            "Iteration 266, loss = 0.60143537\n",
            "Iteration 267, loss = 0.60093029\n",
            "Iteration 268, loss = 0.60041783\n",
            "Iteration 269, loss = 0.59993616\n",
            "Iteration 270, loss = 0.59944090\n",
            "Iteration 271, loss = 0.59895334\n",
            "Iteration 272, loss = 0.59845869\n",
            "Iteration 273, loss = 0.59796705\n",
            "Iteration 274, loss = 0.59748043\n",
            "Iteration 275, loss = 0.59699237\n",
            "Iteration 276, loss = 0.59649583\n",
            "Iteration 277, loss = 0.59599850\n",
            "Iteration 278, loss = 0.59549364\n",
            "Iteration 279, loss = 0.59497840\n",
            "Iteration 280, loss = 0.59447762\n",
            "Iteration 281, loss = 0.59397143\n",
            "Iteration 282, loss = 0.59347333\n",
            "Iteration 283, loss = 0.59297030\n",
            "Iteration 284, loss = 0.59245138\n",
            "Iteration 285, loss = 0.59195465\n",
            "Iteration 286, loss = 0.59144134\n",
            "Iteration 287, loss = 0.59092766\n",
            "Iteration 288, loss = 0.59041446\n",
            "Iteration 289, loss = 0.58990470\n",
            "Iteration 290, loss = 0.58938423\n",
            "Iteration 291, loss = 0.58887761\n",
            "Iteration 292, loss = 0.58834319\n",
            "Iteration 293, loss = 0.58784211\n",
            "Iteration 294, loss = 0.58732144\n",
            "Iteration 295, loss = 0.58680050\n",
            "Iteration 296, loss = 0.58628254\n",
            "Iteration 297, loss = 0.58578027\n",
            "Iteration 298, loss = 0.58524871\n",
            "Iteration 299, loss = 0.58475231\n",
            "Iteration 300, loss = 0.58423510\n",
            "Iteration 301, loss = 0.58371972\n",
            "Iteration 302, loss = 0.58321789\n",
            "Iteration 303, loss = 0.58269148\n",
            "Iteration 304, loss = 0.58218659\n",
            "Iteration 305, loss = 0.58166810\n",
            "Iteration 306, loss = 0.58114507\n",
            "Iteration 307, loss = 0.58065416\n",
            "Iteration 308, loss = 0.58013416\n",
            "Iteration 309, loss = 0.57964456\n",
            "Iteration 310, loss = 0.57912360\n",
            "Iteration 311, loss = 0.57862741\n",
            "Iteration 312, loss = 0.57812094\n",
            "Iteration 313, loss = 0.57762403\n",
            "Iteration 314, loss = 0.57711197\n",
            "Iteration 315, loss = 0.57660488\n",
            "Iteration 316, loss = 0.57609891\n",
            "Iteration 317, loss = 0.57559086\n",
            "Iteration 318, loss = 0.57509532\n",
            "Iteration 319, loss = 0.57458536\n",
            "Iteration 320, loss = 0.57408205\n",
            "Iteration 321, loss = 0.57356083\n",
            "Iteration 322, loss = 0.57305457\n",
            "Iteration 323, loss = 0.57253900\n",
            "Iteration 324, loss = 0.57202022\n",
            "Iteration 325, loss = 0.57151075\n",
            "Iteration 326, loss = 0.57099225\n",
            "Iteration 327, loss = 0.57049032\n",
            "Iteration 328, loss = 0.56996637\n",
            "Iteration 329, loss = 0.56945525\n",
            "Iteration 330, loss = 0.56894954\n",
            "Iteration 331, loss = 0.56842217\n",
            "Iteration 332, loss = 0.56792641\n",
            "Iteration 333, loss = 0.56741316\n",
            "Iteration 334, loss = 0.56691649\n",
            "Iteration 335, loss = 0.56641089\n",
            "Iteration 336, loss = 0.56589781\n",
            "Iteration 337, loss = 0.56540714\n",
            "Iteration 338, loss = 0.56490287\n",
            "Iteration 339, loss = 0.56438037\n",
            "Iteration 340, loss = 0.56389445\n",
            "Iteration 341, loss = 0.56338748\n",
            "Iteration 342, loss = 0.56286658\n",
            "Iteration 343, loss = 0.56236738\n",
            "Iteration 344, loss = 0.56187109\n",
            "Iteration 345, loss = 0.56135456\n",
            "Iteration 346, loss = 0.56086748\n",
            "Iteration 347, loss = 0.56036591\n",
            "Iteration 348, loss = 0.55986620\n",
            "Iteration 349, loss = 0.55936010\n",
            "Iteration 350, loss = 0.55886698\n",
            "Iteration 351, loss = 0.55837920\n",
            "Iteration 352, loss = 0.55788504\n",
            "Iteration 353, loss = 0.55740109\n",
            "Iteration 354, loss = 0.55690009\n",
            "Iteration 355, loss = 0.55640672\n",
            "Iteration 356, loss = 0.55591657\n",
            "Iteration 357, loss = 0.55543306\n",
            "Iteration 358, loss = 0.55494059\n",
            "Iteration 359, loss = 0.55443993\n",
            "Iteration 360, loss = 0.55394235\n",
            "Iteration 361, loss = 0.55347736\n",
            "Iteration 362, loss = 0.55297817\n",
            "Iteration 363, loss = 0.55249039\n",
            "Iteration 364, loss = 0.55200244\n",
            "Iteration 365, loss = 0.55151453\n",
            "Iteration 366, loss = 0.55104351\n",
            "Iteration 367, loss = 0.55053832\n",
            "Iteration 368, loss = 0.55005105\n",
            "Iteration 369, loss = 0.54955646\n",
            "Iteration 370, loss = 0.54907676\n",
            "Iteration 371, loss = 0.54858700\n",
            "Iteration 372, loss = 0.54810778\n",
            "Iteration 373, loss = 0.54763424\n",
            "Iteration 374, loss = 0.54715155\n",
            "Iteration 375, loss = 0.54668349\n",
            "Iteration 376, loss = 0.54619735\n",
            "Iteration 377, loss = 0.54573509\n",
            "Iteration 378, loss = 0.54527317\n",
            "Iteration 379, loss = 0.54478908\n",
            "Iteration 380, loss = 0.54434003\n",
            "Iteration 381, loss = 0.54385710\n",
            "Iteration 382, loss = 0.54339385\n",
            "Iteration 383, loss = 0.54295063\n",
            "Iteration 384, loss = 0.54248854\n",
            "Iteration 385, loss = 0.54203895\n",
            "Iteration 386, loss = 0.54156673\n",
            "Iteration 387, loss = 0.54112023\n",
            "Iteration 388, loss = 0.54067238\n",
            "Iteration 389, loss = 0.54022752\n",
            "Iteration 390, loss = 0.53977885\n",
            "Iteration 391, loss = 0.53932902\n",
            "Iteration 392, loss = 0.53888563\n",
            "Iteration 393, loss = 0.53842521\n",
            "Iteration 394, loss = 0.53797304\n",
            "Iteration 395, loss = 0.53753933\n",
            "Iteration 396, loss = 0.53707715\n",
            "Iteration 397, loss = 0.53662970\n",
            "Iteration 398, loss = 0.53617506\n",
            "Iteration 399, loss = 0.53571374\n",
            "Iteration 400, loss = 0.53526874\n",
            "Iteration 401, loss = 0.53482256\n",
            "Iteration 402, loss = 0.53437671\n",
            "Iteration 403, loss = 0.53391893\n",
            "Iteration 404, loss = 0.53348840\n",
            "Iteration 405, loss = 0.53303506\n",
            "Iteration 406, loss = 0.53260156\n",
            "Iteration 407, loss = 0.53215809\n",
            "Iteration 408, loss = 0.53172221\n",
            "Iteration 409, loss = 0.53128448\n",
            "Iteration 410, loss = 0.53084257\n",
            "Iteration 411, loss = 0.53042722\n",
            "Iteration 412, loss = 0.52998402\n",
            "Iteration 413, loss = 0.52954828\n",
            "Iteration 414, loss = 0.52913737\n",
            "Iteration 415, loss = 0.52872933\n",
            "Iteration 416, loss = 0.52827395\n",
            "Iteration 417, loss = 0.52785437\n",
            "Iteration 418, loss = 0.52742576\n",
            "Iteration 419, loss = 0.52698733\n",
            "Iteration 420, loss = 0.52659302\n",
            "Iteration 421, loss = 0.52612725\n",
            "Iteration 422, loss = 0.52571043\n",
            "Iteration 423, loss = 0.52527863\n",
            "Iteration 424, loss = 0.52484672\n",
            "Iteration 425, loss = 0.52444318\n",
            "Iteration 426, loss = 0.52401954\n",
            "Iteration 427, loss = 0.52359560\n",
            "Iteration 428, loss = 0.52319678\n",
            "Iteration 429, loss = 0.52276993\n",
            "Iteration 430, loss = 0.52237030\n",
            "Iteration 431, loss = 0.52195111\n",
            "Iteration 432, loss = 0.52155639\n",
            "Iteration 433, loss = 0.52115814\n",
            "Iteration 434, loss = 0.52074569\n",
            "Iteration 435, loss = 0.52034775\n",
            "Iteration 436, loss = 0.51995041\n",
            "Iteration 437, loss = 0.51954303\n",
            "Iteration 438, loss = 0.51914549\n",
            "Iteration 439, loss = 0.51875546\n",
            "Iteration 440, loss = 0.51834620\n",
            "Iteration 441, loss = 0.51795068\n",
            "Iteration 442, loss = 0.51756408\n",
            "Iteration 443, loss = 0.51715524\n",
            "Iteration 444, loss = 0.51677299\n",
            "Iteration 445, loss = 0.51636059\n",
            "Iteration 446, loss = 0.51596678\n",
            "Iteration 447, loss = 0.51557840\n",
            "Iteration 448, loss = 0.51517698\n",
            "Iteration 449, loss = 0.51480186\n",
            "Iteration 450, loss = 0.51442199\n",
            "Iteration 451, loss = 0.51402293\n",
            "Iteration 452, loss = 0.51363487\n",
            "Iteration 453, loss = 0.51327236\n",
            "Iteration 454, loss = 0.51286854\n",
            "Iteration 455, loss = 0.51248458\n",
            "Iteration 456, loss = 0.51211691\n",
            "Iteration 457, loss = 0.51172456\n",
            "Iteration 458, loss = 0.51136439\n",
            "Iteration 459, loss = 0.51098952\n",
            "Iteration 460, loss = 0.51060195\n",
            "Iteration 461, loss = 0.51024899\n",
            "Iteration 462, loss = 0.50987248\n",
            "Iteration 463, loss = 0.50949652\n",
            "Iteration 464, loss = 0.50913900\n",
            "Iteration 465, loss = 0.50877605\n",
            "Iteration 466, loss = 0.50841157\n",
            "Iteration 467, loss = 0.50804633\n",
            "Iteration 468, loss = 0.50768733\n",
            "Iteration 469, loss = 0.50732269\n",
            "Iteration 470, loss = 0.50695031\n",
            "Iteration 471, loss = 0.50658514\n",
            "Iteration 472, loss = 0.50624251\n",
            "Iteration 473, loss = 0.50586941\n",
            "Iteration 474, loss = 0.50552319\n",
            "Iteration 475, loss = 0.50516688\n",
            "Iteration 476, loss = 0.50481941\n",
            "Iteration 477, loss = 0.50446428\n",
            "Iteration 478, loss = 0.50413608\n",
            "Iteration 479, loss = 0.50378979\n",
            "Iteration 480, loss = 0.50345986\n",
            "Iteration 481, loss = 0.50310469\n",
            "Iteration 482, loss = 0.50277510\n",
            "Iteration 483, loss = 0.50242640\n",
            "Iteration 484, loss = 0.50209589\n",
            "Iteration 485, loss = 0.50175527\n",
            "Iteration 486, loss = 0.50142465\n",
            "Iteration 487, loss = 0.50108932\n",
            "Iteration 488, loss = 0.50075151\n",
            "Iteration 489, loss = 0.50041357\n",
            "Iteration 490, loss = 0.50007952\n",
            "Iteration 491, loss = 0.49972247\n",
            "Iteration 492, loss = 0.49940759\n",
            "Iteration 493, loss = 0.49907403\n",
            "Iteration 494, loss = 0.49872223\n",
            "Iteration 495, loss = 0.49838651\n",
            "Iteration 496, loss = 0.49808256\n",
            "Iteration 497, loss = 0.49774426\n",
            "Iteration 498, loss = 0.49739558\n",
            "Iteration 499, loss = 0.49707875\n",
            "Iteration 500, loss = 0.49676363\n",
            "Iteration 501, loss = 0.49642852\n",
            "Iteration 502, loss = 0.49609966\n",
            "Iteration 503, loss = 0.49576889\n",
            "Iteration 504, loss = 0.49544600\n",
            "Iteration 505, loss = 0.49512648\n",
            "Iteration 506, loss = 0.49480808\n",
            "Iteration 507, loss = 0.49449487\n",
            "Iteration 508, loss = 0.49417255\n",
            "Iteration 509, loss = 0.49386255\n",
            "Iteration 510, loss = 0.49354334\n",
            "Iteration 511, loss = 0.49324963\n",
            "Iteration 512, loss = 0.49292973\n",
            "Iteration 513, loss = 0.49262099\n",
            "Iteration 514, loss = 0.49231428\n",
            "Iteration 515, loss = 0.49201446\n",
            "Iteration 516, loss = 0.49171804\n",
            "Iteration 517, loss = 0.49141562\n",
            "Iteration 518, loss = 0.49112265\n",
            "Iteration 519, loss = 0.49081919\n",
            "Iteration 520, loss = 0.49053098\n",
            "Iteration 521, loss = 0.49024600\n",
            "Iteration 522, loss = 0.48994317\n",
            "Iteration 523, loss = 0.48966768\n",
            "Iteration 524, loss = 0.48936491\n",
            "Iteration 525, loss = 0.48907686\n",
            "Iteration 526, loss = 0.48878941\n",
            "Iteration 527, loss = 0.48850158\n",
            "Iteration 528, loss = 0.48822829\n",
            "Iteration 529, loss = 0.48792077\n",
            "Iteration 530, loss = 0.48765306\n",
            "Iteration 531, loss = 0.48736297\n",
            "Iteration 532, loss = 0.48708485\n",
            "Iteration 533, loss = 0.48679811\n",
            "Iteration 534, loss = 0.48649566\n",
            "Iteration 535, loss = 0.48624147\n",
            "Iteration 536, loss = 0.48594946\n",
            "Iteration 537, loss = 0.48567371\n",
            "Iteration 538, loss = 0.48540628\n",
            "Iteration 539, loss = 0.48512929\n",
            "Iteration 540, loss = 0.48486196\n",
            "Iteration 541, loss = 0.48457283\n",
            "Iteration 542, loss = 0.48431527\n",
            "Iteration 543, loss = 0.48405993\n",
            "Iteration 544, loss = 0.48380261\n",
            "Iteration 545, loss = 0.48353460\n",
            "Iteration 546, loss = 0.48328627\n",
            "Iteration 547, loss = 0.48303443\n",
            "Iteration 548, loss = 0.48277572\n",
            "Iteration 549, loss = 0.48249781\n",
            "Iteration 550, loss = 0.48225960\n",
            "Iteration 551, loss = 0.48198613\n",
            "Iteration 552, loss = 0.48172342\n",
            "Iteration 553, loss = 0.48148784\n",
            "Iteration 554, loss = 0.48122453\n",
            "Iteration 555, loss = 0.48096936\n",
            "Iteration 556, loss = 0.48073300\n",
            "Iteration 557, loss = 0.48046406\n",
            "Iteration 558, loss = 0.48021375\n",
            "Iteration 559, loss = 0.47996330\n",
            "Iteration 560, loss = 0.47973955\n",
            "Iteration 561, loss = 0.47948362\n",
            "Iteration 562, loss = 0.47924987\n",
            "Iteration 563, loss = 0.47900374\n",
            "Iteration 564, loss = 0.47875199\n",
            "Iteration 565, loss = 0.47852818\n",
            "Iteration 566, loss = 0.47826750\n",
            "Iteration 567, loss = 0.47805009\n",
            "Iteration 568, loss = 0.47780644\n",
            "Iteration 569, loss = 0.47756635\n",
            "Iteration 570, loss = 0.47734363\n",
            "Iteration 571, loss = 0.47710768\n",
            "Iteration 572, loss = 0.47687054\n",
            "Iteration 573, loss = 0.47663660\n",
            "Iteration 574, loss = 0.47639376\n",
            "Iteration 575, loss = 0.47615541\n",
            "Iteration 576, loss = 0.47594137\n",
            "Iteration 577, loss = 0.47569401\n",
            "Iteration 578, loss = 0.47546151\n",
            "Iteration 579, loss = 0.47523384\n",
            "Iteration 580, loss = 0.47501094\n",
            "Iteration 581, loss = 0.47478012\n",
            "Iteration 582, loss = 0.47455091\n",
            "Iteration 583, loss = 0.47432511\n",
            "Iteration 584, loss = 0.47410002\n",
            "Iteration 585, loss = 0.47389098\n",
            "Iteration 586, loss = 0.47364578\n",
            "Iteration 587, loss = 0.47342896\n",
            "Iteration 588, loss = 0.47321710\n",
            "Iteration 589, loss = 0.47299851\n",
            "Iteration 590, loss = 0.47278624\n",
            "Iteration 591, loss = 0.47257912\n",
            "Iteration 592, loss = 0.47235115\n",
            "Iteration 593, loss = 0.47214755\n",
            "Iteration 594, loss = 0.47193704\n",
            "Iteration 595, loss = 0.47171406\n",
            "Iteration 596, loss = 0.47151730\n",
            "Iteration 597, loss = 0.47130944\n",
            "Iteration 598, loss = 0.47110490\n",
            "Iteration 599, loss = 0.47089197\n",
            "Iteration 600, loss = 0.47067673\n",
            "Iteration 601, loss = 0.47046660\n",
            "Iteration 602, loss = 0.47027881\n",
            "Iteration 603, loss = 0.47007255\n",
            "Iteration 604, loss = 0.46986484\n",
            "Iteration 605, loss = 0.46967374\n",
            "Iteration 606, loss = 0.46947759\n",
            "Iteration 607, loss = 0.46928941\n",
            "Iteration 608, loss = 0.46909399\n",
            "Iteration 609, loss = 0.46891387\n",
            "Iteration 610, loss = 0.46872322\n",
            "Iteration 611, loss = 0.46853849\n",
            "Iteration 612, loss = 0.46834795\n",
            "Iteration 613, loss = 0.46815293\n",
            "Iteration 614, loss = 0.46795915\n",
            "Iteration 615, loss = 0.46776941\n",
            "Iteration 616, loss = 0.46758520\n",
            "Iteration 617, loss = 0.46740892\n",
            "Iteration 618, loss = 0.46720637\n",
            "Iteration 619, loss = 0.46701204\n",
            "Iteration 620, loss = 0.46683397\n",
            "Iteration 621, loss = 0.46664502\n",
            "Iteration 622, loss = 0.46646574\n",
            "Iteration 623, loss = 0.46629110\n",
            "Iteration 624, loss = 0.46610873\n",
            "Iteration 625, loss = 0.46592934\n",
            "Iteration 626, loss = 0.46576723\n",
            "Iteration 627, loss = 0.46556933\n",
            "Iteration 628, loss = 0.46540257\n",
            "Iteration 629, loss = 0.46523061\n",
            "Iteration 630, loss = 0.46504649\n",
            "Iteration 631, loss = 0.46487141\n",
            "Iteration 632, loss = 0.46471506\n",
            "Iteration 633, loss = 0.46454432\n",
            "Iteration 634, loss = 0.46437264\n",
            "Iteration 635, loss = 0.46421325\n",
            "Iteration 636, loss = 0.46405253\n",
            "Iteration 637, loss = 0.46389155\n",
            "Iteration 638, loss = 0.46373383\n",
            "Iteration 639, loss = 0.46357384\n",
            "Iteration 640, loss = 0.46340805\n",
            "Iteration 641, loss = 0.46325804\n",
            "Iteration 642, loss = 0.46309193\n",
            "Iteration 643, loss = 0.46293504\n",
            "Iteration 644, loss = 0.46276108\n",
            "Iteration 645, loss = 0.46259822\n",
            "Iteration 646, loss = 0.46245045\n",
            "Iteration 647, loss = 0.46228635\n",
            "Iteration 648, loss = 0.46213211\n",
            "Iteration 649, loss = 0.46197838\n",
            "Iteration 650, loss = 0.46181696\n",
            "Iteration 651, loss = 0.46167493\n",
            "Iteration 652, loss = 0.46151818\n",
            "Iteration 653, loss = 0.46137049\n",
            "Iteration 654, loss = 0.46123836\n",
            "Iteration 655, loss = 0.46106254\n",
            "Iteration 656, loss = 0.46089912\n",
            "Iteration 657, loss = 0.46074369\n",
            "Iteration 658, loss = 0.46060970\n",
            "Iteration 659, loss = 0.46045162\n",
            "Iteration 660, loss = 0.46030448\n",
            "Iteration 661, loss = 0.46016242\n",
            "Iteration 662, loss = 0.46000796\n",
            "Iteration 663, loss = 0.45984469\n",
            "Iteration 664, loss = 0.45970237\n",
            "Iteration 665, loss = 0.45956149\n",
            "Iteration 666, loss = 0.45940593\n",
            "Iteration 667, loss = 0.45925522\n",
            "Iteration 668, loss = 0.45912726\n",
            "Iteration 669, loss = 0.45895400\n",
            "Iteration 670, loss = 0.45881564\n",
            "Iteration 671, loss = 0.45866533\n",
            "Iteration 672, loss = 0.45852461\n",
            "Iteration 673, loss = 0.45838245\n",
            "Iteration 674, loss = 0.45824800\n",
            "Iteration 675, loss = 0.45810525\n",
            "Iteration 676, loss = 0.45797336\n",
            "Iteration 677, loss = 0.45784745\n",
            "Iteration 678, loss = 0.45771276\n",
            "Iteration 679, loss = 0.45759974\n",
            "Iteration 680, loss = 0.45744795\n",
            "Iteration 681, loss = 0.45732469\n",
            "Iteration 682, loss = 0.45718474\n",
            "Iteration 683, loss = 0.45705089\n",
            "Iteration 684, loss = 0.45692034\n",
            "Iteration 685, loss = 0.45679406\n",
            "Iteration 686, loss = 0.45667963\n",
            "Iteration 687, loss = 0.45655876\n",
            "Iteration 688, loss = 0.45641241\n",
            "Iteration 689, loss = 0.45629542\n",
            "Iteration 690, loss = 0.45617723\n",
            "Iteration 691, loss = 0.45603541\n",
            "Iteration 692, loss = 0.45590633\n",
            "Iteration 693, loss = 0.45579634\n",
            "Iteration 694, loss = 0.45568000\n",
            "Iteration 695, loss = 0.45555851\n",
            "Iteration 696, loss = 0.45543575\n",
            "Iteration 697, loss = 0.45531271\n",
            "Iteration 698, loss = 0.45519600\n",
            "Iteration 699, loss = 0.45508039\n",
            "Iteration 700, loss = 0.45496295\n",
            "Iteration 701, loss = 0.45484693\n",
            "Iteration 702, loss = 0.45473754\n",
            "Iteration 703, loss = 0.45462273\n",
            "Iteration 704, loss = 0.45451777\n",
            "Iteration 705, loss = 0.45439639\n",
            "Iteration 706, loss = 0.45429516\n",
            "Iteration 707, loss = 0.45417470\n",
            "Iteration 708, loss = 0.45408193\n",
            "Iteration 709, loss = 0.45394894\n",
            "Iteration 710, loss = 0.45385513\n",
            "Iteration 711, loss = 0.45373747\n",
            "Iteration 712, loss = 0.45363225\n",
            "Iteration 713, loss = 0.45353883\n",
            "Iteration 714, loss = 0.45341226\n",
            "Iteration 715, loss = 0.45330636\n",
            "Iteration 716, loss = 0.45319227\n",
            "Iteration 717, loss = 0.45308728\n",
            "Iteration 718, loss = 0.45296660\n",
            "Iteration 719, loss = 0.45287399\n",
            "Iteration 720, loss = 0.45273792\n",
            "Iteration 721, loss = 0.45263026\n",
            "Iteration 722, loss = 0.45252193\n",
            "Iteration 723, loss = 0.45244507\n",
            "Iteration 724, loss = 0.45232560\n",
            "Iteration 725, loss = 0.45223526\n",
            "Iteration 726, loss = 0.45213196\n",
            "Iteration 727, loss = 0.45205067\n",
            "Iteration 728, loss = 0.45193736\n",
            "Iteration 729, loss = 0.45182689\n",
            "Iteration 730, loss = 0.45172535\n",
            "Iteration 731, loss = 0.45162995\n",
            "Iteration 732, loss = 0.45152754\n",
            "Iteration 733, loss = 0.45142403\n",
            "Iteration 734, loss = 0.45133520\n",
            "Iteration 735, loss = 0.45122646\n",
            "Iteration 736, loss = 0.45113478\n",
            "Iteration 737, loss = 0.45106408\n",
            "Iteration 738, loss = 0.45097184\n",
            "Iteration 739, loss = 0.45085406\n",
            "Iteration 740, loss = 0.45076135\n",
            "Iteration 741, loss = 0.45066810\n",
            "Iteration 742, loss = 0.45056651\n",
            "Iteration 743, loss = 0.45047050\n",
            "Iteration 744, loss = 0.45037555\n",
            "Iteration 745, loss = 0.45028711\n",
            "Iteration 746, loss = 0.45019667\n",
            "Iteration 747, loss = 0.45008743\n",
            "Iteration 748, loss = 0.45000880\n",
            "Iteration 749, loss = 0.44990876\n",
            "Iteration 750, loss = 0.44980811\n",
            "Iteration 751, loss = 0.44972976\n",
            "Iteration 752, loss = 0.44962647\n",
            "Iteration 753, loss = 0.44955287\n",
            "Iteration 754, loss = 0.44945439\n",
            "Iteration 755, loss = 0.44937560\n",
            "Iteration 756, loss = 0.44929204\n",
            "Iteration 757, loss = 0.44920544\n",
            "Iteration 758, loss = 0.44911435\n",
            "Iteration 759, loss = 0.44903386\n",
            "Iteration 760, loss = 0.44894951\n",
            "Iteration 761, loss = 0.44886625\n",
            "Iteration 762, loss = 0.44876930\n",
            "Iteration 763, loss = 0.44868666\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.71553040\n",
            "Iteration 2, loss = 0.71515941\n",
            "Iteration 3, loss = 0.71461811\n",
            "Iteration 4, loss = 0.71397261\n",
            "Iteration 5, loss = 0.71323328\n",
            "Iteration 6, loss = 0.71241959\n",
            "Iteration 7, loss = 0.71158672\n",
            "Iteration 8, loss = 0.71083252\n",
            "Iteration 9, loss = 0.71003981\n",
            "Iteration 10, loss = 0.70924890\n",
            "Iteration 11, loss = 0.70848394\n",
            "Iteration 12, loss = 0.70767405\n",
            "Iteration 13, loss = 0.70691270\n",
            "Iteration 14, loss = 0.70613112\n",
            "Iteration 15, loss = 0.70542799\n",
            "Iteration 16, loss = 0.70469754\n",
            "Iteration 17, loss = 0.70394287\n",
            "Iteration 18, loss = 0.70326795\n",
            "Iteration 19, loss = 0.70257550\n",
            "Iteration 20, loss = 0.70187223\n",
            "Iteration 21, loss = 0.70119610\n",
            "Iteration 22, loss = 0.70052608\n",
            "Iteration 23, loss = 0.69985173\n",
            "Iteration 24, loss = 0.69919758\n",
            "Iteration 25, loss = 0.69855821\n",
            "Iteration 26, loss = 0.69794712\n",
            "Iteration 27, loss = 0.69733605\n",
            "Iteration 28, loss = 0.69669806\n",
            "Iteration 29, loss = 0.69613116\n",
            "Iteration 30, loss = 0.69553432\n",
            "Iteration 31, loss = 0.69495984\n",
            "Iteration 32, loss = 0.69438365\n",
            "Iteration 33, loss = 0.69381598\n",
            "Iteration 34, loss = 0.69327482\n",
            "Iteration 35, loss = 0.69271314\n",
            "Iteration 36, loss = 0.69216598\n",
            "Iteration 37, loss = 0.69163064\n",
            "Iteration 38, loss = 0.69108194\n",
            "Iteration 39, loss = 0.69055327\n",
            "Iteration 40, loss = 0.69004365\n",
            "Iteration 41, loss = 0.68951510\n",
            "Iteration 42, loss = 0.68901408\n",
            "Iteration 43, loss = 0.68850228\n",
            "Iteration 44, loss = 0.68801818\n",
            "Iteration 45, loss = 0.68752113\n",
            "Iteration 46, loss = 0.68704228\n",
            "Iteration 47, loss = 0.68656967\n",
            "Iteration 48, loss = 0.68607708\n",
            "Iteration 49, loss = 0.68560415\n",
            "Iteration 50, loss = 0.68514242\n",
            "Iteration 51, loss = 0.68468499\n",
            "Iteration 52, loss = 0.68423654\n",
            "Iteration 53, loss = 0.68377902\n",
            "Iteration 54, loss = 0.68331830\n",
            "Iteration 55, loss = 0.68288617\n",
            "Iteration 56, loss = 0.68242796\n",
            "Iteration 57, loss = 0.68199040\n",
            "Iteration 58, loss = 0.68155411\n",
            "Iteration 59, loss = 0.68111945\n",
            "Iteration 60, loss = 0.68067760\n",
            "Iteration 61, loss = 0.68024847\n",
            "Iteration 62, loss = 0.67984183\n",
            "Iteration 63, loss = 0.67941955\n",
            "Iteration 64, loss = 0.67899333\n",
            "Iteration 65, loss = 0.67857798\n",
            "Iteration 66, loss = 0.67818498\n",
            "Iteration 67, loss = 0.67775804\n",
            "Iteration 68, loss = 0.67734159\n",
            "Iteration 69, loss = 0.67693124\n",
            "Iteration 70, loss = 0.67652526\n",
            "Iteration 71, loss = 0.67612703\n",
            "Iteration 72, loss = 0.67570400\n",
            "Iteration 73, loss = 0.67530284\n",
            "Iteration 74, loss = 0.67488953\n",
            "Iteration 75, loss = 0.67449087\n",
            "Iteration 76, loss = 0.67409244\n",
            "Iteration 77, loss = 0.67369727\n",
            "Iteration 78, loss = 0.67330992\n",
            "Iteration 79, loss = 0.67290768\n",
            "Iteration 80, loss = 0.67251357\n",
            "Iteration 81, loss = 0.67213597\n",
            "Iteration 82, loss = 0.67172534\n",
            "Iteration 83, loss = 0.67133900\n",
            "Iteration 84, loss = 0.67094600\n",
            "Iteration 85, loss = 0.67054221\n",
            "Iteration 86, loss = 0.67015051\n",
            "Iteration 87, loss = 0.66976742\n",
            "Iteration 88, loss = 0.66937792\n",
            "Iteration 89, loss = 0.66900811\n",
            "Iteration 90, loss = 0.66860074\n",
            "Iteration 91, loss = 0.66821674\n",
            "Iteration 92, loss = 0.66783924\n",
            "Iteration 93, loss = 0.66743579\n",
            "Iteration 94, loss = 0.66704672\n",
            "Iteration 95, loss = 0.66666244\n",
            "Iteration 96, loss = 0.66627835\n",
            "Iteration 97, loss = 0.66589993\n",
            "Iteration 98, loss = 0.66551999\n",
            "Iteration 99, loss = 0.66512777\n",
            "Iteration 100, loss = 0.66474562\n",
            "Iteration 101, loss = 0.66435643\n",
            "Iteration 102, loss = 0.66396716\n",
            "Iteration 103, loss = 0.66358745\n",
            "Iteration 104, loss = 0.66318746\n",
            "Iteration 105, loss = 0.66280168\n",
            "Iteration 106, loss = 0.66242640\n",
            "Iteration 107, loss = 0.66204537\n",
            "Iteration 108, loss = 0.66166031\n",
            "Iteration 109, loss = 0.66126598\n",
            "Iteration 110, loss = 0.66088706\n",
            "Iteration 111, loss = 0.66052295\n",
            "Iteration 112, loss = 0.66013006\n",
            "Iteration 113, loss = 0.65975370\n",
            "Iteration 114, loss = 0.65938713\n",
            "Iteration 115, loss = 0.65899389\n",
            "Iteration 116, loss = 0.65860784\n",
            "Iteration 117, loss = 0.65824006\n",
            "Iteration 118, loss = 0.65786172\n",
            "Iteration 119, loss = 0.65748863\n",
            "Iteration 120, loss = 0.65711081\n",
            "Iteration 121, loss = 0.65672428\n",
            "Iteration 122, loss = 0.65635257\n",
            "Iteration 123, loss = 0.65597060\n",
            "Iteration 124, loss = 0.65557598\n",
            "Iteration 125, loss = 0.65517979\n",
            "Iteration 126, loss = 0.65479765\n",
            "Iteration 127, loss = 0.65440934\n",
            "Iteration 128, loss = 0.65401258\n",
            "Iteration 129, loss = 0.65364495\n",
            "Iteration 130, loss = 0.65324707\n",
            "Iteration 131, loss = 0.65285331\n",
            "Iteration 132, loss = 0.65247455\n",
            "Iteration 133, loss = 0.65208713\n",
            "Iteration 134, loss = 0.65169789\n",
            "Iteration 135, loss = 0.65133076\n",
            "Iteration 136, loss = 0.65095314\n",
            "Iteration 137, loss = 0.65056749\n",
            "Iteration 138, loss = 0.65019208\n",
            "Iteration 139, loss = 0.64981036\n",
            "Iteration 140, loss = 0.64941594\n",
            "Iteration 141, loss = 0.64904250\n",
            "Iteration 142, loss = 0.64864099\n",
            "Iteration 143, loss = 0.64825357\n",
            "Iteration 144, loss = 0.64787406\n",
            "Iteration 145, loss = 0.64745952\n",
            "Iteration 146, loss = 0.64708159\n",
            "Iteration 147, loss = 0.64668652\n",
            "Iteration 148, loss = 0.64628668\n",
            "Iteration 149, loss = 0.64590055\n",
            "Iteration 150, loss = 0.64550608\n",
            "Iteration 151, loss = 0.64512088\n",
            "Iteration 152, loss = 0.64472246\n",
            "Iteration 153, loss = 0.64431379\n",
            "Iteration 154, loss = 0.64389929\n",
            "Iteration 155, loss = 0.64351188\n",
            "Iteration 156, loss = 0.64310594\n",
            "Iteration 157, loss = 0.64270782\n",
            "Iteration 158, loss = 0.64230167\n",
            "Iteration 159, loss = 0.64189725\n",
            "Iteration 160, loss = 0.64148797\n",
            "Iteration 161, loss = 0.64107824\n",
            "Iteration 162, loss = 0.64065975\n",
            "Iteration 163, loss = 0.64025202\n",
            "Iteration 164, loss = 0.63983813\n",
            "Iteration 165, loss = 0.63944577\n",
            "Iteration 166, loss = 0.63901290\n",
            "Iteration 167, loss = 0.63861677\n",
            "Iteration 168, loss = 0.63818801\n",
            "Iteration 169, loss = 0.63777704\n",
            "Iteration 170, loss = 0.63735554\n",
            "Iteration 171, loss = 0.63694181\n",
            "Iteration 172, loss = 0.63651120\n",
            "Iteration 173, loss = 0.63609734\n",
            "Iteration 174, loss = 0.63568653\n",
            "Iteration 175, loss = 0.63525767\n",
            "Iteration 176, loss = 0.63484057\n",
            "Iteration 177, loss = 0.63444421\n",
            "Iteration 178, loss = 0.63401250\n",
            "Iteration 179, loss = 0.63360261\n",
            "Iteration 180, loss = 0.63318935\n",
            "Iteration 181, loss = 0.63276164\n",
            "Iteration 182, loss = 0.63234026\n",
            "Iteration 183, loss = 0.63193037\n",
            "Iteration 184, loss = 0.63149761\n",
            "Iteration 185, loss = 0.63108060\n",
            "Iteration 186, loss = 0.63064468\n",
            "Iteration 187, loss = 0.63020196\n",
            "Iteration 188, loss = 0.62977521\n",
            "Iteration 189, loss = 0.62934078\n",
            "Iteration 190, loss = 0.62889307\n",
            "Iteration 191, loss = 0.62845928\n",
            "Iteration 192, loss = 0.62801085\n",
            "Iteration 193, loss = 0.62756102\n",
            "Iteration 194, loss = 0.62711964\n",
            "Iteration 195, loss = 0.62667057\n",
            "Iteration 196, loss = 0.62622349\n",
            "Iteration 197, loss = 0.62576145\n",
            "Iteration 198, loss = 0.62533008\n",
            "Iteration 199, loss = 0.62487359\n",
            "Iteration 200, loss = 0.62442691\n",
            "Iteration 201, loss = 0.62396539\n",
            "Iteration 202, loss = 0.62352633\n",
            "Iteration 203, loss = 0.62310613\n",
            "Iteration 204, loss = 0.62264145\n",
            "Iteration 205, loss = 0.62220544\n",
            "Iteration 206, loss = 0.62175134\n",
            "Iteration 207, loss = 0.62131610\n",
            "Iteration 208, loss = 0.62086107\n",
            "Iteration 209, loss = 0.62040424\n",
            "Iteration 210, loss = 0.61995293\n",
            "Iteration 211, loss = 0.61951392\n",
            "Iteration 212, loss = 0.61904904\n",
            "Iteration 213, loss = 0.61859079\n",
            "Iteration 214, loss = 0.61813058\n",
            "Iteration 215, loss = 0.61766648\n",
            "Iteration 216, loss = 0.61719085\n",
            "Iteration 217, loss = 0.61672262\n",
            "Iteration 218, loss = 0.61626888\n",
            "Iteration 219, loss = 0.61579001\n",
            "Iteration 220, loss = 0.61532812\n",
            "Iteration 221, loss = 0.61486245\n",
            "Iteration 222, loss = 0.61438745\n",
            "Iteration 223, loss = 0.61391302\n",
            "Iteration 224, loss = 0.61344433\n",
            "Iteration 225, loss = 0.61295809\n",
            "Iteration 226, loss = 0.61249997\n",
            "Iteration 227, loss = 0.61199982\n",
            "Iteration 228, loss = 0.61152354\n",
            "Iteration 229, loss = 0.61106534\n",
            "Iteration 230, loss = 0.61057496\n",
            "Iteration 231, loss = 0.61011352\n",
            "Iteration 232, loss = 0.60962542\n",
            "Iteration 233, loss = 0.60915782\n",
            "Iteration 234, loss = 0.60867586\n",
            "Iteration 235, loss = 0.60819284\n",
            "Iteration 236, loss = 0.60770859\n",
            "Iteration 237, loss = 0.60724594\n",
            "Iteration 238, loss = 0.60676066\n",
            "Iteration 239, loss = 0.60629073\n",
            "Iteration 240, loss = 0.60579489\n",
            "Iteration 241, loss = 0.60530562\n",
            "Iteration 242, loss = 0.60482361\n",
            "Iteration 243, loss = 0.60433420\n",
            "Iteration 244, loss = 0.60383864\n",
            "Iteration 245, loss = 0.60334265\n",
            "Iteration 246, loss = 0.60284723\n",
            "Iteration 247, loss = 0.60235434\n",
            "Iteration 248, loss = 0.60187940\n",
            "Iteration 249, loss = 0.60136097\n",
            "Iteration 250, loss = 0.60088984\n",
            "Iteration 251, loss = 0.60036788\n",
            "Iteration 252, loss = 0.59987845\n",
            "Iteration 253, loss = 0.59940326\n",
            "Iteration 254, loss = 0.59888667\n",
            "Iteration 255, loss = 0.59837948\n",
            "Iteration 256, loss = 0.59790000\n",
            "Iteration 257, loss = 0.59737705\n",
            "Iteration 258, loss = 0.59689770\n",
            "Iteration 259, loss = 0.59639610\n",
            "Iteration 260, loss = 0.59588956\n",
            "Iteration 261, loss = 0.59540102\n",
            "Iteration 262, loss = 0.59489600\n",
            "Iteration 263, loss = 0.59439899\n",
            "Iteration 264, loss = 0.59388405\n",
            "Iteration 265, loss = 0.59338121\n",
            "Iteration 266, loss = 0.59287778\n",
            "Iteration 267, loss = 0.59235477\n",
            "Iteration 268, loss = 0.59182585\n",
            "Iteration 269, loss = 0.59133531\n",
            "Iteration 270, loss = 0.59080473\n",
            "Iteration 271, loss = 0.59031349\n",
            "Iteration 272, loss = 0.58979791\n",
            "Iteration 273, loss = 0.58929254\n",
            "Iteration 274, loss = 0.58880163\n",
            "Iteration 275, loss = 0.58828722\n",
            "Iteration 276, loss = 0.58778692\n",
            "Iteration 277, loss = 0.58727414\n",
            "Iteration 278, loss = 0.58677215\n",
            "Iteration 279, loss = 0.58625634\n",
            "Iteration 280, loss = 0.58575368\n",
            "Iteration 281, loss = 0.58523073\n",
            "Iteration 282, loss = 0.58472445\n",
            "Iteration 283, loss = 0.58422050\n",
            "Iteration 284, loss = 0.58369820\n",
            "Iteration 285, loss = 0.58318301\n",
            "Iteration 286, loss = 0.58266599\n",
            "Iteration 287, loss = 0.58213508\n",
            "Iteration 288, loss = 0.58161682\n",
            "Iteration 289, loss = 0.58108309\n",
            "Iteration 290, loss = 0.58057404\n",
            "Iteration 291, loss = 0.58004380\n",
            "Iteration 292, loss = 0.57950572\n",
            "Iteration 293, loss = 0.57899303\n",
            "Iteration 294, loss = 0.57845594\n",
            "Iteration 295, loss = 0.57792528\n",
            "Iteration 296, loss = 0.57738268\n",
            "Iteration 297, loss = 0.57686501\n",
            "Iteration 298, loss = 0.57631440\n",
            "Iteration 299, loss = 0.57579722\n",
            "Iteration 300, loss = 0.57527385\n",
            "Iteration 301, loss = 0.57473821\n",
            "Iteration 302, loss = 0.57422570\n",
            "Iteration 303, loss = 0.57369097\n",
            "Iteration 304, loss = 0.57315474\n",
            "Iteration 305, loss = 0.57263894\n",
            "Iteration 306, loss = 0.57210096\n",
            "Iteration 307, loss = 0.57158617\n",
            "Iteration 308, loss = 0.57105574\n",
            "Iteration 309, loss = 0.57054475\n",
            "Iteration 310, loss = 0.57002877\n",
            "Iteration 311, loss = 0.56950736\n",
            "Iteration 312, loss = 0.56899211\n",
            "Iteration 313, loss = 0.56848528\n",
            "Iteration 314, loss = 0.56796358\n",
            "Iteration 315, loss = 0.56745036\n",
            "Iteration 316, loss = 0.56691854\n",
            "Iteration 317, loss = 0.56640197\n",
            "Iteration 318, loss = 0.56589050\n",
            "Iteration 319, loss = 0.56536032\n",
            "Iteration 320, loss = 0.56485764\n",
            "Iteration 321, loss = 0.56432070\n",
            "Iteration 322, loss = 0.56380978\n",
            "Iteration 323, loss = 0.56327524\n",
            "Iteration 324, loss = 0.56275060\n",
            "Iteration 325, loss = 0.56222103\n",
            "Iteration 326, loss = 0.56170252\n",
            "Iteration 327, loss = 0.56117455\n",
            "Iteration 328, loss = 0.56064264\n",
            "Iteration 329, loss = 0.56012075\n",
            "Iteration 330, loss = 0.55959749\n",
            "Iteration 331, loss = 0.55907314\n",
            "Iteration 332, loss = 0.55854419\n",
            "Iteration 333, loss = 0.55802600\n",
            "Iteration 334, loss = 0.55749639\n",
            "Iteration 335, loss = 0.55698093\n",
            "Iteration 336, loss = 0.55645581\n",
            "Iteration 337, loss = 0.55594162\n",
            "Iteration 338, loss = 0.55542321\n",
            "Iteration 339, loss = 0.55489281\n",
            "Iteration 340, loss = 0.55439620\n",
            "Iteration 341, loss = 0.55388454\n",
            "Iteration 342, loss = 0.55335613\n",
            "Iteration 343, loss = 0.55284863\n",
            "Iteration 344, loss = 0.55234161\n",
            "Iteration 345, loss = 0.55180607\n",
            "Iteration 346, loss = 0.55129344\n",
            "Iteration 347, loss = 0.55078290\n",
            "Iteration 348, loss = 0.55024888\n",
            "Iteration 349, loss = 0.54972571\n",
            "Iteration 350, loss = 0.54919862\n",
            "Iteration 351, loss = 0.54870177\n",
            "Iteration 352, loss = 0.54817800\n",
            "Iteration 353, loss = 0.54768927\n",
            "Iteration 354, loss = 0.54716113\n",
            "Iteration 355, loss = 0.54665038\n",
            "Iteration 356, loss = 0.54614684\n",
            "Iteration 357, loss = 0.54564828\n",
            "Iteration 358, loss = 0.54512150\n",
            "Iteration 359, loss = 0.54460636\n",
            "Iteration 360, loss = 0.54408749\n",
            "Iteration 361, loss = 0.54359376\n",
            "Iteration 362, loss = 0.54309126\n",
            "Iteration 363, loss = 0.54258586\n",
            "Iteration 364, loss = 0.54207178\n",
            "Iteration 365, loss = 0.54155998\n",
            "Iteration 366, loss = 0.54107638\n",
            "Iteration 367, loss = 0.54057515\n",
            "Iteration 368, loss = 0.54006014\n",
            "Iteration 369, loss = 0.53955316\n",
            "Iteration 370, loss = 0.53905757\n",
            "Iteration 371, loss = 0.53855659\n",
            "Iteration 372, loss = 0.53806838\n",
            "Iteration 373, loss = 0.53757504\n",
            "Iteration 374, loss = 0.53708558\n",
            "Iteration 375, loss = 0.53659612\n",
            "Iteration 376, loss = 0.53609396\n",
            "Iteration 377, loss = 0.53562096\n",
            "Iteration 378, loss = 0.53511825\n",
            "Iteration 379, loss = 0.53464087\n",
            "Iteration 380, loss = 0.53417489\n",
            "Iteration 381, loss = 0.53367954\n",
            "Iteration 382, loss = 0.53319592\n",
            "Iteration 383, loss = 0.53273208\n",
            "Iteration 384, loss = 0.53224587\n",
            "Iteration 385, loss = 0.53176703\n",
            "Iteration 386, loss = 0.53128445\n",
            "Iteration 387, loss = 0.53082500\n",
            "Iteration 388, loss = 0.53035856\n",
            "Iteration 389, loss = 0.52990152\n",
            "Iteration 390, loss = 0.52943144\n",
            "Iteration 391, loss = 0.52896788\n",
            "Iteration 392, loss = 0.52850624\n",
            "Iteration 393, loss = 0.52803261\n",
            "Iteration 394, loss = 0.52755787\n",
            "Iteration 395, loss = 0.52710875\n",
            "Iteration 396, loss = 0.52663331\n",
            "Iteration 397, loss = 0.52616052\n",
            "Iteration 398, loss = 0.52569994\n",
            "Iteration 399, loss = 0.52523122\n",
            "Iteration 400, loss = 0.52476897\n",
            "Iteration 401, loss = 0.52431699\n",
            "Iteration 402, loss = 0.52385603\n",
            "Iteration 403, loss = 0.52339962\n",
            "Iteration 404, loss = 0.52294101\n",
            "Iteration 405, loss = 0.52248774\n",
            "Iteration 406, loss = 0.52205560\n",
            "Iteration 407, loss = 0.52159348\n",
            "Iteration 408, loss = 0.52114604\n",
            "Iteration 409, loss = 0.52069148\n",
            "Iteration 410, loss = 0.52024635\n",
            "Iteration 411, loss = 0.51981400\n",
            "Iteration 412, loss = 0.51935509\n",
            "Iteration 413, loss = 0.51890627\n",
            "Iteration 414, loss = 0.51848493\n",
            "Iteration 415, loss = 0.51803924\n",
            "Iteration 416, loss = 0.51758577\n",
            "Iteration 417, loss = 0.51716471\n",
            "Iteration 418, loss = 0.51671234\n",
            "Iteration 419, loss = 0.51626655\n",
            "Iteration 420, loss = 0.51586530\n",
            "Iteration 421, loss = 0.51537774\n",
            "Iteration 422, loss = 0.51496343\n",
            "Iteration 423, loss = 0.51452028\n",
            "Iteration 424, loss = 0.51408575\n",
            "Iteration 425, loss = 0.51367179\n",
            "Iteration 426, loss = 0.51324209\n",
            "Iteration 427, loss = 0.51281139\n",
            "Iteration 428, loss = 0.51240595\n",
            "Iteration 429, loss = 0.51196610\n",
            "Iteration 430, loss = 0.51156331\n",
            "Iteration 431, loss = 0.51114074\n",
            "Iteration 432, loss = 0.51073981\n",
            "Iteration 433, loss = 0.51033055\n",
            "Iteration 434, loss = 0.50990630\n",
            "Iteration 435, loss = 0.50950808\n",
            "Iteration 436, loss = 0.50909774\n",
            "Iteration 437, loss = 0.50868484\n",
            "Iteration 438, loss = 0.50827405\n",
            "Iteration 439, loss = 0.50787531\n",
            "Iteration 440, loss = 0.50745941\n",
            "Iteration 441, loss = 0.50704377\n",
            "Iteration 442, loss = 0.50664332\n",
            "Iteration 443, loss = 0.50623957\n",
            "Iteration 444, loss = 0.50584040\n",
            "Iteration 445, loss = 0.50541167\n",
            "Iteration 446, loss = 0.50501527\n",
            "Iteration 447, loss = 0.50462708\n",
            "Iteration 448, loss = 0.50421637\n",
            "Iteration 449, loss = 0.50384187\n",
            "Iteration 450, loss = 0.50344894\n",
            "Iteration 451, loss = 0.50307936\n",
            "Iteration 452, loss = 0.50265629\n",
            "Iteration 453, loss = 0.50229822\n",
            "Iteration 454, loss = 0.50188577\n",
            "Iteration 455, loss = 0.50148885\n",
            "Iteration 456, loss = 0.50111522\n",
            "Iteration 457, loss = 0.50072216\n",
            "Iteration 458, loss = 0.50034670\n",
            "Iteration 459, loss = 0.49997911\n",
            "Iteration 460, loss = 0.49957533\n",
            "Iteration 461, loss = 0.49921305\n",
            "Iteration 462, loss = 0.49883161\n",
            "Iteration 463, loss = 0.49846312\n",
            "Iteration 464, loss = 0.49810220\n",
            "Iteration 465, loss = 0.49772455\n",
            "Iteration 466, loss = 0.49736233\n",
            "Iteration 467, loss = 0.49699740\n",
            "Iteration 468, loss = 0.49663534\n",
            "Iteration 469, loss = 0.49626293\n",
            "Iteration 470, loss = 0.49590583\n",
            "Iteration 471, loss = 0.49554506\n",
            "Iteration 472, loss = 0.49519600\n",
            "Iteration 473, loss = 0.49483042\n",
            "Iteration 474, loss = 0.49447251\n",
            "Iteration 475, loss = 0.49412785\n",
            "Iteration 476, loss = 0.49377474\n",
            "Iteration 477, loss = 0.49340713\n",
            "Iteration 478, loss = 0.49307800\n",
            "Iteration 479, loss = 0.49271721\n",
            "Iteration 480, loss = 0.49237031\n",
            "Iteration 481, loss = 0.49202541\n",
            "Iteration 482, loss = 0.49168562\n",
            "Iteration 483, loss = 0.49133024\n",
            "Iteration 484, loss = 0.49099456\n",
            "Iteration 485, loss = 0.49065547\n",
            "Iteration 486, loss = 0.49031075\n",
            "Iteration 487, loss = 0.48998714\n",
            "Iteration 488, loss = 0.48964122\n",
            "Iteration 489, loss = 0.48929830\n",
            "Iteration 490, loss = 0.48896210\n",
            "Iteration 491, loss = 0.48859476\n",
            "Iteration 492, loss = 0.48826784\n",
            "Iteration 493, loss = 0.48793642\n",
            "Iteration 494, loss = 0.48759853\n",
            "Iteration 495, loss = 0.48725868\n",
            "Iteration 496, loss = 0.48695083\n",
            "Iteration 497, loss = 0.48660213\n",
            "Iteration 498, loss = 0.48629557\n",
            "Iteration 499, loss = 0.48595112\n",
            "Iteration 500, loss = 0.48563947\n",
            "Iteration 501, loss = 0.48531017\n",
            "Iteration 502, loss = 0.48498170\n",
            "Iteration 503, loss = 0.48467247\n",
            "Iteration 504, loss = 0.48434270\n",
            "Iteration 505, loss = 0.48401998\n",
            "Iteration 506, loss = 0.48370361\n",
            "Iteration 507, loss = 0.48339135\n",
            "Iteration 508, loss = 0.48308567\n",
            "Iteration 509, loss = 0.48275814\n",
            "Iteration 510, loss = 0.48242769\n",
            "Iteration 511, loss = 0.48212917\n",
            "Iteration 512, loss = 0.48182143\n",
            "Iteration 513, loss = 0.48151212\n",
            "Iteration 514, loss = 0.48120571\n",
            "Iteration 515, loss = 0.48091040\n",
            "Iteration 516, loss = 0.48061014\n",
            "Iteration 517, loss = 0.48030745\n",
            "Iteration 518, loss = 0.48001949\n",
            "Iteration 519, loss = 0.47972298\n",
            "Iteration 520, loss = 0.47942737\n",
            "Iteration 521, loss = 0.47915194\n",
            "Iteration 522, loss = 0.47884697\n",
            "Iteration 523, loss = 0.47857014\n",
            "Iteration 524, loss = 0.47827714\n",
            "Iteration 525, loss = 0.47797994\n",
            "Iteration 526, loss = 0.47771163\n",
            "Iteration 527, loss = 0.47742116\n",
            "Iteration 528, loss = 0.47716832\n",
            "Iteration 529, loss = 0.47684565\n",
            "Iteration 530, loss = 0.47657493\n",
            "Iteration 531, loss = 0.47629500\n",
            "Iteration 532, loss = 0.47603271\n",
            "Iteration 533, loss = 0.47573248\n",
            "Iteration 534, loss = 0.47545209\n",
            "Iteration 535, loss = 0.47518702\n",
            "Iteration 536, loss = 0.47490426\n",
            "Iteration 537, loss = 0.47461770\n",
            "Iteration 538, loss = 0.47435490\n",
            "Iteration 539, loss = 0.47408322\n",
            "Iteration 540, loss = 0.47382944\n",
            "Iteration 541, loss = 0.47354132\n",
            "Iteration 542, loss = 0.47328596\n",
            "Iteration 543, loss = 0.47304841\n",
            "Iteration 544, loss = 0.47278419\n",
            "Iteration 545, loss = 0.47253326\n",
            "Iteration 546, loss = 0.47228573\n",
            "Iteration 547, loss = 0.47203577\n",
            "Iteration 548, loss = 0.47177661\n",
            "Iteration 549, loss = 0.47151500\n",
            "Iteration 550, loss = 0.47125751\n",
            "Iteration 551, loss = 0.47100829\n",
            "Iteration 552, loss = 0.47074878\n",
            "Iteration 553, loss = 0.47051664\n",
            "Iteration 554, loss = 0.47027310\n",
            "Iteration 555, loss = 0.47002482\n",
            "Iteration 556, loss = 0.46977713\n",
            "Iteration 557, loss = 0.46954665\n",
            "Iteration 558, loss = 0.46929773\n",
            "Iteration 559, loss = 0.46906262\n",
            "Iteration 560, loss = 0.46883563\n",
            "Iteration 561, loss = 0.46860139\n",
            "Iteration 562, loss = 0.46837523\n",
            "Iteration 563, loss = 0.46814651\n",
            "Iteration 564, loss = 0.46788701\n",
            "Iteration 565, loss = 0.46767441\n",
            "Iteration 566, loss = 0.46742284\n",
            "Iteration 567, loss = 0.46720226\n",
            "Iteration 568, loss = 0.46697302\n",
            "Iteration 569, loss = 0.46674318\n",
            "Iteration 570, loss = 0.46652234\n",
            "Iteration 571, loss = 0.46629623\n",
            "Iteration 572, loss = 0.46606160\n",
            "Iteration 573, loss = 0.46582426\n",
            "Iteration 574, loss = 0.46560265\n",
            "Iteration 575, loss = 0.46535922\n",
            "Iteration 576, loss = 0.46514471\n",
            "Iteration 577, loss = 0.46490818\n",
            "Iteration 578, loss = 0.46467585\n",
            "Iteration 579, loss = 0.46446322\n",
            "Iteration 580, loss = 0.46423991\n",
            "Iteration 581, loss = 0.46402084\n",
            "Iteration 582, loss = 0.46380646\n",
            "Iteration 583, loss = 0.46358392\n",
            "Iteration 584, loss = 0.46335938\n",
            "Iteration 585, loss = 0.46315394\n",
            "Iteration 586, loss = 0.46291666\n",
            "Iteration 587, loss = 0.46270229\n",
            "Iteration 588, loss = 0.46251260\n",
            "Iteration 589, loss = 0.46229980\n",
            "Iteration 590, loss = 0.46208419\n",
            "Iteration 591, loss = 0.46188734\n",
            "Iteration 592, loss = 0.46167649\n",
            "Iteration 593, loss = 0.46147794\n",
            "Iteration 594, loss = 0.46127419\n",
            "Iteration 595, loss = 0.46107364\n",
            "Iteration 596, loss = 0.46086730\n",
            "Iteration 597, loss = 0.46067045\n",
            "Iteration 598, loss = 0.46047022\n",
            "Iteration 599, loss = 0.46026759\n",
            "Iteration 600, loss = 0.46007512\n",
            "Iteration 601, loss = 0.45987315\n",
            "Iteration 602, loss = 0.45967128\n",
            "Iteration 603, loss = 0.45949369\n",
            "Iteration 604, loss = 0.45928860\n",
            "Iteration 605, loss = 0.45911616\n",
            "Iteration 606, loss = 0.45891431\n",
            "Iteration 607, loss = 0.45873556\n",
            "Iteration 608, loss = 0.45854038\n",
            "Iteration 609, loss = 0.45836607\n",
            "Iteration 610, loss = 0.45817844\n",
            "Iteration 611, loss = 0.45800527\n",
            "Iteration 612, loss = 0.45782043\n",
            "Iteration 613, loss = 0.45763113\n",
            "Iteration 614, loss = 0.45744012\n",
            "Iteration 615, loss = 0.45726212\n",
            "Iteration 616, loss = 0.45708206\n",
            "Iteration 617, loss = 0.45691398\n",
            "Iteration 618, loss = 0.45671153\n",
            "Iteration 619, loss = 0.45653240\n",
            "Iteration 620, loss = 0.45636851\n",
            "Iteration 621, loss = 0.45618079\n",
            "Iteration 622, loss = 0.45601030\n",
            "Iteration 623, loss = 0.45584015\n",
            "Iteration 624, loss = 0.45565721\n",
            "Iteration 625, loss = 0.45550084\n",
            "Iteration 626, loss = 0.45534708\n",
            "Iteration 627, loss = 0.45515432\n",
            "Iteration 628, loss = 0.45500171\n",
            "Iteration 629, loss = 0.45484358\n",
            "Iteration 630, loss = 0.45466869\n",
            "Iteration 631, loss = 0.45450500\n",
            "Iteration 632, loss = 0.45435570\n",
            "Iteration 633, loss = 0.45419973\n",
            "Iteration 634, loss = 0.45404509\n",
            "Iteration 635, loss = 0.45388069\n",
            "Iteration 636, loss = 0.45374038\n",
            "Iteration 637, loss = 0.45357898\n",
            "Iteration 638, loss = 0.45342728\n",
            "Iteration 639, loss = 0.45328288\n",
            "Iteration 640, loss = 0.45312139\n",
            "Iteration 641, loss = 0.45297346\n",
            "Iteration 642, loss = 0.45283166\n",
            "Iteration 643, loss = 0.45267668\n",
            "Iteration 644, loss = 0.45251743\n",
            "Iteration 645, loss = 0.45235774\n",
            "Iteration 646, loss = 0.45221683\n",
            "Iteration 647, loss = 0.45207045\n",
            "Iteration 648, loss = 0.45191993\n",
            "Iteration 649, loss = 0.45176516\n",
            "Iteration 650, loss = 0.45161698\n",
            "Iteration 651, loss = 0.45148890\n",
            "Iteration 652, loss = 0.45132820\n",
            "Iteration 653, loss = 0.45118852\n",
            "Iteration 654, loss = 0.45105425\n",
            "Iteration 655, loss = 0.45089137\n",
            "Iteration 656, loss = 0.45074965\n",
            "Iteration 657, loss = 0.45061110\n",
            "Iteration 658, loss = 0.45046957\n",
            "Iteration 659, loss = 0.45032079\n",
            "Iteration 660, loss = 0.45017771\n",
            "Iteration 661, loss = 0.45005979\n",
            "Iteration 662, loss = 0.44991248\n",
            "Iteration 663, loss = 0.44974996\n",
            "Iteration 664, loss = 0.44962078\n",
            "Iteration 665, loss = 0.44948250\n",
            "Iteration 666, loss = 0.44934194\n",
            "Iteration 667, loss = 0.44919199\n",
            "Iteration 668, loss = 0.44907796\n",
            "Iteration 669, loss = 0.44891711\n",
            "Iteration 670, loss = 0.44879302\n",
            "Iteration 671, loss = 0.44865314\n",
            "Iteration 672, loss = 0.44851546\n",
            "Iteration 673, loss = 0.44837255\n",
            "Iteration 674, loss = 0.44823865\n",
            "Iteration 675, loss = 0.44812019\n",
            "Iteration 676, loss = 0.44799107\n",
            "Iteration 677, loss = 0.44787047\n",
            "Iteration 678, loss = 0.44774887\n",
            "Iteration 679, loss = 0.44761202\n",
            "Iteration 680, loss = 0.44751627\n",
            "Iteration 681, loss = 0.44736894\n",
            "Iteration 682, loss = 0.44723995\n",
            "Iteration 683, loss = 0.44711753\n",
            "Iteration 684, loss = 0.44698909\n",
            "Iteration 685, loss = 0.44686948\n",
            "Iteration 686, loss = 0.44676584\n",
            "Iteration 687, loss = 0.44663802\n",
            "Iteration 688, loss = 0.44650990\n",
            "Iteration 689, loss = 0.44640090\n",
            "Iteration 690, loss = 0.44628427\n",
            "Iteration 691, loss = 0.44615407\n",
            "Iteration 692, loss = 0.44603387\n",
            "Iteration 693, loss = 0.44592200\n",
            "Iteration 694, loss = 0.44583506\n",
            "Iteration 695, loss = 0.44570923\n",
            "Iteration 696, loss = 0.44560024\n",
            "Iteration 697, loss = 0.44547812\n",
            "Iteration 698, loss = 0.44537580\n",
            "Iteration 699, loss = 0.44526998\n",
            "Iteration 700, loss = 0.44516419\n",
            "Iteration 701, loss = 0.44505155\n",
            "Iteration 702, loss = 0.44494383\n",
            "Iteration 703, loss = 0.44483684\n",
            "Iteration 704, loss = 0.44473114\n",
            "Iteration 705, loss = 0.44461905\n",
            "Iteration 706, loss = 0.44452257\n",
            "Iteration 707, loss = 0.44440815\n",
            "Iteration 708, loss = 0.44431594\n",
            "Iteration 709, loss = 0.44419369\n",
            "Iteration 710, loss = 0.44410972\n",
            "Iteration 711, loss = 0.44399149\n",
            "Iteration 712, loss = 0.44389718\n",
            "Iteration 713, loss = 0.44379805\n",
            "Iteration 714, loss = 0.44369407\n",
            "Iteration 715, loss = 0.44359287\n",
            "Iteration 716, loss = 0.44349377\n",
            "Iteration 717, loss = 0.44339416\n",
            "Iteration 718, loss = 0.44329389\n",
            "Iteration 719, loss = 0.44321314\n",
            "Iteration 720, loss = 0.44308180\n",
            "Iteration 721, loss = 0.44297517\n",
            "Iteration 722, loss = 0.44288108\n",
            "Iteration 723, loss = 0.44278786\n",
            "Iteration 724, loss = 0.44268535\n",
            "Iteration 725, loss = 0.44259354\n",
            "Iteration 726, loss = 0.44249827\n",
            "Iteration 727, loss = 0.44244475\n",
            "Iteration 728, loss = 0.44231798\n",
            "Iteration 729, loss = 0.44220738\n",
            "Iteration 730, loss = 0.44211341\n",
            "Iteration 731, loss = 0.44203215\n",
            "Iteration 732, loss = 0.44192823\n",
            "Iteration 733, loss = 0.44184728\n",
            "Iteration 734, loss = 0.44175354\n",
            "Iteration 735, loss = 0.44165649\n",
            "Iteration 736, loss = 0.44157229\n",
            "Iteration 737, loss = 0.44148690\n",
            "Iteration 738, loss = 0.44139239\n",
            "Iteration 739, loss = 0.44130573\n",
            "Iteration 740, loss = 0.44120697\n",
            "Iteration 741, loss = 0.44112627\n",
            "Iteration 742, loss = 0.44103996\n",
            "Iteration 743, loss = 0.44094316\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.71420693\n",
            "Iteration 2, loss = 0.71386241\n",
            "Iteration 3, loss = 0.71335161\n",
            "Iteration 4, loss = 0.71273125\n",
            "Iteration 5, loss = 0.71204451\n",
            "Iteration 6, loss = 0.71127091\n",
            "Iteration 7, loss = 0.71048716\n",
            "Iteration 8, loss = 0.70975892\n",
            "Iteration 9, loss = 0.70901334\n",
            "Iteration 10, loss = 0.70824888\n",
            "Iteration 11, loss = 0.70751045\n",
            "Iteration 12, loss = 0.70675304\n",
            "Iteration 13, loss = 0.70602647\n",
            "Iteration 14, loss = 0.70528796\n",
            "Iteration 15, loss = 0.70461370\n",
            "Iteration 16, loss = 0.70393177\n",
            "Iteration 17, loss = 0.70317835\n",
            "Iteration 18, loss = 0.70251830\n",
            "Iteration 19, loss = 0.70186880\n",
            "Iteration 20, loss = 0.70116813\n",
            "Iteration 21, loss = 0.70050892\n",
            "Iteration 22, loss = 0.69987630\n",
            "Iteration 23, loss = 0.69921265\n",
            "Iteration 24, loss = 0.69857742\n",
            "Iteration 25, loss = 0.69797210\n",
            "Iteration 26, loss = 0.69736537\n",
            "Iteration 27, loss = 0.69679281\n",
            "Iteration 28, loss = 0.69618478\n",
            "Iteration 29, loss = 0.69563314\n",
            "Iteration 30, loss = 0.69505069\n",
            "Iteration 31, loss = 0.69449404\n",
            "Iteration 32, loss = 0.69394256\n",
            "Iteration 33, loss = 0.69340274\n",
            "Iteration 34, loss = 0.69288345\n",
            "Iteration 35, loss = 0.69234023\n",
            "Iteration 36, loss = 0.69181997\n",
            "Iteration 37, loss = 0.69130390\n",
            "Iteration 38, loss = 0.69081534\n",
            "Iteration 39, loss = 0.69030178\n",
            "Iteration 40, loss = 0.68982181\n",
            "Iteration 41, loss = 0.68932887\n",
            "Iteration 42, loss = 0.68884090\n",
            "Iteration 43, loss = 0.68835825\n",
            "Iteration 44, loss = 0.68789544\n",
            "Iteration 45, loss = 0.68743023\n",
            "Iteration 46, loss = 0.68697037\n",
            "Iteration 47, loss = 0.68651889\n",
            "Iteration 48, loss = 0.68605540\n",
            "Iteration 49, loss = 0.68560000\n",
            "Iteration 50, loss = 0.68516984\n",
            "Iteration 51, loss = 0.68473102\n",
            "Iteration 52, loss = 0.68431459\n",
            "Iteration 53, loss = 0.68387708\n",
            "Iteration 54, loss = 0.68344425\n",
            "Iteration 55, loss = 0.68301953\n",
            "Iteration 56, loss = 0.68259457\n",
            "Iteration 57, loss = 0.68216967\n",
            "Iteration 58, loss = 0.68173838\n",
            "Iteration 59, loss = 0.68133605\n",
            "Iteration 60, loss = 0.68092141\n",
            "Iteration 61, loss = 0.68050644\n",
            "Iteration 62, loss = 0.68012330\n",
            "Iteration 63, loss = 0.67969974\n",
            "Iteration 64, loss = 0.67930254\n",
            "Iteration 65, loss = 0.67890251\n",
            "Iteration 66, loss = 0.67852629\n",
            "Iteration 67, loss = 0.67811651\n",
            "Iteration 68, loss = 0.67771706\n",
            "Iteration 69, loss = 0.67733045\n",
            "Iteration 70, loss = 0.67693910\n",
            "Iteration 71, loss = 0.67656009\n",
            "Iteration 72, loss = 0.67614583\n",
            "Iteration 73, loss = 0.67577202\n",
            "Iteration 74, loss = 0.67537269\n",
            "Iteration 75, loss = 0.67498168\n",
            "Iteration 76, loss = 0.67460376\n",
            "Iteration 77, loss = 0.67421487\n",
            "Iteration 78, loss = 0.67384617\n",
            "Iteration 79, loss = 0.67345538\n",
            "Iteration 80, loss = 0.67307275\n",
            "Iteration 81, loss = 0.67270582\n",
            "Iteration 82, loss = 0.67231731\n",
            "Iteration 83, loss = 0.67195379\n",
            "Iteration 84, loss = 0.67157968\n",
            "Iteration 85, loss = 0.67119025\n",
            "Iteration 86, loss = 0.67082156\n",
            "Iteration 87, loss = 0.67046617\n",
            "Iteration 88, loss = 0.67008478\n",
            "Iteration 89, loss = 0.66972362\n",
            "Iteration 90, loss = 0.66934485\n",
            "Iteration 91, loss = 0.66897388\n",
            "Iteration 92, loss = 0.66860825\n",
            "Iteration 93, loss = 0.66822332\n",
            "Iteration 94, loss = 0.66785744\n",
            "Iteration 95, loss = 0.66748407\n",
            "Iteration 96, loss = 0.66711094\n",
            "Iteration 97, loss = 0.66674661\n",
            "Iteration 98, loss = 0.66637867\n",
            "Iteration 99, loss = 0.66600187\n",
            "Iteration 100, loss = 0.66564191\n",
            "Iteration 101, loss = 0.66526332\n",
            "Iteration 102, loss = 0.66489031\n",
            "Iteration 103, loss = 0.66452707\n",
            "Iteration 104, loss = 0.66414261\n",
            "Iteration 105, loss = 0.66376950\n",
            "Iteration 106, loss = 0.66340399\n",
            "Iteration 107, loss = 0.66303323\n",
            "Iteration 108, loss = 0.66265563\n",
            "Iteration 109, loss = 0.66228078\n",
            "Iteration 110, loss = 0.66191088\n",
            "Iteration 111, loss = 0.66154984\n",
            "Iteration 112, loss = 0.66117812\n",
            "Iteration 113, loss = 0.66080616\n",
            "Iteration 114, loss = 0.66046303\n",
            "Iteration 115, loss = 0.66007920\n",
            "Iteration 116, loss = 0.65972068\n",
            "Iteration 117, loss = 0.65936277\n",
            "Iteration 118, loss = 0.65900361\n",
            "Iteration 119, loss = 0.65864297\n",
            "Iteration 120, loss = 0.65828278\n",
            "Iteration 121, loss = 0.65791321\n",
            "Iteration 122, loss = 0.65755325\n",
            "Iteration 123, loss = 0.65718262\n",
            "Iteration 124, loss = 0.65680897\n",
            "Iteration 125, loss = 0.65643044\n",
            "Iteration 126, loss = 0.65606672\n",
            "Iteration 127, loss = 0.65568901\n",
            "Iteration 128, loss = 0.65531508\n",
            "Iteration 129, loss = 0.65495550\n",
            "Iteration 130, loss = 0.65457271\n",
            "Iteration 131, loss = 0.65420086\n",
            "Iteration 132, loss = 0.65382630\n",
            "Iteration 133, loss = 0.65345021\n",
            "Iteration 134, loss = 0.65307672\n",
            "Iteration 135, loss = 0.65272377\n",
            "Iteration 136, loss = 0.65234142\n",
            "Iteration 137, loss = 0.65197643\n",
            "Iteration 138, loss = 0.65160617\n",
            "Iteration 139, loss = 0.65124118\n",
            "Iteration 140, loss = 0.65086176\n",
            "Iteration 141, loss = 0.65049084\n",
            "Iteration 142, loss = 0.65010200\n",
            "Iteration 143, loss = 0.64972461\n",
            "Iteration 144, loss = 0.64935106\n",
            "Iteration 145, loss = 0.64894802\n",
            "Iteration 146, loss = 0.64859076\n",
            "Iteration 147, loss = 0.64819346\n",
            "Iteration 148, loss = 0.64780346\n",
            "Iteration 149, loss = 0.64742406\n",
            "Iteration 150, loss = 0.64704293\n",
            "Iteration 151, loss = 0.64666505\n",
            "Iteration 152, loss = 0.64628191\n",
            "Iteration 153, loss = 0.64587287\n",
            "Iteration 154, loss = 0.64548309\n",
            "Iteration 155, loss = 0.64509864\n",
            "Iteration 156, loss = 0.64472000\n",
            "Iteration 157, loss = 0.64434082\n",
            "Iteration 158, loss = 0.64395387\n",
            "Iteration 159, loss = 0.64355971\n",
            "Iteration 160, loss = 0.64318593\n",
            "Iteration 161, loss = 0.64278362\n",
            "Iteration 162, loss = 0.64238075\n",
            "Iteration 163, loss = 0.64198340\n",
            "Iteration 164, loss = 0.64158570\n",
            "Iteration 165, loss = 0.64120322\n",
            "Iteration 166, loss = 0.64078195\n",
            "Iteration 167, loss = 0.64039855\n",
            "Iteration 168, loss = 0.63998532\n",
            "Iteration 169, loss = 0.63958369\n",
            "Iteration 170, loss = 0.63918376\n",
            "Iteration 171, loss = 0.63878199\n",
            "Iteration 172, loss = 0.63837255\n",
            "Iteration 173, loss = 0.63797192\n",
            "Iteration 174, loss = 0.63757735\n",
            "Iteration 175, loss = 0.63716940\n",
            "Iteration 176, loss = 0.63675925\n",
            "Iteration 177, loss = 0.63637301\n",
            "Iteration 178, loss = 0.63595530\n",
            "Iteration 179, loss = 0.63555688\n",
            "Iteration 180, loss = 0.63515883\n",
            "Iteration 181, loss = 0.63473813\n",
            "Iteration 182, loss = 0.63433543\n",
            "Iteration 183, loss = 0.63392872\n",
            "Iteration 184, loss = 0.63351273\n",
            "Iteration 185, loss = 0.63311331\n",
            "Iteration 186, loss = 0.63267786\n",
            "Iteration 187, loss = 0.63224935\n",
            "Iteration 188, loss = 0.63183606\n",
            "Iteration 189, loss = 0.63141286\n",
            "Iteration 190, loss = 0.63098491\n",
            "Iteration 191, loss = 0.63056026\n",
            "Iteration 192, loss = 0.63014242\n",
            "Iteration 193, loss = 0.62970860\n",
            "Iteration 194, loss = 0.62929341\n",
            "Iteration 195, loss = 0.62886988\n",
            "Iteration 196, loss = 0.62843758\n",
            "Iteration 197, loss = 0.62799912\n",
            "Iteration 198, loss = 0.62758387\n",
            "Iteration 199, loss = 0.62715447\n",
            "Iteration 200, loss = 0.62672614\n",
            "Iteration 201, loss = 0.62628644\n",
            "Iteration 202, loss = 0.62586848\n",
            "Iteration 203, loss = 0.62546187\n",
            "Iteration 204, loss = 0.62501044\n",
            "Iteration 205, loss = 0.62459242\n",
            "Iteration 206, loss = 0.62414769\n",
            "Iteration 207, loss = 0.62373619\n",
            "Iteration 208, loss = 0.62327990\n",
            "Iteration 209, loss = 0.62284400\n",
            "Iteration 210, loss = 0.62240888\n",
            "Iteration 211, loss = 0.62197008\n",
            "Iteration 212, loss = 0.62153047\n",
            "Iteration 213, loss = 0.62108063\n",
            "Iteration 214, loss = 0.62064225\n",
            "Iteration 215, loss = 0.62019560\n",
            "Iteration 216, loss = 0.61972650\n",
            "Iteration 217, loss = 0.61926972\n",
            "Iteration 218, loss = 0.61882476\n",
            "Iteration 219, loss = 0.61836743\n",
            "Iteration 220, loss = 0.61790975\n",
            "Iteration 221, loss = 0.61746895\n",
            "Iteration 222, loss = 0.61701124\n",
            "Iteration 223, loss = 0.61654915\n",
            "Iteration 224, loss = 0.61610236\n",
            "Iteration 225, loss = 0.61563264\n",
            "Iteration 226, loss = 0.61519490\n",
            "Iteration 227, loss = 0.61471384\n",
            "Iteration 228, loss = 0.61425770\n",
            "Iteration 229, loss = 0.61379464\n",
            "Iteration 230, loss = 0.61332321\n",
            "Iteration 231, loss = 0.61287565\n",
            "Iteration 232, loss = 0.61239269\n",
            "Iteration 233, loss = 0.61194400\n",
            "Iteration 234, loss = 0.61146466\n",
            "Iteration 235, loss = 0.61099828\n",
            "Iteration 236, loss = 0.61053391\n",
            "Iteration 237, loss = 0.61006184\n",
            "Iteration 238, loss = 0.60958825\n",
            "Iteration 239, loss = 0.60912971\n",
            "Iteration 240, loss = 0.60864529\n",
            "Iteration 241, loss = 0.60817755\n",
            "Iteration 242, loss = 0.60770590\n",
            "Iteration 243, loss = 0.60723577\n",
            "Iteration 244, loss = 0.60675946\n",
            "Iteration 245, loss = 0.60628781\n",
            "Iteration 246, loss = 0.60580429\n",
            "Iteration 247, loss = 0.60532805\n",
            "Iteration 248, loss = 0.60486492\n",
            "Iteration 249, loss = 0.60437050\n",
            "Iteration 250, loss = 0.60391232\n",
            "Iteration 251, loss = 0.60342759\n",
            "Iteration 252, loss = 0.60295116\n",
            "Iteration 253, loss = 0.60247719\n",
            "Iteration 254, loss = 0.60198980\n",
            "Iteration 255, loss = 0.60150526\n",
            "Iteration 256, loss = 0.60103813\n",
            "Iteration 257, loss = 0.60053237\n",
            "Iteration 258, loss = 0.60005866\n",
            "Iteration 259, loss = 0.59957128\n",
            "Iteration 260, loss = 0.59908295\n",
            "Iteration 261, loss = 0.59860575\n",
            "Iteration 262, loss = 0.59811528\n",
            "Iteration 263, loss = 0.59762321\n",
            "Iteration 264, loss = 0.59713074\n",
            "Iteration 265, loss = 0.59664426\n",
            "Iteration 266, loss = 0.59615436\n",
            "Iteration 267, loss = 0.59566316\n",
            "Iteration 268, loss = 0.59515602\n",
            "Iteration 269, loss = 0.59467733\n",
            "Iteration 270, loss = 0.59418409\n",
            "Iteration 271, loss = 0.59370081\n",
            "Iteration 272, loss = 0.59320060\n",
            "Iteration 273, loss = 0.59271148\n",
            "Iteration 274, loss = 0.59223564\n",
            "Iteration 275, loss = 0.59173930\n",
            "Iteration 276, loss = 0.59124478\n",
            "Iteration 277, loss = 0.59074760\n",
            "Iteration 278, loss = 0.59025713\n",
            "Iteration 279, loss = 0.58974753\n",
            "Iteration 280, loss = 0.58925760\n",
            "Iteration 281, loss = 0.58875158\n",
            "Iteration 282, loss = 0.58824924\n",
            "Iteration 283, loss = 0.58775841\n",
            "Iteration 284, loss = 0.58724564\n",
            "Iteration 285, loss = 0.58674468\n",
            "Iteration 286, loss = 0.58624398\n",
            "Iteration 287, loss = 0.58572614\n",
            "Iteration 288, loss = 0.58521883\n",
            "Iteration 289, loss = 0.58470531\n",
            "Iteration 290, loss = 0.58421217\n",
            "Iteration 291, loss = 0.58370332\n",
            "Iteration 292, loss = 0.58317749\n",
            "Iteration 293, loss = 0.58266820\n",
            "Iteration 294, loss = 0.58216463\n",
            "Iteration 295, loss = 0.58164055\n",
            "Iteration 296, loss = 0.58112423\n",
            "Iteration 297, loss = 0.58062256\n",
            "Iteration 298, loss = 0.58009354\n",
            "Iteration 299, loss = 0.57959317\n",
            "Iteration 300, loss = 0.57906394\n",
            "Iteration 301, loss = 0.57856207\n",
            "Iteration 302, loss = 0.57804706\n",
            "Iteration 303, loss = 0.57753050\n",
            "Iteration 304, loss = 0.57701860\n",
            "Iteration 305, loss = 0.57650780\n",
            "Iteration 306, loss = 0.57598391\n",
            "Iteration 307, loss = 0.57549114\n",
            "Iteration 308, loss = 0.57497791\n",
            "Iteration 309, loss = 0.57446521\n",
            "Iteration 310, loss = 0.57395760\n",
            "Iteration 311, loss = 0.57345321\n",
            "Iteration 312, loss = 0.57295344\n",
            "Iteration 313, loss = 0.57245019\n",
            "Iteration 314, loss = 0.57192182\n",
            "Iteration 315, loss = 0.57142880\n",
            "Iteration 316, loss = 0.57090736\n",
            "Iteration 317, loss = 0.57039944\n",
            "Iteration 318, loss = 0.56990331\n",
            "Iteration 319, loss = 0.56938000\n",
            "Iteration 320, loss = 0.56887020\n",
            "Iteration 321, loss = 0.56835596\n",
            "Iteration 322, loss = 0.56784839\n",
            "Iteration 323, loss = 0.56732813\n",
            "Iteration 324, loss = 0.56681901\n",
            "Iteration 325, loss = 0.56629699\n",
            "Iteration 326, loss = 0.56576793\n",
            "Iteration 327, loss = 0.56525259\n",
            "Iteration 328, loss = 0.56471446\n",
            "Iteration 329, loss = 0.56419408\n",
            "Iteration 330, loss = 0.56367850\n",
            "Iteration 331, loss = 0.56315914\n",
            "Iteration 332, loss = 0.56263396\n",
            "Iteration 333, loss = 0.56211185\n",
            "Iteration 334, loss = 0.56159485\n",
            "Iteration 335, loss = 0.56107730\n",
            "Iteration 336, loss = 0.56056163\n",
            "Iteration 337, loss = 0.56004275\n",
            "Iteration 338, loss = 0.55952882\n",
            "Iteration 339, loss = 0.55898803\n",
            "Iteration 340, loss = 0.55851109\n",
            "Iteration 341, loss = 0.55798670\n",
            "Iteration 342, loss = 0.55745231\n",
            "Iteration 343, loss = 0.55694781\n",
            "Iteration 344, loss = 0.55643867\n",
            "Iteration 345, loss = 0.55592088\n",
            "Iteration 346, loss = 0.55540471\n",
            "Iteration 347, loss = 0.55489196\n",
            "Iteration 348, loss = 0.55438028\n",
            "Iteration 349, loss = 0.55384877\n",
            "Iteration 350, loss = 0.55332675\n",
            "Iteration 351, loss = 0.55283188\n",
            "Iteration 352, loss = 0.55231027\n",
            "Iteration 353, loss = 0.55181214\n",
            "Iteration 354, loss = 0.55129225\n",
            "Iteration 355, loss = 0.55078960\n",
            "Iteration 356, loss = 0.55028318\n",
            "Iteration 357, loss = 0.54978349\n",
            "Iteration 358, loss = 0.54927451\n",
            "Iteration 359, loss = 0.54876813\n",
            "Iteration 360, loss = 0.54826365\n",
            "Iteration 361, loss = 0.54778169\n",
            "Iteration 362, loss = 0.54727294\n",
            "Iteration 363, loss = 0.54677429\n",
            "Iteration 364, loss = 0.54626807\n",
            "Iteration 365, loss = 0.54576974\n",
            "Iteration 366, loss = 0.54529011\n",
            "Iteration 367, loss = 0.54477724\n",
            "Iteration 368, loss = 0.54428676\n",
            "Iteration 369, loss = 0.54377757\n",
            "Iteration 370, loss = 0.54328615\n",
            "Iteration 371, loss = 0.54279452\n",
            "Iteration 372, loss = 0.54229305\n",
            "Iteration 373, loss = 0.54180450\n",
            "Iteration 374, loss = 0.54130667\n",
            "Iteration 375, loss = 0.54081383\n",
            "Iteration 376, loss = 0.54031508\n",
            "Iteration 377, loss = 0.53981178\n",
            "Iteration 378, loss = 0.53933071\n",
            "Iteration 379, loss = 0.53883693\n",
            "Iteration 380, loss = 0.53836451\n",
            "Iteration 381, loss = 0.53786289\n",
            "Iteration 382, loss = 0.53737597\n",
            "Iteration 383, loss = 0.53690411\n",
            "Iteration 384, loss = 0.53641717\n",
            "Iteration 385, loss = 0.53594809\n",
            "Iteration 386, loss = 0.53544491\n",
            "Iteration 387, loss = 0.53498403\n",
            "Iteration 388, loss = 0.53451582\n",
            "Iteration 389, loss = 0.53404566\n",
            "Iteration 390, loss = 0.53357825\n",
            "Iteration 391, loss = 0.53311568\n",
            "Iteration 392, loss = 0.53265167\n",
            "Iteration 393, loss = 0.53217832\n",
            "Iteration 394, loss = 0.53170801\n",
            "Iteration 395, loss = 0.53124788\n",
            "Iteration 396, loss = 0.53078366\n",
            "Iteration 397, loss = 0.53032101\n",
            "Iteration 398, loss = 0.52985278\n",
            "Iteration 399, loss = 0.52938241\n",
            "Iteration 400, loss = 0.52892607\n",
            "Iteration 401, loss = 0.52845106\n",
            "Iteration 402, loss = 0.52799927\n",
            "Iteration 403, loss = 0.52753137\n",
            "Iteration 404, loss = 0.52706802\n",
            "Iteration 405, loss = 0.52661709\n",
            "Iteration 406, loss = 0.52616040\n",
            "Iteration 407, loss = 0.52569934\n",
            "Iteration 408, loss = 0.52523662\n",
            "Iteration 409, loss = 0.52478795\n",
            "Iteration 410, loss = 0.52434485\n",
            "Iteration 411, loss = 0.52388496\n",
            "Iteration 412, loss = 0.52342977\n",
            "Iteration 413, loss = 0.52298806\n",
            "Iteration 414, loss = 0.52254666\n",
            "Iteration 415, loss = 0.52209741\n",
            "Iteration 416, loss = 0.52164571\n",
            "Iteration 417, loss = 0.52122255\n",
            "Iteration 418, loss = 0.52076270\n",
            "Iteration 419, loss = 0.52032127\n",
            "Iteration 420, loss = 0.51990646\n",
            "Iteration 421, loss = 0.51943617\n",
            "Iteration 422, loss = 0.51901076\n",
            "Iteration 423, loss = 0.51855681\n",
            "Iteration 424, loss = 0.51813247\n",
            "Iteration 425, loss = 0.51769550\n",
            "Iteration 426, loss = 0.51727438\n",
            "Iteration 427, loss = 0.51683827\n",
            "Iteration 428, loss = 0.51642875\n",
            "Iteration 429, loss = 0.51598681\n",
            "Iteration 430, loss = 0.51556835\n",
            "Iteration 431, loss = 0.51514735\n",
            "Iteration 432, loss = 0.51474106\n",
            "Iteration 433, loss = 0.51431790\n",
            "Iteration 434, loss = 0.51389780\n",
            "Iteration 435, loss = 0.51348314\n",
            "Iteration 436, loss = 0.51307435\n",
            "Iteration 437, loss = 0.51265435\n",
            "Iteration 438, loss = 0.51224356\n",
            "Iteration 439, loss = 0.51183658\n",
            "Iteration 440, loss = 0.51142301\n",
            "Iteration 441, loss = 0.51100979\n",
            "Iteration 442, loss = 0.51060973\n",
            "Iteration 443, loss = 0.51020034\n",
            "Iteration 444, loss = 0.50980313\n",
            "Iteration 445, loss = 0.50938358\n",
            "Iteration 446, loss = 0.50898560\n",
            "Iteration 447, loss = 0.50859421\n",
            "Iteration 448, loss = 0.50817905\n",
            "Iteration 449, loss = 0.50780565\n",
            "Iteration 450, loss = 0.50739974\n",
            "Iteration 451, loss = 0.50700688\n",
            "Iteration 452, loss = 0.50660507\n",
            "Iteration 453, loss = 0.50624820\n",
            "Iteration 454, loss = 0.50583808\n",
            "Iteration 455, loss = 0.50543284\n",
            "Iteration 456, loss = 0.50505385\n",
            "Iteration 457, loss = 0.50464269\n",
            "Iteration 458, loss = 0.50426750\n",
            "Iteration 459, loss = 0.50388855\n",
            "Iteration 460, loss = 0.50348322\n",
            "Iteration 461, loss = 0.50311067\n",
            "Iteration 462, loss = 0.50273169\n",
            "Iteration 463, loss = 0.50235463\n",
            "Iteration 464, loss = 0.50199562\n",
            "Iteration 465, loss = 0.50161217\n",
            "Iteration 466, loss = 0.50123735\n",
            "Iteration 467, loss = 0.50087478\n",
            "Iteration 468, loss = 0.50050425\n",
            "Iteration 469, loss = 0.50012958\n",
            "Iteration 470, loss = 0.49975309\n",
            "Iteration 471, loss = 0.49938237\n",
            "Iteration 472, loss = 0.49901959\n",
            "Iteration 473, loss = 0.49865509\n",
            "Iteration 474, loss = 0.49829200\n",
            "Iteration 475, loss = 0.49794722\n",
            "Iteration 476, loss = 0.49757202\n",
            "Iteration 477, loss = 0.49720203\n",
            "Iteration 478, loss = 0.49687070\n",
            "Iteration 479, loss = 0.49650811\n",
            "Iteration 480, loss = 0.49614912\n",
            "Iteration 481, loss = 0.49579304\n",
            "Iteration 482, loss = 0.49546026\n",
            "Iteration 483, loss = 0.49508121\n",
            "Iteration 484, loss = 0.49474850\n",
            "Iteration 485, loss = 0.49440611\n",
            "Iteration 486, loss = 0.49406404\n",
            "Iteration 487, loss = 0.49371705\n",
            "Iteration 488, loss = 0.49336851\n",
            "Iteration 489, loss = 0.49303175\n",
            "Iteration 490, loss = 0.49267870\n",
            "Iteration 491, loss = 0.49230719\n",
            "Iteration 492, loss = 0.49197346\n",
            "Iteration 493, loss = 0.49162523\n",
            "Iteration 494, loss = 0.49129315\n",
            "Iteration 495, loss = 0.49093714\n",
            "Iteration 496, loss = 0.49063341\n",
            "Iteration 497, loss = 0.49027852\n",
            "Iteration 498, loss = 0.48995536\n",
            "Iteration 499, loss = 0.48961244\n",
            "Iteration 500, loss = 0.48929030\n",
            "Iteration 501, loss = 0.48895777\n",
            "Iteration 502, loss = 0.48861835\n",
            "Iteration 503, loss = 0.48828666\n",
            "Iteration 504, loss = 0.48794907\n",
            "Iteration 505, loss = 0.48761752\n",
            "Iteration 506, loss = 0.48730194\n",
            "Iteration 507, loss = 0.48697870\n",
            "Iteration 508, loss = 0.48665961\n",
            "Iteration 509, loss = 0.48632993\n",
            "Iteration 510, loss = 0.48600411\n",
            "Iteration 511, loss = 0.48569807\n",
            "Iteration 512, loss = 0.48537862\n",
            "Iteration 513, loss = 0.48506074\n",
            "Iteration 514, loss = 0.48476111\n",
            "Iteration 515, loss = 0.48445377\n",
            "Iteration 516, loss = 0.48415551\n",
            "Iteration 517, loss = 0.48384977\n",
            "Iteration 518, loss = 0.48355158\n",
            "Iteration 519, loss = 0.48323577\n",
            "Iteration 520, loss = 0.48293664\n",
            "Iteration 521, loss = 0.48264993\n",
            "Iteration 522, loss = 0.48234648\n",
            "Iteration 523, loss = 0.48206145\n",
            "Iteration 524, loss = 0.48175710\n",
            "Iteration 525, loss = 0.48146539\n",
            "Iteration 526, loss = 0.48117234\n",
            "Iteration 527, loss = 0.48088793\n",
            "Iteration 528, loss = 0.48062608\n",
            "Iteration 529, loss = 0.48030336\n",
            "Iteration 530, loss = 0.48003359\n",
            "Iteration 531, loss = 0.47974691\n",
            "Iteration 532, loss = 0.47947273\n",
            "Iteration 533, loss = 0.47916720\n",
            "Iteration 534, loss = 0.47888621\n",
            "Iteration 535, loss = 0.47860999\n",
            "Iteration 536, loss = 0.47832910\n",
            "Iteration 537, loss = 0.47804653\n",
            "Iteration 538, loss = 0.47777072\n",
            "Iteration 539, loss = 0.47749296\n",
            "Iteration 540, loss = 0.47724040\n",
            "Iteration 541, loss = 0.47694030\n",
            "Iteration 542, loss = 0.47667800\n",
            "Iteration 543, loss = 0.47643169\n",
            "Iteration 544, loss = 0.47616311\n",
            "Iteration 545, loss = 0.47589888\n",
            "Iteration 546, loss = 0.47565693\n",
            "Iteration 547, loss = 0.47537922\n",
            "Iteration 548, loss = 0.47512009\n",
            "Iteration 549, loss = 0.47485105\n",
            "Iteration 550, loss = 0.47458688\n",
            "Iteration 551, loss = 0.47434527\n",
            "Iteration 552, loss = 0.47407249\n",
            "Iteration 553, loss = 0.47383889\n",
            "Iteration 554, loss = 0.47358954\n",
            "Iteration 555, loss = 0.47333507\n",
            "Iteration 556, loss = 0.47310520\n",
            "Iteration 557, loss = 0.47285364\n",
            "Iteration 558, loss = 0.47260993\n",
            "Iteration 559, loss = 0.47236445\n",
            "Iteration 560, loss = 0.47214355\n",
            "Iteration 561, loss = 0.47188538\n",
            "Iteration 562, loss = 0.47166273\n",
            "Iteration 563, loss = 0.47143332\n",
            "Iteration 564, loss = 0.47115631\n",
            "Iteration 565, loss = 0.47093339\n",
            "Iteration 566, loss = 0.47068735\n",
            "Iteration 567, loss = 0.47046077\n",
            "Iteration 568, loss = 0.47021702\n",
            "Iteration 569, loss = 0.46998108\n",
            "Iteration 570, loss = 0.46975269\n",
            "Iteration 571, loss = 0.46951307\n",
            "Iteration 572, loss = 0.46929243\n",
            "Iteration 573, loss = 0.46903471\n",
            "Iteration 574, loss = 0.46880172\n",
            "Iteration 575, loss = 0.46856228\n",
            "Iteration 576, loss = 0.46833484\n",
            "Iteration 577, loss = 0.46810744\n",
            "Iteration 578, loss = 0.46787473\n",
            "Iteration 579, loss = 0.46764697\n",
            "Iteration 580, loss = 0.46742331\n",
            "Iteration 581, loss = 0.46719995\n",
            "Iteration 582, loss = 0.46698328\n",
            "Iteration 583, loss = 0.46674605\n",
            "Iteration 584, loss = 0.46651327\n",
            "Iteration 585, loss = 0.46629520\n",
            "Iteration 586, loss = 0.46606298\n",
            "Iteration 587, loss = 0.46583270\n",
            "Iteration 588, loss = 0.46563814\n",
            "Iteration 589, loss = 0.46541193\n",
            "Iteration 590, loss = 0.46519077\n",
            "Iteration 591, loss = 0.46497919\n",
            "Iteration 592, loss = 0.46476724\n",
            "Iteration 593, loss = 0.46457138\n",
            "Iteration 594, loss = 0.46435161\n",
            "Iteration 595, loss = 0.46414579\n",
            "Iteration 596, loss = 0.46394069\n",
            "Iteration 597, loss = 0.46372866\n",
            "Iteration 598, loss = 0.46353502\n",
            "Iteration 599, loss = 0.46332061\n",
            "Iteration 600, loss = 0.46311250\n",
            "Iteration 601, loss = 0.46292483\n",
            "Iteration 602, loss = 0.46272484\n",
            "Iteration 603, loss = 0.46254463\n",
            "Iteration 604, loss = 0.46233511\n",
            "Iteration 605, loss = 0.46216191\n",
            "Iteration 606, loss = 0.46195523\n",
            "Iteration 607, loss = 0.46178850\n",
            "Iteration 608, loss = 0.46156394\n",
            "Iteration 609, loss = 0.46138472\n",
            "Iteration 610, loss = 0.46120110\n",
            "Iteration 611, loss = 0.46101992\n",
            "Iteration 612, loss = 0.46082780\n",
            "Iteration 613, loss = 0.46064746\n",
            "Iteration 614, loss = 0.46044721\n",
            "Iteration 615, loss = 0.46026205\n",
            "Iteration 616, loss = 0.46008438\n",
            "Iteration 617, loss = 0.45991685\n",
            "Iteration 618, loss = 0.45971079\n",
            "Iteration 619, loss = 0.45952098\n",
            "Iteration 620, loss = 0.45933449\n",
            "Iteration 621, loss = 0.45915366\n",
            "Iteration 622, loss = 0.45898060\n",
            "Iteration 623, loss = 0.45879100\n",
            "Iteration 624, loss = 0.45860644\n",
            "Iteration 625, loss = 0.45844733\n",
            "Iteration 626, loss = 0.45828253\n",
            "Iteration 627, loss = 0.45807367\n",
            "Iteration 628, loss = 0.45791793\n",
            "Iteration 629, loss = 0.45774242\n",
            "Iteration 630, loss = 0.45757217\n",
            "Iteration 631, loss = 0.45739896\n",
            "Iteration 632, loss = 0.45723879\n",
            "Iteration 633, loss = 0.45708130\n",
            "Iteration 634, loss = 0.45691887\n",
            "Iteration 635, loss = 0.45675090\n",
            "Iteration 636, loss = 0.45661075\n",
            "Iteration 637, loss = 0.45644513\n",
            "Iteration 638, loss = 0.45629916\n",
            "Iteration 639, loss = 0.45613460\n",
            "Iteration 640, loss = 0.45598068\n",
            "Iteration 641, loss = 0.45582560\n",
            "Iteration 642, loss = 0.45568227\n",
            "Iteration 643, loss = 0.45552282\n",
            "Iteration 644, loss = 0.45537131\n",
            "Iteration 645, loss = 0.45521084\n",
            "Iteration 646, loss = 0.45507018\n",
            "Iteration 647, loss = 0.45491880\n",
            "Iteration 648, loss = 0.45476862\n",
            "Iteration 649, loss = 0.45461813\n",
            "Iteration 650, loss = 0.45446971\n",
            "Iteration 651, loss = 0.45433165\n",
            "Iteration 652, loss = 0.45418022\n",
            "Iteration 653, loss = 0.45404045\n",
            "Iteration 654, loss = 0.45390780\n",
            "Iteration 655, loss = 0.45373448\n",
            "Iteration 656, loss = 0.45359218\n",
            "Iteration 657, loss = 0.45344892\n",
            "Iteration 658, loss = 0.45330207\n",
            "Iteration 659, loss = 0.45314940\n",
            "Iteration 660, loss = 0.45300121\n",
            "Iteration 661, loss = 0.45287300\n",
            "Iteration 662, loss = 0.45272589\n",
            "Iteration 663, loss = 0.45256246\n",
            "Iteration 664, loss = 0.45242395\n",
            "Iteration 665, loss = 0.45227481\n",
            "Iteration 666, loss = 0.45214335\n",
            "Iteration 667, loss = 0.45198612\n",
            "Iteration 668, loss = 0.45185621\n",
            "Iteration 669, loss = 0.45170670\n",
            "Iteration 670, loss = 0.45157853\n",
            "Iteration 671, loss = 0.45144248\n",
            "Iteration 672, loss = 0.45129563\n",
            "Iteration 673, loss = 0.45115679\n",
            "Iteration 674, loss = 0.45102507\n",
            "Iteration 675, loss = 0.45089113\n",
            "Iteration 676, loss = 0.45077083\n",
            "Iteration 677, loss = 0.45064305\n",
            "Iteration 678, loss = 0.45052558\n",
            "Iteration 679, loss = 0.45039301\n",
            "Iteration 680, loss = 0.45028068\n",
            "Iteration 681, loss = 0.45015514\n",
            "Iteration 682, loss = 0.45002557\n",
            "Iteration 683, loss = 0.44990121\n",
            "Iteration 684, loss = 0.44977005\n",
            "Iteration 685, loss = 0.44965199\n",
            "Iteration 686, loss = 0.44953681\n",
            "Iteration 687, loss = 0.44940874\n",
            "Iteration 688, loss = 0.44927505\n",
            "Iteration 689, loss = 0.44916837\n",
            "Iteration 690, loss = 0.44904153\n",
            "Iteration 691, loss = 0.44890572\n",
            "Iteration 692, loss = 0.44878712\n",
            "Iteration 693, loss = 0.44866956\n",
            "Iteration 694, loss = 0.44857342\n",
            "Iteration 695, loss = 0.44845862\n",
            "Iteration 696, loss = 0.44833237\n",
            "Iteration 697, loss = 0.44821373\n",
            "Iteration 698, loss = 0.44810276\n",
            "Iteration 699, loss = 0.44799586\n",
            "Iteration 700, loss = 0.44788276\n",
            "Iteration 701, loss = 0.44777021\n",
            "Iteration 702, loss = 0.44767115\n",
            "Iteration 703, loss = 0.44755319\n",
            "Iteration 704, loss = 0.44744134\n",
            "Iteration 705, loss = 0.44732884\n",
            "Iteration 706, loss = 0.44722818\n",
            "Iteration 707, loss = 0.44711167\n",
            "Iteration 708, loss = 0.44702045\n",
            "Iteration 709, loss = 0.44688878\n",
            "Iteration 710, loss = 0.44679718\n",
            "Iteration 711, loss = 0.44668872\n",
            "Iteration 712, loss = 0.44658504\n",
            "Iteration 713, loss = 0.44649840\n",
            "Iteration 714, loss = 0.44636856\n",
            "Iteration 715, loss = 0.44626777\n",
            "Iteration 716, loss = 0.44616119\n",
            "Iteration 717, loss = 0.44606099\n",
            "Iteration 718, loss = 0.44596818\n",
            "Iteration 719, loss = 0.44586589\n",
            "Iteration 720, loss = 0.44575177\n",
            "Iteration 721, loss = 0.44564724\n",
            "Iteration 722, loss = 0.44555062\n",
            "Iteration 723, loss = 0.44544388\n",
            "Iteration 724, loss = 0.44534658\n",
            "Iteration 725, loss = 0.44524261\n",
            "Iteration 726, loss = 0.44515802\n",
            "Iteration 727, loss = 0.44507930\n",
            "Iteration 728, loss = 0.44495597\n",
            "Iteration 729, loss = 0.44484660\n",
            "Iteration 730, loss = 0.44475421\n",
            "Iteration 731, loss = 0.44465960\n",
            "Iteration 732, loss = 0.44456336\n",
            "Iteration 733, loss = 0.44447328\n",
            "Iteration 734, loss = 0.44437671\n",
            "Iteration 735, loss = 0.44427868\n",
            "Iteration 736, loss = 0.44419119\n",
            "Iteration 737, loss = 0.44409535\n",
            "Iteration 738, loss = 0.44401479\n",
            "Iteration 739, loss = 0.44390991\n",
            "Iteration 740, loss = 0.44382337\n",
            "Iteration 741, loss = 0.44373027\n",
            "Iteration 742, loss = 0.44365029\n",
            "Iteration 743, loss = 0.44355197\n",
            "Iteration 744, loss = 0.44346122\n",
            "Iteration 745, loss = 0.44337850\n",
            "Iteration 746, loss = 0.44329139\n",
            "Iteration 747, loss = 0.44320231\n",
            "Iteration 748, loss = 0.44314014\n",
            "Iteration 749, loss = 0.44304081\n",
            "Iteration 750, loss = 0.44295612\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.71412720\n",
            "Iteration 2, loss = 0.71377357\n",
            "Iteration 3, loss = 0.71323626\n",
            "Iteration 4, loss = 0.71261225\n",
            "Iteration 5, loss = 0.71190506\n",
            "Iteration 6, loss = 0.71112610\n",
            "Iteration 7, loss = 0.71033405\n",
            "Iteration 8, loss = 0.70956077\n",
            "Iteration 9, loss = 0.70883549\n",
            "Iteration 10, loss = 0.70804082\n",
            "Iteration 11, loss = 0.70725496\n",
            "Iteration 12, loss = 0.70649384\n",
            "Iteration 13, loss = 0.70575009\n",
            "Iteration 14, loss = 0.70497168\n",
            "Iteration 15, loss = 0.70428038\n",
            "Iteration 16, loss = 0.70356266\n",
            "Iteration 17, loss = 0.70280679\n",
            "Iteration 18, loss = 0.70213762\n",
            "Iteration 19, loss = 0.70143843\n",
            "Iteration 20, loss = 0.70073847\n",
            "Iteration 21, loss = 0.70006874\n",
            "Iteration 22, loss = 0.69940077\n",
            "Iteration 23, loss = 0.69872511\n",
            "Iteration 24, loss = 0.69809844\n",
            "Iteration 25, loss = 0.69745659\n",
            "Iteration 26, loss = 0.69684149\n",
            "Iteration 27, loss = 0.69624679\n",
            "Iteration 28, loss = 0.69562904\n",
            "Iteration 29, loss = 0.69505711\n",
            "Iteration 30, loss = 0.69444849\n",
            "Iteration 31, loss = 0.69388122\n",
            "Iteration 32, loss = 0.69331976\n",
            "Iteration 33, loss = 0.69275302\n",
            "Iteration 34, loss = 0.69222609\n",
            "Iteration 35, loss = 0.69166200\n",
            "Iteration 36, loss = 0.69113010\n",
            "Iteration 37, loss = 0.69059598\n",
            "Iteration 38, loss = 0.69009670\n",
            "Iteration 39, loss = 0.68957028\n",
            "Iteration 40, loss = 0.68907498\n",
            "Iteration 41, loss = 0.68857329\n",
            "Iteration 42, loss = 0.68807592\n",
            "Iteration 43, loss = 0.68757534\n",
            "Iteration 44, loss = 0.68709158\n",
            "Iteration 45, loss = 0.68662155\n",
            "Iteration 46, loss = 0.68612620\n",
            "Iteration 47, loss = 0.68566508\n",
            "Iteration 48, loss = 0.68517586\n",
            "Iteration 49, loss = 0.68471304\n",
            "Iteration 50, loss = 0.68425522\n",
            "Iteration 51, loss = 0.68380005\n",
            "Iteration 52, loss = 0.68336883\n",
            "Iteration 53, loss = 0.68292202\n",
            "Iteration 54, loss = 0.68247323\n",
            "Iteration 55, loss = 0.68202105\n",
            "Iteration 56, loss = 0.68158679\n",
            "Iteration 57, loss = 0.68114865\n",
            "Iteration 58, loss = 0.68070883\n",
            "Iteration 59, loss = 0.68028937\n",
            "Iteration 60, loss = 0.67986263\n",
            "Iteration 61, loss = 0.67943457\n",
            "Iteration 62, loss = 0.67904376\n",
            "Iteration 63, loss = 0.67860021\n",
            "Iteration 64, loss = 0.67817501\n",
            "Iteration 65, loss = 0.67777500\n",
            "Iteration 66, loss = 0.67737813\n",
            "Iteration 67, loss = 0.67696155\n",
            "Iteration 68, loss = 0.67655939\n",
            "Iteration 69, loss = 0.67614699\n",
            "Iteration 70, loss = 0.67575514\n",
            "Iteration 71, loss = 0.67536984\n",
            "Iteration 72, loss = 0.67495919\n",
            "Iteration 73, loss = 0.67456803\n",
            "Iteration 74, loss = 0.67416739\n",
            "Iteration 75, loss = 0.67377036\n",
            "Iteration 76, loss = 0.67339221\n",
            "Iteration 77, loss = 0.67298768\n",
            "Iteration 78, loss = 0.67260230\n",
            "Iteration 79, loss = 0.67220824\n",
            "Iteration 80, loss = 0.67181807\n",
            "Iteration 81, loss = 0.67143282\n",
            "Iteration 82, loss = 0.67103464\n",
            "Iteration 83, loss = 0.67066051\n",
            "Iteration 84, loss = 0.67027640\n",
            "Iteration 85, loss = 0.66987868\n",
            "Iteration 86, loss = 0.66950428\n",
            "Iteration 87, loss = 0.66913410\n",
            "Iteration 88, loss = 0.66875077\n",
            "Iteration 89, loss = 0.66839034\n",
            "Iteration 90, loss = 0.66800290\n",
            "Iteration 91, loss = 0.66762506\n",
            "Iteration 92, loss = 0.66724314\n",
            "Iteration 93, loss = 0.66685923\n",
            "Iteration 94, loss = 0.66647995\n",
            "Iteration 95, loss = 0.66609475\n",
            "Iteration 96, loss = 0.66572703\n",
            "Iteration 97, loss = 0.66534820\n",
            "Iteration 98, loss = 0.66497602\n",
            "Iteration 99, loss = 0.66459268\n",
            "Iteration 100, loss = 0.66422261\n",
            "Iteration 101, loss = 0.66383830\n",
            "Iteration 102, loss = 0.66345782\n",
            "Iteration 103, loss = 0.66309050\n",
            "Iteration 104, loss = 0.66269961\n",
            "Iteration 105, loss = 0.66232104\n",
            "Iteration 106, loss = 0.66195083\n",
            "Iteration 107, loss = 0.66158053\n",
            "Iteration 108, loss = 0.66119728\n",
            "Iteration 109, loss = 0.66082973\n",
            "Iteration 110, loss = 0.66045155\n",
            "Iteration 111, loss = 0.66009044\n",
            "Iteration 112, loss = 0.65971909\n",
            "Iteration 113, loss = 0.65934620\n",
            "Iteration 114, loss = 0.65899524\n",
            "Iteration 115, loss = 0.65860643\n",
            "Iteration 116, loss = 0.65824333\n",
            "Iteration 117, loss = 0.65787457\n",
            "Iteration 118, loss = 0.65751301\n",
            "Iteration 119, loss = 0.65714210\n",
            "Iteration 120, loss = 0.65678500\n",
            "Iteration 121, loss = 0.65641597\n",
            "Iteration 122, loss = 0.65604028\n",
            "Iteration 123, loss = 0.65567522\n",
            "Iteration 124, loss = 0.65529608\n",
            "Iteration 125, loss = 0.65491982\n",
            "Iteration 126, loss = 0.65454471\n",
            "Iteration 127, loss = 0.65416375\n",
            "Iteration 128, loss = 0.65378561\n",
            "Iteration 129, loss = 0.65343160\n",
            "Iteration 130, loss = 0.65303116\n",
            "Iteration 131, loss = 0.65267338\n",
            "Iteration 132, loss = 0.65228963\n",
            "Iteration 133, loss = 0.65191135\n",
            "Iteration 134, loss = 0.65153688\n",
            "Iteration 135, loss = 0.65117878\n",
            "Iteration 136, loss = 0.65080066\n",
            "Iteration 137, loss = 0.65043189\n",
            "Iteration 138, loss = 0.65006024\n",
            "Iteration 139, loss = 0.64969416\n",
            "Iteration 140, loss = 0.64931727\n",
            "Iteration 141, loss = 0.64894894\n",
            "Iteration 142, loss = 0.64855754\n",
            "Iteration 143, loss = 0.64818007\n",
            "Iteration 144, loss = 0.64780881\n",
            "Iteration 145, loss = 0.64741365\n",
            "Iteration 146, loss = 0.64704689\n",
            "Iteration 147, loss = 0.64665691\n",
            "Iteration 148, loss = 0.64626483\n",
            "Iteration 149, loss = 0.64589249\n",
            "Iteration 150, loss = 0.64550180\n",
            "Iteration 151, loss = 0.64513508\n",
            "Iteration 152, loss = 0.64474790\n",
            "Iteration 153, loss = 0.64434519\n",
            "Iteration 154, loss = 0.64395768\n",
            "Iteration 155, loss = 0.64357686\n",
            "Iteration 156, loss = 0.64319563\n",
            "Iteration 157, loss = 0.64281443\n",
            "Iteration 158, loss = 0.64243976\n",
            "Iteration 159, loss = 0.64203954\n",
            "Iteration 160, loss = 0.64167138\n",
            "Iteration 161, loss = 0.64127255\n",
            "Iteration 162, loss = 0.64087688\n",
            "Iteration 163, loss = 0.64047939\n",
            "Iteration 164, loss = 0.64009010\n",
            "Iteration 165, loss = 0.63970300\n",
            "Iteration 166, loss = 0.63929559\n",
            "Iteration 167, loss = 0.63891236\n",
            "Iteration 168, loss = 0.63850092\n",
            "Iteration 169, loss = 0.63810627\n",
            "Iteration 170, loss = 0.63770365\n",
            "Iteration 171, loss = 0.63731028\n",
            "Iteration 172, loss = 0.63691190\n",
            "Iteration 173, loss = 0.63651568\n",
            "Iteration 174, loss = 0.63611423\n",
            "Iteration 175, loss = 0.63570507\n",
            "Iteration 176, loss = 0.63530731\n",
            "Iteration 177, loss = 0.63492484\n",
            "Iteration 178, loss = 0.63450521\n",
            "Iteration 179, loss = 0.63411009\n",
            "Iteration 180, loss = 0.63370692\n",
            "Iteration 181, loss = 0.63330278\n",
            "Iteration 182, loss = 0.63289666\n",
            "Iteration 183, loss = 0.63250740\n",
            "Iteration 184, loss = 0.63209362\n",
            "Iteration 185, loss = 0.63170242\n",
            "Iteration 186, loss = 0.63128642\n",
            "Iteration 187, loss = 0.63086238\n",
            "Iteration 188, loss = 0.63045768\n",
            "Iteration 189, loss = 0.63004324\n",
            "Iteration 190, loss = 0.62962651\n",
            "Iteration 191, loss = 0.62921862\n",
            "Iteration 192, loss = 0.62880977\n",
            "Iteration 193, loss = 0.62837707\n",
            "Iteration 194, loss = 0.62799307\n",
            "Iteration 195, loss = 0.62757450\n",
            "Iteration 196, loss = 0.62715287\n",
            "Iteration 197, loss = 0.62673794\n",
            "Iteration 198, loss = 0.62633340\n",
            "Iteration 199, loss = 0.62592722\n",
            "Iteration 200, loss = 0.62551098\n",
            "Iteration 201, loss = 0.62508070\n",
            "Iteration 202, loss = 0.62467695\n",
            "Iteration 203, loss = 0.62426723\n",
            "Iteration 204, loss = 0.62384097\n",
            "Iteration 205, loss = 0.62342724\n",
            "Iteration 206, loss = 0.62299177\n",
            "Iteration 207, loss = 0.62258180\n",
            "Iteration 208, loss = 0.62214589\n",
            "Iteration 209, loss = 0.62171230\n",
            "Iteration 210, loss = 0.62128640\n",
            "Iteration 211, loss = 0.62086114\n",
            "Iteration 212, loss = 0.62042219\n",
            "Iteration 213, loss = 0.61999925\n",
            "Iteration 214, loss = 0.61957825\n",
            "Iteration 215, loss = 0.61914825\n",
            "Iteration 216, loss = 0.61869602\n",
            "Iteration 217, loss = 0.61825047\n",
            "Iteration 218, loss = 0.61782337\n",
            "Iteration 219, loss = 0.61737744\n",
            "Iteration 220, loss = 0.61693637\n",
            "Iteration 221, loss = 0.61650026\n",
            "Iteration 222, loss = 0.61606549\n",
            "Iteration 223, loss = 0.61561701\n",
            "Iteration 224, loss = 0.61518936\n",
            "Iteration 225, loss = 0.61472495\n",
            "Iteration 226, loss = 0.61429757\n",
            "Iteration 227, loss = 0.61382270\n",
            "Iteration 228, loss = 0.61338258\n",
            "Iteration 229, loss = 0.61292275\n",
            "Iteration 230, loss = 0.61247302\n",
            "Iteration 231, loss = 0.61202787\n",
            "Iteration 232, loss = 0.61156442\n",
            "Iteration 233, loss = 0.61113219\n",
            "Iteration 234, loss = 0.61067696\n",
            "Iteration 235, loss = 0.61021742\n",
            "Iteration 236, loss = 0.60976183\n",
            "Iteration 237, loss = 0.60930872\n",
            "Iteration 238, loss = 0.60884681\n",
            "Iteration 239, loss = 0.60840055\n",
            "Iteration 240, loss = 0.60793084\n",
            "Iteration 241, loss = 0.60746978\n",
            "Iteration 242, loss = 0.60699653\n",
            "Iteration 243, loss = 0.60655513\n",
            "Iteration 244, loss = 0.60608152\n",
            "Iteration 245, loss = 0.60561522\n",
            "Iteration 246, loss = 0.60514701\n",
            "Iteration 247, loss = 0.60467532\n",
            "Iteration 248, loss = 0.60423072\n",
            "Iteration 249, loss = 0.60375557\n",
            "Iteration 250, loss = 0.60329688\n",
            "Iteration 251, loss = 0.60283035\n",
            "Iteration 252, loss = 0.60235843\n",
            "Iteration 253, loss = 0.60189709\n",
            "Iteration 254, loss = 0.60141383\n",
            "Iteration 255, loss = 0.60094649\n",
            "Iteration 256, loss = 0.60047786\n",
            "Iteration 257, loss = 0.59999015\n",
            "Iteration 258, loss = 0.59953297\n",
            "Iteration 259, loss = 0.59905062\n",
            "Iteration 260, loss = 0.59857023\n",
            "Iteration 261, loss = 0.59810502\n",
            "Iteration 262, loss = 0.59762225\n",
            "Iteration 263, loss = 0.59713746\n",
            "Iteration 264, loss = 0.59665923\n",
            "Iteration 265, loss = 0.59617823\n",
            "Iteration 266, loss = 0.59568611\n",
            "Iteration 267, loss = 0.59520070\n",
            "Iteration 268, loss = 0.59469946\n",
            "Iteration 269, loss = 0.59422273\n",
            "Iteration 270, loss = 0.59373157\n",
            "Iteration 271, loss = 0.59324807\n",
            "Iteration 272, loss = 0.59276216\n",
            "Iteration 273, loss = 0.59226203\n",
            "Iteration 274, loss = 0.59178101\n",
            "Iteration 275, loss = 0.59128992\n",
            "Iteration 276, loss = 0.59080073\n",
            "Iteration 277, loss = 0.59029087\n",
            "Iteration 278, loss = 0.58980996\n",
            "Iteration 279, loss = 0.58928243\n",
            "Iteration 280, loss = 0.58879718\n",
            "Iteration 281, loss = 0.58828313\n",
            "Iteration 282, loss = 0.58778523\n",
            "Iteration 283, loss = 0.58730062\n",
            "Iteration 284, loss = 0.58680129\n",
            "Iteration 285, loss = 0.58628988\n",
            "Iteration 286, loss = 0.58581143\n",
            "Iteration 287, loss = 0.58529571\n",
            "Iteration 288, loss = 0.58479558\n",
            "Iteration 289, loss = 0.58429727\n",
            "Iteration 290, loss = 0.58380872\n",
            "Iteration 291, loss = 0.58331500\n",
            "Iteration 292, loss = 0.58279045\n",
            "Iteration 293, loss = 0.58228866\n",
            "Iteration 294, loss = 0.58178628\n",
            "Iteration 295, loss = 0.58126105\n",
            "Iteration 296, loss = 0.58075656\n",
            "Iteration 297, loss = 0.58025987\n",
            "Iteration 298, loss = 0.57972680\n",
            "Iteration 299, loss = 0.57922890\n",
            "Iteration 300, loss = 0.57869655\n",
            "Iteration 301, loss = 0.57819989\n",
            "Iteration 302, loss = 0.57768391\n",
            "Iteration 303, loss = 0.57716781\n",
            "Iteration 304, loss = 0.57666349\n",
            "Iteration 305, loss = 0.57615738\n",
            "Iteration 306, loss = 0.57563629\n",
            "Iteration 307, loss = 0.57514241\n",
            "Iteration 308, loss = 0.57462939\n",
            "Iteration 309, loss = 0.57412337\n",
            "Iteration 310, loss = 0.57360917\n",
            "Iteration 311, loss = 0.57310850\n",
            "Iteration 312, loss = 0.57260261\n",
            "Iteration 313, loss = 0.57208996\n",
            "Iteration 314, loss = 0.57156531\n",
            "Iteration 315, loss = 0.57106275\n",
            "Iteration 316, loss = 0.57054708\n",
            "Iteration 317, loss = 0.57003373\n",
            "Iteration 318, loss = 0.56954050\n",
            "Iteration 319, loss = 0.56901446\n",
            "Iteration 320, loss = 0.56849177\n",
            "Iteration 321, loss = 0.56798003\n",
            "Iteration 322, loss = 0.56745307\n",
            "Iteration 323, loss = 0.56694276\n",
            "Iteration 324, loss = 0.56644225\n",
            "Iteration 325, loss = 0.56592387\n",
            "Iteration 326, loss = 0.56540697\n",
            "Iteration 327, loss = 0.56489280\n",
            "Iteration 328, loss = 0.56436869\n",
            "Iteration 329, loss = 0.56385071\n",
            "Iteration 330, loss = 0.56334252\n",
            "Iteration 331, loss = 0.56284172\n",
            "Iteration 332, loss = 0.56232663\n",
            "Iteration 333, loss = 0.56179975\n",
            "Iteration 334, loss = 0.56128368\n",
            "Iteration 335, loss = 0.56077060\n",
            "Iteration 336, loss = 0.56025093\n",
            "Iteration 337, loss = 0.55974498\n",
            "Iteration 338, loss = 0.55924147\n",
            "Iteration 339, loss = 0.55870614\n",
            "Iteration 340, loss = 0.55823445\n",
            "Iteration 341, loss = 0.55771777\n",
            "Iteration 342, loss = 0.55718945\n",
            "Iteration 343, loss = 0.55670692\n",
            "Iteration 344, loss = 0.55620514\n",
            "Iteration 345, loss = 0.55570329\n",
            "Iteration 346, loss = 0.55520969\n",
            "Iteration 347, loss = 0.55470070\n",
            "Iteration 348, loss = 0.55421055\n",
            "Iteration 349, loss = 0.55369835\n",
            "Iteration 350, loss = 0.55318413\n",
            "Iteration 351, loss = 0.55270034\n",
            "Iteration 352, loss = 0.55220095\n",
            "Iteration 353, loss = 0.55171297\n",
            "Iteration 354, loss = 0.55119619\n",
            "Iteration 355, loss = 0.55071157\n",
            "Iteration 356, loss = 0.55020899\n",
            "Iteration 357, loss = 0.54971547\n",
            "Iteration 358, loss = 0.54921459\n",
            "Iteration 359, loss = 0.54872191\n",
            "Iteration 360, loss = 0.54822207\n",
            "Iteration 361, loss = 0.54774779\n",
            "Iteration 362, loss = 0.54724134\n",
            "Iteration 363, loss = 0.54675723\n",
            "Iteration 364, loss = 0.54627670\n",
            "Iteration 365, loss = 0.54578965\n",
            "Iteration 366, loss = 0.54532753\n",
            "Iteration 367, loss = 0.54484148\n",
            "Iteration 368, loss = 0.54435551\n",
            "Iteration 369, loss = 0.54386713\n",
            "Iteration 370, loss = 0.54339880\n",
            "Iteration 371, loss = 0.54291449\n",
            "Iteration 372, loss = 0.54244332\n",
            "Iteration 373, loss = 0.54197475\n",
            "Iteration 374, loss = 0.54149988\n",
            "Iteration 375, loss = 0.54101682\n",
            "Iteration 376, loss = 0.54054692\n",
            "Iteration 377, loss = 0.54005993\n",
            "Iteration 378, loss = 0.53959585\n",
            "Iteration 379, loss = 0.53912270\n",
            "Iteration 380, loss = 0.53865489\n",
            "Iteration 381, loss = 0.53818822\n",
            "Iteration 382, loss = 0.53772173\n",
            "Iteration 383, loss = 0.53725801\n",
            "Iteration 384, loss = 0.53678927\n",
            "Iteration 385, loss = 0.53635106\n",
            "Iteration 386, loss = 0.53587200\n",
            "Iteration 387, loss = 0.53542782\n",
            "Iteration 388, loss = 0.53497858\n",
            "Iteration 389, loss = 0.53452868\n",
            "Iteration 390, loss = 0.53408306\n",
            "Iteration 391, loss = 0.53363004\n",
            "Iteration 392, loss = 0.53317911\n",
            "Iteration 393, loss = 0.53272328\n",
            "Iteration 394, loss = 0.53227974\n",
            "Iteration 395, loss = 0.53183088\n",
            "Iteration 396, loss = 0.53139912\n",
            "Iteration 397, loss = 0.53095545\n",
            "Iteration 398, loss = 0.53051074\n",
            "Iteration 399, loss = 0.53006810\n",
            "Iteration 400, loss = 0.52961777\n",
            "Iteration 401, loss = 0.52917852\n",
            "Iteration 402, loss = 0.52874056\n",
            "Iteration 403, loss = 0.52829706\n",
            "Iteration 404, loss = 0.52786591\n",
            "Iteration 405, loss = 0.52742546\n",
            "Iteration 406, loss = 0.52700821\n",
            "Iteration 407, loss = 0.52656798\n",
            "Iteration 408, loss = 0.52612912\n",
            "Iteration 409, loss = 0.52569708\n",
            "Iteration 410, loss = 0.52527711\n",
            "Iteration 411, loss = 0.52485106\n",
            "Iteration 412, loss = 0.52442120\n",
            "Iteration 413, loss = 0.52398715\n",
            "Iteration 414, loss = 0.52356539\n",
            "Iteration 415, loss = 0.52313607\n",
            "Iteration 416, loss = 0.52271109\n",
            "Iteration 417, loss = 0.52229859\n",
            "Iteration 418, loss = 0.52187555\n",
            "Iteration 419, loss = 0.52145280\n",
            "Iteration 420, loss = 0.52105341\n",
            "Iteration 421, loss = 0.52061970\n",
            "Iteration 422, loss = 0.52021928\n",
            "Iteration 423, loss = 0.51978766\n",
            "Iteration 424, loss = 0.51938027\n",
            "Iteration 425, loss = 0.51897518\n",
            "Iteration 426, loss = 0.51857584\n",
            "Iteration 427, loss = 0.51817065\n",
            "Iteration 428, loss = 0.51776677\n",
            "Iteration 429, loss = 0.51735561\n",
            "Iteration 430, loss = 0.51696098\n",
            "Iteration 431, loss = 0.51656839\n",
            "Iteration 432, loss = 0.51618037\n",
            "Iteration 433, loss = 0.51579303\n",
            "Iteration 434, loss = 0.51539619\n",
            "Iteration 435, loss = 0.51500170\n",
            "Iteration 436, loss = 0.51462003\n",
            "Iteration 437, loss = 0.51421979\n",
            "Iteration 438, loss = 0.51382980\n",
            "Iteration 439, loss = 0.51344986\n",
            "Iteration 440, loss = 0.51304858\n",
            "Iteration 441, loss = 0.51266812\n",
            "Iteration 442, loss = 0.51228184\n",
            "Iteration 443, loss = 0.51189503\n",
            "Iteration 444, loss = 0.51151524\n",
            "Iteration 445, loss = 0.51112234\n",
            "Iteration 446, loss = 0.51075127\n",
            "Iteration 447, loss = 0.51037519\n",
            "Iteration 448, loss = 0.50999223\n",
            "Iteration 449, loss = 0.50964115\n",
            "Iteration 450, loss = 0.50924587\n",
            "Iteration 451, loss = 0.50888412\n",
            "Iteration 452, loss = 0.50851100\n",
            "Iteration 453, loss = 0.50816620\n",
            "Iteration 454, loss = 0.50778243\n",
            "Iteration 455, loss = 0.50741474\n",
            "Iteration 456, loss = 0.50705389\n",
            "Iteration 457, loss = 0.50667144\n",
            "Iteration 458, loss = 0.50631449\n",
            "Iteration 459, loss = 0.50595655\n",
            "Iteration 460, loss = 0.50557223\n",
            "Iteration 461, loss = 0.50522978\n",
            "Iteration 462, loss = 0.50487021\n",
            "Iteration 463, loss = 0.50450986\n",
            "Iteration 464, loss = 0.50417682\n",
            "Iteration 465, loss = 0.50381559\n",
            "Iteration 466, loss = 0.50346367\n",
            "Iteration 467, loss = 0.50312043\n",
            "Iteration 468, loss = 0.50277456\n",
            "Iteration 469, loss = 0.50242313\n",
            "Iteration 470, loss = 0.50207963\n",
            "Iteration 471, loss = 0.50172798\n",
            "Iteration 472, loss = 0.50139171\n",
            "Iteration 473, loss = 0.50104730\n",
            "Iteration 474, loss = 0.50071974\n",
            "Iteration 475, loss = 0.50038582\n",
            "Iteration 476, loss = 0.50003437\n",
            "Iteration 477, loss = 0.49969533\n",
            "Iteration 478, loss = 0.49937507\n",
            "Iteration 479, loss = 0.49902132\n",
            "Iteration 480, loss = 0.49869473\n",
            "Iteration 481, loss = 0.49836257\n",
            "Iteration 482, loss = 0.49804735\n",
            "Iteration 483, loss = 0.49770383\n",
            "Iteration 484, loss = 0.49739481\n",
            "Iteration 485, loss = 0.49707230\n",
            "Iteration 486, loss = 0.49676338\n",
            "Iteration 487, loss = 0.49643257\n",
            "Iteration 488, loss = 0.49610426\n",
            "Iteration 489, loss = 0.49578376\n",
            "Iteration 490, loss = 0.49545476\n",
            "Iteration 491, loss = 0.49510985\n",
            "Iteration 492, loss = 0.49478835\n",
            "Iteration 493, loss = 0.49447019\n",
            "Iteration 494, loss = 0.49415663\n",
            "Iteration 495, loss = 0.49382206\n",
            "Iteration 496, loss = 0.49352795\n",
            "Iteration 497, loss = 0.49319888\n",
            "Iteration 498, loss = 0.49290551\n",
            "Iteration 499, loss = 0.49257602\n",
            "Iteration 500, loss = 0.49227673\n",
            "Iteration 501, loss = 0.49195689\n",
            "Iteration 502, loss = 0.49163922\n",
            "Iteration 503, loss = 0.49131225\n",
            "Iteration 504, loss = 0.49100263\n",
            "Iteration 505, loss = 0.49067478\n",
            "Iteration 506, loss = 0.49038360\n",
            "Iteration 507, loss = 0.49007587\n",
            "Iteration 508, loss = 0.48976380\n",
            "Iteration 509, loss = 0.48945888\n",
            "Iteration 510, loss = 0.48917084\n",
            "Iteration 511, loss = 0.48887421\n",
            "Iteration 512, loss = 0.48858091\n",
            "Iteration 513, loss = 0.48829246\n",
            "Iteration 514, loss = 0.48799242\n",
            "Iteration 515, loss = 0.48771959\n",
            "Iteration 516, loss = 0.48743976\n",
            "Iteration 517, loss = 0.48714836\n",
            "Iteration 518, loss = 0.48686265\n",
            "Iteration 519, loss = 0.48656765\n",
            "Iteration 520, loss = 0.48628947\n",
            "Iteration 521, loss = 0.48601217\n",
            "Iteration 522, loss = 0.48573027\n",
            "Iteration 523, loss = 0.48545274\n",
            "Iteration 524, loss = 0.48516782\n",
            "Iteration 525, loss = 0.48488706\n",
            "Iteration 526, loss = 0.48460436\n",
            "Iteration 527, loss = 0.48433965\n",
            "Iteration 528, loss = 0.48407798\n",
            "Iteration 529, loss = 0.48378245\n",
            "Iteration 530, loss = 0.48352008\n",
            "Iteration 531, loss = 0.48324826\n",
            "Iteration 532, loss = 0.48298639\n",
            "Iteration 533, loss = 0.48269746\n",
            "Iteration 534, loss = 0.48243419\n",
            "Iteration 535, loss = 0.48216156\n",
            "Iteration 536, loss = 0.48190441\n",
            "Iteration 537, loss = 0.48164307\n",
            "Iteration 538, loss = 0.48137331\n",
            "Iteration 539, loss = 0.48112569\n",
            "Iteration 540, loss = 0.48085734\n",
            "Iteration 541, loss = 0.48059431\n",
            "Iteration 542, loss = 0.48034968\n",
            "Iteration 543, loss = 0.48010068\n",
            "Iteration 544, loss = 0.47985181\n",
            "Iteration 545, loss = 0.47960482\n",
            "Iteration 546, loss = 0.47936106\n",
            "Iteration 547, loss = 0.47910286\n",
            "Iteration 548, loss = 0.47886033\n",
            "Iteration 549, loss = 0.47859743\n",
            "Iteration 550, loss = 0.47834911\n",
            "Iteration 551, loss = 0.47810445\n",
            "Iteration 552, loss = 0.47785219\n",
            "Iteration 553, loss = 0.47762117\n",
            "Iteration 554, loss = 0.47738685\n",
            "Iteration 555, loss = 0.47714522\n",
            "Iteration 556, loss = 0.47692039\n",
            "Iteration 557, loss = 0.47667511\n",
            "Iteration 558, loss = 0.47644332\n",
            "Iteration 559, loss = 0.47620898\n",
            "Iteration 560, loss = 0.47599683\n",
            "Iteration 561, loss = 0.47575576\n",
            "Iteration 562, loss = 0.47554880\n",
            "Iteration 563, loss = 0.47532083\n",
            "Iteration 564, loss = 0.47506434\n",
            "Iteration 565, loss = 0.47485633\n",
            "Iteration 566, loss = 0.47462772\n",
            "Iteration 567, loss = 0.47442330\n",
            "Iteration 568, loss = 0.47417650\n",
            "Iteration 569, loss = 0.47395474\n",
            "Iteration 570, loss = 0.47374173\n",
            "Iteration 571, loss = 0.47350473\n",
            "Iteration 572, loss = 0.47331166\n",
            "Iteration 573, loss = 0.47305459\n",
            "Iteration 574, loss = 0.47283363\n",
            "Iteration 575, loss = 0.47261634\n",
            "Iteration 576, loss = 0.47239396\n",
            "Iteration 577, loss = 0.47217647\n",
            "Iteration 578, loss = 0.47195042\n",
            "Iteration 579, loss = 0.47172922\n",
            "Iteration 580, loss = 0.47151721\n",
            "Iteration 581, loss = 0.47130414\n",
            "Iteration 582, loss = 0.47108331\n",
            "Iteration 583, loss = 0.47085849\n",
            "Iteration 584, loss = 0.47063667\n",
            "Iteration 585, loss = 0.47043140\n",
            "Iteration 586, loss = 0.47020054\n",
            "Iteration 587, loss = 0.46998375\n",
            "Iteration 588, loss = 0.46979126\n",
            "Iteration 589, loss = 0.46957284\n",
            "Iteration 590, loss = 0.46937373\n",
            "Iteration 591, loss = 0.46917186\n",
            "Iteration 592, loss = 0.46896613\n",
            "Iteration 593, loss = 0.46878000\n",
            "Iteration 594, loss = 0.46857684\n",
            "Iteration 595, loss = 0.46837530\n",
            "Iteration 596, loss = 0.46817623\n",
            "Iteration 597, loss = 0.46797787\n",
            "Iteration 598, loss = 0.46778588\n",
            "Iteration 599, loss = 0.46758097\n",
            "Iteration 600, loss = 0.46738230\n",
            "Iteration 601, loss = 0.46719814\n",
            "Iteration 602, loss = 0.46699531\n",
            "Iteration 603, loss = 0.46682480\n",
            "Iteration 604, loss = 0.46661955\n",
            "Iteration 605, loss = 0.46644305\n",
            "Iteration 606, loss = 0.46625539\n",
            "Iteration 607, loss = 0.46607177\n",
            "Iteration 608, loss = 0.46585963\n",
            "Iteration 609, loss = 0.46568627\n",
            "Iteration 610, loss = 0.46549763\n",
            "Iteration 611, loss = 0.46532120\n",
            "Iteration 612, loss = 0.46513141\n",
            "Iteration 613, loss = 0.46494635\n",
            "Iteration 614, loss = 0.46475680\n",
            "Iteration 615, loss = 0.46459150\n",
            "Iteration 616, loss = 0.46440123\n",
            "Iteration 617, loss = 0.46423460\n",
            "Iteration 618, loss = 0.46403107\n",
            "Iteration 619, loss = 0.46385329\n",
            "Iteration 620, loss = 0.46367326\n",
            "Iteration 621, loss = 0.46350031\n",
            "Iteration 622, loss = 0.46332531\n",
            "Iteration 623, loss = 0.46314922\n",
            "Iteration 624, loss = 0.46296792\n",
            "Iteration 625, loss = 0.46281251\n",
            "Iteration 626, loss = 0.46264857\n",
            "Iteration 627, loss = 0.46244880\n",
            "Iteration 628, loss = 0.46228920\n",
            "Iteration 629, loss = 0.46212079\n",
            "Iteration 630, loss = 0.46195275\n",
            "Iteration 631, loss = 0.46178310\n",
            "Iteration 632, loss = 0.46162730\n",
            "Iteration 633, loss = 0.46148032\n",
            "Iteration 634, loss = 0.46130984\n",
            "Iteration 635, loss = 0.46114756\n",
            "Iteration 636, loss = 0.46100802\n",
            "Iteration 637, loss = 0.46084841\n",
            "Iteration 638, loss = 0.46071025\n",
            "Iteration 639, loss = 0.46054734\n",
            "Iteration 640, loss = 0.46040246\n",
            "Iteration 641, loss = 0.46024654\n",
            "Iteration 642, loss = 0.46010105\n",
            "Iteration 643, loss = 0.45995613\n",
            "Iteration 644, loss = 0.45980364\n",
            "Iteration 645, loss = 0.45965757\n",
            "Iteration 646, loss = 0.45952084\n",
            "Iteration 647, loss = 0.45936894\n",
            "Iteration 648, loss = 0.45923149\n",
            "Iteration 649, loss = 0.45907825\n",
            "Iteration 650, loss = 0.45893239\n",
            "Iteration 651, loss = 0.45879975\n",
            "Iteration 652, loss = 0.45865760\n",
            "Iteration 653, loss = 0.45851808\n",
            "Iteration 654, loss = 0.45838779\n",
            "Iteration 655, loss = 0.45822052\n",
            "Iteration 656, loss = 0.45808339\n",
            "Iteration 657, loss = 0.45794405\n",
            "Iteration 658, loss = 0.45779836\n",
            "Iteration 659, loss = 0.45765368\n",
            "Iteration 660, loss = 0.45750919\n",
            "Iteration 661, loss = 0.45737927\n",
            "Iteration 662, loss = 0.45723936\n",
            "Iteration 663, loss = 0.45708886\n",
            "Iteration 664, loss = 0.45694552\n",
            "Iteration 665, loss = 0.45679284\n",
            "Iteration 666, loss = 0.45665786\n",
            "Iteration 667, loss = 0.45650672\n",
            "Iteration 668, loss = 0.45637323\n",
            "Iteration 669, loss = 0.45622953\n",
            "Iteration 670, loss = 0.45609830\n",
            "Iteration 671, loss = 0.45595747\n",
            "Iteration 672, loss = 0.45581996\n",
            "Iteration 673, loss = 0.45568737\n",
            "Iteration 674, loss = 0.45555113\n",
            "Iteration 675, loss = 0.45541839\n",
            "Iteration 676, loss = 0.45530414\n",
            "Iteration 677, loss = 0.45517856\n",
            "Iteration 678, loss = 0.45505846\n",
            "Iteration 679, loss = 0.45493231\n",
            "Iteration 680, loss = 0.45481714\n",
            "Iteration 681, loss = 0.45470467\n",
            "Iteration 682, loss = 0.45457982\n",
            "Iteration 683, loss = 0.45445568\n",
            "Iteration 684, loss = 0.45432769\n",
            "Iteration 685, loss = 0.45421360\n",
            "Iteration 686, loss = 0.45409902\n",
            "Iteration 687, loss = 0.45396948\n",
            "Iteration 688, loss = 0.45384551\n",
            "Iteration 689, loss = 0.45373007\n",
            "Iteration 690, loss = 0.45360923\n",
            "Iteration 691, loss = 0.45347485\n",
            "Iteration 692, loss = 0.45335785\n",
            "Iteration 693, loss = 0.45323799\n",
            "Iteration 694, loss = 0.45313579\n",
            "Iteration 695, loss = 0.45303587\n",
            "Iteration 696, loss = 0.45290289\n",
            "Iteration 697, loss = 0.45278662\n",
            "Iteration 698, loss = 0.45267325\n",
            "Iteration 699, loss = 0.45257339\n",
            "Iteration 700, loss = 0.45245501\n",
            "Iteration 701, loss = 0.45234528\n",
            "Iteration 702, loss = 0.45224603\n",
            "Iteration 703, loss = 0.45213486\n",
            "Iteration 704, loss = 0.45202725\n",
            "Iteration 705, loss = 0.45191027\n",
            "Iteration 706, loss = 0.45181503\n",
            "Iteration 707, loss = 0.45169885\n",
            "Iteration 708, loss = 0.45159832\n",
            "Iteration 709, loss = 0.45148494\n",
            "Iteration 710, loss = 0.45138156\n",
            "Iteration 711, loss = 0.45128368\n",
            "Iteration 712, loss = 0.45117394\n",
            "Iteration 713, loss = 0.45108698\n",
            "Iteration 714, loss = 0.45095355\n",
            "Iteration 715, loss = 0.45085924\n",
            "Iteration 716, loss = 0.45075709\n",
            "Iteration 717, loss = 0.45064986\n",
            "Iteration 718, loss = 0.45056159\n",
            "Iteration 719, loss = 0.45045491\n",
            "Iteration 720, loss = 0.45034312\n",
            "Iteration 721, loss = 0.45023654\n",
            "Iteration 722, loss = 0.45013844\n",
            "Iteration 723, loss = 0.45003516\n",
            "Iteration 724, loss = 0.44994842\n",
            "Iteration 725, loss = 0.44983805\n",
            "Iteration 726, loss = 0.44975584\n",
            "Iteration 727, loss = 0.44966500\n",
            "Iteration 728, loss = 0.44956128\n",
            "Iteration 729, loss = 0.44944957\n",
            "Iteration 730, loss = 0.44935494\n",
            "Iteration 731, loss = 0.44927374\n",
            "Iteration 732, loss = 0.44915921\n",
            "Iteration 733, loss = 0.44907503\n",
            "Iteration 734, loss = 0.44897306\n",
            "Iteration 735, loss = 0.44887826\n",
            "Iteration 736, loss = 0.44879087\n",
            "Iteration 737, loss = 0.44869452\n",
            "Iteration 738, loss = 0.44860942\n",
            "Iteration 739, loss = 0.44850730\n",
            "Iteration 740, loss = 0.44841878\n",
            "Iteration 741, loss = 0.44832808\n",
            "Iteration 742, loss = 0.44824209\n",
            "Iteration 743, loss = 0.44814414\n",
            "Iteration 744, loss = 0.44805613\n",
            "Iteration 745, loss = 0.44797061\n",
            "Iteration 746, loss = 0.44788234\n",
            "Iteration 747, loss = 0.44779421\n",
            "Iteration 748, loss = 0.44772488\n",
            "Iteration 749, loss = 0.44762739\n",
            "Iteration 750, loss = 0.44754378\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.71557133\n",
            "Iteration 2, loss = 0.71522472\n",
            "Iteration 3, loss = 0.71469984\n",
            "Iteration 4, loss = 0.71407233\n",
            "Iteration 5, loss = 0.71336579\n",
            "Iteration 6, loss = 0.71260588\n",
            "Iteration 7, loss = 0.71183386\n",
            "Iteration 8, loss = 0.71105461\n",
            "Iteration 9, loss = 0.71034494\n",
            "Iteration 10, loss = 0.70957786\n",
            "Iteration 11, loss = 0.70879611\n",
            "Iteration 12, loss = 0.70803636\n",
            "Iteration 13, loss = 0.70728493\n",
            "Iteration 14, loss = 0.70653370\n",
            "Iteration 15, loss = 0.70583991\n",
            "Iteration 16, loss = 0.70515966\n",
            "Iteration 17, loss = 0.70439439\n",
            "Iteration 18, loss = 0.70374034\n",
            "Iteration 19, loss = 0.70303991\n",
            "Iteration 20, loss = 0.70236768\n",
            "Iteration 21, loss = 0.70171090\n",
            "Iteration 22, loss = 0.70105291\n",
            "Iteration 23, loss = 0.70041901\n",
            "Iteration 24, loss = 0.69978023\n",
            "Iteration 25, loss = 0.69916995\n",
            "Iteration 26, loss = 0.69856802\n",
            "Iteration 27, loss = 0.69798692\n",
            "Iteration 28, loss = 0.69739844\n",
            "Iteration 29, loss = 0.69683404\n",
            "Iteration 30, loss = 0.69626212\n",
            "Iteration 31, loss = 0.69571499\n",
            "Iteration 32, loss = 0.69516580\n",
            "Iteration 33, loss = 0.69463085\n",
            "Iteration 34, loss = 0.69410001\n",
            "Iteration 35, loss = 0.69357050\n",
            "Iteration 36, loss = 0.69305057\n",
            "Iteration 37, loss = 0.69253392\n",
            "Iteration 38, loss = 0.69205199\n",
            "Iteration 39, loss = 0.69153685\n",
            "Iteration 40, loss = 0.69105016\n",
            "Iteration 41, loss = 0.69055354\n",
            "Iteration 42, loss = 0.69008109\n",
            "Iteration 43, loss = 0.68960158\n",
            "Iteration 44, loss = 0.68913663\n",
            "Iteration 45, loss = 0.68868071\n",
            "Iteration 46, loss = 0.68820485\n",
            "Iteration 47, loss = 0.68775271\n",
            "Iteration 48, loss = 0.68729281\n",
            "Iteration 49, loss = 0.68684585\n",
            "Iteration 50, loss = 0.68639083\n",
            "Iteration 51, loss = 0.68596180\n",
            "Iteration 52, loss = 0.68551614\n",
            "Iteration 53, loss = 0.68510604\n",
            "Iteration 54, loss = 0.68465605\n",
            "Iteration 55, loss = 0.68421637\n",
            "Iteration 56, loss = 0.68378996\n",
            "Iteration 57, loss = 0.68336094\n",
            "Iteration 58, loss = 0.68293469\n",
            "Iteration 59, loss = 0.68251464\n",
            "Iteration 60, loss = 0.68210015\n",
            "Iteration 61, loss = 0.68168122\n",
            "Iteration 62, loss = 0.68130963\n",
            "Iteration 63, loss = 0.68087982\n",
            "Iteration 64, loss = 0.68046726\n",
            "Iteration 65, loss = 0.68006776\n",
            "Iteration 66, loss = 0.67969702\n",
            "Iteration 67, loss = 0.67930035\n",
            "Iteration 68, loss = 0.67890945\n",
            "Iteration 69, loss = 0.67851358\n",
            "Iteration 70, loss = 0.67813494\n",
            "Iteration 71, loss = 0.67776086\n",
            "Iteration 72, loss = 0.67737219\n",
            "Iteration 73, loss = 0.67698954\n",
            "Iteration 74, loss = 0.67661088\n",
            "Iteration 75, loss = 0.67622447\n",
            "Iteration 76, loss = 0.67585447\n",
            "Iteration 77, loss = 0.67546651\n",
            "Iteration 78, loss = 0.67509602\n",
            "Iteration 79, loss = 0.67471318\n",
            "Iteration 80, loss = 0.67434472\n",
            "Iteration 81, loss = 0.67397181\n",
            "Iteration 82, loss = 0.67358443\n",
            "Iteration 83, loss = 0.67320982\n",
            "Iteration 84, loss = 0.67284175\n",
            "Iteration 85, loss = 0.67245834\n",
            "Iteration 86, loss = 0.67208091\n",
            "Iteration 87, loss = 0.67172619\n",
            "Iteration 88, loss = 0.67134953\n",
            "Iteration 89, loss = 0.67098902\n",
            "Iteration 90, loss = 0.67061131\n",
            "Iteration 91, loss = 0.67023897\n",
            "Iteration 92, loss = 0.66986459\n",
            "Iteration 93, loss = 0.66949579\n",
            "Iteration 94, loss = 0.66912856\n",
            "Iteration 95, loss = 0.66874351\n",
            "Iteration 96, loss = 0.66837703\n",
            "Iteration 97, loss = 0.66801442\n",
            "Iteration 98, loss = 0.66764008\n",
            "Iteration 99, loss = 0.66726285\n",
            "Iteration 100, loss = 0.66690070\n",
            "Iteration 101, loss = 0.66652177\n",
            "Iteration 102, loss = 0.66615299\n",
            "Iteration 103, loss = 0.66579379\n",
            "Iteration 104, loss = 0.66541324\n",
            "Iteration 105, loss = 0.66505036\n",
            "Iteration 106, loss = 0.66470008\n",
            "Iteration 107, loss = 0.66432065\n",
            "Iteration 108, loss = 0.66395362\n",
            "Iteration 109, loss = 0.66358578\n",
            "Iteration 110, loss = 0.66322133\n",
            "Iteration 111, loss = 0.66286148\n",
            "Iteration 112, loss = 0.66249579\n",
            "Iteration 113, loss = 0.66213226\n",
            "Iteration 114, loss = 0.66176918\n",
            "Iteration 115, loss = 0.66140053\n",
            "Iteration 116, loss = 0.66103991\n",
            "Iteration 117, loss = 0.66067190\n",
            "Iteration 118, loss = 0.66031467\n",
            "Iteration 119, loss = 0.65995381\n",
            "Iteration 120, loss = 0.65959606\n",
            "Iteration 121, loss = 0.65923011\n",
            "Iteration 122, loss = 0.65886573\n",
            "Iteration 123, loss = 0.65851301\n",
            "Iteration 124, loss = 0.65813116\n",
            "Iteration 125, loss = 0.65776983\n",
            "Iteration 126, loss = 0.65739159\n",
            "Iteration 127, loss = 0.65701590\n",
            "Iteration 128, loss = 0.65664191\n",
            "Iteration 129, loss = 0.65629105\n",
            "Iteration 130, loss = 0.65589543\n",
            "Iteration 131, loss = 0.65553344\n",
            "Iteration 132, loss = 0.65515667\n",
            "Iteration 133, loss = 0.65477276\n",
            "Iteration 134, loss = 0.65439870\n",
            "Iteration 135, loss = 0.65404425\n",
            "Iteration 136, loss = 0.65367331\n",
            "Iteration 137, loss = 0.65329501\n",
            "Iteration 138, loss = 0.65292955\n",
            "Iteration 139, loss = 0.65256534\n",
            "Iteration 140, loss = 0.65219099\n",
            "Iteration 141, loss = 0.65182760\n",
            "Iteration 142, loss = 0.65144203\n",
            "Iteration 143, loss = 0.65106746\n",
            "Iteration 144, loss = 0.65069449\n",
            "Iteration 145, loss = 0.65030905\n",
            "Iteration 146, loss = 0.64992885\n",
            "Iteration 147, loss = 0.64954971\n",
            "Iteration 148, loss = 0.64916095\n",
            "Iteration 149, loss = 0.64878231\n",
            "Iteration 150, loss = 0.64839614\n",
            "Iteration 151, loss = 0.64802200\n",
            "Iteration 152, loss = 0.64763948\n",
            "Iteration 153, loss = 0.64724012\n",
            "Iteration 154, loss = 0.64684871\n",
            "Iteration 155, loss = 0.64646320\n",
            "Iteration 156, loss = 0.64608115\n",
            "Iteration 157, loss = 0.64570053\n",
            "Iteration 158, loss = 0.64532635\n",
            "Iteration 159, loss = 0.64493318\n",
            "Iteration 160, loss = 0.64455764\n",
            "Iteration 161, loss = 0.64416514\n",
            "Iteration 162, loss = 0.64376202\n",
            "Iteration 163, loss = 0.64336940\n",
            "Iteration 164, loss = 0.64298121\n",
            "Iteration 165, loss = 0.64259349\n",
            "Iteration 166, loss = 0.64218738\n",
            "Iteration 167, loss = 0.64180625\n",
            "Iteration 168, loss = 0.64139171\n",
            "Iteration 169, loss = 0.64099739\n",
            "Iteration 170, loss = 0.64059706\n",
            "Iteration 171, loss = 0.64019735\n",
            "Iteration 172, loss = 0.63980081\n",
            "Iteration 173, loss = 0.63939750\n",
            "Iteration 174, loss = 0.63900621\n",
            "Iteration 175, loss = 0.63859278\n",
            "Iteration 176, loss = 0.63818920\n",
            "Iteration 177, loss = 0.63780183\n",
            "Iteration 178, loss = 0.63737124\n",
            "Iteration 179, loss = 0.63697358\n",
            "Iteration 180, loss = 0.63656118\n",
            "Iteration 181, loss = 0.63615119\n",
            "Iteration 182, loss = 0.63572847\n",
            "Iteration 183, loss = 0.63532580\n",
            "Iteration 184, loss = 0.63490531\n",
            "Iteration 185, loss = 0.63450810\n",
            "Iteration 186, loss = 0.63408579\n",
            "Iteration 187, loss = 0.63365400\n",
            "Iteration 188, loss = 0.63324139\n",
            "Iteration 189, loss = 0.63281422\n",
            "Iteration 190, loss = 0.63238484\n",
            "Iteration 191, loss = 0.63196930\n",
            "Iteration 192, loss = 0.63154731\n",
            "Iteration 193, loss = 0.63111095\n",
            "Iteration 194, loss = 0.63070642\n",
            "Iteration 195, loss = 0.63027950\n",
            "Iteration 196, loss = 0.62984750\n",
            "Iteration 197, loss = 0.62942847\n",
            "Iteration 198, loss = 0.62900337\n",
            "Iteration 199, loss = 0.62858987\n",
            "Iteration 200, loss = 0.62816854\n",
            "Iteration 201, loss = 0.62772229\n",
            "Iteration 202, loss = 0.62730834\n",
            "Iteration 203, loss = 0.62688160\n",
            "Iteration 204, loss = 0.62644738\n",
            "Iteration 205, loss = 0.62601389\n",
            "Iteration 206, loss = 0.62556316\n",
            "Iteration 207, loss = 0.62514951\n",
            "Iteration 208, loss = 0.62468750\n",
            "Iteration 209, loss = 0.62424977\n",
            "Iteration 210, loss = 0.62380701\n",
            "Iteration 211, loss = 0.62336927\n",
            "Iteration 212, loss = 0.62292256\n",
            "Iteration 213, loss = 0.62247332\n",
            "Iteration 214, loss = 0.62203627\n",
            "Iteration 215, loss = 0.62158585\n",
            "Iteration 216, loss = 0.62112613\n",
            "Iteration 217, loss = 0.62066342\n",
            "Iteration 218, loss = 0.62022528\n",
            "Iteration 219, loss = 0.61975657\n",
            "Iteration 220, loss = 0.61930146\n",
            "Iteration 221, loss = 0.61885011\n",
            "Iteration 222, loss = 0.61839245\n",
            "Iteration 223, loss = 0.61792848\n",
            "Iteration 224, loss = 0.61747680\n",
            "Iteration 225, loss = 0.61700205\n",
            "Iteration 226, loss = 0.61654957\n",
            "Iteration 227, loss = 0.61606798\n",
            "Iteration 228, loss = 0.61561007\n",
            "Iteration 229, loss = 0.61514082\n",
            "Iteration 230, loss = 0.61466906\n",
            "Iteration 231, loss = 0.61421212\n",
            "Iteration 232, loss = 0.61373169\n",
            "Iteration 233, loss = 0.61328015\n",
            "Iteration 234, loss = 0.61280197\n",
            "Iteration 235, loss = 0.61232987\n",
            "Iteration 236, loss = 0.61184305\n",
            "Iteration 237, loss = 0.61137885\n",
            "Iteration 238, loss = 0.61090087\n",
            "Iteration 239, loss = 0.61043297\n",
            "Iteration 240, loss = 0.60994941\n",
            "Iteration 241, loss = 0.60946828\n",
            "Iteration 242, loss = 0.60897550\n",
            "Iteration 243, loss = 0.60850686\n",
            "Iteration 244, loss = 0.60802808\n",
            "Iteration 245, loss = 0.60753768\n",
            "Iteration 246, loss = 0.60704819\n",
            "Iteration 247, loss = 0.60655924\n",
            "Iteration 248, loss = 0.60609938\n",
            "Iteration 249, loss = 0.60560463\n",
            "Iteration 250, loss = 0.60512396\n",
            "Iteration 251, loss = 0.60464970\n",
            "Iteration 252, loss = 0.60415667\n",
            "Iteration 253, loss = 0.60366847\n",
            "Iteration 254, loss = 0.60317780\n",
            "Iteration 255, loss = 0.60268522\n",
            "Iteration 256, loss = 0.60220371\n",
            "Iteration 257, loss = 0.60169565\n",
            "Iteration 258, loss = 0.60122845\n",
            "Iteration 259, loss = 0.60072139\n",
            "Iteration 260, loss = 0.60023597\n",
            "Iteration 261, loss = 0.59974950\n",
            "Iteration 262, loss = 0.59924786\n",
            "Iteration 263, loss = 0.59874752\n",
            "Iteration 264, loss = 0.59825021\n",
            "Iteration 265, loss = 0.59775764\n",
            "Iteration 266, loss = 0.59723266\n",
            "Iteration 267, loss = 0.59673612\n",
            "Iteration 268, loss = 0.59623288\n",
            "Iteration 269, loss = 0.59574516\n",
            "Iteration 270, loss = 0.59522969\n",
            "Iteration 271, loss = 0.59474062\n",
            "Iteration 272, loss = 0.59424185\n",
            "Iteration 273, loss = 0.59372519\n",
            "Iteration 274, loss = 0.59325041\n",
            "Iteration 275, loss = 0.59274024\n",
            "Iteration 276, loss = 0.59224231\n",
            "Iteration 277, loss = 0.59172225\n",
            "Iteration 278, loss = 0.59124042\n",
            "Iteration 279, loss = 0.59070157\n",
            "Iteration 280, loss = 0.59020038\n",
            "Iteration 281, loss = 0.58968628\n",
            "Iteration 282, loss = 0.58917558\n",
            "Iteration 283, loss = 0.58868271\n",
            "Iteration 284, loss = 0.58817418\n",
            "Iteration 285, loss = 0.58765659\n",
            "Iteration 286, loss = 0.58716854\n",
            "Iteration 287, loss = 0.58665039\n",
            "Iteration 288, loss = 0.58614453\n",
            "Iteration 289, loss = 0.58563032\n",
            "Iteration 290, loss = 0.58513239\n",
            "Iteration 291, loss = 0.58463958\n",
            "Iteration 292, loss = 0.58410081\n",
            "Iteration 293, loss = 0.58358728\n",
            "Iteration 294, loss = 0.58308225\n",
            "Iteration 295, loss = 0.58255434\n",
            "Iteration 296, loss = 0.58203848\n",
            "Iteration 297, loss = 0.58153129\n",
            "Iteration 298, loss = 0.58100624\n",
            "Iteration 299, loss = 0.58050183\n",
            "Iteration 300, loss = 0.57996924\n",
            "Iteration 301, loss = 0.57946470\n",
            "Iteration 302, loss = 0.57895224\n",
            "Iteration 303, loss = 0.57843358\n",
            "Iteration 304, loss = 0.57793349\n",
            "Iteration 305, loss = 0.57741564\n",
            "Iteration 306, loss = 0.57690370\n",
            "Iteration 307, loss = 0.57639064\n",
            "Iteration 308, loss = 0.57587757\n",
            "Iteration 309, loss = 0.57537776\n",
            "Iteration 310, loss = 0.57485791\n",
            "Iteration 311, loss = 0.57436250\n",
            "Iteration 312, loss = 0.57384775\n",
            "Iteration 313, loss = 0.57334253\n",
            "Iteration 314, loss = 0.57282803\n",
            "Iteration 315, loss = 0.57232201\n",
            "Iteration 316, loss = 0.57181119\n",
            "Iteration 317, loss = 0.57129209\n",
            "Iteration 318, loss = 0.57080274\n",
            "Iteration 319, loss = 0.57028459\n",
            "Iteration 320, loss = 0.56976202\n",
            "Iteration 321, loss = 0.56924812\n",
            "Iteration 322, loss = 0.56872331\n",
            "Iteration 323, loss = 0.56821733\n",
            "Iteration 324, loss = 0.56772210\n",
            "Iteration 325, loss = 0.56720495\n",
            "Iteration 326, loss = 0.56668590\n",
            "Iteration 327, loss = 0.56617689\n",
            "Iteration 328, loss = 0.56566435\n",
            "Iteration 329, loss = 0.56514636\n",
            "Iteration 330, loss = 0.56463361\n",
            "Iteration 331, loss = 0.56413443\n",
            "Iteration 332, loss = 0.56361255\n",
            "Iteration 333, loss = 0.56310218\n",
            "Iteration 334, loss = 0.56259022\n",
            "Iteration 335, loss = 0.56208635\n",
            "Iteration 336, loss = 0.56157921\n",
            "Iteration 337, loss = 0.56108775\n",
            "Iteration 338, loss = 0.56058926\n",
            "Iteration 339, loss = 0.56007034\n",
            "Iteration 340, loss = 0.55959260\n",
            "Iteration 341, loss = 0.55910210\n",
            "Iteration 342, loss = 0.55857964\n",
            "Iteration 343, loss = 0.55809682\n",
            "Iteration 344, loss = 0.55759652\n",
            "Iteration 345, loss = 0.55710919\n",
            "Iteration 346, loss = 0.55662389\n",
            "Iteration 347, loss = 0.55612407\n",
            "Iteration 348, loss = 0.55564367\n",
            "Iteration 349, loss = 0.55513358\n",
            "Iteration 350, loss = 0.55463334\n",
            "Iteration 351, loss = 0.55416431\n",
            "Iteration 352, loss = 0.55367494\n",
            "Iteration 353, loss = 0.55318936\n",
            "Iteration 354, loss = 0.55268949\n",
            "Iteration 355, loss = 0.55220983\n",
            "Iteration 356, loss = 0.55171598\n",
            "Iteration 357, loss = 0.55122792\n",
            "Iteration 358, loss = 0.55073673\n",
            "Iteration 359, loss = 0.55024505\n",
            "Iteration 360, loss = 0.54976213\n",
            "Iteration 361, loss = 0.54928261\n",
            "Iteration 362, loss = 0.54878757\n",
            "Iteration 363, loss = 0.54831998\n",
            "Iteration 364, loss = 0.54783925\n",
            "Iteration 365, loss = 0.54734714\n",
            "Iteration 366, loss = 0.54689699\n",
            "Iteration 367, loss = 0.54642454\n",
            "Iteration 368, loss = 0.54594322\n",
            "Iteration 369, loss = 0.54546889\n",
            "Iteration 370, loss = 0.54500251\n",
            "Iteration 371, loss = 0.54454228\n",
            "Iteration 372, loss = 0.54407388\n",
            "Iteration 373, loss = 0.54360977\n",
            "Iteration 374, loss = 0.54315402\n",
            "Iteration 375, loss = 0.54267740\n",
            "Iteration 376, loss = 0.54220878\n",
            "Iteration 377, loss = 0.54174172\n",
            "Iteration 378, loss = 0.54127387\n",
            "Iteration 379, loss = 0.54081764\n",
            "Iteration 380, loss = 0.54035131\n",
            "Iteration 381, loss = 0.53988709\n",
            "Iteration 382, loss = 0.53942900\n",
            "Iteration 383, loss = 0.53897835\n",
            "Iteration 384, loss = 0.53851109\n",
            "Iteration 385, loss = 0.53807150\n",
            "Iteration 386, loss = 0.53761171\n",
            "Iteration 387, loss = 0.53716707\n",
            "Iteration 388, loss = 0.53672814\n",
            "Iteration 389, loss = 0.53628401\n",
            "Iteration 390, loss = 0.53584642\n",
            "Iteration 391, loss = 0.53539726\n",
            "Iteration 392, loss = 0.53495931\n",
            "Iteration 393, loss = 0.53451248\n",
            "Iteration 394, loss = 0.53407225\n",
            "Iteration 395, loss = 0.53364385\n",
            "Iteration 396, loss = 0.53320972\n",
            "Iteration 397, loss = 0.53277768\n",
            "Iteration 398, loss = 0.53234288\n",
            "Iteration 399, loss = 0.53190535\n",
            "Iteration 400, loss = 0.53145612\n",
            "Iteration 401, loss = 0.53103088\n",
            "Iteration 402, loss = 0.53060350\n",
            "Iteration 403, loss = 0.53016719\n",
            "Iteration 404, loss = 0.52973243\n",
            "Iteration 405, loss = 0.52929701\n",
            "Iteration 406, loss = 0.52888792\n",
            "Iteration 407, loss = 0.52844759\n",
            "Iteration 408, loss = 0.52801186\n",
            "Iteration 409, loss = 0.52758317\n",
            "Iteration 410, loss = 0.52715744\n",
            "Iteration 411, loss = 0.52672747\n",
            "Iteration 412, loss = 0.52631131\n",
            "Iteration 413, loss = 0.52586805\n",
            "Iteration 414, loss = 0.52545165\n",
            "Iteration 415, loss = 0.52502076\n",
            "Iteration 416, loss = 0.52458900\n",
            "Iteration 417, loss = 0.52418622\n",
            "Iteration 418, loss = 0.52375525\n",
            "Iteration 419, loss = 0.52332743\n",
            "Iteration 420, loss = 0.52292423\n",
            "Iteration 421, loss = 0.52249023\n",
            "Iteration 422, loss = 0.52208481\n",
            "Iteration 423, loss = 0.52165508\n",
            "Iteration 424, loss = 0.52123778\n",
            "Iteration 425, loss = 0.52083424\n",
            "Iteration 426, loss = 0.52043259\n",
            "Iteration 427, loss = 0.52002185\n",
            "Iteration 428, loss = 0.51961641\n",
            "Iteration 429, loss = 0.51920276\n",
            "Iteration 430, loss = 0.51880493\n",
            "Iteration 431, loss = 0.51840487\n",
            "Iteration 432, loss = 0.51800598\n",
            "Iteration 433, loss = 0.51762168\n",
            "Iteration 434, loss = 0.51722717\n",
            "Iteration 435, loss = 0.51683575\n",
            "Iteration 436, loss = 0.51645984\n",
            "Iteration 437, loss = 0.51606005\n",
            "Iteration 438, loss = 0.51567505\n",
            "Iteration 439, loss = 0.51529579\n",
            "Iteration 440, loss = 0.51489397\n",
            "Iteration 441, loss = 0.51452000\n",
            "Iteration 442, loss = 0.51413168\n",
            "Iteration 443, loss = 0.51373632\n",
            "Iteration 444, loss = 0.51336382\n",
            "Iteration 445, loss = 0.51295377\n",
            "Iteration 446, loss = 0.51258968\n",
            "Iteration 447, loss = 0.51220671\n",
            "Iteration 448, loss = 0.51182642\n",
            "Iteration 449, loss = 0.51146007\n",
            "Iteration 450, loss = 0.51108028\n",
            "Iteration 451, loss = 0.51070483\n",
            "Iteration 452, loss = 0.51033346\n",
            "Iteration 453, loss = 0.51000395\n",
            "Iteration 454, loss = 0.50959659\n",
            "Iteration 455, loss = 0.50923380\n",
            "Iteration 456, loss = 0.50887718\n",
            "Iteration 457, loss = 0.50848381\n",
            "Iteration 458, loss = 0.50813099\n",
            "Iteration 459, loss = 0.50775873\n",
            "Iteration 460, loss = 0.50737589\n",
            "Iteration 461, loss = 0.50702699\n",
            "Iteration 462, loss = 0.50666547\n",
            "Iteration 463, loss = 0.50630174\n",
            "Iteration 464, loss = 0.50595408\n",
            "Iteration 465, loss = 0.50559620\n",
            "Iteration 466, loss = 0.50524223\n",
            "Iteration 467, loss = 0.50489528\n",
            "Iteration 468, loss = 0.50455381\n",
            "Iteration 469, loss = 0.50419157\n",
            "Iteration 470, loss = 0.50384085\n",
            "Iteration 471, loss = 0.50349210\n",
            "Iteration 472, loss = 0.50314879\n",
            "Iteration 473, loss = 0.50279978\n",
            "Iteration 474, loss = 0.50246124\n",
            "Iteration 475, loss = 0.50212573\n",
            "Iteration 476, loss = 0.50177282\n",
            "Iteration 477, loss = 0.50143474\n",
            "Iteration 478, loss = 0.50109917\n",
            "Iteration 479, loss = 0.50075645\n",
            "Iteration 480, loss = 0.50041802\n",
            "Iteration 481, loss = 0.50008320\n",
            "Iteration 482, loss = 0.49976261\n",
            "Iteration 483, loss = 0.49940503\n",
            "Iteration 484, loss = 0.49909676\n",
            "Iteration 485, loss = 0.49876605\n",
            "Iteration 486, loss = 0.49844992\n",
            "Iteration 487, loss = 0.49811348\n",
            "Iteration 488, loss = 0.49778434\n",
            "Iteration 489, loss = 0.49744834\n",
            "Iteration 490, loss = 0.49711993\n",
            "Iteration 491, loss = 0.49677321\n",
            "Iteration 492, loss = 0.49644572\n",
            "Iteration 493, loss = 0.49610986\n",
            "Iteration 494, loss = 0.49579668\n",
            "Iteration 495, loss = 0.49546833\n",
            "Iteration 496, loss = 0.49516649\n",
            "Iteration 497, loss = 0.49482936\n",
            "Iteration 498, loss = 0.49453554\n",
            "Iteration 499, loss = 0.49421021\n",
            "Iteration 500, loss = 0.49391149\n",
            "Iteration 501, loss = 0.49359143\n",
            "Iteration 502, loss = 0.49327377\n",
            "Iteration 503, loss = 0.49295133\n",
            "Iteration 504, loss = 0.49263359\n",
            "Iteration 505, loss = 0.49231921\n",
            "Iteration 506, loss = 0.49202086\n",
            "Iteration 507, loss = 0.49171540\n",
            "Iteration 508, loss = 0.49141013\n",
            "Iteration 509, loss = 0.49110034\n",
            "Iteration 510, loss = 0.49081007\n",
            "Iteration 511, loss = 0.49050631\n",
            "Iteration 512, loss = 0.49020574\n",
            "Iteration 513, loss = 0.48991494\n",
            "Iteration 514, loss = 0.48961011\n",
            "Iteration 515, loss = 0.48932223\n",
            "Iteration 516, loss = 0.48902561\n",
            "Iteration 517, loss = 0.48872729\n",
            "Iteration 518, loss = 0.48843255\n",
            "Iteration 519, loss = 0.48813233\n",
            "Iteration 520, loss = 0.48783857\n",
            "Iteration 521, loss = 0.48755933\n",
            "Iteration 522, loss = 0.48726283\n",
            "Iteration 523, loss = 0.48698361\n",
            "Iteration 524, loss = 0.48669242\n",
            "Iteration 525, loss = 0.48640501\n",
            "Iteration 526, loss = 0.48612439\n",
            "Iteration 527, loss = 0.48585039\n",
            "Iteration 528, loss = 0.48557303\n",
            "Iteration 529, loss = 0.48527732\n",
            "Iteration 530, loss = 0.48500862\n",
            "Iteration 531, loss = 0.48473767\n",
            "Iteration 532, loss = 0.48445732\n",
            "Iteration 533, loss = 0.48416911\n",
            "Iteration 534, loss = 0.48389680\n",
            "Iteration 535, loss = 0.48362469\n",
            "Iteration 536, loss = 0.48335880\n",
            "Iteration 537, loss = 0.48310096\n",
            "Iteration 538, loss = 0.48282362\n",
            "Iteration 539, loss = 0.48256897\n",
            "Iteration 540, loss = 0.48230355\n",
            "Iteration 541, loss = 0.48203782\n",
            "Iteration 542, loss = 0.48178564\n",
            "Iteration 543, loss = 0.48153646\n",
            "Iteration 544, loss = 0.48126740\n",
            "Iteration 545, loss = 0.48101856\n",
            "Iteration 546, loss = 0.48076323\n",
            "Iteration 547, loss = 0.48050447\n",
            "Iteration 548, loss = 0.48025490\n",
            "Iteration 549, loss = 0.47999548\n",
            "Iteration 550, loss = 0.47974376\n",
            "Iteration 551, loss = 0.47949659\n",
            "Iteration 552, loss = 0.47924530\n",
            "Iteration 553, loss = 0.47900379\n",
            "Iteration 554, loss = 0.47876677\n",
            "Iteration 555, loss = 0.47853375\n",
            "Iteration 556, loss = 0.47828161\n",
            "Iteration 557, loss = 0.47803868\n",
            "Iteration 558, loss = 0.47779897\n",
            "Iteration 559, loss = 0.47755981\n",
            "Iteration 560, loss = 0.47733423\n",
            "Iteration 561, loss = 0.47709196\n",
            "Iteration 562, loss = 0.47686832\n",
            "Iteration 563, loss = 0.47664683\n",
            "Iteration 564, loss = 0.47637999\n",
            "Iteration 565, loss = 0.47616947\n",
            "Iteration 566, loss = 0.47592822\n",
            "Iteration 567, loss = 0.47572372\n",
            "Iteration 568, loss = 0.47547731\n",
            "Iteration 569, loss = 0.47525151\n",
            "Iteration 570, loss = 0.47503292\n",
            "Iteration 571, loss = 0.47479269\n",
            "Iteration 572, loss = 0.47460706\n",
            "Iteration 573, loss = 0.47433616\n",
            "Iteration 574, loss = 0.47410942\n",
            "Iteration 575, loss = 0.47388777\n",
            "Iteration 576, loss = 0.47365920\n",
            "Iteration 577, loss = 0.47345141\n",
            "Iteration 578, loss = 0.47320736\n",
            "Iteration 579, loss = 0.47298836\n",
            "Iteration 580, loss = 0.47277343\n",
            "Iteration 581, loss = 0.47255071\n",
            "Iteration 582, loss = 0.47233352\n",
            "Iteration 583, loss = 0.47210554\n",
            "Iteration 584, loss = 0.47188607\n",
            "Iteration 585, loss = 0.47167190\n",
            "Iteration 586, loss = 0.47144342\n",
            "Iteration 587, loss = 0.47122494\n",
            "Iteration 588, loss = 0.47101656\n",
            "Iteration 589, loss = 0.47080556\n",
            "Iteration 590, loss = 0.47060876\n",
            "Iteration 591, loss = 0.47039760\n",
            "Iteration 592, loss = 0.47018999\n",
            "Iteration 593, loss = 0.46999284\n",
            "Iteration 594, loss = 0.46978272\n",
            "Iteration 595, loss = 0.46958039\n",
            "Iteration 596, loss = 0.46938038\n",
            "Iteration 597, loss = 0.46917866\n",
            "Iteration 598, loss = 0.46897572\n",
            "Iteration 599, loss = 0.46877950\n",
            "Iteration 600, loss = 0.46857085\n",
            "Iteration 601, loss = 0.46838529\n",
            "Iteration 602, loss = 0.46819036\n",
            "Iteration 603, loss = 0.46800978\n",
            "Iteration 604, loss = 0.46780908\n",
            "Iteration 605, loss = 0.46763390\n",
            "Iteration 606, loss = 0.46744237\n",
            "Iteration 607, loss = 0.46725432\n",
            "Iteration 608, loss = 0.46704406\n",
            "Iteration 609, loss = 0.46686338\n",
            "Iteration 610, loss = 0.46667558\n",
            "Iteration 611, loss = 0.46650164\n",
            "Iteration 612, loss = 0.46630967\n",
            "Iteration 613, loss = 0.46611163\n",
            "Iteration 614, loss = 0.46591782\n",
            "Iteration 615, loss = 0.46574132\n",
            "Iteration 616, loss = 0.46555800\n",
            "Iteration 617, loss = 0.46539047\n",
            "Iteration 618, loss = 0.46517900\n",
            "Iteration 619, loss = 0.46499350\n",
            "Iteration 620, loss = 0.46481271\n",
            "Iteration 621, loss = 0.46463793\n",
            "Iteration 622, loss = 0.46445695\n",
            "Iteration 623, loss = 0.46427960\n",
            "Iteration 624, loss = 0.46409323\n",
            "Iteration 625, loss = 0.46392281\n",
            "Iteration 626, loss = 0.46377151\n",
            "Iteration 627, loss = 0.46356443\n",
            "Iteration 628, loss = 0.46339817\n",
            "Iteration 629, loss = 0.46322761\n",
            "Iteration 630, loss = 0.46304587\n",
            "Iteration 631, loss = 0.46287375\n",
            "Iteration 632, loss = 0.46271143\n",
            "Iteration 633, loss = 0.46256056\n",
            "Iteration 634, loss = 0.46238739\n",
            "Iteration 635, loss = 0.46221314\n",
            "Iteration 636, loss = 0.46206654\n",
            "Iteration 637, loss = 0.46189831\n",
            "Iteration 638, loss = 0.46174647\n",
            "Iteration 639, loss = 0.46158600\n",
            "Iteration 640, loss = 0.46142709\n",
            "Iteration 641, loss = 0.46127317\n",
            "Iteration 642, loss = 0.46111730\n",
            "Iteration 643, loss = 0.46096103\n",
            "Iteration 644, loss = 0.46080767\n",
            "Iteration 645, loss = 0.46065296\n",
            "Iteration 646, loss = 0.46050925\n",
            "Iteration 647, loss = 0.46035105\n",
            "Iteration 648, loss = 0.46020756\n",
            "Iteration 649, loss = 0.46005277\n",
            "Iteration 650, loss = 0.45989626\n",
            "Iteration 651, loss = 0.45975780\n",
            "Iteration 652, loss = 0.45960608\n",
            "Iteration 653, loss = 0.45946223\n",
            "Iteration 654, loss = 0.45933081\n",
            "Iteration 655, loss = 0.45916045\n",
            "Iteration 656, loss = 0.45902339\n",
            "Iteration 657, loss = 0.45887620\n",
            "Iteration 658, loss = 0.45872813\n",
            "Iteration 659, loss = 0.45858455\n",
            "Iteration 660, loss = 0.45844437\n",
            "Iteration 661, loss = 0.45830180\n",
            "Iteration 662, loss = 0.45816137\n",
            "Iteration 663, loss = 0.45800041\n",
            "Iteration 664, loss = 0.45785302\n",
            "Iteration 665, loss = 0.45770614\n",
            "Iteration 666, loss = 0.45756504\n",
            "Iteration 667, loss = 0.45740663\n",
            "Iteration 668, loss = 0.45726611\n",
            "Iteration 669, loss = 0.45711496\n",
            "Iteration 670, loss = 0.45697264\n",
            "Iteration 671, loss = 0.45683370\n",
            "Iteration 672, loss = 0.45669668\n",
            "Iteration 673, loss = 0.45655479\n",
            "Iteration 674, loss = 0.45641883\n",
            "Iteration 675, loss = 0.45628497\n",
            "Iteration 676, loss = 0.45615922\n",
            "Iteration 677, loss = 0.45603852\n",
            "Iteration 678, loss = 0.45591275\n",
            "Iteration 679, loss = 0.45578861\n",
            "Iteration 680, loss = 0.45566352\n",
            "Iteration 681, loss = 0.45553994\n",
            "Iteration 682, loss = 0.45540997\n",
            "Iteration 683, loss = 0.45529658\n",
            "Iteration 684, loss = 0.45515370\n",
            "Iteration 685, loss = 0.45504169\n",
            "Iteration 686, loss = 0.45492227\n",
            "Iteration 687, loss = 0.45479470\n",
            "Iteration 688, loss = 0.45466738\n",
            "Iteration 689, loss = 0.45454904\n",
            "Iteration 690, loss = 0.45442296\n",
            "Iteration 691, loss = 0.45428993\n",
            "Iteration 692, loss = 0.45416559\n",
            "Iteration 693, loss = 0.45404564\n",
            "Iteration 694, loss = 0.45393971\n",
            "Iteration 695, loss = 0.45383270\n",
            "Iteration 696, loss = 0.45369548\n",
            "Iteration 697, loss = 0.45356984\n",
            "Iteration 698, loss = 0.45345464\n",
            "Iteration 699, loss = 0.45334381\n",
            "Iteration 700, loss = 0.45322317\n",
            "Iteration 701, loss = 0.45311111\n",
            "Iteration 702, loss = 0.45300324\n",
            "Iteration 703, loss = 0.45288777\n",
            "Iteration 704, loss = 0.45277032\n",
            "Iteration 705, loss = 0.45265126\n",
            "Iteration 706, loss = 0.45255249\n",
            "Iteration 707, loss = 0.45243527\n",
            "Iteration 708, loss = 0.45233311\n",
            "Iteration 709, loss = 0.45221863\n",
            "Iteration 710, loss = 0.45210644\n",
            "Iteration 711, loss = 0.45199563\n",
            "Iteration 712, loss = 0.45188500\n",
            "Iteration 713, loss = 0.45178075\n",
            "Iteration 714, loss = 0.45165678\n",
            "Iteration 715, loss = 0.45155450\n",
            "Iteration 716, loss = 0.45143834\n",
            "Iteration 717, loss = 0.45132951\n",
            "Iteration 718, loss = 0.45122920\n",
            "Iteration 719, loss = 0.45112515\n",
            "Iteration 720, loss = 0.45100232\n",
            "Iteration 721, loss = 0.45089496\n",
            "Iteration 722, loss = 0.45078684\n",
            "Iteration 723, loss = 0.45068275\n",
            "Iteration 724, loss = 0.45057588\n",
            "Iteration 725, loss = 0.45047012\n",
            "Iteration 726, loss = 0.45037930\n",
            "Iteration 727, loss = 0.45027563\n",
            "Iteration 728, loss = 0.45017912\n",
            "Iteration 729, loss = 0.45005966\n",
            "Iteration 730, loss = 0.44996158\n",
            "Iteration 731, loss = 0.44986345\n",
            "Iteration 732, loss = 0.44975548\n",
            "Iteration 733, loss = 0.44967288\n",
            "Iteration 734, loss = 0.44956235\n",
            "Iteration 735, loss = 0.44946734\n",
            "Iteration 736, loss = 0.44937160\n",
            "Iteration 737, loss = 0.44927592\n",
            "Iteration 738, loss = 0.44918185\n",
            "Iteration 739, loss = 0.44908366\n",
            "Iteration 740, loss = 0.44898890\n",
            "Iteration 741, loss = 0.44889391\n",
            "Iteration 742, loss = 0.44880498\n",
            "Iteration 743, loss = 0.44870117\n",
            "Iteration 744, loss = 0.44860320\n",
            "Iteration 745, loss = 0.44851386\n",
            "Iteration 746, loss = 0.44842177\n",
            "Iteration 747, loss = 0.44832466\n",
            "Iteration 748, loss = 0.44826065\n",
            "Iteration 749, loss = 0.44814783\n",
            "Iteration 750, loss = 0.44806011\n",
            "Iteration 751, loss = 0.44796561\n",
            "Iteration 752, loss = 0.44786625\n",
            "Iteration 753, loss = 0.44778961\n",
            "Iteration 754, loss = 0.44770167\n",
            "Iteration 755, loss = 0.44763645\n",
            "Iteration 756, loss = 0.44754415\n",
            "Iteration 757, loss = 0.44746841\n",
            "Iteration 758, loss = 0.44737441\n",
            "Iteration 759, loss = 0.44728598\n",
            "Iteration 760, loss = 0.44720684\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69481829\n",
            "Iteration 2, loss = 0.69444909\n",
            "Iteration 3, loss = 0.69407690\n",
            "Iteration 4, loss = 0.69371192\n",
            "Iteration 5, loss = 0.69333479\n",
            "Iteration 6, loss = 0.69298723\n",
            "Iteration 7, loss = 0.69266712\n",
            "Iteration 8, loss = 0.69231240\n",
            "Iteration 9, loss = 0.69200480\n",
            "Iteration 10, loss = 0.69167765\n",
            "Iteration 11, loss = 0.69131884\n",
            "Iteration 12, loss = 0.69098264\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69472309\n",
            "Iteration 2, loss = 0.69434367\n",
            "Iteration 3, loss = 0.69397299\n",
            "Iteration 4, loss = 0.69361484\n",
            "Iteration 5, loss = 0.69322995\n",
            "Iteration 6, loss = 0.69289390\n",
            "Iteration 7, loss = 0.69255480\n",
            "Iteration 8, loss = 0.69221433\n",
            "Iteration 9, loss = 0.69190139\n",
            "Iteration 10, loss = 0.69155964\n",
            "Iteration 11, loss = 0.69120553\n",
            "Iteration 12, loss = 0.69086993\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69459895\n",
            "Iteration 2, loss = 0.69418306\n",
            "Iteration 3, loss = 0.69381772\n",
            "Iteration 4, loss = 0.69344965\n",
            "Iteration 5, loss = 0.69307784\n",
            "Iteration 6, loss = 0.69274224\n",
            "Iteration 7, loss = 0.69240447\n",
            "Iteration 8, loss = 0.69207223\n",
            "Iteration 9, loss = 0.69174678\n",
            "Iteration 10, loss = 0.69140644\n",
            "Iteration 11, loss = 0.69106454\n",
            "Iteration 12, loss = 0.69072542\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69470200\n",
            "Iteration 2, loss = 0.69428016\n",
            "Iteration 3, loss = 0.69389849\n",
            "Iteration 4, loss = 0.69353959\n",
            "Iteration 5, loss = 0.69316858\n",
            "Iteration 6, loss = 0.69279835\n",
            "Iteration 7, loss = 0.69246205\n",
            "Iteration 8, loss = 0.69212603\n",
            "Iteration 9, loss = 0.69180658\n",
            "Iteration 10, loss = 0.69145865\n",
            "Iteration 11, loss = 0.69111892\n",
            "Iteration 12, loss = 0.69077270\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69486222\n",
            "Iteration 2, loss = 0.69440173\n",
            "Iteration 3, loss = 0.69405436\n",
            "Iteration 4, loss = 0.69370789\n",
            "Iteration 5, loss = 0.69334119\n",
            "Iteration 6, loss = 0.69298727\n",
            "Iteration 7, loss = 0.69265666\n",
            "Iteration 8, loss = 0.69231392\n",
            "Iteration 9, loss = 0.69199990\n",
            "Iteration 10, loss = 0.69163630\n",
            "Iteration 11, loss = 0.69130766\n",
            "Iteration 12, loss = 0.69097020\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69456510\n",
            "Iteration 2, loss = 0.69413215\n",
            "Iteration 3, loss = 0.69375891\n",
            "Iteration 4, loss = 0.69343161\n",
            "Iteration 5, loss = 0.69305278\n",
            "Iteration 6, loss = 0.69270355\n",
            "Iteration 7, loss = 0.69240020\n",
            "Iteration 8, loss = 0.69205971\n",
            "Iteration 9, loss = 0.69173530\n",
            "Iteration 10, loss = 0.69139766\n",
            "Iteration 11, loss = 0.69107223\n",
            "Iteration 12, loss = 0.69073003\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69482954\n",
            "Iteration 2, loss = 0.69433428\n",
            "Iteration 3, loss = 0.69398311\n",
            "Iteration 4, loss = 0.69363138\n",
            "Iteration 5, loss = 0.69327792\n",
            "Iteration 6, loss = 0.69289743\n",
            "Iteration 7, loss = 0.69255484\n",
            "Iteration 8, loss = 0.69220412\n",
            "Iteration 9, loss = 0.69187069\n",
            "Iteration 10, loss = 0.69150950\n",
            "Iteration 11, loss = 0.69118467\n",
            "Iteration 12, loss = 0.69082296\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69463349\n",
            "Iteration 2, loss = 0.69420115\n",
            "Iteration 3, loss = 0.69384235\n",
            "Iteration 4, loss = 0.69349440\n",
            "Iteration 5, loss = 0.69315017\n",
            "Iteration 6, loss = 0.69277350\n",
            "Iteration 7, loss = 0.69245117\n",
            "Iteration 8, loss = 0.69208836\n",
            "Iteration 9, loss = 0.69176151\n",
            "Iteration 10, loss = 0.69140902\n",
            "Iteration 11, loss = 0.69108148\n",
            "Iteration 12, loss = 0.69072973\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69460286\n",
            "Iteration 2, loss = 0.69417046\n",
            "Iteration 3, loss = 0.69380062\n",
            "Iteration 4, loss = 0.69346539\n",
            "Iteration 5, loss = 0.69311080\n",
            "Iteration 6, loss = 0.69274491\n",
            "Iteration 7, loss = 0.69241642\n",
            "Iteration 8, loss = 0.69206563\n",
            "Iteration 9, loss = 0.69174727\n",
            "Iteration 10, loss = 0.69139108\n",
            "Iteration 11, loss = 0.69104824\n",
            "Iteration 12, loss = 0.69070188\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69471213\n",
            "Iteration 2, loss = 0.69428274\n",
            "Iteration 3, loss = 0.69390825\n",
            "Iteration 4, loss = 0.69355857\n",
            "Iteration 5, loss = 0.69319836\n",
            "Iteration 6, loss = 0.69283788\n",
            "Iteration 7, loss = 0.69250190\n",
            "Iteration 8, loss = 0.69215479\n",
            "Iteration 9, loss = 0.69182963\n",
            "Iteration 10, loss = 0.69147758\n",
            "Iteration 11, loss = 0.69112667\n",
            "Iteration 12, loss = 0.69078330\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.70857880\n",
            "Iteration 2, loss = 0.70826311\n",
            "Iteration 3, loss = 0.70780428\n",
            "Iteration 4, loss = 0.70720499\n",
            "Iteration 5, loss = 0.70652058\n",
            "Iteration 6, loss = 0.70585271\n",
            "Iteration 7, loss = 0.70517637\n",
            "Iteration 8, loss = 0.70445202\n",
            "Iteration 9, loss = 0.70383121\n",
            "Iteration 10, loss = 0.70317274\n",
            "Iteration 11, loss = 0.70247814\n",
            "Iteration 12, loss = 0.70183408\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.70870432\n",
            "Iteration 2, loss = 0.70839286\n",
            "Iteration 3, loss = 0.70795537\n",
            "Iteration 4, loss = 0.70736678\n",
            "Iteration 5, loss = 0.70669238\n",
            "Iteration 6, loss = 0.70603522\n",
            "Iteration 7, loss = 0.70534679\n",
            "Iteration 8, loss = 0.70466291\n",
            "Iteration 9, loss = 0.70403120\n",
            "Iteration 10, loss = 0.70337226\n",
            "Iteration 11, loss = 0.70270222\n",
            "Iteration 12, loss = 0.70206283\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.70752616\n",
            "Iteration 2, loss = 0.70720512\n",
            "Iteration 3, loss = 0.70673126\n",
            "Iteration 4, loss = 0.70613413\n",
            "Iteration 5, loss = 0.70546524\n",
            "Iteration 6, loss = 0.70480438\n",
            "Iteration 7, loss = 0.70410213\n",
            "Iteration 8, loss = 0.70342417\n",
            "Iteration 9, loss = 0.70277359\n",
            "Iteration 10, loss = 0.70210317\n",
            "Iteration 11, loss = 0.70142110\n",
            "Iteration 12, loss = 0.70077420\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.70879290\n",
            "Iteration 2, loss = 0.70845766\n",
            "Iteration 3, loss = 0.70795847\n",
            "Iteration 4, loss = 0.70735628\n",
            "Iteration 5, loss = 0.70666131\n",
            "Iteration 6, loss = 0.70595089\n",
            "Iteration 7, loss = 0.70521242\n",
            "Iteration 8, loss = 0.70450521\n",
            "Iteration 9, loss = 0.70382982\n",
            "Iteration 10, loss = 0.70311914\n",
            "Iteration 11, loss = 0.70240837\n",
            "Iteration 12, loss = 0.70171045\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.70897564\n",
            "Iteration 2, loss = 0.70864647\n",
            "Iteration 3, loss = 0.70818320\n",
            "Iteration 4, loss = 0.70760995\n",
            "Iteration 5, loss = 0.70696379\n",
            "Iteration 6, loss = 0.70630040\n",
            "Iteration 7, loss = 0.70561209\n",
            "Iteration 8, loss = 0.70498376\n",
            "Iteration 9, loss = 0.70434414\n",
            "Iteration 10, loss = 0.70367512\n",
            "Iteration 11, loss = 0.70303045\n",
            "Iteration 12, loss = 0.70238624\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.70729967\n",
            "Iteration 2, loss = 0.70699506\n",
            "Iteration 3, loss = 0.70655116\n",
            "Iteration 4, loss = 0.70602638\n",
            "Iteration 5, loss = 0.70541636\n",
            "Iteration 6, loss = 0.70479826\n",
            "Iteration 7, loss = 0.70415854\n",
            "Iteration 8, loss = 0.70357148\n",
            "Iteration 9, loss = 0.70296412\n",
            "Iteration 10, loss = 0.70235304\n",
            "Iteration 11, loss = 0.70173462\n",
            "Iteration 12, loss = 0.70112345\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.70828738\n",
            "Iteration 2, loss = 0.70794251\n",
            "Iteration 3, loss = 0.70744927\n",
            "Iteration 4, loss = 0.70685523\n",
            "Iteration 5, loss = 0.70617248\n",
            "Iteration 6, loss = 0.70544216\n",
            "Iteration 7, loss = 0.70468493\n",
            "Iteration 8, loss = 0.70402084\n",
            "Iteration 9, loss = 0.70332192\n",
            "Iteration 10, loss = 0.70262110\n",
            "Iteration 11, loss = 0.70195526\n",
            "Iteration 12, loss = 0.70123485\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.70734176\n",
            "Iteration 2, loss = 0.70702247\n",
            "Iteration 3, loss = 0.70656070\n",
            "Iteration 4, loss = 0.70598915\n",
            "Iteration 5, loss = 0.70536366\n",
            "Iteration 6, loss = 0.70467284\n",
            "Iteration 7, loss = 0.70397106\n",
            "Iteration 8, loss = 0.70333129\n",
            "Iteration 9, loss = 0.70268087\n",
            "Iteration 10, loss = 0.70200942\n",
            "Iteration 11, loss = 0.70137463\n",
            "Iteration 12, loss = 0.70071883\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.70698970\n",
            "Iteration 2, loss = 0.70664712\n",
            "Iteration 3, loss = 0.70614016\n",
            "Iteration 4, loss = 0.70554660\n",
            "Iteration 5, loss = 0.70487664\n",
            "Iteration 6, loss = 0.70415060\n",
            "Iteration 7, loss = 0.70340917\n",
            "Iteration 8, loss = 0.70269721\n",
            "Iteration 9, loss = 0.70203865\n",
            "Iteration 10, loss = 0.70130711\n",
            "Iteration 11, loss = 0.70058839\n",
            "Iteration 12, loss = 0.69989626\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.70876503\n",
            "Iteration 2, loss = 0.70844825\n",
            "Iteration 3, loss = 0.70798171\n",
            "Iteration 4, loss = 0.70741803\n",
            "Iteration 5, loss = 0.70678013\n",
            "Iteration 6, loss = 0.70611321\n",
            "Iteration 7, loss = 0.70543309\n",
            "Iteration 8, loss = 0.70475354\n",
            "Iteration 9, loss = 0.70415150\n",
            "Iteration 10, loss = 0.70349149\n",
            "Iteration 11, loss = 0.70281701\n",
            "Iteration 12, loss = 0.70217291\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69481829\n",
            "Iteration 2, loss = 0.69444909\n",
            "Iteration 3, loss = 0.69407690\n",
            "Iteration 4, loss = 0.69371192\n",
            "Iteration 5, loss = 0.69333479\n",
            "Iteration 6, loss = 0.69298723\n",
            "Iteration 7, loss = 0.69266712\n",
            "Iteration 8, loss = 0.69231240\n",
            "Iteration 9, loss = 0.69200480\n",
            "Iteration 10, loss = 0.69167765\n",
            "Iteration 11, loss = 0.69131884\n",
            "Iteration 12, loss = 0.69098264\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69472309\n",
            "Iteration 2, loss = 0.69434367\n",
            "Iteration 3, loss = 0.69397299\n",
            "Iteration 4, loss = 0.69361484\n",
            "Iteration 5, loss = 0.69322995\n",
            "Iteration 6, loss = 0.69289390\n",
            "Iteration 7, loss = 0.69255480\n",
            "Iteration 8, loss = 0.69221433\n",
            "Iteration 9, loss = 0.69190139\n",
            "Iteration 10, loss = 0.69155964\n",
            "Iteration 11, loss = 0.69120553\n",
            "Iteration 12, loss = 0.69086993\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69459895\n",
            "Iteration 2, loss = 0.69418306\n",
            "Iteration 3, loss = 0.69381772\n",
            "Iteration 4, loss = 0.69344965\n",
            "Iteration 5, loss = 0.69307784\n",
            "Iteration 6, loss = 0.69274224\n",
            "Iteration 7, loss = 0.69240447\n",
            "Iteration 8, loss = 0.69207223\n",
            "Iteration 9, loss = 0.69174678\n",
            "Iteration 10, loss = 0.69140644\n",
            "Iteration 11, loss = 0.69106454\n",
            "Iteration 12, loss = 0.69072542\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69470200\n",
            "Iteration 2, loss = 0.69428016\n",
            "Iteration 3, loss = 0.69389849\n",
            "Iteration 4, loss = 0.69353959\n",
            "Iteration 5, loss = 0.69316858\n",
            "Iteration 6, loss = 0.69279835\n",
            "Iteration 7, loss = 0.69246205\n",
            "Iteration 8, loss = 0.69212603\n",
            "Iteration 9, loss = 0.69180658\n",
            "Iteration 10, loss = 0.69145865\n",
            "Iteration 11, loss = 0.69111892\n",
            "Iteration 12, loss = 0.69077270\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69486222\n",
            "Iteration 2, loss = 0.69440173\n",
            "Iteration 3, loss = 0.69405436\n",
            "Iteration 4, loss = 0.69370789\n",
            "Iteration 5, loss = 0.69334119\n",
            "Iteration 6, loss = 0.69298727\n",
            "Iteration 7, loss = 0.69265666\n",
            "Iteration 8, loss = 0.69231392\n",
            "Iteration 9, loss = 0.69199990\n",
            "Iteration 10, loss = 0.69163630\n",
            "Iteration 11, loss = 0.69130766\n",
            "Iteration 12, loss = 0.69097020\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69456510\n",
            "Iteration 2, loss = 0.69413215\n",
            "Iteration 3, loss = 0.69375891\n",
            "Iteration 4, loss = 0.69343161\n",
            "Iteration 5, loss = 0.69305278\n",
            "Iteration 6, loss = 0.69270355\n",
            "Iteration 7, loss = 0.69240020\n",
            "Iteration 8, loss = 0.69205971\n",
            "Iteration 9, loss = 0.69173530\n",
            "Iteration 10, loss = 0.69139766\n",
            "Iteration 11, loss = 0.69107223\n",
            "Iteration 12, loss = 0.69073003\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69482954\n",
            "Iteration 2, loss = 0.69433428\n",
            "Iteration 3, loss = 0.69398311\n",
            "Iteration 4, loss = 0.69363138\n",
            "Iteration 5, loss = 0.69327792\n",
            "Iteration 6, loss = 0.69289743\n",
            "Iteration 7, loss = 0.69255484\n",
            "Iteration 8, loss = 0.69220412\n",
            "Iteration 9, loss = 0.69187069\n",
            "Iteration 10, loss = 0.69150950\n",
            "Iteration 11, loss = 0.69118467\n",
            "Iteration 12, loss = 0.69082296\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69463349\n",
            "Iteration 2, loss = 0.69420115\n",
            "Iteration 3, loss = 0.69384235\n",
            "Iteration 4, loss = 0.69349440\n",
            "Iteration 5, loss = 0.69315017\n",
            "Iteration 6, loss = 0.69277350\n",
            "Iteration 7, loss = 0.69245117\n",
            "Iteration 8, loss = 0.69208836\n",
            "Iteration 9, loss = 0.69176151\n",
            "Iteration 10, loss = 0.69140902\n",
            "Iteration 11, loss = 0.69108148\n",
            "Iteration 12, loss = 0.69072973\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69460286\n",
            "Iteration 2, loss = 0.69417046\n",
            "Iteration 3, loss = 0.69380062\n",
            "Iteration 4, loss = 0.69346539\n",
            "Iteration 5, loss = 0.69311080\n",
            "Iteration 6, loss = 0.69274491\n",
            "Iteration 7, loss = 0.69241642\n",
            "Iteration 8, loss = 0.69206563\n",
            "Iteration 9, loss = 0.69174727\n",
            "Iteration 10, loss = 0.69139108\n",
            "Iteration 11, loss = 0.69104824\n",
            "Iteration 12, loss = 0.69070188\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69471213\n",
            "Iteration 2, loss = 0.69428274\n",
            "Iteration 3, loss = 0.69390825\n",
            "Iteration 4, loss = 0.69355857\n",
            "Iteration 5, loss = 0.69319836\n",
            "Iteration 6, loss = 0.69283788\n",
            "Iteration 7, loss = 0.69250190\n",
            "Iteration 8, loss = 0.69215479\n",
            "Iteration 9, loss = 0.69182963\n",
            "Iteration 10, loss = 0.69147758\n",
            "Iteration 11, loss = 0.69112667\n",
            "Iteration 12, loss = 0.69078330\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.70726748\n",
            "Iteration 2, loss = 0.70474285\n",
            "Iteration 3, loss = 0.70214483\n",
            "Iteration 4, loss = 0.69993801\n",
            "Iteration 5, loss = 0.69760512\n",
            "Iteration 6, loss = 0.69523673\n",
            "Iteration 7, loss = 0.69302907\n",
            "Iteration 8, loss = 0.69069492\n",
            "Iteration 9, loss = 0.68855522\n",
            "Iteration 10, loss = 0.68650624\n",
            "Iteration 11, loss = 0.68469661\n",
            "Iteration 12, loss = 0.68294287\n",
            "Iteration 13, loss = 0.68105488\n",
            "Iteration 14, loss = 0.67938544\n",
            "Iteration 15, loss = 0.67774353\n",
            "Iteration 16, loss = 0.67638207\n",
            "Iteration 17, loss = 0.67488343\n",
            "Iteration 18, loss = 0.67357352\n",
            "Iteration 19, loss = 0.67219227\n",
            "Iteration 20, loss = 0.67087926\n",
            "Iteration 21, loss = 0.66961376\n",
            "Iteration 22, loss = 0.66830656\n",
            "Iteration 23, loss = 0.66703262\n",
            "Iteration 24, loss = 0.66572444\n",
            "Iteration 25, loss = 0.66452435\n",
            "Iteration 26, loss = 0.66325318\n",
            "Iteration 27, loss = 0.66196006\n",
            "Iteration 28, loss = 0.66063807\n",
            "Iteration 29, loss = 0.65933878\n",
            "Iteration 30, loss = 0.65804512\n",
            "Iteration 31, loss = 0.65674049\n",
            "Iteration 32, loss = 0.65539863\n",
            "Iteration 33, loss = 0.65408655\n",
            "Iteration 34, loss = 0.65279113\n",
            "Iteration 35, loss = 0.65145861\n",
            "Iteration 36, loss = 0.65014005\n",
            "Iteration 37, loss = 0.64886977\n",
            "Iteration 38, loss = 0.64745512\n",
            "Iteration 39, loss = 0.64608045\n",
            "Iteration 40, loss = 0.64479803\n",
            "Iteration 41, loss = 0.64332743\n",
            "Iteration 42, loss = 0.64191602\n",
            "Iteration 43, loss = 0.64049443\n",
            "Iteration 44, loss = 0.63910440\n",
            "Iteration 45, loss = 0.63775095\n",
            "Iteration 46, loss = 0.63643092\n",
            "Iteration 47, loss = 0.63502632\n",
            "Iteration 48, loss = 0.63361504\n",
            "Iteration 49, loss = 0.63221307\n",
            "Iteration 50, loss = 0.63081191\n",
            "Iteration 51, loss = 0.62943018\n",
            "Iteration 52, loss = 0.62802355\n",
            "Iteration 53, loss = 0.62660563\n",
            "Iteration 54, loss = 0.62504413\n",
            "Iteration 55, loss = 0.62357597\n",
            "Iteration 56, loss = 0.62203861\n",
            "Iteration 57, loss = 0.62056461\n",
            "Iteration 58, loss = 0.61900895\n",
            "Iteration 59, loss = 0.61748222\n",
            "Iteration 60, loss = 0.61595047\n",
            "Iteration 61, loss = 0.61441222\n",
            "Iteration 62, loss = 0.61279187\n",
            "Iteration 63, loss = 0.61133758\n",
            "Iteration 64, loss = 0.60975173\n",
            "Iteration 65, loss = 0.60826608\n",
            "Iteration 66, loss = 0.60665775\n",
            "Iteration 67, loss = 0.60519153\n",
            "Iteration 68, loss = 0.60354429\n",
            "Iteration 69, loss = 0.60193952\n",
            "Iteration 70, loss = 0.60038454\n",
            "Iteration 71, loss = 0.59875342\n",
            "Iteration 72, loss = 0.59724172\n",
            "Iteration 73, loss = 0.59568940\n",
            "Iteration 74, loss = 0.59411755\n",
            "Iteration 75, loss = 0.59247741\n",
            "Iteration 76, loss = 0.59099312\n",
            "Iteration 77, loss = 0.58950039\n",
            "Iteration 78, loss = 0.58798962\n",
            "Iteration 79, loss = 0.58644068\n",
            "Iteration 80, loss = 0.58490456\n",
            "Iteration 81, loss = 0.58334570\n",
            "Iteration 82, loss = 0.58185184\n",
            "Iteration 83, loss = 0.58034850\n",
            "Iteration 84, loss = 0.57873498\n",
            "Iteration 85, loss = 0.57727232\n",
            "Iteration 86, loss = 0.57572464\n",
            "Iteration 87, loss = 0.57419009\n",
            "Iteration 88, loss = 0.57266928\n",
            "Iteration 89, loss = 0.57127440\n",
            "Iteration 90, loss = 0.56981067\n",
            "Iteration 91, loss = 0.56833181\n",
            "Iteration 92, loss = 0.56689415\n",
            "Iteration 93, loss = 0.56547485\n",
            "Iteration 94, loss = 0.56405420\n",
            "Iteration 95, loss = 0.56269036\n",
            "Iteration 96, loss = 0.56130434\n",
            "Iteration 97, loss = 0.55994609\n",
            "Iteration 98, loss = 0.55847735\n",
            "Iteration 99, loss = 0.55712999\n",
            "Iteration 100, loss = 0.55588485\n",
            "Iteration 101, loss = 0.55444629\n",
            "Iteration 102, loss = 0.55305484\n",
            "Iteration 103, loss = 0.55173634\n",
            "Iteration 104, loss = 0.55040718\n",
            "Iteration 105, loss = 0.54908623\n",
            "Iteration 106, loss = 0.54779022\n",
            "Iteration 107, loss = 0.54637598\n",
            "Iteration 108, loss = 0.54499591\n",
            "Iteration 109, loss = 0.54365887\n",
            "Iteration 110, loss = 0.54219262\n",
            "Iteration 111, loss = 0.54080409\n",
            "Iteration 112, loss = 0.53941023\n",
            "Iteration 113, loss = 0.53802546\n",
            "Iteration 114, loss = 0.53669375\n",
            "Iteration 115, loss = 0.53530705\n",
            "Iteration 116, loss = 0.53384714\n",
            "Iteration 117, loss = 0.53250136\n",
            "Iteration 118, loss = 0.53108438\n",
            "Iteration 119, loss = 0.52969609\n",
            "Iteration 120, loss = 0.52829965\n",
            "Iteration 121, loss = 0.52702573\n",
            "Iteration 122, loss = 0.52558707\n",
            "Iteration 123, loss = 0.52422791\n",
            "Iteration 124, loss = 0.52287870\n",
            "Iteration 125, loss = 0.52165300\n",
            "Iteration 126, loss = 0.52033111\n",
            "Iteration 127, loss = 0.51901550\n",
            "Iteration 128, loss = 0.51777467\n",
            "Iteration 129, loss = 0.51646278\n",
            "Iteration 130, loss = 0.51522600\n",
            "Iteration 131, loss = 0.51391277\n",
            "Iteration 132, loss = 0.51264701\n",
            "Iteration 133, loss = 0.51138166\n",
            "Iteration 134, loss = 0.51012193\n",
            "Iteration 135, loss = 0.50882427\n",
            "Iteration 136, loss = 0.50756554\n",
            "Iteration 137, loss = 0.50630796\n",
            "Iteration 138, loss = 0.50509099\n",
            "Iteration 139, loss = 0.50381698\n",
            "Iteration 140, loss = 0.50266805\n",
            "Iteration 141, loss = 0.50139717\n",
            "Iteration 142, loss = 0.50007585\n",
            "Iteration 143, loss = 0.49894433\n",
            "Iteration 144, loss = 0.49771623\n",
            "Iteration 145, loss = 0.49660244\n",
            "Iteration 146, loss = 0.49541280\n",
            "Iteration 147, loss = 0.49424033\n",
            "Iteration 148, loss = 0.49310363\n",
            "Iteration 149, loss = 0.49195481\n",
            "Iteration 150, loss = 0.49085094\n",
            "Iteration 151, loss = 0.48986791\n",
            "Iteration 152, loss = 0.48863106\n",
            "Iteration 153, loss = 0.48755010\n",
            "Iteration 154, loss = 0.48646138\n",
            "Iteration 155, loss = 0.48535707\n",
            "Iteration 156, loss = 0.48428317\n",
            "Iteration 157, loss = 0.48321350\n",
            "Iteration 158, loss = 0.48216948\n",
            "Iteration 159, loss = 0.48110691\n",
            "Iteration 160, loss = 0.48008091\n",
            "Iteration 161, loss = 0.47901442\n",
            "Iteration 162, loss = 0.47801466\n",
            "Iteration 163, loss = 0.47707978\n",
            "Iteration 164, loss = 0.47597242\n",
            "Iteration 165, loss = 0.47503223\n",
            "Iteration 166, loss = 0.47404161\n",
            "Iteration 167, loss = 0.47305255\n",
            "Iteration 168, loss = 0.47216254\n",
            "Iteration 169, loss = 0.47118064\n",
            "Iteration 170, loss = 0.47025015\n",
            "Iteration 171, loss = 0.46932089\n",
            "Iteration 172, loss = 0.46841021\n",
            "Iteration 173, loss = 0.46742683\n",
            "Iteration 174, loss = 0.46650759\n",
            "Iteration 175, loss = 0.46563740\n",
            "Iteration 176, loss = 0.46473193\n",
            "Iteration 177, loss = 0.46388042\n",
            "Iteration 178, loss = 0.46299942\n",
            "Iteration 179, loss = 0.46216704\n",
            "Iteration 180, loss = 0.46132760\n",
            "Iteration 181, loss = 0.46041923\n",
            "Iteration 182, loss = 0.45959316\n",
            "Iteration 183, loss = 0.45882760\n",
            "Iteration 184, loss = 0.45796585\n",
            "Iteration 185, loss = 0.45717177\n",
            "Iteration 186, loss = 0.45627386\n",
            "Iteration 187, loss = 0.45555115\n",
            "Iteration 188, loss = 0.45471792\n",
            "Iteration 189, loss = 0.45397741\n",
            "Iteration 190, loss = 0.45325848\n",
            "Iteration 191, loss = 0.45249158\n",
            "Iteration 192, loss = 0.45172759\n",
            "Iteration 193, loss = 0.45097589\n",
            "Iteration 194, loss = 0.45028413\n",
            "Iteration 195, loss = 0.44957016\n",
            "Iteration 196, loss = 0.44887277\n",
            "Iteration 197, loss = 0.44817100\n",
            "Iteration 198, loss = 0.44747904\n",
            "Iteration 199, loss = 0.44683509\n",
            "Iteration 200, loss = 0.44619128\n",
            "Iteration 201, loss = 0.44554872\n",
            "Iteration 202, loss = 0.44491219\n",
            "Iteration 203, loss = 0.44429224\n",
            "Iteration 204, loss = 0.44360764\n",
            "Iteration 205, loss = 0.44306632\n",
            "Iteration 206, loss = 0.44244926\n",
            "Iteration 207, loss = 0.44186095\n",
            "Iteration 208, loss = 0.44129836\n",
            "Iteration 209, loss = 0.44077840\n",
            "Iteration 210, loss = 0.44014398\n",
            "Iteration 211, loss = 0.43957975\n",
            "Iteration 212, loss = 0.43899426\n",
            "Iteration 213, loss = 0.43846406\n",
            "Iteration 214, loss = 0.43798486\n",
            "Iteration 215, loss = 0.43742513\n",
            "Iteration 216, loss = 0.43690507\n",
            "Iteration 217, loss = 0.43644291\n",
            "Iteration 218, loss = 0.43592701\n",
            "Iteration 219, loss = 0.43544055\n",
            "Iteration 220, loss = 0.43493354\n",
            "Iteration 221, loss = 0.43448900\n",
            "Iteration 222, loss = 0.43403955\n",
            "Iteration 223, loss = 0.43349304\n",
            "Iteration 224, loss = 0.43313868\n",
            "Iteration 225, loss = 0.43267771\n",
            "Iteration 226, loss = 0.43226457\n",
            "Iteration 227, loss = 0.43190560\n",
            "Iteration 228, loss = 0.43140698\n",
            "Iteration 229, loss = 0.43102583\n",
            "Iteration 230, loss = 0.43058993\n",
            "Iteration 231, loss = 0.43027971\n",
            "Iteration 232, loss = 0.42982543\n",
            "Iteration 233, loss = 0.42949565\n",
            "Iteration 234, loss = 0.42907401\n",
            "Iteration 235, loss = 0.42873265\n",
            "Iteration 236, loss = 0.42843124\n",
            "Iteration 237, loss = 0.42799594\n",
            "Iteration 238, loss = 0.42767539\n",
            "Iteration 239, loss = 0.42729510\n",
            "Iteration 240, loss = 0.42698767\n",
            "Iteration 241, loss = 0.42664784\n",
            "Iteration 242, loss = 0.42633936\n",
            "Iteration 243, loss = 0.42599563\n",
            "Iteration 244, loss = 0.42567545\n",
            "Iteration 245, loss = 0.42537337\n",
            "Iteration 246, loss = 0.42504698\n",
            "Iteration 247, loss = 0.42472648\n",
            "Iteration 248, loss = 0.42444412\n",
            "Iteration 249, loss = 0.42418261\n",
            "Iteration 250, loss = 0.42384238\n",
            "Iteration 251, loss = 0.42355618\n",
            "Iteration 252, loss = 0.42324035\n",
            "Iteration 253, loss = 0.42296336\n",
            "Iteration 254, loss = 0.42273104\n",
            "Iteration 255, loss = 0.42239509\n",
            "Iteration 256, loss = 0.42219839\n",
            "Iteration 257, loss = 0.42191599\n",
            "Iteration 258, loss = 0.42161044\n",
            "Iteration 259, loss = 0.42139319\n",
            "Iteration 260, loss = 0.42120867\n",
            "Iteration 261, loss = 0.42088843\n",
            "Iteration 262, loss = 0.42062382\n",
            "Iteration 263, loss = 0.42044495\n",
            "Iteration 264, loss = 0.42016865\n",
            "Iteration 265, loss = 0.41996423\n",
            "Iteration 266, loss = 0.41966387\n",
            "Iteration 267, loss = 0.41942695\n",
            "Iteration 268, loss = 0.41924000\n",
            "Iteration 269, loss = 0.41905326\n",
            "Iteration 270, loss = 0.41879869\n",
            "Iteration 271, loss = 0.41854889\n",
            "Iteration 272, loss = 0.41837171\n",
            "Iteration 273, loss = 0.41812696\n",
            "Iteration 274, loss = 0.41792099\n",
            "Iteration 275, loss = 0.41773810\n",
            "Iteration 276, loss = 0.41753250\n",
            "Iteration 277, loss = 0.41733221\n",
            "Iteration 278, loss = 0.41717911\n",
            "Iteration 279, loss = 0.41695621\n",
            "Iteration 280, loss = 0.41677905\n",
            "Iteration 281, loss = 0.41664709\n",
            "Iteration 282, loss = 0.41643229\n",
            "Iteration 283, loss = 0.41627530\n",
            "Iteration 284, loss = 0.41610869\n",
            "Iteration 285, loss = 0.41592374\n",
            "Iteration 286, loss = 0.41580545\n",
            "Iteration 287, loss = 0.41558745\n",
            "Iteration 288, loss = 0.41544424\n",
            "Iteration 289, loss = 0.41529035\n",
            "Iteration 290, loss = 0.41518089\n",
            "Iteration 291, loss = 0.41512465\n",
            "Iteration 292, loss = 0.41481667\n",
            "Iteration 293, loss = 0.41467708\n",
            "Iteration 294, loss = 0.41453556\n",
            "Iteration 295, loss = 0.41442410\n",
            "Iteration 296, loss = 0.41429475\n",
            "Iteration 297, loss = 0.41415380\n",
            "Iteration 298, loss = 0.41405185\n",
            "Iteration 299, loss = 0.41389750\n",
            "Iteration 300, loss = 0.41380732\n",
            "Iteration 301, loss = 0.41365328\n",
            "Iteration 302, loss = 0.41366093\n",
            "Iteration 303, loss = 0.41347150\n",
            "Iteration 304, loss = 0.41334837\n",
            "Iteration 305, loss = 0.41327888\n",
            "Iteration 306, loss = 0.41309255\n",
            "Iteration 307, loss = 0.41298656\n",
            "Iteration 308, loss = 0.41283383\n",
            "Iteration 309, loss = 0.41273841\n",
            "Iteration 310, loss = 0.41263576\n",
            "Iteration 311, loss = 0.41252926\n",
            "Iteration 312, loss = 0.41239005\n",
            "Iteration 313, loss = 0.41239868\n",
            "Iteration 314, loss = 0.41219773\n",
            "Iteration 315, loss = 0.41210210\n",
            "Iteration 316, loss = 0.41197572\n",
            "Iteration 317, loss = 0.41194720\n",
            "Iteration 318, loss = 0.41183972\n",
            "Iteration 319, loss = 0.41177534\n",
            "Iteration 320, loss = 0.41165467\n",
            "Iteration 321, loss = 0.41153623\n",
            "Iteration 322, loss = 0.41144290\n",
            "Iteration 323, loss = 0.41136278\n",
            "Iteration 324, loss = 0.41123441\n",
            "Iteration 325, loss = 0.41115830\n",
            "Iteration 326, loss = 0.41106202\n",
            "Iteration 327, loss = 0.41102501\n",
            "Iteration 328, loss = 0.41089273\n",
            "Iteration 329, loss = 0.41082226\n",
            "Iteration 330, loss = 0.41071455\n",
            "Iteration 331, loss = 0.41068555\n",
            "Iteration 332, loss = 0.41064065\n",
            "Iteration 333, loss = 0.41058815\n",
            "Iteration 334, loss = 0.41042460\n",
            "Iteration 335, loss = 0.41036663\n",
            "Iteration 336, loss = 0.41026318\n",
            "Iteration 337, loss = 0.41020290\n",
            "Iteration 338, loss = 0.41010134\n",
            "Iteration 339, loss = 0.41005362\n",
            "Iteration 340, loss = 0.40994747\n",
            "Iteration 341, loss = 0.40991571\n",
            "Iteration 342, loss = 0.40979443\n",
            "Iteration 343, loss = 0.40976327\n",
            "Iteration 344, loss = 0.40968543\n",
            "Iteration 345, loss = 0.40960111\n",
            "Iteration 346, loss = 0.40957369\n",
            "Iteration 347, loss = 0.40953309\n",
            "Iteration 348, loss = 0.40943039\n",
            "Iteration 349, loss = 0.40937629\n",
            "Iteration 350, loss = 0.40934556\n",
            "Iteration 351, loss = 0.40922981\n",
            "Iteration 352, loss = 0.40923577\n",
            "Iteration 353, loss = 0.40912045\n",
            "Iteration 354, loss = 0.40906869\n",
            "Iteration 355, loss = 0.40901704\n",
            "Iteration 356, loss = 0.40898123\n",
            "Iteration 357, loss = 0.40890485\n",
            "Iteration 358, loss = 0.40885915\n",
            "Iteration 359, loss = 0.40877587\n",
            "Iteration 360, loss = 0.40878808\n",
            "Iteration 361, loss = 0.40868710\n",
            "Iteration 362, loss = 0.40864186\n",
            "Iteration 363, loss = 0.40860231\n",
            "Iteration 364, loss = 0.40854525\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Melhores hiperparâmetros encontrados através do Random Search:\n",
            "{'tol': 0.0001, 'solver': 'adam', 'max_iter': 1000, 'activation': 'relu'}\n",
            "Melhor pontuação (acurácia) encontrada através do Random Search: 0.8144736842105263\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ajuste do modelo MLP aos dados de treinamento\n",
        "rna_best_random = MLPClassifier(**best_params_random)\n",
        "rna_best_random.fit(X_train_oversampled, y_train_oversampled)\n",
        "\n",
        "# Predições nos dados de teste usando o modelo com melhores hiperparâmetros encontrados pelo Random Search\n",
        "pred_random = rna_best_random.predict(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "slq4FtuwOs7v",
        "outputId": "fffc5940-b681-40a2-d24d-83086d7e31e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cálculo e impressão da acurácia nos dados de teste\n",
        "accuracy_random = accuracy_score(y_test, pred_random)\n",
        "print(\"Acurácia do modelo MLP com melhores hiperparâmetros pelo Random Search:\", accuracy_random)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYntJpO2QjNi",
        "outputId": "1bb0b321-e4f5-408b-99c1-56696307e071"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acurácia do modelo MLP com melhores hiperparâmetros pelo Random Search: 0.7649253731343284\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Matriz de confusão para o modelo com melhores hiperparâmetros pelo Random Search\n",
        "print(\"Matriz de Confusão - Random Search\")\n",
        "cm_rna_random = confusion_matrix(y_test, pred_random)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ULBdoi06Qo8m",
        "outputId": "67df8d70-4005-4464-c7c7-1dc80660537c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matriz de Confusão - Random Search\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtém a média das acurácias (10 folds) referente ao conjunto treino\n",
        "rna1_random = r_results.loc[r_search.best_index_,'mean_test_score']"
      ],
      "metadata": {
        "id": "x5YW3ZzESaFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3Bx5kmMn-0a"
      },
      "source": [
        "## Rede Neural (Scikit) - 2 camadas\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rna2 = MLPClassifier(hidden_layer_sizes=(3,3), activation='relu', solver='sgd', max_iter =1000,\n",
        "                              tol=0.0001, random_state = 3, verbose = True)"
      ],
      "metadata": {
        "id": "qlXM15-fCjj_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rna2.fit(X_train_oversampled, y_train_oversampled)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3hoRD7DADKm-",
        "outputId": "bb32490f-23f9-4244-e903-db6a4c64957b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 0.81105531\n",
            "Iteration 2, loss = 0.80990128\n",
            "Iteration 3, loss = 0.80815600\n",
            "Iteration 4, loss = 0.80606531\n",
            "Iteration 5, loss = 0.80366632\n",
            "Iteration 6, loss = 0.80117983\n",
            "Iteration 7, loss = 0.79861464\n",
            "Iteration 8, loss = 0.79609273\n",
            "Iteration 9, loss = 0.79355391\n",
            "Iteration 10, loss = 0.79099263\n",
            "Iteration 11, loss = 0.78840054\n",
            "Iteration 12, loss = 0.78584426\n",
            "Iteration 13, loss = 0.78346353\n",
            "Iteration 14, loss = 0.78110317\n",
            "Iteration 15, loss = 0.77874012\n",
            "Iteration 16, loss = 0.77632135\n",
            "Iteration 17, loss = 0.77410531\n",
            "Iteration 18, loss = 0.77184272\n",
            "Iteration 19, loss = 0.76964690\n",
            "Iteration 20, loss = 0.76752217\n",
            "Iteration 21, loss = 0.76537005\n",
            "Iteration 22, loss = 0.76339980\n",
            "Iteration 23, loss = 0.76134291\n",
            "Iteration 24, loss = 0.75926013\n",
            "Iteration 25, loss = 0.75735537\n",
            "Iteration 26, loss = 0.75554467\n",
            "Iteration 27, loss = 0.75358030\n",
            "Iteration 28, loss = 0.75177958\n",
            "Iteration 29, loss = 0.74995433\n",
            "Iteration 30, loss = 0.74819515\n",
            "Iteration 31, loss = 0.74649678\n",
            "Iteration 32, loss = 0.74479649\n",
            "Iteration 33, loss = 0.74318539\n",
            "Iteration 34, loss = 0.74157028\n",
            "Iteration 35, loss = 0.73998081\n",
            "Iteration 36, loss = 0.73842991\n",
            "Iteration 37, loss = 0.73689779\n",
            "Iteration 38, loss = 0.73540198\n",
            "Iteration 39, loss = 0.73402460\n",
            "Iteration 40, loss = 0.73256567\n",
            "Iteration 41, loss = 0.73120199\n",
            "Iteration 42, loss = 0.72980755\n",
            "Iteration 43, loss = 0.72845738\n",
            "Iteration 44, loss = 0.72718263\n",
            "Iteration 45, loss = 0.72599004\n",
            "Iteration 46, loss = 0.72468366\n",
            "Iteration 47, loss = 0.72351947\n",
            "Iteration 48, loss = 0.72228555\n",
            "Iteration 49, loss = 0.72122780\n",
            "Iteration 50, loss = 0.72005901\n",
            "Iteration 51, loss = 0.71890458\n",
            "Iteration 52, loss = 0.71786203\n",
            "Iteration 53, loss = 0.71678482\n",
            "Iteration 54, loss = 0.71582341\n",
            "Iteration 55, loss = 0.71474495\n",
            "Iteration 56, loss = 0.71379169\n",
            "Iteration 57, loss = 0.71282094\n",
            "Iteration 58, loss = 0.71190340\n",
            "Iteration 59, loss = 0.71099959\n",
            "Iteration 60, loss = 0.71003973\n",
            "Iteration 61, loss = 0.70918654\n",
            "Iteration 62, loss = 0.70836994\n",
            "Iteration 63, loss = 0.70752405\n",
            "Iteration 64, loss = 0.70677116\n",
            "Iteration 65, loss = 0.70593142\n",
            "Iteration 66, loss = 0.70513934\n",
            "Iteration 67, loss = 0.70438894\n",
            "Iteration 68, loss = 0.70368277\n",
            "Iteration 69, loss = 0.70288350\n",
            "Iteration 70, loss = 0.70221178\n",
            "Iteration 71, loss = 0.70152622\n",
            "Iteration 72, loss = 0.70089686\n",
            "Iteration 73, loss = 0.70015061\n",
            "Iteration 74, loss = 0.69953450\n",
            "Iteration 75, loss = 0.69888403\n",
            "Iteration 76, loss = 0.69827013\n",
            "Iteration 77, loss = 0.69762844\n",
            "Iteration 78, loss = 0.69707592\n",
            "Iteration 79, loss = 0.69645296\n",
            "Iteration 80, loss = 0.69591460\n",
            "Iteration 81, loss = 0.69533183\n",
            "Iteration 82, loss = 0.69483442\n",
            "Iteration 83, loss = 0.69424022\n",
            "Iteration 84, loss = 0.69376062\n",
            "Iteration 85, loss = 0.69320390\n",
            "Iteration 86, loss = 0.69265350\n",
            "Iteration 87, loss = 0.69219999\n",
            "Iteration 88, loss = 0.69167762\n",
            "Iteration 89, loss = 0.69120673\n",
            "Iteration 90, loss = 0.69073929\n",
            "Iteration 91, loss = 0.69027343\n",
            "Iteration 92, loss = 0.68979466\n",
            "Iteration 93, loss = 0.68931711\n",
            "Iteration 94, loss = 0.68889794\n",
            "Iteration 95, loss = 0.68845183\n",
            "Iteration 96, loss = 0.68800558\n",
            "Iteration 97, loss = 0.68759928\n",
            "Iteration 98, loss = 0.68716826\n",
            "Iteration 99, loss = 0.68671904\n",
            "Iteration 100, loss = 0.68633105\n",
            "Iteration 101, loss = 0.68588960\n",
            "Iteration 102, loss = 0.68552026\n",
            "Iteration 103, loss = 0.68508852\n",
            "Iteration 104, loss = 0.68466335\n",
            "Iteration 105, loss = 0.68427930\n",
            "Iteration 106, loss = 0.68389053\n",
            "Iteration 107, loss = 0.68354618\n",
            "Iteration 108, loss = 0.68311659\n",
            "Iteration 109, loss = 0.68273097\n",
            "Iteration 110, loss = 0.68236934\n",
            "Iteration 111, loss = 0.68195494\n",
            "Iteration 112, loss = 0.68161304\n",
            "Iteration 113, loss = 0.68125208\n",
            "Iteration 114, loss = 0.68088019\n",
            "Iteration 115, loss = 0.68049466\n",
            "Iteration 116, loss = 0.68016081\n",
            "Iteration 117, loss = 0.67975447\n",
            "Iteration 118, loss = 0.67942596\n",
            "Iteration 119, loss = 0.67906560\n",
            "Iteration 120, loss = 0.67869384\n",
            "Iteration 121, loss = 0.67839314\n",
            "Iteration 122, loss = 0.67797159\n",
            "Iteration 123, loss = 0.67763221\n",
            "Iteration 124, loss = 0.67726705\n",
            "Iteration 125, loss = 0.67692851\n",
            "Iteration 126, loss = 0.67655035\n",
            "Iteration 127, loss = 0.67618681\n",
            "Iteration 128, loss = 0.67585536\n",
            "Iteration 129, loss = 0.67548897\n",
            "Iteration 130, loss = 0.67512271\n",
            "Iteration 131, loss = 0.67475997\n",
            "Iteration 132, loss = 0.67438294\n",
            "Iteration 133, loss = 0.67403080\n",
            "Iteration 134, loss = 0.67365429\n",
            "Iteration 135, loss = 0.67331007\n",
            "Iteration 136, loss = 0.67294161\n",
            "Iteration 137, loss = 0.67260171\n",
            "Iteration 138, loss = 0.67225373\n",
            "Iteration 139, loss = 0.67188908\n",
            "Iteration 140, loss = 0.67155383\n",
            "Iteration 141, loss = 0.67122789\n",
            "Iteration 142, loss = 0.67088744\n",
            "Iteration 143, loss = 0.67055829\n",
            "Iteration 144, loss = 0.67025291\n",
            "Iteration 145, loss = 0.66992957\n",
            "Iteration 146, loss = 0.66962838\n",
            "Iteration 147, loss = 0.66929511\n",
            "Iteration 148, loss = 0.66899008\n",
            "Iteration 149, loss = 0.66867943\n",
            "Iteration 150, loss = 0.66835319\n",
            "Iteration 151, loss = 0.66805299\n",
            "Iteration 152, loss = 0.66776029\n",
            "Iteration 153, loss = 0.66742532\n",
            "Iteration 154, loss = 0.66711553\n",
            "Iteration 155, loss = 0.66679392\n",
            "Iteration 156, loss = 0.66648811\n",
            "Iteration 157, loss = 0.66618017\n",
            "Iteration 158, loss = 0.66586217\n",
            "Iteration 159, loss = 0.66555674\n",
            "Iteration 160, loss = 0.66524466\n",
            "Iteration 161, loss = 0.66495298\n",
            "Iteration 162, loss = 0.66463513\n",
            "Iteration 163, loss = 0.66432203\n",
            "Iteration 164, loss = 0.66402377\n",
            "Iteration 165, loss = 0.66370442\n",
            "Iteration 166, loss = 0.66340472\n",
            "Iteration 167, loss = 0.66304637\n",
            "Iteration 168, loss = 0.66274929\n",
            "Iteration 169, loss = 0.66241620\n",
            "Iteration 170, loss = 0.66209562\n",
            "Iteration 171, loss = 0.66174147\n",
            "Iteration 172, loss = 0.66141238\n",
            "Iteration 173, loss = 0.66106141\n",
            "Iteration 174, loss = 0.66073225\n",
            "Iteration 175, loss = 0.66038637\n",
            "Iteration 176, loss = 0.66003564\n",
            "Iteration 177, loss = 0.65968803\n",
            "Iteration 178, loss = 0.65934575\n",
            "Iteration 179, loss = 0.65896546\n",
            "Iteration 180, loss = 0.65861296\n",
            "Iteration 181, loss = 0.65826043\n",
            "Iteration 182, loss = 0.65788334\n",
            "Iteration 183, loss = 0.65751482\n",
            "Iteration 184, loss = 0.65714062\n",
            "Iteration 185, loss = 0.65678155\n",
            "Iteration 186, loss = 0.65639928\n",
            "Iteration 187, loss = 0.65602324\n",
            "Iteration 188, loss = 0.65561449\n",
            "Iteration 189, loss = 0.65524969\n",
            "Iteration 190, loss = 0.65484772\n",
            "Iteration 191, loss = 0.65445755\n",
            "Iteration 192, loss = 0.65405255\n",
            "Iteration 193, loss = 0.65364712\n",
            "Iteration 194, loss = 0.65323513\n",
            "Iteration 195, loss = 0.65281972\n",
            "Iteration 196, loss = 0.65239529\n",
            "Iteration 197, loss = 0.65198734\n",
            "Iteration 198, loss = 0.65157260\n",
            "Iteration 199, loss = 0.65114106\n",
            "Iteration 200, loss = 0.65069468\n",
            "Iteration 201, loss = 0.65026412\n",
            "Iteration 202, loss = 0.64982538\n",
            "Iteration 203, loss = 0.64939066\n",
            "Iteration 204, loss = 0.64894032\n",
            "Iteration 205, loss = 0.64850914\n",
            "Iteration 206, loss = 0.64805607\n",
            "Iteration 207, loss = 0.64758311\n",
            "Iteration 208, loss = 0.64714396\n",
            "Iteration 209, loss = 0.64668306\n",
            "Iteration 210, loss = 0.64621703\n",
            "Iteration 211, loss = 0.64573354\n",
            "Iteration 212, loss = 0.64524245\n",
            "Iteration 213, loss = 0.64475652\n",
            "Iteration 214, loss = 0.64424781\n",
            "Iteration 215, loss = 0.64373379\n",
            "Iteration 216, loss = 0.64319488\n",
            "Iteration 217, loss = 0.64266649\n",
            "Iteration 218, loss = 0.64206797\n",
            "Iteration 219, loss = 0.64151776\n",
            "Iteration 220, loss = 0.64092391\n",
            "Iteration 221, loss = 0.64037265\n",
            "Iteration 222, loss = 0.63975718\n",
            "Iteration 223, loss = 0.63912984\n",
            "Iteration 224, loss = 0.63853706\n",
            "Iteration 225, loss = 0.63793201\n",
            "Iteration 226, loss = 0.63731673\n",
            "Iteration 227, loss = 0.63667981\n",
            "Iteration 228, loss = 0.63605254\n",
            "Iteration 229, loss = 0.63548422\n",
            "Iteration 230, loss = 0.63491101\n",
            "Iteration 231, loss = 0.63436313\n",
            "Iteration 232, loss = 0.63384657\n",
            "Iteration 233, loss = 0.63332476\n",
            "Iteration 234, loss = 0.63282727\n",
            "Iteration 235, loss = 0.63230356\n",
            "Iteration 236, loss = 0.63180998\n",
            "Iteration 237, loss = 0.63127311\n",
            "Iteration 238, loss = 0.63075699\n",
            "Iteration 239, loss = 0.63023158\n",
            "Iteration 240, loss = 0.62973458\n",
            "Iteration 241, loss = 0.62918236\n",
            "Iteration 242, loss = 0.62865634\n",
            "Iteration 243, loss = 0.62816565\n",
            "Iteration 244, loss = 0.62760675\n",
            "Iteration 245, loss = 0.62707141\n",
            "Iteration 246, loss = 0.62655442\n",
            "Iteration 247, loss = 0.62598974\n",
            "Iteration 248, loss = 0.62548134\n",
            "Iteration 249, loss = 0.62493703\n",
            "Iteration 250, loss = 0.62437932\n",
            "Iteration 251, loss = 0.62386563\n",
            "Iteration 252, loss = 0.62332898\n",
            "Iteration 253, loss = 0.62278621\n",
            "Iteration 254, loss = 0.62224948\n",
            "Iteration 255, loss = 0.62168880\n",
            "Iteration 256, loss = 0.62112503\n",
            "Iteration 257, loss = 0.62057610\n",
            "Iteration 258, loss = 0.62003773\n",
            "Iteration 259, loss = 0.61948636\n",
            "Iteration 260, loss = 0.61889958\n",
            "Iteration 261, loss = 0.61836188\n",
            "Iteration 262, loss = 0.61777524\n",
            "Iteration 263, loss = 0.61720011\n",
            "Iteration 264, loss = 0.61662947\n",
            "Iteration 265, loss = 0.61608685\n",
            "Iteration 266, loss = 0.61548945\n",
            "Iteration 267, loss = 0.61492095\n",
            "Iteration 268, loss = 0.61435218\n",
            "Iteration 269, loss = 0.61376289\n",
            "Iteration 270, loss = 0.61319248\n",
            "Iteration 271, loss = 0.61258297\n",
            "Iteration 272, loss = 0.61200159\n",
            "Iteration 273, loss = 0.61138297\n",
            "Iteration 274, loss = 0.61080426\n",
            "Iteration 275, loss = 0.61017667\n",
            "Iteration 276, loss = 0.60957150\n",
            "Iteration 277, loss = 0.60897010\n",
            "Iteration 278, loss = 0.60834491\n",
            "Iteration 279, loss = 0.60771371\n",
            "Iteration 280, loss = 0.60708973\n",
            "Iteration 281, loss = 0.60643169\n",
            "Iteration 282, loss = 0.60579512\n",
            "Iteration 283, loss = 0.60511582\n",
            "Iteration 284, loss = 0.60449287\n",
            "Iteration 285, loss = 0.60382258\n",
            "Iteration 286, loss = 0.60313398\n",
            "Iteration 287, loss = 0.60248242\n",
            "Iteration 288, loss = 0.60181967\n",
            "Iteration 289, loss = 0.60116508\n",
            "Iteration 290, loss = 0.60047590\n",
            "Iteration 291, loss = 0.59982407\n",
            "Iteration 292, loss = 0.59912996\n",
            "Iteration 293, loss = 0.59848021\n",
            "Iteration 294, loss = 0.59781648\n",
            "Iteration 295, loss = 0.59718429\n",
            "Iteration 296, loss = 0.59646798\n",
            "Iteration 297, loss = 0.59581060\n",
            "Iteration 298, loss = 0.59513374\n",
            "Iteration 299, loss = 0.59446694\n",
            "Iteration 300, loss = 0.59377480\n",
            "Iteration 301, loss = 0.59310287\n",
            "Iteration 302, loss = 0.59244196\n",
            "Iteration 303, loss = 0.59174310\n",
            "Iteration 304, loss = 0.59105021\n",
            "Iteration 305, loss = 0.59037709\n",
            "Iteration 306, loss = 0.58969449\n",
            "Iteration 307, loss = 0.58902063\n",
            "Iteration 308, loss = 0.58829292\n",
            "Iteration 309, loss = 0.58759674\n",
            "Iteration 310, loss = 0.58689558\n",
            "Iteration 311, loss = 0.58618570\n",
            "Iteration 312, loss = 0.58548115\n",
            "Iteration 313, loss = 0.58478276\n",
            "Iteration 314, loss = 0.58405463\n",
            "Iteration 315, loss = 0.58332405\n",
            "Iteration 316, loss = 0.58261664\n",
            "Iteration 317, loss = 0.58188762\n",
            "Iteration 318, loss = 0.58114455\n",
            "Iteration 319, loss = 0.58041725\n",
            "Iteration 320, loss = 0.57969540\n",
            "Iteration 321, loss = 0.57896613\n",
            "Iteration 322, loss = 0.57821393\n",
            "Iteration 323, loss = 0.57748007\n",
            "Iteration 324, loss = 0.57676592\n",
            "Iteration 325, loss = 0.57603881\n",
            "Iteration 326, loss = 0.57528967\n",
            "Iteration 327, loss = 0.57456406\n",
            "Iteration 328, loss = 0.57385719\n",
            "Iteration 329, loss = 0.57314300\n",
            "Iteration 330, loss = 0.57240733\n",
            "Iteration 331, loss = 0.57168525\n",
            "Iteration 332, loss = 0.57098608\n",
            "Iteration 333, loss = 0.57024034\n",
            "Iteration 334, loss = 0.56953659\n",
            "Iteration 335, loss = 0.56880576\n",
            "Iteration 336, loss = 0.56804294\n",
            "Iteration 337, loss = 0.56735858\n",
            "Iteration 338, loss = 0.56664229\n",
            "Iteration 339, loss = 0.56591627\n",
            "Iteration 340, loss = 0.56519568\n",
            "Iteration 341, loss = 0.56450089\n",
            "Iteration 342, loss = 0.56382776\n",
            "Iteration 343, loss = 0.56312869\n",
            "Iteration 344, loss = 0.56244612\n",
            "Iteration 345, loss = 0.56177041\n",
            "Iteration 346, loss = 0.56111463\n",
            "Iteration 347, loss = 0.56043281\n",
            "Iteration 348, loss = 0.55975209\n",
            "Iteration 349, loss = 0.55907015\n",
            "Iteration 350, loss = 0.55840824\n",
            "Iteration 351, loss = 0.55775609\n",
            "Iteration 352, loss = 0.55708467\n",
            "Iteration 353, loss = 0.55643620\n",
            "Iteration 354, loss = 0.55579621\n",
            "Iteration 355, loss = 0.55512851\n",
            "Iteration 356, loss = 0.55449799\n",
            "Iteration 357, loss = 0.55382418\n",
            "Iteration 358, loss = 0.55318771\n",
            "Iteration 359, loss = 0.55256907\n",
            "Iteration 360, loss = 0.55190087\n",
            "Iteration 361, loss = 0.55128692\n",
            "Iteration 362, loss = 0.55065788\n",
            "Iteration 363, loss = 0.55000547\n",
            "Iteration 364, loss = 0.54939175\n",
            "Iteration 365, loss = 0.54876260\n",
            "Iteration 366, loss = 0.54814669\n",
            "Iteration 367, loss = 0.54753014\n",
            "Iteration 368, loss = 0.54686913\n",
            "Iteration 369, loss = 0.54624944\n",
            "Iteration 370, loss = 0.54561347\n",
            "Iteration 371, loss = 0.54498466\n",
            "Iteration 372, loss = 0.54437488\n",
            "Iteration 373, loss = 0.54372852\n",
            "Iteration 374, loss = 0.54313715\n",
            "Iteration 375, loss = 0.54249562\n",
            "Iteration 376, loss = 0.54186464\n",
            "Iteration 377, loss = 0.54125408\n",
            "Iteration 378, loss = 0.54065530\n",
            "Iteration 379, loss = 0.54002652\n",
            "Iteration 380, loss = 0.53940624\n",
            "Iteration 381, loss = 0.53881165\n",
            "Iteration 382, loss = 0.53820299\n",
            "Iteration 383, loss = 0.53758766\n",
            "Iteration 384, loss = 0.53696916\n",
            "Iteration 385, loss = 0.53639553\n",
            "Iteration 386, loss = 0.53575654\n",
            "Iteration 387, loss = 0.53517685\n",
            "Iteration 388, loss = 0.53457119\n",
            "Iteration 389, loss = 0.53397579\n",
            "Iteration 390, loss = 0.53338881\n",
            "Iteration 391, loss = 0.53278512\n",
            "Iteration 392, loss = 0.53218750\n",
            "Iteration 393, loss = 0.53160481\n",
            "Iteration 394, loss = 0.53100519\n",
            "Iteration 395, loss = 0.53043456\n",
            "Iteration 396, loss = 0.52985070\n",
            "Iteration 397, loss = 0.52928197\n",
            "Iteration 398, loss = 0.52871030\n",
            "Iteration 399, loss = 0.52812107\n",
            "Iteration 400, loss = 0.52755889\n",
            "Iteration 401, loss = 0.52701205\n",
            "Iteration 402, loss = 0.52643923\n",
            "Iteration 403, loss = 0.52587957\n",
            "Iteration 404, loss = 0.52531622\n",
            "Iteration 405, loss = 0.52476839\n",
            "Iteration 406, loss = 0.52422734\n",
            "Iteration 407, loss = 0.52365382\n",
            "Iteration 408, loss = 0.52311255\n",
            "Iteration 409, loss = 0.52255518\n",
            "Iteration 410, loss = 0.52201494\n",
            "Iteration 411, loss = 0.52150140\n",
            "Iteration 412, loss = 0.52094481\n",
            "Iteration 413, loss = 0.52042334\n",
            "Iteration 414, loss = 0.51986123\n",
            "Iteration 415, loss = 0.51934522\n",
            "Iteration 416, loss = 0.51880842\n",
            "Iteration 417, loss = 0.51830907\n",
            "Iteration 418, loss = 0.51777183\n",
            "Iteration 419, loss = 0.51724324\n",
            "Iteration 420, loss = 0.51671937\n",
            "Iteration 421, loss = 0.51621227\n",
            "Iteration 422, loss = 0.51572017\n",
            "Iteration 423, loss = 0.51518437\n",
            "Iteration 424, loss = 0.51467177\n",
            "Iteration 425, loss = 0.51420604\n",
            "Iteration 426, loss = 0.51367765\n",
            "Iteration 427, loss = 0.51316528\n",
            "Iteration 428, loss = 0.51269368\n",
            "Iteration 429, loss = 0.51217139\n",
            "Iteration 430, loss = 0.51169307\n",
            "Iteration 431, loss = 0.51118264\n",
            "Iteration 432, loss = 0.51072440\n",
            "Iteration 433, loss = 0.51022462\n",
            "Iteration 434, loss = 0.50972997\n",
            "Iteration 435, loss = 0.50925197\n",
            "Iteration 436, loss = 0.50876425\n",
            "Iteration 437, loss = 0.50827368\n",
            "Iteration 438, loss = 0.50778057\n",
            "Iteration 439, loss = 0.50729578\n",
            "Iteration 440, loss = 0.50683634\n",
            "Iteration 441, loss = 0.50631994\n",
            "Iteration 442, loss = 0.50588673\n",
            "Iteration 443, loss = 0.50538060\n",
            "Iteration 444, loss = 0.50491039\n",
            "Iteration 445, loss = 0.50443233\n",
            "Iteration 446, loss = 0.50397570\n",
            "Iteration 447, loss = 0.50349154\n",
            "Iteration 448, loss = 0.50304476\n",
            "Iteration 449, loss = 0.50259599\n",
            "Iteration 450, loss = 0.50216798\n",
            "Iteration 451, loss = 0.50172315\n",
            "Iteration 452, loss = 0.50128812\n",
            "Iteration 453, loss = 0.50085362\n",
            "Iteration 454, loss = 0.50046447\n",
            "Iteration 455, loss = 0.50004974\n",
            "Iteration 456, loss = 0.49963619\n",
            "Iteration 457, loss = 0.49922326\n",
            "Iteration 458, loss = 0.49880356\n",
            "Iteration 459, loss = 0.49843900\n",
            "Iteration 460, loss = 0.49806626\n",
            "Iteration 461, loss = 0.49763104\n",
            "Iteration 462, loss = 0.49729166\n",
            "Iteration 463, loss = 0.49687414\n",
            "Iteration 464, loss = 0.49649866\n",
            "Iteration 465, loss = 0.49611431\n",
            "Iteration 466, loss = 0.49576198\n",
            "Iteration 467, loss = 0.49539264\n",
            "Iteration 468, loss = 0.49504505\n",
            "Iteration 469, loss = 0.49468776\n",
            "Iteration 470, loss = 0.49431554\n",
            "Iteration 471, loss = 0.49396363\n",
            "Iteration 472, loss = 0.49360872\n",
            "Iteration 473, loss = 0.49327447\n",
            "Iteration 474, loss = 0.49294514\n",
            "Iteration 475, loss = 0.49260111\n",
            "Iteration 476, loss = 0.49226846\n",
            "Iteration 477, loss = 0.49194598\n",
            "Iteration 478, loss = 0.49162576\n",
            "Iteration 479, loss = 0.49132016\n",
            "Iteration 480, loss = 0.49100052\n",
            "Iteration 481, loss = 0.49068334\n",
            "Iteration 482, loss = 0.49036853\n",
            "Iteration 483, loss = 0.49007039\n",
            "Iteration 484, loss = 0.48976642\n",
            "Iteration 485, loss = 0.48947559\n",
            "Iteration 486, loss = 0.48915886\n",
            "Iteration 487, loss = 0.48887192\n",
            "Iteration 488, loss = 0.48859081\n",
            "Iteration 489, loss = 0.48830069\n",
            "Iteration 490, loss = 0.48801556\n",
            "Iteration 491, loss = 0.48771878\n",
            "Iteration 492, loss = 0.48743587\n",
            "Iteration 493, loss = 0.48715607\n",
            "Iteration 494, loss = 0.48690136\n",
            "Iteration 495, loss = 0.48658818\n",
            "Iteration 496, loss = 0.48632322\n",
            "Iteration 497, loss = 0.48604855\n",
            "Iteration 498, loss = 0.48579564\n",
            "Iteration 499, loss = 0.48551975\n",
            "Iteration 500, loss = 0.48526028\n",
            "Iteration 501, loss = 0.48501062\n",
            "Iteration 502, loss = 0.48473432\n",
            "Iteration 503, loss = 0.48447157\n",
            "Iteration 504, loss = 0.48423316\n",
            "Iteration 505, loss = 0.48397570\n",
            "Iteration 506, loss = 0.48373041\n",
            "Iteration 507, loss = 0.48348254\n",
            "Iteration 508, loss = 0.48324030\n",
            "Iteration 509, loss = 0.48298590\n",
            "Iteration 510, loss = 0.48274844\n",
            "Iteration 511, loss = 0.48250688\n",
            "Iteration 512, loss = 0.48226266\n",
            "Iteration 513, loss = 0.48203339\n",
            "Iteration 514, loss = 0.48179507\n",
            "Iteration 515, loss = 0.48156142\n",
            "Iteration 516, loss = 0.48132649\n",
            "Iteration 517, loss = 0.48110575\n",
            "Iteration 518, loss = 0.48085700\n",
            "Iteration 519, loss = 0.48062502\n",
            "Iteration 520, loss = 0.48040920\n",
            "Iteration 521, loss = 0.48016089\n",
            "Iteration 522, loss = 0.47993638\n",
            "Iteration 523, loss = 0.47971693\n",
            "Iteration 524, loss = 0.47949481\n",
            "Iteration 525, loss = 0.47926559\n",
            "Iteration 526, loss = 0.47903668\n",
            "Iteration 527, loss = 0.47882554\n",
            "Iteration 528, loss = 0.47860803\n",
            "Iteration 529, loss = 0.47839885\n",
            "Iteration 530, loss = 0.47816144\n",
            "Iteration 531, loss = 0.47792784\n",
            "Iteration 532, loss = 0.47774024\n",
            "Iteration 533, loss = 0.47746049\n",
            "Iteration 534, loss = 0.47725672\n",
            "Iteration 535, loss = 0.47696455\n",
            "Iteration 536, loss = 0.47668574\n",
            "Iteration 537, loss = 0.47641648\n",
            "Iteration 538, loss = 0.47615635\n",
            "Iteration 539, loss = 0.47588424\n",
            "Iteration 540, loss = 0.47563598\n",
            "Iteration 541, loss = 0.47536377\n",
            "Iteration 542, loss = 0.47516421\n",
            "Iteration 543, loss = 0.47489819\n",
            "Iteration 544, loss = 0.47466960\n",
            "Iteration 545, loss = 0.47441946\n",
            "Iteration 546, loss = 0.47419499\n",
            "Iteration 547, loss = 0.47397625\n",
            "Iteration 548, loss = 0.47375523\n",
            "Iteration 549, loss = 0.47351510\n",
            "Iteration 550, loss = 0.47329717\n",
            "Iteration 551, loss = 0.47309143\n",
            "Iteration 552, loss = 0.47286452\n",
            "Iteration 553, loss = 0.47265511\n",
            "Iteration 554, loss = 0.47241986\n",
            "Iteration 555, loss = 0.47220728\n",
            "Iteration 556, loss = 0.47199783\n",
            "Iteration 557, loss = 0.47177998\n",
            "Iteration 558, loss = 0.47156413\n",
            "Iteration 559, loss = 0.47135100\n",
            "Iteration 560, loss = 0.47112251\n",
            "Iteration 561, loss = 0.47092374\n",
            "Iteration 562, loss = 0.47070759\n",
            "Iteration 563, loss = 0.47051975\n",
            "Iteration 564, loss = 0.47028299\n",
            "Iteration 565, loss = 0.47008621\n",
            "Iteration 566, loss = 0.46990478\n",
            "Iteration 567, loss = 0.46968309\n",
            "Iteration 568, loss = 0.46946382\n",
            "Iteration 569, loss = 0.46927095\n",
            "Iteration 570, loss = 0.46908378\n",
            "Iteration 571, loss = 0.46887607\n",
            "Iteration 572, loss = 0.46868307\n",
            "Iteration 573, loss = 0.46849112\n",
            "Iteration 574, loss = 0.46829634\n",
            "Iteration 575, loss = 0.46808796\n",
            "Iteration 576, loss = 0.46790248\n",
            "Iteration 577, loss = 0.46773379\n",
            "Iteration 578, loss = 0.46752567\n",
            "Iteration 579, loss = 0.46732898\n",
            "Iteration 580, loss = 0.46715823\n",
            "Iteration 581, loss = 0.46696629\n",
            "Iteration 582, loss = 0.46676477\n",
            "Iteration 583, loss = 0.46659314\n",
            "Iteration 584, loss = 0.46639715\n",
            "Iteration 585, loss = 0.46620225\n",
            "Iteration 586, loss = 0.46603454\n",
            "Iteration 587, loss = 0.46582649\n",
            "Iteration 588, loss = 0.46566142\n",
            "Iteration 589, loss = 0.46546634\n",
            "Iteration 590, loss = 0.46529267\n",
            "Iteration 591, loss = 0.46510936\n",
            "Iteration 592, loss = 0.46494829\n",
            "Iteration 593, loss = 0.46474692\n",
            "Iteration 594, loss = 0.46456782\n",
            "Iteration 595, loss = 0.46438893\n",
            "Iteration 596, loss = 0.46421479\n",
            "Iteration 597, loss = 0.46403936\n",
            "Iteration 598, loss = 0.46386586\n",
            "Iteration 599, loss = 0.46370145\n",
            "Iteration 600, loss = 0.46352967\n",
            "Iteration 601, loss = 0.46333947\n",
            "Iteration 602, loss = 0.46318505\n",
            "Iteration 603, loss = 0.46301819\n",
            "Iteration 604, loss = 0.46284154\n",
            "Iteration 605, loss = 0.46269383\n",
            "Iteration 606, loss = 0.46251020\n",
            "Iteration 607, loss = 0.46235370\n",
            "Iteration 608, loss = 0.46219479\n",
            "Iteration 609, loss = 0.46202672\n",
            "Iteration 610, loss = 0.46186891\n",
            "Iteration 611, loss = 0.46170398\n",
            "Iteration 612, loss = 0.46159619\n",
            "Iteration 613, loss = 0.46139985\n",
            "Iteration 614, loss = 0.46124521\n",
            "Iteration 615, loss = 0.46111678\n",
            "Iteration 616, loss = 0.46095774\n",
            "Iteration 617, loss = 0.46078975\n",
            "Iteration 618, loss = 0.46064093\n",
            "Iteration 619, loss = 0.46052328\n",
            "Iteration 620, loss = 0.46034464\n",
            "Iteration 621, loss = 0.46020493\n",
            "Iteration 622, loss = 0.46009920\n",
            "Iteration 623, loss = 0.45990703\n",
            "Iteration 624, loss = 0.45977119\n",
            "Iteration 625, loss = 0.45962325\n",
            "Iteration 626, loss = 0.45947674\n",
            "Iteration 627, loss = 0.45934776\n",
            "Iteration 628, loss = 0.45919088\n",
            "Iteration 629, loss = 0.45904197\n",
            "Iteration 630, loss = 0.45889962\n",
            "Iteration 631, loss = 0.45876260\n",
            "Iteration 632, loss = 0.45862587\n",
            "Iteration 633, loss = 0.45848220\n",
            "Iteration 634, loss = 0.45833722\n",
            "Iteration 635, loss = 0.45818146\n",
            "Iteration 636, loss = 0.45805349\n",
            "Iteration 637, loss = 0.45791132\n",
            "Iteration 638, loss = 0.45779505\n",
            "Iteration 639, loss = 0.45763094\n",
            "Iteration 640, loss = 0.45751332\n",
            "Iteration 641, loss = 0.45735381\n",
            "Iteration 642, loss = 0.45723150\n",
            "Iteration 643, loss = 0.45709371\n",
            "Iteration 644, loss = 0.45695606\n",
            "Iteration 645, loss = 0.45682516\n",
            "Iteration 646, loss = 0.45671932\n",
            "Iteration 647, loss = 0.45657330\n",
            "Iteration 648, loss = 0.45643764\n",
            "Iteration 649, loss = 0.45629458\n",
            "Iteration 650, loss = 0.45617932\n",
            "Iteration 651, loss = 0.45607331\n",
            "Iteration 652, loss = 0.45593699\n",
            "Iteration 653, loss = 0.45581167\n",
            "Iteration 654, loss = 0.45566913\n",
            "Iteration 655, loss = 0.45554071\n",
            "Iteration 656, loss = 0.45541639\n",
            "Iteration 657, loss = 0.45529353\n",
            "Iteration 658, loss = 0.45516193\n",
            "Iteration 659, loss = 0.45503547\n",
            "Iteration 660, loss = 0.45492030\n",
            "Iteration 661, loss = 0.45479138\n",
            "Iteration 662, loss = 0.45465803\n",
            "Iteration 663, loss = 0.45453769\n",
            "Iteration 664, loss = 0.45442218\n",
            "Iteration 665, loss = 0.45430950\n",
            "Iteration 666, loss = 0.45419064\n",
            "Iteration 667, loss = 0.45405869\n",
            "Iteration 668, loss = 0.45392739\n",
            "Iteration 669, loss = 0.45382367\n",
            "Iteration 670, loss = 0.45369621\n",
            "Iteration 671, loss = 0.45358262\n",
            "Iteration 672, loss = 0.45345445\n",
            "Iteration 673, loss = 0.45334560\n",
            "Iteration 674, loss = 0.45322130\n",
            "Iteration 675, loss = 0.45314713\n",
            "Iteration 676, loss = 0.45299665\n",
            "Iteration 677, loss = 0.45287082\n",
            "Iteration 678, loss = 0.45277066\n",
            "Iteration 679, loss = 0.45264718\n",
            "Iteration 680, loss = 0.45252641\n",
            "Iteration 681, loss = 0.45243418\n",
            "Iteration 682, loss = 0.45232315\n",
            "Iteration 683, loss = 0.45217443\n",
            "Iteration 684, loss = 0.45206051\n",
            "Iteration 685, loss = 0.45196409\n",
            "Iteration 686, loss = 0.45184218\n",
            "Iteration 687, loss = 0.45172950\n",
            "Iteration 688, loss = 0.45162840\n",
            "Iteration 689, loss = 0.45152708\n",
            "Iteration 690, loss = 0.45139601\n",
            "Iteration 691, loss = 0.45128254\n",
            "Iteration 692, loss = 0.45117339\n",
            "Iteration 693, loss = 0.45107483\n",
            "Iteration 694, loss = 0.45096651\n",
            "Iteration 695, loss = 0.45084521\n",
            "Iteration 696, loss = 0.45073976\n",
            "Iteration 697, loss = 0.45063169\n",
            "Iteration 698, loss = 0.45052656\n",
            "Iteration 699, loss = 0.45043924\n",
            "Iteration 700, loss = 0.45033647\n",
            "Iteration 701, loss = 0.45020902\n",
            "Iteration 702, loss = 0.45011644\n",
            "Iteration 703, loss = 0.45000198\n",
            "Iteration 704, loss = 0.44989885\n",
            "Iteration 705, loss = 0.44979389\n",
            "Iteration 706, loss = 0.44968296\n",
            "Iteration 707, loss = 0.44958074\n",
            "Iteration 708, loss = 0.44946784\n",
            "Iteration 709, loss = 0.44937821\n",
            "Iteration 710, loss = 0.44928835\n",
            "Iteration 711, loss = 0.44916358\n",
            "Iteration 712, loss = 0.44904991\n",
            "Iteration 713, loss = 0.44896459\n",
            "Iteration 714, loss = 0.44885836\n",
            "Iteration 715, loss = 0.44876770\n",
            "Iteration 716, loss = 0.44864809\n",
            "Iteration 717, loss = 0.44854069\n",
            "Iteration 718, loss = 0.44844766\n",
            "Iteration 719, loss = 0.44835093\n",
            "Iteration 720, loss = 0.44824843\n",
            "Iteration 721, loss = 0.44815044\n",
            "Iteration 722, loss = 0.44804051\n",
            "Iteration 723, loss = 0.44795805\n",
            "Iteration 724, loss = 0.44784843\n",
            "Iteration 725, loss = 0.44774525\n",
            "Iteration 726, loss = 0.44765175\n",
            "Iteration 727, loss = 0.44756075\n",
            "Iteration 728, loss = 0.44746290\n",
            "Iteration 729, loss = 0.44735505\n",
            "Iteration 730, loss = 0.44726480\n",
            "Iteration 731, loss = 0.44716396\n",
            "Iteration 732, loss = 0.44708218\n",
            "Iteration 733, loss = 0.44698176\n",
            "Iteration 734, loss = 0.44689510\n",
            "Iteration 735, loss = 0.44678474\n",
            "Iteration 736, loss = 0.44669564\n",
            "Iteration 737, loss = 0.44661006\n",
            "Iteration 738, loss = 0.44651080\n",
            "Iteration 739, loss = 0.44642873\n",
            "Iteration 740, loss = 0.44632564\n",
            "Iteration 741, loss = 0.44623660\n",
            "Iteration 742, loss = 0.44614525\n",
            "Iteration 743, loss = 0.44607705\n",
            "Iteration 744, loss = 0.44596797\n",
            "Iteration 745, loss = 0.44588265\n",
            "Iteration 746, loss = 0.44579834\n",
            "Iteration 747, loss = 0.44571953\n",
            "Iteration 748, loss = 0.44563420\n",
            "Iteration 749, loss = 0.44554989\n",
            "Iteration 750, loss = 0.44545677\n",
            "Iteration 751, loss = 0.44539288\n",
            "Iteration 752, loss = 0.44528888\n",
            "Iteration 753, loss = 0.44521172\n",
            "Iteration 754, loss = 0.44513524\n",
            "Iteration 755, loss = 0.44505330\n",
            "Iteration 756, loss = 0.44496243\n",
            "Iteration 757, loss = 0.44489250\n",
            "Iteration 758, loss = 0.44481155\n",
            "Iteration 759, loss = 0.44472772\n",
            "Iteration 760, loss = 0.44466633\n",
            "Iteration 761, loss = 0.44457071\n",
            "Iteration 762, loss = 0.44449132\n",
            "Iteration 763, loss = 0.44441757\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(hidden_layer_sizes=(3, 3), max_iter=1000, random_state=3,\n",
              "              solver='sgd', verbose=True)"
            ],
            "text/html": [
              "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(hidden_layer_sizes=(3, 3), max_iter=1000, random_state=3,\n",
              "              solver=&#x27;sgd&#x27;, verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(hidden_layer_sizes=(3, 3), max_iter=1000, random_state=3,\n",
              "              solver=&#x27;sgd&#x27;, verbose=True)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "previsoes2 = rna2.predict(X_test)"
      ],
      "metadata": {
        "id": "4MqHNsQ4DOfj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rna2_teste = accuracy_score(y_test, previsoes2)"
      ],
      "metadata": {
        "id": "NW11SMqbDasQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(rna2_teste)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHQEagbNDc13",
        "outputId": "73b1bfe2-9613-4ef2-873d-617f95d318c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7723880597014925\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cm_rna2_teste = confusion_matrix(y_test, previsoes2)"
      ],
      "metadata": {
        "id": "XO8CpYM4Dkgg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_test, previsoes2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDpC4zdnD2zM",
        "outputId": "ce09617f-806f-4e66-9f81-ad323c47e1d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.83      0.82       169\n",
            "           1       0.70      0.67      0.68        99\n",
            "\n",
            "    accuracy                           0.77       268\n",
            "   macro avg       0.76      0.75      0.75       268\n",
            "weighted avg       0.77      0.77      0.77       268\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "previsoes_treino2 = rna2.predict(X_train_oversampled)"
      ],
      "metadata": {
        "id": "cbn9UkxVFDGr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rna2_treino = accuracy_score(y_train_oversampled, previsoes_treino2)"
      ],
      "metadata": {
        "id": "GSMSU-CEFDGr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm_rna2_treino = confusion_matrix(y_train_oversampled, previsoes_treino2)"
      ],
      "metadata": {
        "id": "dTeAKX21FDGr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GridSearch"
      ],
      "metadata": {
        "id": "zxnaZzt0U5-2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Grid Search\n",
        "param_grid = {\n",
        "    'activation': ['relu', 'tanh', 'logistic'],\n",
        "    'solver': ['sgd', 'adam'],\n",
        "    'max_iter': [1000, 2000],\n",
        "    'tol': [0.0001, 0.001],\n",
        "}\n",
        "\n",
        "g_search = GridSearchCV(estimator=rna2, param_grid=param_grid, cv=10)\n",
        "g_search.fit(X_train_oversampled, y_train_oversampled)\n",
        "\n",
        "best_params_grid2 = g_search.best_params_\n",
        "best_score_grid2 = g_search.best_score_\n",
        "\n",
        "print(\"Melhores hiperparâmetros encontrados através do Grid Search:\")\n",
        "print(best_params_grid2)\n",
        "print(\"Melhor pontuação (acurácia) encontrada através do Grid Search:\", best_score_grid2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6f11c66-8b8b-48a3-e906-9ceb5b68c6b9",
        "id": "7OYEDTl2U5-3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mA saída de streaming foi truncada nas últimas 5000 linhas.\u001b[0m\n",
            "Iteration 247, loss = 0.56346102\n",
            "Iteration 248, loss = 0.56254016\n",
            "Iteration 249, loss = 0.56157539\n",
            "Iteration 250, loss = 0.56065497\n",
            "Iteration 251, loss = 0.55966122\n",
            "Iteration 252, loss = 0.55872129\n",
            "Iteration 253, loss = 0.55778043\n",
            "Iteration 254, loss = 0.55681335\n",
            "Iteration 255, loss = 0.55589612\n",
            "Iteration 256, loss = 0.55497291\n",
            "Iteration 257, loss = 0.55400158\n",
            "Iteration 258, loss = 0.55306000\n",
            "Iteration 259, loss = 0.55212939\n",
            "Iteration 260, loss = 0.55120276\n",
            "Iteration 261, loss = 0.55020712\n",
            "Iteration 262, loss = 0.54929448\n",
            "Iteration 263, loss = 0.54833807\n",
            "Iteration 264, loss = 0.54739058\n",
            "Iteration 265, loss = 0.54648054\n",
            "Iteration 266, loss = 0.54550055\n",
            "Iteration 267, loss = 0.54458424\n",
            "Iteration 268, loss = 0.54365022\n",
            "Iteration 269, loss = 0.54272918\n",
            "Iteration 270, loss = 0.54180953\n",
            "Iteration 271, loss = 0.54089530\n",
            "Iteration 272, loss = 0.54005650\n",
            "Iteration 273, loss = 0.53908898\n",
            "Iteration 274, loss = 0.53817226\n",
            "Iteration 275, loss = 0.53727531\n",
            "Iteration 276, loss = 0.53636678\n",
            "Iteration 277, loss = 0.53549103\n",
            "Iteration 278, loss = 0.53460724\n",
            "Iteration 279, loss = 0.53370744\n",
            "Iteration 280, loss = 0.53284847\n",
            "Iteration 281, loss = 0.53196788\n",
            "Iteration 282, loss = 0.53109109\n",
            "Iteration 283, loss = 0.53021475\n",
            "Iteration 284, loss = 0.52935378\n",
            "Iteration 285, loss = 0.52850814\n",
            "Iteration 286, loss = 0.52767412\n",
            "Iteration 287, loss = 0.52676336\n",
            "Iteration 288, loss = 0.52589147\n",
            "Iteration 289, loss = 0.52505059\n",
            "Iteration 290, loss = 0.52418059\n",
            "Iteration 291, loss = 0.52338343\n",
            "Iteration 292, loss = 0.52248482\n",
            "Iteration 293, loss = 0.52163645\n",
            "Iteration 294, loss = 0.52080430\n",
            "Iteration 295, loss = 0.51993919\n",
            "Iteration 296, loss = 0.51911833\n",
            "Iteration 297, loss = 0.51828427\n",
            "Iteration 298, loss = 0.51743280\n",
            "Iteration 299, loss = 0.51659781\n",
            "Iteration 300, loss = 0.51575821\n",
            "Iteration 301, loss = 0.51495806\n",
            "Iteration 302, loss = 0.51404706\n",
            "Iteration 303, loss = 0.51327217\n",
            "Iteration 304, loss = 0.51242604\n",
            "Iteration 305, loss = 0.51160759\n",
            "Iteration 306, loss = 0.51079157\n",
            "Iteration 307, loss = 0.50996417\n",
            "Iteration 308, loss = 0.50915633\n",
            "Iteration 309, loss = 0.50835278\n",
            "Iteration 310, loss = 0.50756060\n",
            "Iteration 311, loss = 0.50669815\n",
            "Iteration 312, loss = 0.50589351\n",
            "Iteration 313, loss = 0.50510735\n",
            "Iteration 314, loss = 0.50432368\n",
            "Iteration 315, loss = 0.50351701\n",
            "Iteration 316, loss = 0.50275203\n",
            "Iteration 317, loss = 0.50196670\n",
            "Iteration 318, loss = 0.50117869\n",
            "Iteration 319, loss = 0.50042510\n",
            "Iteration 320, loss = 0.49966193\n",
            "Iteration 321, loss = 0.49889110\n",
            "Iteration 322, loss = 0.49814077\n",
            "Iteration 323, loss = 0.49738140\n",
            "Iteration 324, loss = 0.49661271\n",
            "Iteration 325, loss = 0.49586495\n",
            "Iteration 326, loss = 0.49513460\n",
            "Iteration 327, loss = 0.49443218\n",
            "Iteration 328, loss = 0.49364668\n",
            "Iteration 329, loss = 0.49294159\n",
            "Iteration 330, loss = 0.49217485\n",
            "Iteration 331, loss = 0.49147166\n",
            "Iteration 332, loss = 0.49073846\n",
            "Iteration 333, loss = 0.48998166\n",
            "Iteration 334, loss = 0.48928209\n",
            "Iteration 335, loss = 0.48854979\n",
            "Iteration 336, loss = 0.48780888\n",
            "Iteration 337, loss = 0.48712873\n",
            "Iteration 338, loss = 0.48640433\n",
            "Iteration 339, loss = 0.48570943\n",
            "Iteration 340, loss = 0.48499926\n",
            "Iteration 341, loss = 0.48434097\n",
            "Iteration 342, loss = 0.48362861\n",
            "Iteration 343, loss = 0.48293575\n",
            "Iteration 344, loss = 0.48224084\n",
            "Iteration 345, loss = 0.48156409\n",
            "Iteration 346, loss = 0.48087358\n",
            "Iteration 347, loss = 0.48021575\n",
            "Iteration 348, loss = 0.47954022\n",
            "Iteration 349, loss = 0.47883649\n",
            "Iteration 350, loss = 0.47818102\n",
            "Iteration 351, loss = 0.47751058\n",
            "Iteration 352, loss = 0.47681230\n",
            "Iteration 353, loss = 0.47617138\n",
            "Iteration 354, loss = 0.47553642\n",
            "Iteration 355, loss = 0.47488576\n",
            "Iteration 356, loss = 0.47423940\n",
            "Iteration 357, loss = 0.47361252\n",
            "Iteration 358, loss = 0.47296652\n",
            "Iteration 359, loss = 0.47236070\n",
            "Iteration 360, loss = 0.47175695\n",
            "Iteration 361, loss = 0.47111253\n",
            "Iteration 362, loss = 0.47051271\n",
            "Iteration 363, loss = 0.46988397\n",
            "Iteration 364, loss = 0.46927463\n",
            "Iteration 365, loss = 0.46861446\n",
            "Iteration 366, loss = 0.46802918\n",
            "Iteration 367, loss = 0.46743653\n",
            "Iteration 368, loss = 0.46684513\n",
            "Iteration 369, loss = 0.46623314\n",
            "Iteration 370, loss = 0.46569440\n",
            "Iteration 371, loss = 0.46508558\n",
            "Iteration 372, loss = 0.46450272\n",
            "Iteration 373, loss = 0.46392512\n",
            "Iteration 374, loss = 0.46339494\n",
            "Iteration 375, loss = 0.46280986\n",
            "Iteration 376, loss = 0.46223257\n",
            "Iteration 377, loss = 0.46170790\n",
            "Iteration 378, loss = 0.46115164\n",
            "Iteration 379, loss = 0.46060503\n",
            "Iteration 380, loss = 0.46012443\n",
            "Iteration 381, loss = 0.45954625\n",
            "Iteration 382, loss = 0.45905681\n",
            "Iteration 383, loss = 0.45853891\n",
            "Iteration 384, loss = 0.45799815\n",
            "Iteration 385, loss = 0.45752930\n",
            "Iteration 386, loss = 0.45699155\n",
            "Iteration 387, loss = 0.45649667\n",
            "Iteration 388, loss = 0.45598144\n",
            "Iteration 389, loss = 0.45548867\n",
            "Iteration 390, loss = 0.45497732\n",
            "Iteration 391, loss = 0.45448697\n",
            "Iteration 392, loss = 0.45400244\n",
            "Iteration 393, loss = 0.45352697\n",
            "Iteration 394, loss = 0.45302939\n",
            "Iteration 395, loss = 0.45254882\n",
            "Iteration 396, loss = 0.45204862\n",
            "Iteration 397, loss = 0.45159524\n",
            "Iteration 398, loss = 0.45115032\n",
            "Iteration 399, loss = 0.45066534\n",
            "Iteration 400, loss = 0.45020142\n",
            "Iteration 401, loss = 0.44974270\n",
            "Iteration 402, loss = 0.44928956\n",
            "Iteration 403, loss = 0.44884556\n",
            "Iteration 404, loss = 0.44840774\n",
            "Iteration 405, loss = 0.44793306\n",
            "Iteration 406, loss = 0.44750494\n",
            "Iteration 407, loss = 0.44708263\n",
            "Iteration 408, loss = 0.44665888\n",
            "Iteration 409, loss = 0.44620654\n",
            "Iteration 410, loss = 0.44578264\n",
            "Iteration 411, loss = 0.44535446\n",
            "Iteration 412, loss = 0.44492236\n",
            "Iteration 413, loss = 0.44450478\n",
            "Iteration 414, loss = 0.44409622\n",
            "Iteration 415, loss = 0.44368275\n",
            "Iteration 416, loss = 0.44328305\n",
            "Iteration 417, loss = 0.44290567\n",
            "Iteration 418, loss = 0.44250100\n",
            "Iteration 419, loss = 0.44212989\n",
            "Iteration 420, loss = 0.44174353\n",
            "Iteration 421, loss = 0.44136978\n",
            "Iteration 422, loss = 0.44099418\n",
            "Iteration 423, loss = 0.44061396\n",
            "Iteration 424, loss = 0.44021315\n",
            "Iteration 425, loss = 0.43983394\n",
            "Iteration 426, loss = 0.43946970\n",
            "Iteration 427, loss = 0.43912905\n",
            "Iteration 428, loss = 0.43878755\n",
            "Iteration 429, loss = 0.43843892\n",
            "Iteration 430, loss = 0.43806835\n",
            "Iteration 431, loss = 0.43766754\n",
            "Iteration 432, loss = 0.43731963\n",
            "Iteration 433, loss = 0.43698525\n",
            "Iteration 434, loss = 0.43663695\n",
            "Iteration 435, loss = 0.43629183\n",
            "Iteration 436, loss = 0.43595341\n",
            "Iteration 437, loss = 0.43564276\n",
            "Iteration 438, loss = 0.43529101\n",
            "Iteration 439, loss = 0.43498112\n",
            "Iteration 440, loss = 0.43462304\n",
            "Iteration 441, loss = 0.43429660\n",
            "Iteration 442, loss = 0.43398970\n",
            "Iteration 443, loss = 0.43365993\n",
            "Iteration 444, loss = 0.43336522\n",
            "Iteration 445, loss = 0.43301556\n",
            "Iteration 446, loss = 0.43272425\n",
            "Iteration 447, loss = 0.43244802\n",
            "Iteration 448, loss = 0.43214568\n",
            "Iteration 449, loss = 0.43185177\n",
            "Iteration 450, loss = 0.43152754\n",
            "Iteration 451, loss = 0.43121198\n",
            "Iteration 452, loss = 0.43089135\n",
            "Iteration 453, loss = 0.43059086\n",
            "Iteration 454, loss = 0.43034485\n",
            "Iteration 455, loss = 0.43003293\n",
            "Iteration 456, loss = 0.42977088\n",
            "Iteration 457, loss = 0.42947523\n",
            "Iteration 458, loss = 0.42919346\n",
            "Iteration 459, loss = 0.42891405\n",
            "Iteration 460, loss = 0.42867610\n",
            "Iteration 461, loss = 0.42835777\n",
            "Iteration 462, loss = 0.42809575\n",
            "Iteration 463, loss = 0.42781125\n",
            "Iteration 464, loss = 0.42753398\n",
            "Iteration 465, loss = 0.42725922\n",
            "Iteration 466, loss = 0.42701011\n",
            "Iteration 467, loss = 0.42674069\n",
            "Iteration 468, loss = 0.42649557\n",
            "Iteration 469, loss = 0.42621875\n",
            "Iteration 470, loss = 0.42594371\n",
            "Iteration 471, loss = 0.42564994\n",
            "Iteration 472, loss = 0.42544653\n",
            "Iteration 473, loss = 0.42518432\n",
            "Iteration 474, loss = 0.42490818\n",
            "Iteration 475, loss = 0.42465456\n",
            "Iteration 476, loss = 0.42444170\n",
            "Iteration 477, loss = 0.42416912\n",
            "Iteration 478, loss = 0.42393798\n",
            "Iteration 479, loss = 0.42368129\n",
            "Iteration 480, loss = 0.42344095\n",
            "Iteration 481, loss = 0.42317952\n",
            "Iteration 482, loss = 0.42294904\n",
            "Iteration 483, loss = 0.42268872\n",
            "Iteration 484, loss = 0.42244482\n",
            "Iteration 485, loss = 0.42222110\n",
            "Iteration 486, loss = 0.42196864\n",
            "Iteration 487, loss = 0.42174409\n",
            "Iteration 488, loss = 0.42153738\n",
            "Iteration 489, loss = 0.42128513\n",
            "Iteration 490, loss = 0.42110329\n",
            "Iteration 491, loss = 0.42085765\n",
            "Iteration 492, loss = 0.42064556\n",
            "Iteration 493, loss = 0.42044368\n",
            "Iteration 494, loss = 0.42023487\n",
            "Iteration 495, loss = 0.42004022\n",
            "Iteration 496, loss = 0.41981280\n",
            "Iteration 497, loss = 0.41955497\n",
            "Iteration 498, loss = 0.41932506\n",
            "Iteration 499, loss = 0.41912303\n",
            "Iteration 500, loss = 0.41888586\n",
            "Iteration 501, loss = 0.41869853\n",
            "Iteration 502, loss = 0.41847418\n",
            "Iteration 503, loss = 0.41827663\n",
            "Iteration 504, loss = 0.41804293\n",
            "Iteration 505, loss = 0.41784761\n",
            "Iteration 506, loss = 0.41763227\n",
            "Iteration 507, loss = 0.41743583\n",
            "Iteration 508, loss = 0.41726033\n",
            "Iteration 509, loss = 0.41705661\n",
            "Iteration 510, loss = 0.41683160\n",
            "Iteration 511, loss = 0.41664062\n",
            "Iteration 512, loss = 0.41643379\n",
            "Iteration 513, loss = 0.41624420\n",
            "Iteration 514, loss = 0.41605982\n",
            "Iteration 515, loss = 0.41586834\n",
            "Iteration 516, loss = 0.41566137\n",
            "Iteration 517, loss = 0.41547458\n",
            "Iteration 518, loss = 0.41528322\n",
            "Iteration 519, loss = 0.41508734\n",
            "Iteration 520, loss = 0.41489106\n",
            "Iteration 521, loss = 0.41471178\n",
            "Iteration 522, loss = 0.41453243\n",
            "Iteration 523, loss = 0.41434386\n",
            "Iteration 524, loss = 0.41417902\n",
            "Iteration 525, loss = 0.41400728\n",
            "Iteration 526, loss = 0.41382618\n",
            "Iteration 527, loss = 0.41365046\n",
            "Iteration 528, loss = 0.41345790\n",
            "Iteration 529, loss = 0.41326756\n",
            "Iteration 530, loss = 0.41311676\n",
            "Iteration 531, loss = 0.41293379\n",
            "Iteration 532, loss = 0.41273636\n",
            "Iteration 533, loss = 0.41257361\n",
            "Iteration 534, loss = 0.41238609\n",
            "Iteration 535, loss = 0.41221477\n",
            "Iteration 536, loss = 0.41205426\n",
            "Iteration 537, loss = 0.41185323\n",
            "Iteration 538, loss = 0.41168787\n",
            "Iteration 539, loss = 0.41152813\n",
            "Iteration 540, loss = 0.41146417\n",
            "Iteration 541, loss = 0.41124990\n",
            "Iteration 542, loss = 0.41109333\n",
            "Iteration 543, loss = 0.41092373\n",
            "Iteration 544, loss = 0.41073170\n",
            "Iteration 545, loss = 0.41055885\n",
            "Iteration 546, loss = 0.41036452\n",
            "Iteration 547, loss = 0.41018781\n",
            "Iteration 548, loss = 0.41003692\n",
            "Iteration 549, loss = 0.40986644\n",
            "Iteration 550, loss = 0.40971959\n",
            "Iteration 551, loss = 0.40955685\n",
            "Iteration 552, loss = 0.40938032\n",
            "Iteration 553, loss = 0.40922639\n",
            "Iteration 554, loss = 0.40906922\n",
            "Iteration 555, loss = 0.40897159\n",
            "Iteration 556, loss = 0.40879152\n",
            "Iteration 557, loss = 0.40861928\n",
            "Iteration 558, loss = 0.40844770\n",
            "Iteration 559, loss = 0.40833477\n",
            "Iteration 560, loss = 0.40819066\n",
            "Iteration 561, loss = 0.40805517\n",
            "Iteration 562, loss = 0.40792838\n",
            "Iteration 563, loss = 0.40779601\n",
            "Iteration 564, loss = 0.40767848\n",
            "Iteration 565, loss = 0.40755085\n",
            "Iteration 566, loss = 0.40735495\n",
            "Iteration 567, loss = 0.40721251\n",
            "Iteration 568, loss = 0.40704670\n",
            "Iteration 569, loss = 0.40691562\n",
            "Iteration 570, loss = 0.40679335\n",
            "Iteration 571, loss = 0.40666901\n",
            "Iteration 572, loss = 0.40653001\n",
            "Iteration 573, loss = 0.40638365\n",
            "Iteration 574, loss = 0.40624523\n",
            "Iteration 575, loss = 0.40615603\n",
            "Iteration 576, loss = 0.40601986\n",
            "Iteration 577, loss = 0.40591248\n",
            "Iteration 578, loss = 0.40578097\n",
            "Iteration 579, loss = 0.40564505\n",
            "Iteration 580, loss = 0.40552385\n",
            "Iteration 581, loss = 0.40538942\n",
            "Iteration 582, loss = 0.40533607\n",
            "Iteration 583, loss = 0.40515632\n",
            "Iteration 584, loss = 0.40498356\n",
            "Iteration 585, loss = 0.40489165\n",
            "Iteration 586, loss = 0.40474587\n",
            "Iteration 587, loss = 0.40465825\n",
            "Iteration 588, loss = 0.40453153\n",
            "Iteration 589, loss = 0.40442292\n",
            "Iteration 590, loss = 0.40428843\n",
            "Iteration 591, loss = 0.40417472\n",
            "Iteration 592, loss = 0.40405922\n",
            "Iteration 593, loss = 0.40391777\n",
            "Iteration 594, loss = 0.40378379\n",
            "Iteration 595, loss = 0.40367381\n",
            "Iteration 596, loss = 0.40354915\n",
            "Iteration 597, loss = 0.40342611\n",
            "Iteration 598, loss = 0.40331552\n",
            "Iteration 599, loss = 0.40321804\n",
            "Iteration 600, loss = 0.40309919\n",
            "Iteration 601, loss = 0.40299602\n",
            "Iteration 602, loss = 0.40289436\n",
            "Iteration 603, loss = 0.40276631\n",
            "Iteration 604, loss = 0.40266359\n",
            "Iteration 605, loss = 0.40255138\n",
            "Iteration 606, loss = 0.40248554\n",
            "Iteration 607, loss = 0.40235661\n",
            "Iteration 608, loss = 0.40223424\n",
            "Iteration 609, loss = 0.40216178\n",
            "Iteration 610, loss = 0.40205632\n",
            "Iteration 611, loss = 0.40197755\n",
            "Iteration 612, loss = 0.40184894\n",
            "Iteration 613, loss = 0.40174230\n",
            "Iteration 614, loss = 0.40161255\n",
            "Iteration 615, loss = 0.40154216\n",
            "Iteration 616, loss = 0.40142618\n",
            "Iteration 617, loss = 0.40133833\n",
            "Iteration 618, loss = 0.40123291\n",
            "Iteration 619, loss = 0.40115846\n",
            "Iteration 620, loss = 0.40105393\n",
            "Iteration 621, loss = 0.40097109\n",
            "Iteration 622, loss = 0.40091598\n",
            "Iteration 623, loss = 0.40083294\n",
            "Iteration 624, loss = 0.40073463\n",
            "Iteration 625, loss = 0.40064926\n",
            "Iteration 626, loss = 0.40062114\n",
            "Iteration 627, loss = 0.40043751\n",
            "Iteration 628, loss = 0.40031575\n",
            "Iteration 629, loss = 0.40023898\n",
            "Iteration 630, loss = 0.40014364\n",
            "Iteration 631, loss = 0.40007043\n",
            "Iteration 632, loss = 0.40000593\n",
            "Iteration 633, loss = 0.39991252\n",
            "Iteration 634, loss = 0.39983657\n",
            "Iteration 635, loss = 0.39974375\n",
            "Iteration 636, loss = 0.39963374\n",
            "Iteration 637, loss = 0.39953560\n",
            "Iteration 638, loss = 0.39944186\n",
            "Iteration 639, loss = 0.39936714\n",
            "Iteration 640, loss = 0.39932992\n",
            "Iteration 641, loss = 0.39928224\n",
            "Iteration 642, loss = 0.39919404\n",
            "Iteration 643, loss = 0.39914535\n",
            "Iteration 644, loss = 0.39905708\n",
            "Iteration 645, loss = 0.39896125\n",
            "Iteration 646, loss = 0.39892339\n",
            "Iteration 647, loss = 0.39882469\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.71663519\n",
            "Iteration 2, loss = 0.71557696\n",
            "Iteration 3, loss = 0.71449648\n",
            "Iteration 4, loss = 0.71344198\n",
            "Iteration 5, loss = 0.71238511\n",
            "Iteration 6, loss = 0.71133791\n",
            "Iteration 7, loss = 0.71037898\n",
            "Iteration 8, loss = 0.70947064\n",
            "Iteration 9, loss = 0.70845961\n",
            "Iteration 10, loss = 0.70758910\n",
            "Iteration 11, loss = 0.70678246\n",
            "Iteration 12, loss = 0.70594977\n",
            "Iteration 13, loss = 0.70520965\n",
            "Iteration 14, loss = 0.70439865\n",
            "Iteration 15, loss = 0.70376180\n",
            "Iteration 16, loss = 0.70311275\n",
            "Iteration 17, loss = 0.70250225\n",
            "Iteration 18, loss = 0.70185428\n",
            "Iteration 19, loss = 0.70123690\n",
            "Iteration 20, loss = 0.70069557\n",
            "Iteration 21, loss = 0.70023446\n",
            "Iteration 22, loss = 0.69965582\n",
            "Iteration 23, loss = 0.69920340\n",
            "Iteration 24, loss = 0.69874283\n",
            "Iteration 25, loss = 0.69827023\n",
            "Iteration 26, loss = 0.69790691\n",
            "Iteration 27, loss = 0.69749424\n",
            "Iteration 28, loss = 0.69708464\n",
            "Iteration 29, loss = 0.69679621\n",
            "Iteration 30, loss = 0.69644921\n",
            "Iteration 31, loss = 0.69611694\n",
            "Iteration 32, loss = 0.69586942\n",
            "Iteration 33, loss = 0.69553745\n",
            "Iteration 34, loss = 0.69529445\n",
            "Iteration 35, loss = 0.69502839\n",
            "Iteration 36, loss = 0.69477694\n",
            "Iteration 37, loss = 0.69452064\n",
            "Iteration 38, loss = 0.69436191\n",
            "Iteration 39, loss = 0.69414656\n",
            "Iteration 40, loss = 0.69393575\n",
            "Iteration 41, loss = 0.69378961\n",
            "Iteration 42, loss = 0.69361823\n",
            "Iteration 43, loss = 0.69351973\n",
            "Iteration 44, loss = 0.69335147\n",
            "Iteration 45, loss = 0.69323693\n",
            "Iteration 46, loss = 0.69305465\n",
            "Iteration 47, loss = 0.69294652\n",
            "Iteration 48, loss = 0.69283228\n",
            "Iteration 49, loss = 0.69269728\n",
            "Iteration 50, loss = 0.69257000\n",
            "Iteration 51, loss = 0.69247262\n",
            "Iteration 52, loss = 0.69232576\n",
            "Iteration 53, loss = 0.69219409\n",
            "Iteration 54, loss = 0.69208664\n",
            "Iteration 55, loss = 0.69197802\n",
            "Iteration 56, loss = 0.69184001\n",
            "Iteration 57, loss = 0.69170177\n",
            "Iteration 58, loss = 0.69158603\n",
            "Iteration 59, loss = 0.69148529\n",
            "Iteration 60, loss = 0.69132696\n",
            "Iteration 61, loss = 0.69117649\n",
            "Iteration 62, loss = 0.69104746\n",
            "Iteration 63, loss = 0.69091533\n",
            "Iteration 64, loss = 0.69076033\n",
            "Iteration 65, loss = 0.69063290\n",
            "Iteration 66, loss = 0.69047081\n",
            "Iteration 67, loss = 0.69031439\n",
            "Iteration 68, loss = 0.69015732\n",
            "Iteration 69, loss = 0.68998925\n",
            "Iteration 70, loss = 0.68981898\n",
            "Iteration 71, loss = 0.68965099\n",
            "Iteration 72, loss = 0.68946434\n",
            "Iteration 73, loss = 0.68928277\n",
            "Iteration 74, loss = 0.68908948\n",
            "Iteration 75, loss = 0.68889668\n",
            "Iteration 76, loss = 0.68868201\n",
            "Iteration 77, loss = 0.68846275\n",
            "Iteration 78, loss = 0.68824037\n",
            "Iteration 79, loss = 0.68800068\n",
            "Iteration 80, loss = 0.68776996\n",
            "Iteration 81, loss = 0.68753459\n",
            "Iteration 82, loss = 0.68727378\n",
            "Iteration 83, loss = 0.68702590\n",
            "Iteration 84, loss = 0.68676140\n",
            "Iteration 85, loss = 0.68648589\n",
            "Iteration 86, loss = 0.68620921\n",
            "Iteration 87, loss = 0.68593789\n",
            "Iteration 88, loss = 0.68565084\n",
            "Iteration 89, loss = 0.68532775\n",
            "Iteration 90, loss = 0.68503340\n",
            "Iteration 91, loss = 0.68471981\n",
            "Iteration 92, loss = 0.68440476\n",
            "Iteration 93, loss = 0.68407350\n",
            "Iteration 94, loss = 0.68374088\n",
            "Iteration 95, loss = 0.68340189\n",
            "Iteration 96, loss = 0.68305117\n",
            "Iteration 97, loss = 0.68266657\n",
            "Iteration 98, loss = 0.68230340\n",
            "Iteration 99, loss = 0.68190524\n",
            "Iteration 100, loss = 0.68150373\n",
            "Iteration 101, loss = 0.68109979\n",
            "Iteration 102, loss = 0.68072182\n",
            "Iteration 103, loss = 0.68029843\n",
            "Iteration 104, loss = 0.67990561\n",
            "Iteration 105, loss = 0.67946541\n",
            "Iteration 106, loss = 0.67904439\n",
            "Iteration 107, loss = 0.67861366\n",
            "Iteration 108, loss = 0.67814258\n",
            "Iteration 109, loss = 0.67771234\n",
            "Iteration 110, loss = 0.67724966\n",
            "Iteration 111, loss = 0.67679293\n",
            "Iteration 112, loss = 0.67632675\n",
            "Iteration 113, loss = 0.67583772\n",
            "Iteration 114, loss = 0.67531975\n",
            "Iteration 115, loss = 0.67481914\n",
            "Iteration 116, loss = 0.67427563\n",
            "Iteration 117, loss = 0.67375465\n",
            "Iteration 118, loss = 0.67320643\n",
            "Iteration 119, loss = 0.67266232\n",
            "Iteration 120, loss = 0.67206960\n",
            "Iteration 121, loss = 0.67153040\n",
            "Iteration 122, loss = 0.67092850\n",
            "Iteration 123, loss = 0.67036359\n",
            "Iteration 124, loss = 0.66977054\n",
            "Iteration 125, loss = 0.66915429\n",
            "Iteration 126, loss = 0.66853269\n",
            "Iteration 127, loss = 0.66791729\n",
            "Iteration 128, loss = 0.66733763\n",
            "Iteration 129, loss = 0.66671825\n",
            "Iteration 130, loss = 0.66608054\n",
            "Iteration 131, loss = 0.66544068\n",
            "Iteration 132, loss = 0.66477775\n",
            "Iteration 133, loss = 0.66411362\n",
            "Iteration 134, loss = 0.66345370\n",
            "Iteration 135, loss = 0.66278396\n",
            "Iteration 136, loss = 0.66210476\n",
            "Iteration 137, loss = 0.66140013\n",
            "Iteration 138, loss = 0.66071917\n",
            "Iteration 139, loss = 0.66002408\n",
            "Iteration 140, loss = 0.65935648\n",
            "Iteration 141, loss = 0.65863922\n",
            "Iteration 142, loss = 0.65792021\n",
            "Iteration 143, loss = 0.65721724\n",
            "Iteration 144, loss = 0.65648772\n",
            "Iteration 145, loss = 0.65576180\n",
            "Iteration 146, loss = 0.65503099\n",
            "Iteration 147, loss = 0.65426905\n",
            "Iteration 148, loss = 0.65350605\n",
            "Iteration 149, loss = 0.65279327\n",
            "Iteration 150, loss = 0.65197577\n",
            "Iteration 151, loss = 0.65123762\n",
            "Iteration 152, loss = 0.65044130\n",
            "Iteration 153, loss = 0.64968454\n",
            "Iteration 154, loss = 0.64888142\n",
            "Iteration 155, loss = 0.64810611\n",
            "Iteration 156, loss = 0.64730273\n",
            "Iteration 157, loss = 0.64650751\n",
            "Iteration 158, loss = 0.64569095\n",
            "Iteration 159, loss = 0.64492005\n",
            "Iteration 160, loss = 0.64408381\n",
            "Iteration 161, loss = 0.64328019\n",
            "Iteration 162, loss = 0.64246436\n",
            "Iteration 163, loss = 0.64163665\n",
            "Iteration 164, loss = 0.64079238\n",
            "Iteration 165, loss = 0.63994006\n",
            "Iteration 166, loss = 0.63909041\n",
            "Iteration 167, loss = 0.63825207\n",
            "Iteration 168, loss = 0.63738634\n",
            "Iteration 169, loss = 0.63653617\n",
            "Iteration 170, loss = 0.63570069\n",
            "Iteration 171, loss = 0.63481481\n",
            "Iteration 172, loss = 0.63395276\n",
            "Iteration 173, loss = 0.63309368\n",
            "Iteration 174, loss = 0.63223088\n",
            "Iteration 175, loss = 0.63135490\n",
            "Iteration 176, loss = 0.63048315\n",
            "Iteration 177, loss = 0.62958374\n",
            "Iteration 178, loss = 0.62873299\n",
            "Iteration 179, loss = 0.62784026\n",
            "Iteration 180, loss = 0.62694374\n",
            "Iteration 181, loss = 0.62609431\n",
            "Iteration 182, loss = 0.62513921\n",
            "Iteration 183, loss = 0.62425266\n",
            "Iteration 184, loss = 0.62333363\n",
            "Iteration 185, loss = 0.62246655\n",
            "Iteration 186, loss = 0.62155081\n",
            "Iteration 187, loss = 0.62063343\n",
            "Iteration 188, loss = 0.61972225\n",
            "Iteration 189, loss = 0.61881972\n",
            "Iteration 190, loss = 0.61791271\n",
            "Iteration 191, loss = 0.61700663\n",
            "Iteration 192, loss = 0.61605716\n",
            "Iteration 193, loss = 0.61518055\n",
            "Iteration 194, loss = 0.61425156\n",
            "Iteration 195, loss = 0.61338247\n",
            "Iteration 196, loss = 0.61244166\n",
            "Iteration 197, loss = 0.61154249\n",
            "Iteration 198, loss = 0.61064540\n",
            "Iteration 199, loss = 0.60975192\n",
            "Iteration 200, loss = 0.60885455\n",
            "Iteration 201, loss = 0.60794243\n",
            "Iteration 202, loss = 0.60702716\n",
            "Iteration 203, loss = 0.60610924\n",
            "Iteration 204, loss = 0.60521488\n",
            "Iteration 205, loss = 0.60428637\n",
            "Iteration 206, loss = 0.60336981\n",
            "Iteration 207, loss = 0.60243430\n",
            "Iteration 208, loss = 0.60153380\n",
            "Iteration 209, loss = 0.60063170\n",
            "Iteration 210, loss = 0.59973162\n",
            "Iteration 211, loss = 0.59877876\n",
            "Iteration 212, loss = 0.59789998\n",
            "Iteration 213, loss = 0.59697554\n",
            "Iteration 214, loss = 0.59609617\n",
            "Iteration 215, loss = 0.59513166\n",
            "Iteration 216, loss = 0.59423009\n",
            "Iteration 217, loss = 0.59327273\n",
            "Iteration 218, loss = 0.59239664\n",
            "Iteration 219, loss = 0.59146561\n",
            "Iteration 220, loss = 0.59055882\n",
            "Iteration 221, loss = 0.58964702\n",
            "Iteration 222, loss = 0.58872791\n",
            "Iteration 223, loss = 0.58780433\n",
            "Iteration 224, loss = 0.58689848\n",
            "Iteration 225, loss = 0.58598071\n",
            "Iteration 226, loss = 0.58504486\n",
            "Iteration 227, loss = 0.58410660\n",
            "Iteration 228, loss = 0.58317695\n",
            "Iteration 229, loss = 0.58226966\n",
            "Iteration 230, loss = 0.58137709\n",
            "Iteration 231, loss = 0.58042342\n",
            "Iteration 232, loss = 0.57955763\n",
            "Iteration 233, loss = 0.57859756\n",
            "Iteration 234, loss = 0.57770271\n",
            "Iteration 235, loss = 0.57675033\n",
            "Iteration 236, loss = 0.57587589\n",
            "Iteration 237, loss = 0.57490765\n",
            "Iteration 238, loss = 0.57403076\n",
            "Iteration 239, loss = 0.57313497\n",
            "Iteration 240, loss = 0.57223755\n",
            "Iteration 241, loss = 0.57133009\n",
            "Iteration 242, loss = 0.57039587\n",
            "Iteration 243, loss = 0.56952597\n",
            "Iteration 244, loss = 0.56860625\n",
            "Iteration 245, loss = 0.56777456\n",
            "Iteration 246, loss = 0.56680652\n",
            "Iteration 247, loss = 0.56591034\n",
            "Iteration 248, loss = 0.56503026\n",
            "Iteration 249, loss = 0.56412596\n",
            "Iteration 250, loss = 0.56325278\n",
            "Iteration 251, loss = 0.56232289\n",
            "Iteration 252, loss = 0.56143387\n",
            "Iteration 253, loss = 0.56056100\n",
            "Iteration 254, loss = 0.55965664\n",
            "Iteration 255, loss = 0.55877712\n",
            "Iteration 256, loss = 0.55789236\n",
            "Iteration 257, loss = 0.55698697\n",
            "Iteration 258, loss = 0.55608443\n",
            "Iteration 259, loss = 0.55522349\n",
            "Iteration 260, loss = 0.55433876\n",
            "Iteration 261, loss = 0.55345099\n",
            "Iteration 262, loss = 0.55254278\n",
            "Iteration 263, loss = 0.55171191\n",
            "Iteration 264, loss = 0.55081851\n",
            "Iteration 265, loss = 0.54996655\n",
            "Iteration 266, loss = 0.54906203\n",
            "Iteration 267, loss = 0.54823294\n",
            "Iteration 268, loss = 0.54736540\n",
            "Iteration 269, loss = 0.54652236\n",
            "Iteration 270, loss = 0.54563700\n",
            "Iteration 271, loss = 0.54481551\n",
            "Iteration 272, loss = 0.54399447\n",
            "Iteration 273, loss = 0.54314211\n",
            "Iteration 274, loss = 0.54230449\n",
            "Iteration 275, loss = 0.54149512\n",
            "Iteration 276, loss = 0.54068754\n",
            "Iteration 277, loss = 0.53985996\n",
            "Iteration 278, loss = 0.53903169\n",
            "Iteration 279, loss = 0.53824198\n",
            "Iteration 280, loss = 0.53745673\n",
            "Iteration 281, loss = 0.53666400\n",
            "Iteration 282, loss = 0.53583851\n",
            "Iteration 283, loss = 0.53504585\n",
            "Iteration 284, loss = 0.53423765\n",
            "Iteration 285, loss = 0.53349523\n",
            "Iteration 286, loss = 0.53269005\n",
            "Iteration 287, loss = 0.53189922\n",
            "Iteration 288, loss = 0.53107643\n",
            "Iteration 289, loss = 0.53031126\n",
            "Iteration 290, loss = 0.52950843\n",
            "Iteration 291, loss = 0.52877314\n",
            "Iteration 292, loss = 0.52799273\n",
            "Iteration 293, loss = 0.52722625\n",
            "Iteration 294, loss = 0.52645829\n",
            "Iteration 295, loss = 0.52570952\n",
            "Iteration 296, loss = 0.52495433\n",
            "Iteration 297, loss = 0.52418593\n",
            "Iteration 298, loss = 0.52341540\n",
            "Iteration 299, loss = 0.52266424\n",
            "Iteration 300, loss = 0.52191903\n",
            "Iteration 301, loss = 0.52114775\n",
            "Iteration 302, loss = 0.52037195\n",
            "Iteration 303, loss = 0.51965096\n",
            "Iteration 304, loss = 0.51888694\n",
            "Iteration 305, loss = 0.51817377\n",
            "Iteration 306, loss = 0.51743763\n",
            "Iteration 307, loss = 0.51669560\n",
            "Iteration 308, loss = 0.51597178\n",
            "Iteration 309, loss = 0.51526454\n",
            "Iteration 310, loss = 0.51455628\n",
            "Iteration 311, loss = 0.51381550\n",
            "Iteration 312, loss = 0.51308299\n",
            "Iteration 313, loss = 0.51235729\n",
            "Iteration 314, loss = 0.51168138\n",
            "Iteration 315, loss = 0.51097704\n",
            "Iteration 316, loss = 0.51026072\n",
            "Iteration 317, loss = 0.50955244\n",
            "Iteration 318, loss = 0.50885671\n",
            "Iteration 319, loss = 0.50819381\n",
            "Iteration 320, loss = 0.50746877\n",
            "Iteration 321, loss = 0.50680664\n",
            "Iteration 322, loss = 0.50612306\n",
            "Iteration 323, loss = 0.50547447\n",
            "Iteration 324, loss = 0.50478295\n",
            "Iteration 325, loss = 0.50413202\n",
            "Iteration 326, loss = 0.50347264\n",
            "Iteration 327, loss = 0.50282106\n",
            "Iteration 328, loss = 0.50215913\n",
            "Iteration 329, loss = 0.50152869\n",
            "Iteration 330, loss = 0.50085989\n",
            "Iteration 331, loss = 0.50024703\n",
            "Iteration 332, loss = 0.49956753\n",
            "Iteration 333, loss = 0.49893158\n",
            "Iteration 334, loss = 0.49831785\n",
            "Iteration 335, loss = 0.49766351\n",
            "Iteration 336, loss = 0.49704729\n",
            "Iteration 337, loss = 0.49643294\n",
            "Iteration 338, loss = 0.49578176\n",
            "Iteration 339, loss = 0.49516691\n",
            "Iteration 340, loss = 0.49453136\n",
            "Iteration 341, loss = 0.49393166\n",
            "Iteration 342, loss = 0.49331687\n",
            "Iteration 343, loss = 0.49266777\n",
            "Iteration 344, loss = 0.49204410\n",
            "Iteration 345, loss = 0.49144258\n",
            "Iteration 346, loss = 0.49083059\n",
            "Iteration 347, loss = 0.49022215\n",
            "Iteration 348, loss = 0.48965494\n",
            "Iteration 349, loss = 0.48900343\n",
            "Iteration 350, loss = 0.48840449\n",
            "Iteration 351, loss = 0.48780983\n",
            "Iteration 352, loss = 0.48722906\n",
            "Iteration 353, loss = 0.48664200\n",
            "Iteration 354, loss = 0.48604804\n",
            "Iteration 355, loss = 0.48546322\n",
            "Iteration 356, loss = 0.48488562\n",
            "Iteration 357, loss = 0.48432931\n",
            "Iteration 358, loss = 0.48373828\n",
            "Iteration 359, loss = 0.48318691\n",
            "Iteration 360, loss = 0.48263293\n",
            "Iteration 361, loss = 0.48207141\n",
            "Iteration 362, loss = 0.48153779\n",
            "Iteration 363, loss = 0.48097165\n",
            "Iteration 364, loss = 0.48041779\n",
            "Iteration 365, loss = 0.47987113\n",
            "Iteration 366, loss = 0.47932880\n",
            "Iteration 367, loss = 0.47877512\n",
            "Iteration 368, loss = 0.47827055\n",
            "Iteration 369, loss = 0.47772881\n",
            "Iteration 370, loss = 0.47723043\n",
            "Iteration 371, loss = 0.47667990\n",
            "Iteration 372, loss = 0.47615754\n",
            "Iteration 373, loss = 0.47562013\n",
            "Iteration 374, loss = 0.47513513\n",
            "Iteration 375, loss = 0.47462158\n",
            "Iteration 376, loss = 0.47409193\n",
            "Iteration 377, loss = 0.47359165\n",
            "Iteration 378, loss = 0.47310638\n",
            "Iteration 379, loss = 0.47260984\n",
            "Iteration 380, loss = 0.47217850\n",
            "Iteration 381, loss = 0.47164594\n",
            "Iteration 382, loss = 0.47119817\n",
            "Iteration 383, loss = 0.47072538\n",
            "Iteration 384, loss = 0.47026340\n",
            "Iteration 385, loss = 0.46983061\n",
            "Iteration 386, loss = 0.46933139\n",
            "Iteration 387, loss = 0.46887383\n",
            "Iteration 388, loss = 0.46840447\n",
            "Iteration 389, loss = 0.46798374\n",
            "Iteration 390, loss = 0.46749131\n",
            "Iteration 391, loss = 0.46705702\n",
            "Iteration 392, loss = 0.46660347\n",
            "Iteration 393, loss = 0.46616860\n",
            "Iteration 394, loss = 0.46573154\n",
            "Iteration 395, loss = 0.46525680\n",
            "Iteration 396, loss = 0.46481851\n",
            "Iteration 397, loss = 0.46438498\n",
            "Iteration 398, loss = 0.46398619\n",
            "Iteration 399, loss = 0.46354559\n",
            "Iteration 400, loss = 0.46314427\n",
            "Iteration 401, loss = 0.46271856\n",
            "Iteration 402, loss = 0.46231138\n",
            "Iteration 403, loss = 0.46193955\n",
            "Iteration 404, loss = 0.46151942\n",
            "Iteration 405, loss = 0.46110483\n",
            "Iteration 406, loss = 0.46070658\n",
            "Iteration 407, loss = 0.46030650\n",
            "Iteration 408, loss = 0.45992396\n",
            "Iteration 409, loss = 0.45954222\n",
            "Iteration 410, loss = 0.45915326\n",
            "Iteration 411, loss = 0.45875222\n",
            "Iteration 412, loss = 0.45840892\n",
            "Iteration 413, loss = 0.45799281\n",
            "Iteration 414, loss = 0.45759298\n",
            "Iteration 415, loss = 0.45723280\n",
            "Iteration 416, loss = 0.45684548\n",
            "Iteration 417, loss = 0.45650089\n",
            "Iteration 418, loss = 0.45613667\n",
            "Iteration 419, loss = 0.45580768\n",
            "Iteration 420, loss = 0.45544389\n",
            "Iteration 421, loss = 0.45508111\n",
            "Iteration 422, loss = 0.45472114\n",
            "Iteration 423, loss = 0.45440457\n",
            "Iteration 424, loss = 0.45401089\n",
            "Iteration 425, loss = 0.45365430\n",
            "Iteration 426, loss = 0.45330943\n",
            "Iteration 427, loss = 0.45302348\n",
            "Iteration 428, loss = 0.45269300\n",
            "Iteration 429, loss = 0.45238488\n",
            "Iteration 430, loss = 0.45203495\n",
            "Iteration 431, loss = 0.45169163\n",
            "Iteration 432, loss = 0.45136255\n",
            "Iteration 433, loss = 0.45104188\n",
            "Iteration 434, loss = 0.45069307\n",
            "Iteration 435, loss = 0.45040602\n",
            "Iteration 436, loss = 0.45003656\n",
            "Iteration 437, loss = 0.44975724\n",
            "Iteration 438, loss = 0.44940680\n",
            "Iteration 439, loss = 0.44912434\n",
            "Iteration 440, loss = 0.44879370\n",
            "Iteration 441, loss = 0.44851316\n",
            "Iteration 442, loss = 0.44821629\n",
            "Iteration 443, loss = 0.44792986\n",
            "Iteration 444, loss = 0.44765808\n",
            "Iteration 445, loss = 0.44733257\n",
            "Iteration 446, loss = 0.44704389\n",
            "Iteration 447, loss = 0.44677854\n",
            "Iteration 448, loss = 0.44647241\n",
            "Iteration 449, loss = 0.44616587\n",
            "Iteration 450, loss = 0.44588985\n",
            "Iteration 451, loss = 0.44558689\n",
            "Iteration 452, loss = 0.44526754\n",
            "Iteration 453, loss = 0.44497236\n",
            "Iteration 454, loss = 0.44478016\n",
            "Iteration 455, loss = 0.44442715\n",
            "Iteration 456, loss = 0.44417445\n",
            "Iteration 457, loss = 0.44387770\n",
            "Iteration 458, loss = 0.44361031\n",
            "Iteration 459, loss = 0.44336050\n",
            "Iteration 460, loss = 0.44308768\n",
            "Iteration 461, loss = 0.44281066\n",
            "Iteration 462, loss = 0.44256251\n",
            "Iteration 463, loss = 0.44229206\n",
            "Iteration 464, loss = 0.44202278\n",
            "Iteration 465, loss = 0.44175442\n",
            "Iteration 466, loss = 0.44152946\n",
            "Iteration 467, loss = 0.44127840\n",
            "Iteration 468, loss = 0.44100239\n",
            "Iteration 469, loss = 0.44075650\n",
            "Iteration 470, loss = 0.44052483\n",
            "Iteration 471, loss = 0.44023527\n",
            "Iteration 472, loss = 0.44001370\n",
            "Iteration 473, loss = 0.43976739\n",
            "Iteration 474, loss = 0.43950339\n",
            "Iteration 475, loss = 0.43929309\n",
            "Iteration 476, loss = 0.43905203\n",
            "Iteration 477, loss = 0.43877772\n",
            "Iteration 478, loss = 0.43857053\n",
            "Iteration 479, loss = 0.43829571\n",
            "Iteration 480, loss = 0.43806346\n",
            "Iteration 481, loss = 0.43780596\n",
            "Iteration 482, loss = 0.43758388\n",
            "Iteration 483, loss = 0.43733857\n",
            "Iteration 484, loss = 0.43714433\n",
            "Iteration 485, loss = 0.43687698\n",
            "Iteration 486, loss = 0.43665102\n",
            "Iteration 487, loss = 0.43642334\n",
            "Iteration 488, loss = 0.43621635\n",
            "Iteration 489, loss = 0.43596822\n",
            "Iteration 490, loss = 0.43576858\n",
            "Iteration 491, loss = 0.43554888\n",
            "Iteration 492, loss = 0.43531249\n",
            "Iteration 493, loss = 0.43510691\n",
            "Iteration 494, loss = 0.43486952\n",
            "Iteration 495, loss = 0.43465906\n",
            "Iteration 496, loss = 0.43448551\n",
            "Iteration 497, loss = 0.43418382\n",
            "Iteration 498, loss = 0.43396070\n",
            "Iteration 499, loss = 0.43377272\n",
            "Iteration 500, loss = 0.43351390\n",
            "Iteration 501, loss = 0.43336295\n",
            "Iteration 502, loss = 0.43309101\n",
            "Iteration 503, loss = 0.43289222\n",
            "Iteration 504, loss = 0.43267124\n",
            "Iteration 505, loss = 0.43247650\n",
            "Iteration 506, loss = 0.43227711\n",
            "Iteration 507, loss = 0.43208226\n",
            "Iteration 508, loss = 0.43187569\n",
            "Iteration 509, loss = 0.43169643\n",
            "Iteration 510, loss = 0.43147517\n",
            "Iteration 511, loss = 0.43128611\n",
            "Iteration 512, loss = 0.43105567\n",
            "Iteration 513, loss = 0.43088146\n",
            "Iteration 514, loss = 0.43066492\n",
            "Iteration 515, loss = 0.43046998\n",
            "Iteration 516, loss = 0.43026594\n",
            "Iteration 517, loss = 0.43006765\n",
            "Iteration 518, loss = 0.42986835\n",
            "Iteration 519, loss = 0.42966583\n",
            "Iteration 520, loss = 0.42948110\n",
            "Iteration 521, loss = 0.42930307\n",
            "Iteration 522, loss = 0.42910146\n",
            "Iteration 523, loss = 0.42891350\n",
            "Iteration 524, loss = 0.42872765\n",
            "Iteration 525, loss = 0.42854678\n",
            "Iteration 526, loss = 0.42838005\n",
            "Iteration 527, loss = 0.42817632\n",
            "Iteration 528, loss = 0.42801256\n",
            "Iteration 529, loss = 0.42779933\n",
            "Iteration 530, loss = 0.42765296\n",
            "Iteration 531, loss = 0.42748354\n",
            "Iteration 532, loss = 0.42728926\n",
            "Iteration 533, loss = 0.42711043\n",
            "Iteration 534, loss = 0.42693335\n",
            "Iteration 535, loss = 0.42677679\n",
            "Iteration 536, loss = 0.42662410\n",
            "Iteration 537, loss = 0.42642986\n",
            "Iteration 538, loss = 0.42627294\n",
            "Iteration 539, loss = 0.42613151\n",
            "Iteration 540, loss = 0.42602125\n",
            "Iteration 541, loss = 0.42582853\n",
            "Iteration 542, loss = 0.42568754\n",
            "Iteration 543, loss = 0.42552335\n",
            "Iteration 544, loss = 0.42533493\n",
            "Iteration 545, loss = 0.42518502\n",
            "Iteration 546, loss = 0.42500218\n",
            "Iteration 547, loss = 0.42482819\n",
            "Iteration 548, loss = 0.42469386\n",
            "Iteration 549, loss = 0.42450679\n",
            "Iteration 550, loss = 0.42434649\n",
            "Iteration 551, loss = 0.42420436\n",
            "Iteration 552, loss = 0.42404572\n",
            "Iteration 553, loss = 0.42389458\n",
            "Iteration 554, loss = 0.42374855\n",
            "Iteration 555, loss = 0.42361836\n",
            "Iteration 556, loss = 0.42347163\n",
            "Iteration 557, loss = 0.42332537\n",
            "Iteration 558, loss = 0.42316224\n",
            "Iteration 559, loss = 0.42303394\n",
            "Iteration 560, loss = 0.42290730\n",
            "Iteration 561, loss = 0.42278963\n",
            "Iteration 562, loss = 0.42265801\n",
            "Iteration 563, loss = 0.42251846\n",
            "Iteration 564, loss = 0.42244667\n",
            "Iteration 565, loss = 0.42232055\n",
            "Iteration 566, loss = 0.42214687\n",
            "Iteration 567, loss = 0.42196579\n",
            "Iteration 568, loss = 0.42182470\n",
            "Iteration 569, loss = 0.42175287\n",
            "Iteration 570, loss = 0.42159985\n",
            "Iteration 571, loss = 0.42148874\n",
            "Iteration 572, loss = 0.42135898\n",
            "Iteration 573, loss = 0.42124041\n",
            "Iteration 574, loss = 0.42112524\n",
            "Iteration 575, loss = 0.42101186\n",
            "Iteration 576, loss = 0.42089048\n",
            "Iteration 577, loss = 0.42078609\n",
            "Iteration 578, loss = 0.42071037\n",
            "Iteration 579, loss = 0.42057768\n",
            "Iteration 580, loss = 0.42046267\n",
            "Iteration 581, loss = 0.42032799\n",
            "Iteration 582, loss = 0.42030407\n",
            "Iteration 583, loss = 0.42013278\n",
            "Iteration 584, loss = 0.42001386\n",
            "Iteration 585, loss = 0.41990313\n",
            "Iteration 586, loss = 0.41978911\n",
            "Iteration 587, loss = 0.41967738\n",
            "Iteration 588, loss = 0.41958992\n",
            "Iteration 589, loss = 0.41952649\n",
            "Iteration 590, loss = 0.41936891\n",
            "Iteration 591, loss = 0.41929405\n",
            "Iteration 592, loss = 0.41918962\n",
            "Iteration 593, loss = 0.41904150\n",
            "Iteration 594, loss = 0.41897047\n",
            "Iteration 595, loss = 0.41888574\n",
            "Iteration 596, loss = 0.41875982\n",
            "Iteration 597, loss = 0.41866675\n",
            "Iteration 598, loss = 0.41856666\n",
            "Iteration 599, loss = 0.41852067\n",
            "Iteration 600, loss = 0.41838028\n",
            "Iteration 601, loss = 0.41828204\n",
            "Iteration 602, loss = 0.41823183\n",
            "Iteration 603, loss = 0.41810983\n",
            "Iteration 604, loss = 0.41802409\n",
            "Iteration 605, loss = 0.41794146\n",
            "Iteration 606, loss = 0.41787058\n",
            "Iteration 607, loss = 0.41776677\n",
            "Iteration 608, loss = 0.41768092\n",
            "Iteration 609, loss = 0.41760860\n",
            "Iteration 610, loss = 0.41754322\n",
            "Iteration 611, loss = 0.41743878\n",
            "Iteration 612, loss = 0.41735865\n",
            "Iteration 613, loss = 0.41725281\n",
            "Iteration 614, loss = 0.41714214\n",
            "Iteration 615, loss = 0.41708123\n",
            "Iteration 616, loss = 0.41695552\n",
            "Iteration 617, loss = 0.41687209\n",
            "Iteration 618, loss = 0.41678433\n",
            "Iteration 619, loss = 0.41670854\n",
            "Iteration 620, loss = 0.41661215\n",
            "Iteration 621, loss = 0.41658285\n",
            "Iteration 622, loss = 0.41650880\n",
            "Iteration 623, loss = 0.41641843\n",
            "Iteration 624, loss = 0.41634229\n",
            "Iteration 625, loss = 0.41628216\n",
            "Iteration 626, loss = 0.41622920\n",
            "Iteration 627, loss = 0.41612259\n",
            "Iteration 628, loss = 0.41601873\n",
            "Iteration 629, loss = 0.41595194\n",
            "Iteration 630, loss = 0.41588921\n",
            "Iteration 631, loss = 0.41581461\n",
            "Iteration 632, loss = 0.41574267\n",
            "Iteration 633, loss = 0.41567867\n",
            "Iteration 634, loss = 0.41559186\n",
            "Iteration 635, loss = 0.41550031\n",
            "Iteration 636, loss = 0.41543201\n",
            "Iteration 637, loss = 0.41535393\n",
            "Iteration 638, loss = 0.41529983\n",
            "Iteration 639, loss = 0.41525453\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.71667603\n",
            "Iteration 2, loss = 0.71559423\n",
            "Iteration 3, loss = 0.71437674\n",
            "Iteration 4, loss = 0.71335734\n",
            "Iteration 5, loss = 0.71221318\n",
            "Iteration 6, loss = 0.71130978\n",
            "Iteration 7, loss = 0.71025995\n",
            "Iteration 8, loss = 0.70938996\n",
            "Iteration 9, loss = 0.70836877\n",
            "Iteration 10, loss = 0.70756772\n",
            "Iteration 11, loss = 0.70669141\n",
            "Iteration 12, loss = 0.70592366\n",
            "Iteration 13, loss = 0.70514384\n",
            "Iteration 14, loss = 0.70439348\n",
            "Iteration 15, loss = 0.70370222\n",
            "Iteration 16, loss = 0.70305212\n",
            "Iteration 17, loss = 0.70242707\n",
            "Iteration 18, loss = 0.70173405\n",
            "Iteration 19, loss = 0.70114060\n",
            "Iteration 20, loss = 0.70053021\n",
            "Iteration 21, loss = 0.70007146\n",
            "Iteration 22, loss = 0.69949596\n",
            "Iteration 23, loss = 0.69896764\n",
            "Iteration 24, loss = 0.69850439\n",
            "Iteration 25, loss = 0.69801833\n",
            "Iteration 26, loss = 0.69768973\n",
            "Iteration 27, loss = 0.69724651\n",
            "Iteration 28, loss = 0.69685807\n",
            "Iteration 29, loss = 0.69657883\n",
            "Iteration 30, loss = 0.69621052\n",
            "Iteration 31, loss = 0.69592883\n",
            "Iteration 32, loss = 0.69567483\n",
            "Iteration 33, loss = 0.69537889\n",
            "Iteration 34, loss = 0.69513565\n",
            "Iteration 35, loss = 0.69487378\n",
            "Iteration 36, loss = 0.69468275\n",
            "Iteration 37, loss = 0.69443610\n",
            "Iteration 38, loss = 0.69425444\n",
            "Iteration 39, loss = 0.69406982\n",
            "Iteration 40, loss = 0.69388563\n",
            "Iteration 41, loss = 0.69375214\n",
            "Iteration 42, loss = 0.69356512\n",
            "Iteration 43, loss = 0.69345842\n",
            "Iteration 44, loss = 0.69325401\n",
            "Iteration 45, loss = 0.69316783\n",
            "Iteration 46, loss = 0.69296071\n",
            "Iteration 47, loss = 0.69286192\n",
            "Iteration 48, loss = 0.69271866\n",
            "Iteration 49, loss = 0.69260030\n",
            "Iteration 50, loss = 0.69247041\n",
            "Iteration 51, loss = 0.69236096\n",
            "Iteration 52, loss = 0.69222298\n",
            "Iteration 53, loss = 0.69210848\n",
            "Iteration 54, loss = 0.69200310\n",
            "Iteration 55, loss = 0.69187397\n",
            "Iteration 56, loss = 0.69174354\n",
            "Iteration 57, loss = 0.69160682\n",
            "Iteration 58, loss = 0.69148847\n",
            "Iteration 59, loss = 0.69136799\n",
            "Iteration 60, loss = 0.69121335\n",
            "Iteration 61, loss = 0.69106394\n",
            "Iteration 62, loss = 0.69093375\n",
            "Iteration 63, loss = 0.69078504\n",
            "Iteration 64, loss = 0.69063456\n",
            "Iteration 65, loss = 0.69047385\n",
            "Iteration 66, loss = 0.69031989\n",
            "Iteration 67, loss = 0.69015107\n",
            "Iteration 68, loss = 0.68997104\n",
            "Iteration 69, loss = 0.68980375\n",
            "Iteration 70, loss = 0.68961694\n",
            "Iteration 71, loss = 0.68943013\n",
            "Iteration 72, loss = 0.68924190\n",
            "Iteration 73, loss = 0.68903691\n",
            "Iteration 74, loss = 0.68882466\n",
            "Iteration 75, loss = 0.68862838\n",
            "Iteration 76, loss = 0.68838458\n",
            "Iteration 77, loss = 0.68815487\n",
            "Iteration 78, loss = 0.68792156\n",
            "Iteration 79, loss = 0.68765662\n",
            "Iteration 80, loss = 0.68740953\n",
            "Iteration 81, loss = 0.68715258\n",
            "Iteration 82, loss = 0.68687057\n",
            "Iteration 83, loss = 0.68659821\n",
            "Iteration 84, loss = 0.68630942\n",
            "Iteration 85, loss = 0.68602317\n",
            "Iteration 86, loss = 0.68570895\n",
            "Iteration 87, loss = 0.68539584\n",
            "Iteration 88, loss = 0.68507740\n",
            "Iteration 89, loss = 0.68474669\n",
            "Iteration 90, loss = 0.68441246\n",
            "Iteration 91, loss = 0.68406110\n",
            "Iteration 92, loss = 0.68369979\n",
            "Iteration 93, loss = 0.68334131\n",
            "Iteration 94, loss = 0.68297807\n",
            "Iteration 95, loss = 0.68261190\n",
            "Iteration 96, loss = 0.68222990\n",
            "Iteration 97, loss = 0.68181146\n",
            "Iteration 98, loss = 0.68142560\n",
            "Iteration 99, loss = 0.68100538\n",
            "Iteration 100, loss = 0.68058984\n",
            "Iteration 101, loss = 0.68015156\n",
            "Iteration 102, loss = 0.67973552\n",
            "Iteration 103, loss = 0.67928493\n",
            "Iteration 104, loss = 0.67885028\n",
            "Iteration 105, loss = 0.67839048\n",
            "Iteration 106, loss = 0.67791717\n",
            "Iteration 107, loss = 0.67745760\n",
            "Iteration 108, loss = 0.67694098\n",
            "Iteration 109, loss = 0.67644848\n",
            "Iteration 110, loss = 0.67595221\n",
            "Iteration 111, loss = 0.67545023\n",
            "Iteration 112, loss = 0.67494455\n",
            "Iteration 113, loss = 0.67441056\n",
            "Iteration 114, loss = 0.67389198\n",
            "Iteration 115, loss = 0.67336150\n",
            "Iteration 116, loss = 0.67281298\n",
            "Iteration 117, loss = 0.67225369\n",
            "Iteration 118, loss = 0.67169970\n",
            "Iteration 119, loss = 0.67112067\n",
            "Iteration 120, loss = 0.67052637\n",
            "Iteration 121, loss = 0.66997130\n",
            "Iteration 122, loss = 0.66933185\n",
            "Iteration 123, loss = 0.66874167\n",
            "Iteration 124, loss = 0.66811677\n",
            "Iteration 125, loss = 0.66751855\n",
            "Iteration 126, loss = 0.66684675\n",
            "Iteration 127, loss = 0.66620878\n",
            "Iteration 128, loss = 0.66560928\n",
            "Iteration 129, loss = 0.66492361\n",
            "Iteration 130, loss = 0.66427919\n",
            "Iteration 131, loss = 0.66362247\n",
            "Iteration 132, loss = 0.66292683\n",
            "Iteration 133, loss = 0.66224454\n",
            "Iteration 134, loss = 0.66155090\n",
            "Iteration 135, loss = 0.66089497\n",
            "Iteration 136, loss = 0.66017250\n",
            "Iteration 137, loss = 0.65944985\n",
            "Iteration 138, loss = 0.65873505\n",
            "Iteration 139, loss = 0.65803342\n",
            "Iteration 140, loss = 0.65734031\n",
            "Iteration 141, loss = 0.65657344\n",
            "Iteration 142, loss = 0.65583637\n",
            "Iteration 143, loss = 0.65512151\n",
            "Iteration 144, loss = 0.65436195\n",
            "Iteration 145, loss = 0.65361415\n",
            "Iteration 146, loss = 0.65285724\n",
            "Iteration 147, loss = 0.65208385\n",
            "Iteration 148, loss = 0.65128874\n",
            "Iteration 149, loss = 0.65051835\n",
            "Iteration 150, loss = 0.64972622\n",
            "Iteration 151, loss = 0.64896653\n",
            "Iteration 152, loss = 0.64815822\n",
            "Iteration 153, loss = 0.64738128\n",
            "Iteration 154, loss = 0.64655037\n",
            "Iteration 155, loss = 0.64578433\n",
            "Iteration 156, loss = 0.64496553\n",
            "Iteration 157, loss = 0.64413533\n",
            "Iteration 158, loss = 0.64332091\n",
            "Iteration 159, loss = 0.64252667\n",
            "Iteration 160, loss = 0.64170258\n",
            "Iteration 161, loss = 0.64087129\n",
            "Iteration 162, loss = 0.64004970\n",
            "Iteration 163, loss = 0.63919779\n",
            "Iteration 164, loss = 0.63837025\n",
            "Iteration 165, loss = 0.63749909\n",
            "Iteration 166, loss = 0.63666098\n",
            "Iteration 167, loss = 0.63580668\n",
            "Iteration 168, loss = 0.63495366\n",
            "Iteration 169, loss = 0.63410980\n",
            "Iteration 170, loss = 0.63327980\n",
            "Iteration 171, loss = 0.63241952\n",
            "Iteration 172, loss = 0.63156285\n",
            "Iteration 173, loss = 0.63070685\n",
            "Iteration 174, loss = 0.62985676\n",
            "Iteration 175, loss = 0.62899405\n",
            "Iteration 176, loss = 0.62813525\n",
            "Iteration 177, loss = 0.62729567\n",
            "Iteration 178, loss = 0.62643309\n",
            "Iteration 179, loss = 0.62554536\n",
            "Iteration 180, loss = 0.62467919\n",
            "Iteration 181, loss = 0.62386574\n",
            "Iteration 182, loss = 0.62296469\n",
            "Iteration 183, loss = 0.62208340\n",
            "Iteration 184, loss = 0.62119262\n",
            "Iteration 185, loss = 0.62035157\n",
            "Iteration 186, loss = 0.61949864\n",
            "Iteration 187, loss = 0.61858448\n",
            "Iteration 188, loss = 0.61772639\n",
            "Iteration 189, loss = 0.61685301\n",
            "Iteration 190, loss = 0.61596031\n",
            "Iteration 191, loss = 0.61511226\n",
            "Iteration 192, loss = 0.61418283\n",
            "Iteration 193, loss = 0.61332118\n",
            "Iteration 194, loss = 0.61239401\n",
            "Iteration 195, loss = 0.61156770\n",
            "Iteration 196, loss = 0.61065150\n",
            "Iteration 197, loss = 0.60975916\n",
            "Iteration 198, loss = 0.60888963\n",
            "Iteration 199, loss = 0.60804894\n",
            "Iteration 200, loss = 0.60718748\n",
            "Iteration 201, loss = 0.60630163\n",
            "Iteration 202, loss = 0.60542502\n",
            "Iteration 203, loss = 0.60456045\n",
            "Iteration 204, loss = 0.60368986\n",
            "Iteration 205, loss = 0.60281828\n",
            "Iteration 206, loss = 0.60191670\n",
            "Iteration 207, loss = 0.60104387\n",
            "Iteration 208, loss = 0.60016251\n",
            "Iteration 209, loss = 0.59931300\n",
            "Iteration 210, loss = 0.59845683\n",
            "Iteration 211, loss = 0.59758065\n",
            "Iteration 212, loss = 0.59671197\n",
            "Iteration 213, loss = 0.59585080\n",
            "Iteration 214, loss = 0.59500025\n",
            "Iteration 215, loss = 0.59411519\n",
            "Iteration 216, loss = 0.59325350\n",
            "Iteration 217, loss = 0.59236652\n",
            "Iteration 218, loss = 0.59152712\n",
            "Iteration 219, loss = 0.59064790\n",
            "Iteration 220, loss = 0.58975993\n",
            "Iteration 221, loss = 0.58891228\n",
            "Iteration 222, loss = 0.58802993\n",
            "Iteration 223, loss = 0.58714315\n",
            "Iteration 224, loss = 0.58627025\n",
            "Iteration 225, loss = 0.58541003\n",
            "Iteration 226, loss = 0.58452143\n",
            "Iteration 227, loss = 0.58363898\n",
            "Iteration 228, loss = 0.58275733\n",
            "Iteration 229, loss = 0.58188736\n",
            "Iteration 230, loss = 0.58104623\n",
            "Iteration 231, loss = 0.58014807\n",
            "Iteration 232, loss = 0.57929772\n",
            "Iteration 233, loss = 0.57844324\n",
            "Iteration 234, loss = 0.57759161\n",
            "Iteration 235, loss = 0.57670143\n",
            "Iteration 236, loss = 0.57586862\n",
            "Iteration 237, loss = 0.57500749\n",
            "Iteration 238, loss = 0.57412168\n",
            "Iteration 239, loss = 0.57330496\n",
            "Iteration 240, loss = 0.57241880\n",
            "Iteration 241, loss = 0.57156593\n",
            "Iteration 242, loss = 0.57066912\n",
            "Iteration 243, loss = 0.56982805\n",
            "Iteration 244, loss = 0.56893829\n",
            "Iteration 245, loss = 0.56813430\n",
            "Iteration 246, loss = 0.56719333\n",
            "Iteration 247, loss = 0.56634653\n",
            "Iteration 248, loss = 0.56546726\n",
            "Iteration 249, loss = 0.56460949\n",
            "Iteration 250, loss = 0.56373647\n",
            "Iteration 251, loss = 0.56285869\n",
            "Iteration 252, loss = 0.56197985\n",
            "Iteration 253, loss = 0.56112044\n",
            "Iteration 254, loss = 0.56025108\n",
            "Iteration 255, loss = 0.55940391\n",
            "Iteration 256, loss = 0.55853663\n",
            "Iteration 257, loss = 0.55763671\n",
            "Iteration 258, loss = 0.55675274\n",
            "Iteration 259, loss = 0.55590648\n",
            "Iteration 260, loss = 0.55503150\n",
            "Iteration 261, loss = 0.55413315\n",
            "Iteration 262, loss = 0.55325544\n",
            "Iteration 263, loss = 0.55240356\n",
            "Iteration 264, loss = 0.55154600\n",
            "Iteration 265, loss = 0.55067630\n",
            "Iteration 266, loss = 0.54976176\n",
            "Iteration 267, loss = 0.54891860\n",
            "Iteration 268, loss = 0.54805557\n",
            "Iteration 269, loss = 0.54718718\n",
            "Iteration 270, loss = 0.54631560\n",
            "Iteration 271, loss = 0.54545610\n",
            "Iteration 272, loss = 0.54459932\n",
            "Iteration 273, loss = 0.54376344\n",
            "Iteration 274, loss = 0.54289504\n",
            "Iteration 275, loss = 0.54208009\n",
            "Iteration 276, loss = 0.54121009\n",
            "Iteration 277, loss = 0.54034770\n",
            "Iteration 278, loss = 0.53951539\n",
            "Iteration 279, loss = 0.53865623\n",
            "Iteration 280, loss = 0.53784231\n",
            "Iteration 281, loss = 0.53700361\n",
            "Iteration 282, loss = 0.53616756\n",
            "Iteration 283, loss = 0.53535028\n",
            "Iteration 284, loss = 0.53454524\n",
            "Iteration 285, loss = 0.53368977\n",
            "Iteration 286, loss = 0.53288190\n",
            "Iteration 287, loss = 0.53207149\n",
            "Iteration 288, loss = 0.53119381\n",
            "Iteration 289, loss = 0.53039622\n",
            "Iteration 290, loss = 0.52952983\n",
            "Iteration 291, loss = 0.52875797\n",
            "Iteration 292, loss = 0.52788138\n",
            "Iteration 293, loss = 0.52709853\n",
            "Iteration 294, loss = 0.52628432\n",
            "Iteration 295, loss = 0.52545247\n",
            "Iteration 296, loss = 0.52462975\n",
            "Iteration 297, loss = 0.52382638\n",
            "Iteration 298, loss = 0.52299020\n",
            "Iteration 299, loss = 0.52220550\n",
            "Iteration 300, loss = 0.52137253\n",
            "Iteration 301, loss = 0.52057580\n",
            "Iteration 302, loss = 0.51976073\n",
            "Iteration 303, loss = 0.51897381\n",
            "Iteration 304, loss = 0.51816461\n",
            "Iteration 305, loss = 0.51736239\n",
            "Iteration 306, loss = 0.51659134\n",
            "Iteration 307, loss = 0.51577239\n",
            "Iteration 308, loss = 0.51499078\n",
            "Iteration 309, loss = 0.51420958\n",
            "Iteration 310, loss = 0.51343960\n",
            "Iteration 311, loss = 0.51259715\n",
            "Iteration 312, loss = 0.51183405\n",
            "Iteration 313, loss = 0.51101615\n",
            "Iteration 314, loss = 0.51027135\n",
            "Iteration 315, loss = 0.50950366\n",
            "Iteration 316, loss = 0.50871659\n",
            "Iteration 317, loss = 0.50794244\n",
            "Iteration 318, loss = 0.50716353\n",
            "Iteration 319, loss = 0.50640881\n",
            "Iteration 320, loss = 0.50564244\n",
            "Iteration 321, loss = 0.50490136\n",
            "Iteration 322, loss = 0.50416743\n",
            "Iteration 323, loss = 0.50341070\n",
            "Iteration 324, loss = 0.50268123\n",
            "Iteration 325, loss = 0.50194473\n",
            "Iteration 326, loss = 0.50123789\n",
            "Iteration 327, loss = 0.50050289\n",
            "Iteration 328, loss = 0.49982324\n",
            "Iteration 329, loss = 0.49914222\n",
            "Iteration 330, loss = 0.49834239\n",
            "Iteration 331, loss = 0.49768466\n",
            "Iteration 332, loss = 0.49694234\n",
            "Iteration 333, loss = 0.49622447\n",
            "Iteration 334, loss = 0.49554121\n",
            "Iteration 335, loss = 0.49478952\n",
            "Iteration 336, loss = 0.49410010\n",
            "Iteration 337, loss = 0.49342247\n",
            "Iteration 338, loss = 0.49270782\n",
            "Iteration 339, loss = 0.49200007\n",
            "Iteration 340, loss = 0.49128879\n",
            "Iteration 341, loss = 0.49064107\n",
            "Iteration 342, loss = 0.48994136\n",
            "Iteration 343, loss = 0.48924199\n",
            "Iteration 344, loss = 0.48854698\n",
            "Iteration 345, loss = 0.48786618\n",
            "Iteration 346, loss = 0.48718793\n",
            "Iteration 347, loss = 0.48652000\n",
            "Iteration 348, loss = 0.48586033\n",
            "Iteration 349, loss = 0.48515477\n",
            "Iteration 350, loss = 0.48451423\n",
            "Iteration 351, loss = 0.48386133\n",
            "Iteration 352, loss = 0.48320021\n",
            "Iteration 353, loss = 0.48251737\n",
            "Iteration 354, loss = 0.48187225\n",
            "Iteration 355, loss = 0.48120634\n",
            "Iteration 356, loss = 0.48057246\n",
            "Iteration 357, loss = 0.47991021\n",
            "Iteration 358, loss = 0.47927178\n",
            "Iteration 359, loss = 0.47861668\n",
            "Iteration 360, loss = 0.47799248\n",
            "Iteration 361, loss = 0.47737130\n",
            "Iteration 362, loss = 0.47676045\n",
            "Iteration 363, loss = 0.47612225\n",
            "Iteration 364, loss = 0.47549537\n",
            "Iteration 365, loss = 0.47486995\n",
            "Iteration 366, loss = 0.47424893\n",
            "Iteration 367, loss = 0.47367551\n",
            "Iteration 368, loss = 0.47307033\n",
            "Iteration 369, loss = 0.47249049\n",
            "Iteration 370, loss = 0.47189118\n",
            "Iteration 371, loss = 0.47131873\n",
            "Iteration 372, loss = 0.47073983\n",
            "Iteration 373, loss = 0.47013458\n",
            "Iteration 374, loss = 0.46958852\n",
            "Iteration 375, loss = 0.46899316\n",
            "Iteration 376, loss = 0.46843202\n",
            "Iteration 377, loss = 0.46784716\n",
            "Iteration 378, loss = 0.46729669\n",
            "Iteration 379, loss = 0.46674839\n",
            "Iteration 380, loss = 0.46628900\n",
            "Iteration 381, loss = 0.46564063\n",
            "Iteration 382, loss = 0.46514984\n",
            "Iteration 383, loss = 0.46459614\n",
            "Iteration 384, loss = 0.46405316\n",
            "Iteration 385, loss = 0.46357885\n",
            "Iteration 386, loss = 0.46301603\n",
            "Iteration 387, loss = 0.46251438\n",
            "Iteration 388, loss = 0.46195357\n",
            "Iteration 389, loss = 0.46148111\n",
            "Iteration 390, loss = 0.46096158\n",
            "Iteration 391, loss = 0.46045803\n",
            "Iteration 392, loss = 0.45995558\n",
            "Iteration 393, loss = 0.45941978\n",
            "Iteration 394, loss = 0.45896398\n",
            "Iteration 395, loss = 0.45840337\n",
            "Iteration 396, loss = 0.45790203\n",
            "Iteration 397, loss = 0.45741215\n",
            "Iteration 398, loss = 0.45691282\n",
            "Iteration 399, loss = 0.45642006\n",
            "Iteration 400, loss = 0.45596018\n",
            "Iteration 401, loss = 0.45548293\n",
            "Iteration 402, loss = 0.45499777\n",
            "Iteration 403, loss = 0.45457712\n",
            "Iteration 404, loss = 0.45408972\n",
            "Iteration 405, loss = 0.45361526\n",
            "Iteration 406, loss = 0.45316348\n",
            "Iteration 407, loss = 0.45271772\n",
            "Iteration 408, loss = 0.45226593\n",
            "Iteration 409, loss = 0.45181904\n",
            "Iteration 410, loss = 0.45137586\n",
            "Iteration 411, loss = 0.45090005\n",
            "Iteration 412, loss = 0.45049015\n",
            "Iteration 413, loss = 0.45004385\n",
            "Iteration 414, loss = 0.44960224\n",
            "Iteration 415, loss = 0.44917594\n",
            "Iteration 416, loss = 0.44874517\n",
            "Iteration 417, loss = 0.44834461\n",
            "Iteration 418, loss = 0.44791347\n",
            "Iteration 419, loss = 0.44751943\n",
            "Iteration 420, loss = 0.44709212\n",
            "Iteration 421, loss = 0.44671280\n",
            "Iteration 422, loss = 0.44629726\n",
            "Iteration 423, loss = 0.44594795\n",
            "Iteration 424, loss = 0.44548511\n",
            "Iteration 425, loss = 0.44508749\n",
            "Iteration 426, loss = 0.44469686\n",
            "Iteration 427, loss = 0.44436558\n",
            "Iteration 428, loss = 0.44395895\n",
            "Iteration 429, loss = 0.44362105\n",
            "Iteration 430, loss = 0.44325068\n",
            "Iteration 431, loss = 0.44287022\n",
            "Iteration 432, loss = 0.44251429\n",
            "Iteration 433, loss = 0.44215199\n",
            "Iteration 434, loss = 0.44178840\n",
            "Iteration 435, loss = 0.44143367\n",
            "Iteration 436, loss = 0.44104989\n",
            "Iteration 437, loss = 0.44070984\n",
            "Iteration 438, loss = 0.44034498\n",
            "Iteration 439, loss = 0.44003545\n",
            "Iteration 440, loss = 0.43966990\n",
            "Iteration 441, loss = 0.43934100\n",
            "Iteration 442, loss = 0.43903437\n",
            "Iteration 443, loss = 0.43869003\n",
            "Iteration 444, loss = 0.43840368\n",
            "Iteration 445, loss = 0.43803749\n",
            "Iteration 446, loss = 0.43773098\n",
            "Iteration 447, loss = 0.43743234\n",
            "Iteration 448, loss = 0.43713496\n",
            "Iteration 449, loss = 0.43679505\n",
            "Iteration 450, loss = 0.43649825\n",
            "Iteration 451, loss = 0.43617533\n",
            "Iteration 452, loss = 0.43584195\n",
            "Iteration 453, loss = 0.43550544\n",
            "Iteration 454, loss = 0.43527100\n",
            "Iteration 455, loss = 0.43491316\n",
            "Iteration 456, loss = 0.43462501\n",
            "Iteration 457, loss = 0.43432815\n",
            "Iteration 458, loss = 0.43406070\n",
            "Iteration 459, loss = 0.43376608\n",
            "Iteration 460, loss = 0.43352059\n",
            "Iteration 461, loss = 0.43321824\n",
            "Iteration 462, loss = 0.43294190\n",
            "Iteration 463, loss = 0.43266314\n",
            "Iteration 464, loss = 0.43240684\n",
            "Iteration 465, loss = 0.43212925\n",
            "Iteration 466, loss = 0.43185867\n",
            "Iteration 467, loss = 0.43159967\n",
            "Iteration 468, loss = 0.43136434\n",
            "Iteration 469, loss = 0.43106412\n",
            "Iteration 470, loss = 0.43083960\n",
            "Iteration 471, loss = 0.43054705\n",
            "Iteration 472, loss = 0.43029981\n",
            "Iteration 473, loss = 0.43007048\n",
            "Iteration 474, loss = 0.42981392\n",
            "Iteration 475, loss = 0.42955386\n",
            "Iteration 476, loss = 0.42932780\n",
            "Iteration 477, loss = 0.42908722\n",
            "Iteration 478, loss = 0.42886798\n",
            "Iteration 479, loss = 0.42860268\n",
            "Iteration 480, loss = 0.42838695\n",
            "Iteration 481, loss = 0.42813993\n",
            "Iteration 482, loss = 0.42791491\n",
            "Iteration 483, loss = 0.42768650\n",
            "Iteration 484, loss = 0.42751177\n",
            "Iteration 485, loss = 0.42725302\n",
            "Iteration 486, loss = 0.42702814\n",
            "Iteration 487, loss = 0.42683202\n",
            "Iteration 488, loss = 0.42661392\n",
            "Iteration 489, loss = 0.42638219\n",
            "Iteration 490, loss = 0.42619134\n",
            "Iteration 491, loss = 0.42600440\n",
            "Iteration 492, loss = 0.42579206\n",
            "Iteration 493, loss = 0.42558738\n",
            "Iteration 494, loss = 0.42535876\n",
            "Iteration 495, loss = 0.42516989\n",
            "Iteration 496, loss = 0.42499410\n",
            "Iteration 497, loss = 0.42476816\n",
            "Iteration 498, loss = 0.42456517\n",
            "Iteration 499, loss = 0.42436835\n",
            "Iteration 500, loss = 0.42415535\n",
            "Iteration 501, loss = 0.42399627\n",
            "Iteration 502, loss = 0.42379241\n",
            "Iteration 503, loss = 0.42368171\n",
            "Iteration 504, loss = 0.42344100\n",
            "Iteration 505, loss = 0.42327109\n",
            "Iteration 506, loss = 0.42308366\n",
            "Iteration 507, loss = 0.42291144\n",
            "Iteration 508, loss = 0.42271676\n",
            "Iteration 509, loss = 0.42253768\n",
            "Iteration 510, loss = 0.42237667\n",
            "Iteration 511, loss = 0.42218366\n",
            "Iteration 512, loss = 0.42201668\n",
            "Iteration 513, loss = 0.42183610\n",
            "Iteration 514, loss = 0.42164146\n",
            "Iteration 515, loss = 0.42149928\n",
            "Iteration 516, loss = 0.42131520\n",
            "Iteration 517, loss = 0.42119230\n",
            "Iteration 518, loss = 0.42100260\n",
            "Iteration 519, loss = 0.42083930\n",
            "Iteration 520, loss = 0.42066379\n",
            "Iteration 521, loss = 0.42049922\n",
            "Iteration 522, loss = 0.42036290\n",
            "Iteration 523, loss = 0.42017678\n",
            "Iteration 524, loss = 0.42004146\n",
            "Iteration 525, loss = 0.41987216\n",
            "Iteration 526, loss = 0.41973113\n",
            "Iteration 527, loss = 0.41958581\n",
            "Iteration 528, loss = 0.41946291\n",
            "Iteration 529, loss = 0.41929093\n",
            "Iteration 530, loss = 0.41913569\n",
            "Iteration 531, loss = 0.41900474\n",
            "Iteration 532, loss = 0.41886260\n",
            "Iteration 533, loss = 0.41870273\n",
            "Iteration 534, loss = 0.41855619\n",
            "Iteration 535, loss = 0.41840811\n",
            "Iteration 536, loss = 0.41825735\n",
            "Iteration 537, loss = 0.41811893\n",
            "Iteration 538, loss = 0.41800422\n",
            "Iteration 539, loss = 0.41785480\n",
            "Iteration 540, loss = 0.41772889\n",
            "Iteration 541, loss = 0.41756254\n",
            "Iteration 542, loss = 0.41742353\n",
            "Iteration 543, loss = 0.41729560\n",
            "Iteration 544, loss = 0.41716300\n",
            "Iteration 545, loss = 0.41703729\n",
            "Iteration 546, loss = 0.41690661\n",
            "Iteration 547, loss = 0.41675979\n",
            "Iteration 548, loss = 0.41664482\n",
            "Iteration 549, loss = 0.41648389\n",
            "Iteration 550, loss = 0.41633723\n",
            "Iteration 551, loss = 0.41620715\n",
            "Iteration 552, loss = 0.41607485\n",
            "Iteration 553, loss = 0.41592894\n",
            "Iteration 554, loss = 0.41583277\n",
            "Iteration 555, loss = 0.41568829\n",
            "Iteration 556, loss = 0.41559282\n",
            "Iteration 557, loss = 0.41544137\n",
            "Iteration 558, loss = 0.41529213\n",
            "Iteration 559, loss = 0.41516378\n",
            "Iteration 560, loss = 0.41503886\n",
            "Iteration 561, loss = 0.41493918\n",
            "Iteration 562, loss = 0.41482521\n",
            "Iteration 563, loss = 0.41472647\n",
            "Iteration 564, loss = 0.41464323\n",
            "Iteration 565, loss = 0.41460646\n",
            "Iteration 566, loss = 0.41443289\n",
            "Iteration 567, loss = 0.41424548\n",
            "Iteration 568, loss = 0.41412396\n",
            "Iteration 569, loss = 0.41402531\n",
            "Iteration 570, loss = 0.41392059\n",
            "Iteration 571, loss = 0.41384217\n",
            "Iteration 572, loss = 0.41372289\n",
            "Iteration 573, loss = 0.41361323\n",
            "Iteration 574, loss = 0.41349706\n",
            "Iteration 575, loss = 0.41339663\n",
            "Iteration 576, loss = 0.41331444\n",
            "Iteration 577, loss = 0.41317028\n",
            "Iteration 578, loss = 0.41306005\n",
            "Iteration 579, loss = 0.41297503\n",
            "Iteration 580, loss = 0.41285717\n",
            "Iteration 581, loss = 0.41274944\n",
            "Iteration 582, loss = 0.41265905\n",
            "Iteration 583, loss = 0.41258007\n",
            "Iteration 584, loss = 0.41245169\n",
            "Iteration 585, loss = 0.41236375\n",
            "Iteration 586, loss = 0.41226387\n",
            "Iteration 587, loss = 0.41215463\n",
            "Iteration 588, loss = 0.41207096\n",
            "Iteration 589, loss = 0.41201102\n",
            "Iteration 590, loss = 0.41190232\n",
            "Iteration 591, loss = 0.41180481\n",
            "Iteration 592, loss = 0.41173354\n",
            "Iteration 593, loss = 0.41160180\n",
            "Iteration 594, loss = 0.41152726\n",
            "Iteration 595, loss = 0.41142464\n",
            "Iteration 596, loss = 0.41133927\n",
            "Iteration 597, loss = 0.41126056\n",
            "Iteration 598, loss = 0.41117311\n",
            "Iteration 599, loss = 0.41107476\n",
            "Iteration 600, loss = 0.41098073\n",
            "Iteration 601, loss = 0.41092595\n",
            "Iteration 602, loss = 0.41087307\n",
            "Iteration 603, loss = 0.41075108\n",
            "Iteration 604, loss = 0.41069595\n",
            "Iteration 605, loss = 0.41058895\n",
            "Iteration 606, loss = 0.41051265\n",
            "Iteration 607, loss = 0.41042749\n",
            "Iteration 608, loss = 0.41032721\n",
            "Iteration 609, loss = 0.41025391\n",
            "Iteration 610, loss = 0.41019866\n",
            "Iteration 611, loss = 0.41007398\n",
            "Iteration 612, loss = 0.41002261\n",
            "Iteration 613, loss = 0.40994888\n",
            "Iteration 614, loss = 0.40982722\n",
            "Iteration 615, loss = 0.40978967\n",
            "Iteration 616, loss = 0.40968871\n",
            "Iteration 617, loss = 0.40961015\n",
            "Iteration 618, loss = 0.40951350\n",
            "Iteration 619, loss = 0.40945639\n",
            "Iteration 620, loss = 0.40934814\n",
            "Iteration 621, loss = 0.40931049\n",
            "Iteration 622, loss = 0.40923169\n",
            "Iteration 623, loss = 0.40915008\n",
            "Iteration 624, loss = 0.40906985\n",
            "Iteration 625, loss = 0.40901838\n",
            "Iteration 626, loss = 0.40894832\n",
            "Iteration 627, loss = 0.40887152\n",
            "Iteration 628, loss = 0.40878529\n",
            "Iteration 629, loss = 0.40871804\n",
            "Iteration 630, loss = 0.40864438\n",
            "Iteration 631, loss = 0.40863346\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.71657573\n",
            "Iteration 2, loss = 0.71565874\n",
            "Iteration 3, loss = 0.71448115\n",
            "Iteration 4, loss = 0.71347398\n",
            "Iteration 5, loss = 0.71237619\n",
            "Iteration 6, loss = 0.71143606\n",
            "Iteration 7, loss = 0.71038825\n",
            "Iteration 8, loss = 0.70954211\n",
            "Iteration 9, loss = 0.70849149\n",
            "Iteration 10, loss = 0.70773278\n",
            "Iteration 11, loss = 0.70681970\n",
            "Iteration 12, loss = 0.70604967\n",
            "Iteration 13, loss = 0.70533603\n",
            "Iteration 14, loss = 0.70449123\n",
            "Iteration 15, loss = 0.70379363\n",
            "Iteration 16, loss = 0.70314333\n",
            "Iteration 17, loss = 0.70253867\n",
            "Iteration 18, loss = 0.70188220\n",
            "Iteration 19, loss = 0.70129819\n",
            "Iteration 20, loss = 0.70072890\n",
            "Iteration 21, loss = 0.70030840\n",
            "Iteration 22, loss = 0.69974314\n",
            "Iteration 23, loss = 0.69918473\n",
            "Iteration 24, loss = 0.69876489\n",
            "Iteration 25, loss = 0.69831431\n",
            "Iteration 26, loss = 0.69796404\n",
            "Iteration 27, loss = 0.69758494\n",
            "Iteration 28, loss = 0.69710251\n",
            "Iteration 29, loss = 0.69686192\n",
            "Iteration 30, loss = 0.69649738\n",
            "Iteration 31, loss = 0.69615057\n",
            "Iteration 32, loss = 0.69590957\n",
            "Iteration 33, loss = 0.69562074\n",
            "Iteration 34, loss = 0.69535048\n",
            "Iteration 35, loss = 0.69512595\n",
            "Iteration 36, loss = 0.69490041\n",
            "Iteration 37, loss = 0.69468930\n",
            "Iteration 38, loss = 0.69446028\n",
            "Iteration 39, loss = 0.69429937\n",
            "Iteration 40, loss = 0.69411113\n",
            "Iteration 41, loss = 0.69396489\n",
            "Iteration 42, loss = 0.69380603\n",
            "Iteration 43, loss = 0.69365179\n",
            "Iteration 44, loss = 0.69349815\n",
            "Iteration 45, loss = 0.69340930\n",
            "Iteration 46, loss = 0.69318392\n",
            "Iteration 47, loss = 0.69305680\n",
            "Iteration 48, loss = 0.69291022\n",
            "Iteration 49, loss = 0.69274377\n",
            "Iteration 50, loss = 0.69259657\n",
            "Iteration 51, loss = 0.69246117\n",
            "Iteration 52, loss = 0.69225666\n",
            "Iteration 53, loss = 0.69211959\n",
            "Iteration 54, loss = 0.69197607\n",
            "Iteration 55, loss = 0.69184761\n",
            "Iteration 56, loss = 0.69168210\n",
            "Iteration 57, loss = 0.69153598\n",
            "Iteration 58, loss = 0.69141738\n",
            "Iteration 59, loss = 0.69124974\n",
            "Iteration 60, loss = 0.69110406\n",
            "Iteration 61, loss = 0.69094797\n",
            "Iteration 62, loss = 0.69083109\n",
            "Iteration 63, loss = 0.69063743\n",
            "Iteration 64, loss = 0.69048321\n",
            "Iteration 65, loss = 0.69032044\n",
            "Iteration 66, loss = 0.69013950\n",
            "Iteration 67, loss = 0.68995906\n",
            "Iteration 68, loss = 0.68977027\n",
            "Iteration 69, loss = 0.68958487\n",
            "Iteration 70, loss = 0.68940351\n",
            "Iteration 71, loss = 0.68919876\n",
            "Iteration 72, loss = 0.68899546\n",
            "Iteration 73, loss = 0.68877780\n",
            "Iteration 74, loss = 0.68856149\n",
            "Iteration 75, loss = 0.68834672\n",
            "Iteration 76, loss = 0.68810027\n",
            "Iteration 77, loss = 0.68786184\n",
            "Iteration 78, loss = 0.68760068\n",
            "Iteration 79, loss = 0.68734441\n",
            "Iteration 80, loss = 0.68707667\n",
            "Iteration 81, loss = 0.68680712\n",
            "Iteration 82, loss = 0.68651688\n",
            "Iteration 83, loss = 0.68622322\n",
            "Iteration 84, loss = 0.68591534\n",
            "Iteration 85, loss = 0.68561550\n",
            "Iteration 86, loss = 0.68528799\n",
            "Iteration 87, loss = 0.68496566\n",
            "Iteration 88, loss = 0.68463559\n",
            "Iteration 89, loss = 0.68428422\n",
            "Iteration 90, loss = 0.68394987\n",
            "Iteration 91, loss = 0.68357129\n",
            "Iteration 92, loss = 0.68320765\n",
            "Iteration 93, loss = 0.68282545\n",
            "Iteration 94, loss = 0.68244890\n",
            "Iteration 95, loss = 0.68205719\n",
            "Iteration 96, loss = 0.68167151\n",
            "Iteration 97, loss = 0.68123435\n",
            "Iteration 98, loss = 0.68083016\n",
            "Iteration 99, loss = 0.68039295\n",
            "Iteration 100, loss = 0.67996364\n",
            "Iteration 101, loss = 0.67950812\n",
            "Iteration 102, loss = 0.67905847\n",
            "Iteration 103, loss = 0.67859322\n",
            "Iteration 104, loss = 0.67812882\n",
            "Iteration 105, loss = 0.67765749\n",
            "Iteration 106, loss = 0.67716825\n",
            "Iteration 107, loss = 0.67666073\n",
            "Iteration 108, loss = 0.67616285\n",
            "Iteration 109, loss = 0.67562841\n",
            "Iteration 110, loss = 0.67512088\n",
            "Iteration 111, loss = 0.67460400\n",
            "Iteration 112, loss = 0.67405643\n",
            "Iteration 113, loss = 0.67352850\n",
            "Iteration 114, loss = 0.67295595\n",
            "Iteration 115, loss = 0.67239875\n",
            "Iteration 116, loss = 0.67183112\n",
            "Iteration 117, loss = 0.67124381\n",
            "Iteration 118, loss = 0.67066523\n",
            "Iteration 119, loss = 0.67006030\n",
            "Iteration 120, loss = 0.66945280\n",
            "Iteration 121, loss = 0.66882399\n",
            "Iteration 122, loss = 0.66822368\n",
            "Iteration 123, loss = 0.66760433\n",
            "Iteration 124, loss = 0.66694306\n",
            "Iteration 125, loss = 0.66631639\n",
            "Iteration 126, loss = 0.66562109\n",
            "Iteration 127, loss = 0.66496606\n",
            "Iteration 128, loss = 0.66432582\n",
            "Iteration 129, loss = 0.66363925\n",
            "Iteration 130, loss = 0.66297300\n",
            "Iteration 131, loss = 0.66228682\n",
            "Iteration 132, loss = 0.66157988\n",
            "Iteration 133, loss = 0.66086558\n",
            "Iteration 134, loss = 0.66013334\n",
            "Iteration 135, loss = 0.65943620\n",
            "Iteration 136, loss = 0.65868106\n",
            "Iteration 137, loss = 0.65793798\n",
            "Iteration 138, loss = 0.65718588\n",
            "Iteration 139, loss = 0.65644245\n",
            "Iteration 140, loss = 0.65572040\n",
            "Iteration 141, loss = 0.65492842\n",
            "Iteration 142, loss = 0.65417469\n",
            "Iteration 143, loss = 0.65340592\n",
            "Iteration 144, loss = 0.65261621\n",
            "Iteration 145, loss = 0.65184635\n",
            "Iteration 146, loss = 0.65106728\n",
            "Iteration 147, loss = 0.65026560\n",
            "Iteration 148, loss = 0.64943089\n",
            "Iteration 149, loss = 0.64865456\n",
            "Iteration 150, loss = 0.64780288\n",
            "Iteration 151, loss = 0.64698023\n",
            "Iteration 152, loss = 0.64616637\n",
            "Iteration 153, loss = 0.64533702\n",
            "Iteration 154, loss = 0.64446090\n",
            "Iteration 155, loss = 0.64365088\n",
            "Iteration 156, loss = 0.64278423\n",
            "Iteration 157, loss = 0.64191996\n",
            "Iteration 158, loss = 0.64105054\n",
            "Iteration 159, loss = 0.64020909\n",
            "Iteration 160, loss = 0.63931662\n",
            "Iteration 161, loss = 0.63847733\n",
            "Iteration 162, loss = 0.63758873\n",
            "Iteration 163, loss = 0.63673453\n",
            "Iteration 164, loss = 0.63586018\n",
            "Iteration 165, loss = 0.63496851\n",
            "Iteration 166, loss = 0.63408961\n",
            "Iteration 167, loss = 0.63322924\n",
            "Iteration 168, loss = 0.63233228\n",
            "Iteration 169, loss = 0.63146681\n",
            "Iteration 170, loss = 0.63059872\n",
            "Iteration 171, loss = 0.62972267\n",
            "Iteration 172, loss = 0.62883825\n",
            "Iteration 173, loss = 0.62795337\n",
            "Iteration 174, loss = 0.62705387\n",
            "Iteration 175, loss = 0.62614335\n",
            "Iteration 176, loss = 0.62527041\n",
            "Iteration 177, loss = 0.62439184\n",
            "Iteration 178, loss = 0.62346860\n",
            "Iteration 179, loss = 0.62257972\n",
            "Iteration 180, loss = 0.62167122\n",
            "Iteration 181, loss = 0.62082015\n",
            "Iteration 182, loss = 0.61990394\n",
            "Iteration 183, loss = 0.61896917\n",
            "Iteration 184, loss = 0.61805580\n",
            "Iteration 185, loss = 0.61716291\n",
            "Iteration 186, loss = 0.61627146\n",
            "Iteration 187, loss = 0.61535566\n",
            "Iteration 188, loss = 0.61444548\n",
            "Iteration 189, loss = 0.61355728\n",
            "Iteration 190, loss = 0.61263570\n",
            "Iteration 191, loss = 0.61176498\n",
            "Iteration 192, loss = 0.61082196\n",
            "Iteration 193, loss = 0.60990281\n",
            "Iteration 194, loss = 0.60898003\n",
            "Iteration 195, loss = 0.60807245\n",
            "Iteration 196, loss = 0.60715192\n",
            "Iteration 197, loss = 0.60619970\n",
            "Iteration 198, loss = 0.60529730\n",
            "Iteration 199, loss = 0.60437294\n",
            "Iteration 200, loss = 0.60347124\n",
            "Iteration 201, loss = 0.60253482\n",
            "Iteration 202, loss = 0.60163606\n",
            "Iteration 203, loss = 0.60072386\n",
            "Iteration 204, loss = 0.59979500\n",
            "Iteration 205, loss = 0.59891153\n",
            "Iteration 206, loss = 0.59798939\n",
            "Iteration 207, loss = 0.59708376\n",
            "Iteration 208, loss = 0.59617538\n",
            "Iteration 209, loss = 0.59528642\n",
            "Iteration 210, loss = 0.59441045\n",
            "Iteration 211, loss = 0.59345385\n",
            "Iteration 212, loss = 0.59257464\n",
            "Iteration 213, loss = 0.59166882\n",
            "Iteration 214, loss = 0.59076752\n",
            "Iteration 215, loss = 0.58985111\n",
            "Iteration 216, loss = 0.58895221\n",
            "Iteration 217, loss = 0.58804203\n",
            "Iteration 218, loss = 0.58715412\n",
            "Iteration 219, loss = 0.58626061\n",
            "Iteration 220, loss = 0.58533321\n",
            "Iteration 221, loss = 0.58444663\n",
            "Iteration 222, loss = 0.58354377\n",
            "Iteration 223, loss = 0.58264304\n",
            "Iteration 224, loss = 0.58176651\n",
            "Iteration 225, loss = 0.58086221\n",
            "Iteration 226, loss = 0.57994687\n",
            "Iteration 227, loss = 0.57906759\n",
            "Iteration 228, loss = 0.57818637\n",
            "Iteration 229, loss = 0.57726514\n",
            "Iteration 230, loss = 0.57642283\n",
            "Iteration 231, loss = 0.57550458\n",
            "Iteration 232, loss = 0.57461569\n",
            "Iteration 233, loss = 0.57374684\n",
            "Iteration 234, loss = 0.57286843\n",
            "Iteration 235, loss = 0.57196657\n",
            "Iteration 236, loss = 0.57111012\n",
            "Iteration 237, loss = 0.57019957\n",
            "Iteration 238, loss = 0.56929029\n",
            "Iteration 239, loss = 0.56844709\n",
            "Iteration 240, loss = 0.56754435\n",
            "Iteration 241, loss = 0.56666446\n",
            "Iteration 242, loss = 0.56577059\n",
            "Iteration 243, loss = 0.56492590\n",
            "Iteration 244, loss = 0.56401447\n",
            "Iteration 245, loss = 0.56315987\n",
            "Iteration 246, loss = 0.56223761\n",
            "Iteration 247, loss = 0.56137114\n",
            "Iteration 248, loss = 0.56050323\n",
            "Iteration 249, loss = 0.55962769\n",
            "Iteration 250, loss = 0.55874838\n",
            "Iteration 251, loss = 0.55785204\n",
            "Iteration 252, loss = 0.55697576\n",
            "Iteration 253, loss = 0.55613177\n",
            "Iteration 254, loss = 0.55524124\n",
            "Iteration 255, loss = 0.55438518\n",
            "Iteration 256, loss = 0.55350536\n",
            "Iteration 257, loss = 0.55265104\n",
            "Iteration 258, loss = 0.55175343\n",
            "Iteration 259, loss = 0.55089778\n",
            "Iteration 260, loss = 0.55005961\n",
            "Iteration 261, loss = 0.54914763\n",
            "Iteration 262, loss = 0.54829926\n",
            "Iteration 263, loss = 0.54746931\n",
            "Iteration 264, loss = 0.54661221\n",
            "Iteration 265, loss = 0.54579497\n",
            "Iteration 266, loss = 0.54486533\n",
            "Iteration 267, loss = 0.54403982\n",
            "Iteration 268, loss = 0.54318716\n",
            "Iteration 269, loss = 0.54234300\n",
            "Iteration 270, loss = 0.54148569\n",
            "Iteration 271, loss = 0.54065956\n",
            "Iteration 272, loss = 0.53979872\n",
            "Iteration 273, loss = 0.53899969\n",
            "Iteration 274, loss = 0.53814964\n",
            "Iteration 275, loss = 0.53734111\n",
            "Iteration 276, loss = 0.53651609\n",
            "Iteration 277, loss = 0.53566728\n",
            "Iteration 278, loss = 0.53486450\n",
            "Iteration 279, loss = 0.53403260\n",
            "Iteration 280, loss = 0.53322034\n",
            "Iteration 281, loss = 0.53240195\n",
            "Iteration 282, loss = 0.53158541\n",
            "Iteration 283, loss = 0.53077069\n",
            "Iteration 284, loss = 0.52997620\n",
            "Iteration 285, loss = 0.52914076\n",
            "Iteration 286, loss = 0.52835999\n",
            "Iteration 287, loss = 0.52756500\n",
            "Iteration 288, loss = 0.52668561\n",
            "Iteration 289, loss = 0.52590909\n",
            "Iteration 290, loss = 0.52508653\n",
            "Iteration 291, loss = 0.52432536\n",
            "Iteration 292, loss = 0.52349594\n",
            "Iteration 293, loss = 0.52269865\n",
            "Iteration 294, loss = 0.52194454\n",
            "Iteration 295, loss = 0.52115072\n",
            "Iteration 296, loss = 0.52037258\n",
            "Iteration 297, loss = 0.51958322\n",
            "Iteration 298, loss = 0.51878121\n",
            "Iteration 299, loss = 0.51802619\n",
            "Iteration 300, loss = 0.51720406\n",
            "Iteration 301, loss = 0.51644652\n",
            "Iteration 302, loss = 0.51565539\n",
            "Iteration 303, loss = 0.51488999\n",
            "Iteration 304, loss = 0.51413921\n",
            "Iteration 305, loss = 0.51334980\n",
            "Iteration 306, loss = 0.51261379\n",
            "Iteration 307, loss = 0.51182061\n",
            "Iteration 308, loss = 0.51105557\n",
            "Iteration 309, loss = 0.51033014\n",
            "Iteration 310, loss = 0.50957980\n",
            "Iteration 311, loss = 0.50880380\n",
            "Iteration 312, loss = 0.50807114\n",
            "Iteration 313, loss = 0.50729649\n",
            "Iteration 314, loss = 0.50659802\n",
            "Iteration 315, loss = 0.50583193\n",
            "Iteration 316, loss = 0.50509430\n",
            "Iteration 317, loss = 0.50436557\n",
            "Iteration 318, loss = 0.50364508\n",
            "Iteration 319, loss = 0.50290205\n",
            "Iteration 320, loss = 0.50220176\n",
            "Iteration 321, loss = 0.50148408\n",
            "Iteration 322, loss = 0.50075695\n",
            "Iteration 323, loss = 0.50009913\n",
            "Iteration 324, loss = 0.49935401\n",
            "Iteration 325, loss = 0.49869124\n",
            "Iteration 326, loss = 0.49798604\n",
            "Iteration 327, loss = 0.49730815\n",
            "Iteration 328, loss = 0.49663655\n",
            "Iteration 329, loss = 0.49596758\n",
            "Iteration 330, loss = 0.49526026\n",
            "Iteration 331, loss = 0.49460786\n",
            "Iteration 332, loss = 0.49392217\n",
            "Iteration 333, loss = 0.49327754\n",
            "Iteration 334, loss = 0.49258094\n",
            "Iteration 335, loss = 0.49190305\n",
            "Iteration 336, loss = 0.49121796\n",
            "Iteration 337, loss = 0.49057501\n",
            "Iteration 338, loss = 0.48988316\n",
            "Iteration 339, loss = 0.48921924\n",
            "Iteration 340, loss = 0.48854745\n",
            "Iteration 341, loss = 0.48790960\n",
            "Iteration 342, loss = 0.48729658\n",
            "Iteration 343, loss = 0.48663415\n",
            "Iteration 344, loss = 0.48599797\n",
            "Iteration 345, loss = 0.48538041\n",
            "Iteration 346, loss = 0.48474091\n",
            "Iteration 347, loss = 0.48407555\n",
            "Iteration 348, loss = 0.48349344\n",
            "Iteration 349, loss = 0.48285069\n",
            "Iteration 350, loss = 0.48224464\n",
            "Iteration 351, loss = 0.48161679\n",
            "Iteration 352, loss = 0.48099758\n",
            "Iteration 353, loss = 0.48040510\n",
            "Iteration 354, loss = 0.47980876\n",
            "Iteration 355, loss = 0.47916089\n",
            "Iteration 356, loss = 0.47857864\n",
            "Iteration 357, loss = 0.47799352\n",
            "Iteration 358, loss = 0.47736387\n",
            "Iteration 359, loss = 0.47676768\n",
            "Iteration 360, loss = 0.47617675\n",
            "Iteration 361, loss = 0.47558325\n",
            "Iteration 362, loss = 0.47501193\n",
            "Iteration 363, loss = 0.47441529\n",
            "Iteration 364, loss = 0.47385710\n",
            "Iteration 365, loss = 0.47325413\n",
            "Iteration 366, loss = 0.47270059\n",
            "Iteration 367, loss = 0.47214115\n",
            "Iteration 368, loss = 0.47162346\n",
            "Iteration 369, loss = 0.47105227\n",
            "Iteration 370, loss = 0.47051438\n",
            "Iteration 371, loss = 0.46998379\n",
            "Iteration 372, loss = 0.46946397\n",
            "Iteration 373, loss = 0.46889701\n",
            "Iteration 374, loss = 0.46836963\n",
            "Iteration 375, loss = 0.46786196\n",
            "Iteration 376, loss = 0.46733625\n",
            "Iteration 377, loss = 0.46684076\n",
            "Iteration 378, loss = 0.46631785\n",
            "Iteration 379, loss = 0.46582563\n",
            "Iteration 380, loss = 0.46537927\n",
            "Iteration 381, loss = 0.46481597\n",
            "Iteration 382, loss = 0.46435829\n",
            "Iteration 383, loss = 0.46388768\n",
            "Iteration 384, loss = 0.46336012\n",
            "Iteration 385, loss = 0.46291834\n",
            "Iteration 386, loss = 0.46242098\n",
            "Iteration 387, loss = 0.46191155\n",
            "Iteration 388, loss = 0.46144944\n",
            "Iteration 389, loss = 0.46098696\n",
            "Iteration 390, loss = 0.46047399\n",
            "Iteration 391, loss = 0.46003081\n",
            "Iteration 392, loss = 0.45958390\n",
            "Iteration 393, loss = 0.45907626\n",
            "Iteration 394, loss = 0.45863547\n",
            "Iteration 395, loss = 0.45815400\n",
            "Iteration 396, loss = 0.45773190\n",
            "Iteration 397, loss = 0.45729037\n",
            "Iteration 398, loss = 0.45686838\n",
            "Iteration 399, loss = 0.45646486\n",
            "Iteration 400, loss = 0.45602209\n",
            "Iteration 401, loss = 0.45558896\n",
            "Iteration 402, loss = 0.45516636\n",
            "Iteration 403, loss = 0.45478247\n",
            "Iteration 404, loss = 0.45434357\n",
            "Iteration 405, loss = 0.45391950\n",
            "Iteration 406, loss = 0.45351607\n",
            "Iteration 407, loss = 0.45307325\n",
            "Iteration 408, loss = 0.45264751\n",
            "Iteration 409, loss = 0.45225487\n",
            "Iteration 410, loss = 0.45184128\n",
            "Iteration 411, loss = 0.45139957\n",
            "Iteration 412, loss = 0.45105295\n",
            "Iteration 413, loss = 0.45063986\n",
            "Iteration 414, loss = 0.45023466\n",
            "Iteration 415, loss = 0.44986674\n",
            "Iteration 416, loss = 0.44943952\n",
            "Iteration 417, loss = 0.44911206\n",
            "Iteration 418, loss = 0.44873623\n",
            "Iteration 419, loss = 0.44837379\n",
            "Iteration 420, loss = 0.44799429\n",
            "Iteration 421, loss = 0.44763069\n",
            "Iteration 422, loss = 0.44727543\n",
            "Iteration 423, loss = 0.44692891\n",
            "Iteration 424, loss = 0.44655982\n",
            "Iteration 425, loss = 0.44619754\n",
            "Iteration 426, loss = 0.44583682\n",
            "Iteration 427, loss = 0.44549961\n",
            "Iteration 428, loss = 0.44513480\n",
            "Iteration 429, loss = 0.44483493\n",
            "Iteration 430, loss = 0.44447470\n",
            "Iteration 431, loss = 0.44417305\n",
            "Iteration 432, loss = 0.44387023\n",
            "Iteration 433, loss = 0.44351284\n",
            "Iteration 434, loss = 0.44322972\n",
            "Iteration 435, loss = 0.44284887\n",
            "Iteration 436, loss = 0.44251210\n",
            "Iteration 437, loss = 0.44222257\n",
            "Iteration 438, loss = 0.44190042\n",
            "Iteration 439, loss = 0.44160381\n",
            "Iteration 440, loss = 0.44128793\n",
            "Iteration 441, loss = 0.44097353\n",
            "Iteration 442, loss = 0.44067723\n",
            "Iteration 443, loss = 0.44034802\n",
            "Iteration 444, loss = 0.44007149\n",
            "Iteration 445, loss = 0.43973190\n",
            "Iteration 446, loss = 0.43943993\n",
            "Iteration 447, loss = 0.43916273\n",
            "Iteration 448, loss = 0.43886337\n",
            "Iteration 449, loss = 0.43858542\n",
            "Iteration 450, loss = 0.43827249\n",
            "Iteration 451, loss = 0.43800661\n",
            "Iteration 452, loss = 0.43770819\n",
            "Iteration 453, loss = 0.43742324\n",
            "Iteration 454, loss = 0.43712520\n",
            "Iteration 455, loss = 0.43682465\n",
            "Iteration 456, loss = 0.43656507\n",
            "Iteration 457, loss = 0.43628950\n",
            "Iteration 458, loss = 0.43605189\n",
            "Iteration 459, loss = 0.43574571\n",
            "Iteration 460, loss = 0.43550676\n",
            "Iteration 461, loss = 0.43520907\n",
            "Iteration 462, loss = 0.43496059\n",
            "Iteration 463, loss = 0.43468638\n",
            "Iteration 464, loss = 0.43443813\n",
            "Iteration 465, loss = 0.43416827\n",
            "Iteration 466, loss = 0.43390335\n",
            "Iteration 467, loss = 0.43364114\n",
            "Iteration 468, loss = 0.43337883\n",
            "Iteration 469, loss = 0.43312843\n",
            "Iteration 470, loss = 0.43287114\n",
            "Iteration 471, loss = 0.43260765\n",
            "Iteration 472, loss = 0.43232963\n",
            "Iteration 473, loss = 0.43211857\n",
            "Iteration 474, loss = 0.43185716\n",
            "Iteration 475, loss = 0.43158086\n",
            "Iteration 476, loss = 0.43134835\n",
            "Iteration 477, loss = 0.43110568\n",
            "Iteration 478, loss = 0.43084291\n",
            "Iteration 479, loss = 0.43060320\n",
            "Iteration 480, loss = 0.43035654\n",
            "Iteration 481, loss = 0.43009955\n",
            "Iteration 482, loss = 0.42984638\n",
            "Iteration 483, loss = 0.42965827\n",
            "Iteration 484, loss = 0.42942398\n",
            "Iteration 485, loss = 0.42918801\n",
            "Iteration 486, loss = 0.42895123\n",
            "Iteration 487, loss = 0.42872844\n",
            "Iteration 488, loss = 0.42848977\n",
            "Iteration 489, loss = 0.42827285\n",
            "Iteration 490, loss = 0.42805127\n",
            "Iteration 491, loss = 0.42783900\n",
            "Iteration 492, loss = 0.42759989\n",
            "Iteration 493, loss = 0.42736877\n",
            "Iteration 494, loss = 0.42712510\n",
            "Iteration 495, loss = 0.42688776\n",
            "Iteration 496, loss = 0.42670876\n",
            "Iteration 497, loss = 0.42644404\n",
            "Iteration 498, loss = 0.42621489\n",
            "Iteration 499, loss = 0.42599582\n",
            "Iteration 500, loss = 0.42578961\n",
            "Iteration 501, loss = 0.42559340\n",
            "Iteration 502, loss = 0.42533488\n",
            "Iteration 503, loss = 0.42516962\n",
            "Iteration 504, loss = 0.42491069\n",
            "Iteration 505, loss = 0.42468546\n",
            "Iteration 506, loss = 0.42448097\n",
            "Iteration 507, loss = 0.42428107\n",
            "Iteration 508, loss = 0.42403493\n",
            "Iteration 509, loss = 0.42385747\n",
            "Iteration 510, loss = 0.42364075\n",
            "Iteration 511, loss = 0.42345478\n",
            "Iteration 512, loss = 0.42320896\n",
            "Iteration 513, loss = 0.42301433\n",
            "Iteration 514, loss = 0.42281046\n",
            "Iteration 515, loss = 0.42264428\n",
            "Iteration 516, loss = 0.42239400\n",
            "Iteration 517, loss = 0.42220643\n",
            "Iteration 518, loss = 0.42199221\n",
            "Iteration 519, loss = 0.42178513\n",
            "Iteration 520, loss = 0.42158562\n",
            "Iteration 521, loss = 0.42136970\n",
            "Iteration 522, loss = 0.42116509\n",
            "Iteration 523, loss = 0.42097246\n",
            "Iteration 524, loss = 0.42077930\n",
            "Iteration 525, loss = 0.42055979\n",
            "Iteration 526, loss = 0.42038961\n",
            "Iteration 527, loss = 0.42019575\n",
            "Iteration 528, loss = 0.42001008\n",
            "Iteration 529, loss = 0.41982181\n",
            "Iteration 530, loss = 0.41960259\n",
            "Iteration 531, loss = 0.41940554\n",
            "Iteration 532, loss = 0.41924410\n",
            "Iteration 533, loss = 0.41906054\n",
            "Iteration 534, loss = 0.41886319\n",
            "Iteration 535, loss = 0.41870149\n",
            "Iteration 536, loss = 0.41852428\n",
            "Iteration 537, loss = 0.41833099\n",
            "Iteration 538, loss = 0.41816153\n",
            "Iteration 539, loss = 0.41800012\n",
            "Iteration 540, loss = 0.41783466\n",
            "Iteration 541, loss = 0.41764989\n",
            "Iteration 542, loss = 0.41749181\n",
            "Iteration 543, loss = 0.41731374\n",
            "Iteration 544, loss = 0.41712476\n",
            "Iteration 545, loss = 0.41694726\n",
            "Iteration 546, loss = 0.41678240\n",
            "Iteration 547, loss = 0.41660534\n",
            "Iteration 548, loss = 0.41644362\n",
            "Iteration 549, loss = 0.41632796\n",
            "Iteration 550, loss = 0.41610570\n",
            "Iteration 551, loss = 0.41593319\n",
            "Iteration 552, loss = 0.41581336\n",
            "Iteration 553, loss = 0.41561793\n",
            "Iteration 554, loss = 0.41546044\n",
            "Iteration 555, loss = 0.41529512\n",
            "Iteration 556, loss = 0.41513897\n",
            "Iteration 557, loss = 0.41495009\n",
            "Iteration 558, loss = 0.41479869\n",
            "Iteration 559, loss = 0.41467522\n",
            "Iteration 560, loss = 0.41452017\n",
            "Iteration 561, loss = 0.41440365\n",
            "Iteration 562, loss = 0.41424771\n",
            "Iteration 563, loss = 0.41411833\n",
            "Iteration 564, loss = 0.41399450\n",
            "Iteration 565, loss = 0.41388776\n",
            "Iteration 566, loss = 0.41373693\n",
            "Iteration 567, loss = 0.41356183\n",
            "Iteration 568, loss = 0.41340883\n",
            "Iteration 569, loss = 0.41328605\n",
            "Iteration 570, loss = 0.41316745\n",
            "Iteration 571, loss = 0.41302985\n",
            "Iteration 572, loss = 0.41289187\n",
            "Iteration 573, loss = 0.41280669\n",
            "Iteration 574, loss = 0.41267545\n",
            "Iteration 575, loss = 0.41254794\n",
            "Iteration 576, loss = 0.41240685\n",
            "Iteration 577, loss = 0.41229139\n",
            "Iteration 578, loss = 0.41216708\n",
            "Iteration 579, loss = 0.41204203\n",
            "Iteration 580, loss = 0.41187634\n",
            "Iteration 581, loss = 0.41176688\n",
            "Iteration 582, loss = 0.41166573\n",
            "Iteration 583, loss = 0.41151942\n",
            "Iteration 584, loss = 0.41141960\n",
            "Iteration 585, loss = 0.41131867\n",
            "Iteration 586, loss = 0.41117279\n",
            "Iteration 587, loss = 0.41104683\n",
            "Iteration 588, loss = 0.41094280\n",
            "Iteration 589, loss = 0.41088342\n",
            "Iteration 590, loss = 0.41075569\n",
            "Iteration 591, loss = 0.41064606\n",
            "Iteration 592, loss = 0.41054430\n",
            "Iteration 593, loss = 0.41039153\n",
            "Iteration 594, loss = 0.41027405\n",
            "Iteration 595, loss = 0.41017845\n",
            "Iteration 596, loss = 0.41006627\n",
            "Iteration 597, loss = 0.40997899\n",
            "Iteration 598, loss = 0.40986309\n",
            "Iteration 599, loss = 0.40977521\n",
            "Iteration 600, loss = 0.40966294\n",
            "Iteration 601, loss = 0.40955912\n",
            "Iteration 602, loss = 0.40947812\n",
            "Iteration 603, loss = 0.40935810\n",
            "Iteration 604, loss = 0.40925623\n",
            "Iteration 605, loss = 0.40918980\n",
            "Iteration 606, loss = 0.40908049\n",
            "Iteration 607, loss = 0.40900654\n",
            "Iteration 608, loss = 0.40889871\n",
            "Iteration 609, loss = 0.40880920\n",
            "Iteration 610, loss = 0.40871759\n",
            "Iteration 611, loss = 0.40861155\n",
            "Iteration 612, loss = 0.40853229\n",
            "Iteration 613, loss = 0.40843564\n",
            "Iteration 614, loss = 0.40827421\n",
            "Iteration 615, loss = 0.40830789\n",
            "Iteration 616, loss = 0.40814448\n",
            "Iteration 617, loss = 0.40807304\n",
            "Iteration 618, loss = 0.40794622\n",
            "Iteration 619, loss = 0.40786841\n",
            "Iteration 620, loss = 0.40777107\n",
            "Iteration 621, loss = 0.40768423\n",
            "Iteration 622, loss = 0.40761547\n",
            "Iteration 623, loss = 0.40751582\n",
            "Iteration 624, loss = 0.40741709\n",
            "Iteration 625, loss = 0.40734139\n",
            "Iteration 626, loss = 0.40726970\n",
            "Iteration 627, loss = 0.40716073\n",
            "Iteration 628, loss = 0.40706265\n",
            "Iteration 629, loss = 0.40697508\n",
            "Iteration 630, loss = 0.40692189\n",
            "Iteration 631, loss = 0.40692121\n",
            "Iteration 632, loss = 0.40678587\n",
            "Iteration 633, loss = 0.40669043\n",
            "Iteration 634, loss = 0.40661583\n",
            "Iteration 635, loss = 0.40651709\n",
            "Iteration 636, loss = 0.40643671\n",
            "Iteration 637, loss = 0.40635578\n",
            "Iteration 638, loss = 0.40632513\n",
            "Iteration 639, loss = 0.40624636\n",
            "Iteration 640, loss = 0.40615828\n",
            "Iteration 641, loss = 0.40610525\n",
            "Iteration 642, loss = 0.40603192\n",
            "Iteration 643, loss = 0.40594044\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.71664268\n",
            "Iteration 2, loss = 0.71559298\n",
            "Iteration 3, loss = 0.71444289\n",
            "Iteration 4, loss = 0.71337931\n",
            "Iteration 5, loss = 0.71225573\n",
            "Iteration 6, loss = 0.71128171\n",
            "Iteration 7, loss = 0.71024947\n",
            "Iteration 8, loss = 0.70937098\n",
            "Iteration 9, loss = 0.70839796\n",
            "Iteration 10, loss = 0.70756519\n",
            "Iteration 11, loss = 0.70664584\n",
            "Iteration 12, loss = 0.70592521\n",
            "Iteration 13, loss = 0.70519011\n",
            "Iteration 14, loss = 0.70440313\n",
            "Iteration 15, loss = 0.70367570\n",
            "Iteration 16, loss = 0.70306183\n",
            "Iteration 17, loss = 0.70242838\n",
            "Iteration 18, loss = 0.70176612\n",
            "Iteration 19, loss = 0.70118013\n",
            "Iteration 20, loss = 0.70062589\n",
            "Iteration 21, loss = 0.70013926\n",
            "Iteration 22, loss = 0.69961720\n",
            "Iteration 23, loss = 0.69909769\n",
            "Iteration 24, loss = 0.69867055\n",
            "Iteration 25, loss = 0.69825701\n",
            "Iteration 26, loss = 0.69791136\n",
            "Iteration 27, loss = 0.69754286\n",
            "Iteration 28, loss = 0.69707525\n",
            "Iteration 29, loss = 0.69682028\n",
            "Iteration 30, loss = 0.69643165\n",
            "Iteration 31, loss = 0.69610200\n",
            "Iteration 32, loss = 0.69585482\n",
            "Iteration 33, loss = 0.69551299\n",
            "Iteration 34, loss = 0.69523534\n",
            "Iteration 35, loss = 0.69499387\n",
            "Iteration 36, loss = 0.69471555\n",
            "Iteration 37, loss = 0.69455111\n",
            "Iteration 38, loss = 0.69435510\n",
            "Iteration 39, loss = 0.69414233\n",
            "Iteration 40, loss = 0.69398007\n",
            "Iteration 41, loss = 0.69381250\n",
            "Iteration 42, loss = 0.69364464\n",
            "Iteration 43, loss = 0.69351667\n",
            "Iteration 44, loss = 0.69335580\n",
            "Iteration 45, loss = 0.69325452\n",
            "Iteration 46, loss = 0.69304957\n",
            "Iteration 47, loss = 0.69290760\n",
            "Iteration 48, loss = 0.69279845\n",
            "Iteration 49, loss = 0.69261178\n",
            "Iteration 50, loss = 0.69248967\n",
            "Iteration 51, loss = 0.69235528\n",
            "Iteration 52, loss = 0.69217542\n",
            "Iteration 53, loss = 0.69204875\n",
            "Iteration 54, loss = 0.69193255\n",
            "Iteration 55, loss = 0.69182965\n",
            "Iteration 56, loss = 0.69164865\n",
            "Iteration 57, loss = 0.69151634\n",
            "Iteration 58, loss = 0.69139838\n",
            "Iteration 59, loss = 0.69126185\n",
            "Iteration 60, loss = 0.69111210\n",
            "Iteration 61, loss = 0.69096523\n",
            "Iteration 62, loss = 0.69084700\n",
            "Iteration 63, loss = 0.69066978\n",
            "Iteration 64, loss = 0.69052007\n",
            "Iteration 65, loss = 0.69036558\n",
            "Iteration 66, loss = 0.69019014\n",
            "Iteration 67, loss = 0.69002772\n",
            "Iteration 68, loss = 0.68983770\n",
            "Iteration 69, loss = 0.68966188\n",
            "Iteration 70, loss = 0.68947591\n",
            "Iteration 71, loss = 0.68928659\n",
            "Iteration 72, loss = 0.68908866\n",
            "Iteration 73, loss = 0.68890652\n",
            "Iteration 74, loss = 0.68869360\n",
            "Iteration 75, loss = 0.68848025\n",
            "Iteration 76, loss = 0.68826099\n",
            "Iteration 77, loss = 0.68802523\n",
            "Iteration 78, loss = 0.68778848\n",
            "Iteration 79, loss = 0.68753732\n",
            "Iteration 80, loss = 0.68729458\n",
            "Iteration 81, loss = 0.68703574\n",
            "Iteration 82, loss = 0.68676812\n",
            "Iteration 83, loss = 0.68649216\n",
            "Iteration 84, loss = 0.68620372\n",
            "Iteration 85, loss = 0.68591904\n",
            "Iteration 86, loss = 0.68562025\n",
            "Iteration 87, loss = 0.68531100\n",
            "Iteration 88, loss = 0.68500351\n",
            "Iteration 89, loss = 0.68467160\n",
            "Iteration 90, loss = 0.68436121\n",
            "Iteration 91, loss = 0.68399064\n",
            "Iteration 92, loss = 0.68364502\n",
            "Iteration 93, loss = 0.68329230\n",
            "Iteration 94, loss = 0.68292488\n",
            "Iteration 95, loss = 0.68255587\n",
            "Iteration 96, loss = 0.68219602\n",
            "Iteration 97, loss = 0.68176998\n",
            "Iteration 98, loss = 0.68139716\n",
            "Iteration 99, loss = 0.68097885\n",
            "Iteration 100, loss = 0.68057418\n",
            "Iteration 101, loss = 0.68015475\n",
            "Iteration 102, loss = 0.67973188\n",
            "Iteration 103, loss = 0.67929240\n",
            "Iteration 104, loss = 0.67885148\n",
            "Iteration 105, loss = 0.67839663\n",
            "Iteration 106, loss = 0.67793807\n",
            "Iteration 107, loss = 0.67744833\n",
            "Iteration 108, loss = 0.67695436\n",
            "Iteration 109, loss = 0.67646646\n",
            "Iteration 110, loss = 0.67596287\n",
            "Iteration 111, loss = 0.67547014\n",
            "Iteration 112, loss = 0.67495931\n",
            "Iteration 113, loss = 0.67442700\n",
            "Iteration 114, loss = 0.67389329\n",
            "Iteration 115, loss = 0.67337051\n",
            "Iteration 116, loss = 0.67281116\n",
            "Iteration 117, loss = 0.67225315\n",
            "Iteration 118, loss = 0.67168587\n",
            "Iteration 119, loss = 0.67110671\n",
            "Iteration 120, loss = 0.67054085\n",
            "Iteration 121, loss = 0.66992798\n",
            "Iteration 122, loss = 0.66933490\n",
            "Iteration 123, loss = 0.66872109\n",
            "Iteration 124, loss = 0.66812486\n",
            "Iteration 125, loss = 0.66750099\n",
            "Iteration 126, loss = 0.66683881\n",
            "Iteration 127, loss = 0.66619991\n",
            "Iteration 128, loss = 0.66557770\n",
            "Iteration 129, loss = 0.66491679\n",
            "Iteration 130, loss = 0.66425350\n",
            "Iteration 131, loss = 0.66360460\n",
            "Iteration 132, loss = 0.66291829\n",
            "Iteration 133, loss = 0.66222465\n",
            "Iteration 134, loss = 0.66153580\n",
            "Iteration 135, loss = 0.66085848\n",
            "Iteration 136, loss = 0.66014688\n",
            "Iteration 137, loss = 0.65942137\n",
            "Iteration 138, loss = 0.65871888\n",
            "Iteration 139, loss = 0.65799572\n",
            "Iteration 140, loss = 0.65729027\n",
            "Iteration 141, loss = 0.65655705\n",
            "Iteration 142, loss = 0.65582375\n",
            "Iteration 143, loss = 0.65507418\n",
            "Iteration 144, loss = 0.65433683\n",
            "Iteration 145, loss = 0.65359828\n",
            "Iteration 146, loss = 0.65284065\n",
            "Iteration 147, loss = 0.65205961\n",
            "Iteration 148, loss = 0.65128326\n",
            "Iteration 149, loss = 0.65053267\n",
            "Iteration 150, loss = 0.64970867\n",
            "Iteration 151, loss = 0.64892253\n",
            "Iteration 152, loss = 0.64814299\n",
            "Iteration 153, loss = 0.64734373\n",
            "Iteration 154, loss = 0.64653037\n",
            "Iteration 155, loss = 0.64571994\n",
            "Iteration 156, loss = 0.64491442\n",
            "Iteration 157, loss = 0.64407507\n",
            "Iteration 158, loss = 0.64324432\n",
            "Iteration 159, loss = 0.64243236\n",
            "Iteration 160, loss = 0.64155543\n",
            "Iteration 161, loss = 0.64073970\n",
            "Iteration 162, loss = 0.63988092\n",
            "Iteration 163, loss = 0.63902670\n",
            "Iteration 164, loss = 0.63817788\n",
            "Iteration 165, loss = 0.63732822\n",
            "Iteration 166, loss = 0.63643121\n",
            "Iteration 167, loss = 0.63561448\n",
            "Iteration 168, loss = 0.63472781\n",
            "Iteration 169, loss = 0.63391279\n",
            "Iteration 170, loss = 0.63301694\n",
            "Iteration 171, loss = 0.63217232\n",
            "Iteration 172, loss = 0.63131147\n",
            "Iteration 173, loss = 0.63045441\n",
            "Iteration 174, loss = 0.62955483\n",
            "Iteration 175, loss = 0.62866633\n",
            "Iteration 176, loss = 0.62780355\n",
            "Iteration 177, loss = 0.62694744\n",
            "Iteration 178, loss = 0.62606144\n",
            "Iteration 179, loss = 0.62517490\n",
            "Iteration 180, loss = 0.62428358\n",
            "Iteration 181, loss = 0.62341191\n",
            "Iteration 182, loss = 0.62253255\n",
            "Iteration 183, loss = 0.62162941\n",
            "Iteration 184, loss = 0.62073014\n",
            "Iteration 185, loss = 0.61983443\n",
            "Iteration 186, loss = 0.61897218\n",
            "Iteration 187, loss = 0.61806847\n",
            "Iteration 188, loss = 0.61718438\n",
            "Iteration 189, loss = 0.61630226\n",
            "Iteration 190, loss = 0.61539156\n",
            "Iteration 191, loss = 0.61455044\n",
            "Iteration 192, loss = 0.61359780\n",
            "Iteration 193, loss = 0.61273118\n",
            "Iteration 194, loss = 0.61177629\n",
            "Iteration 195, loss = 0.61089233\n",
            "Iteration 196, loss = 0.61000025\n",
            "Iteration 197, loss = 0.60906779\n",
            "Iteration 198, loss = 0.60815758\n",
            "Iteration 199, loss = 0.60728641\n",
            "Iteration 200, loss = 0.60636973\n",
            "Iteration 201, loss = 0.60545129\n",
            "Iteration 202, loss = 0.60458922\n",
            "Iteration 203, loss = 0.60369836\n",
            "Iteration 204, loss = 0.60280034\n",
            "Iteration 205, loss = 0.60193793\n",
            "Iteration 206, loss = 0.60100900\n",
            "Iteration 207, loss = 0.60013243\n",
            "Iteration 208, loss = 0.59923451\n",
            "Iteration 209, loss = 0.59837547\n",
            "Iteration 210, loss = 0.59747664\n",
            "Iteration 211, loss = 0.59655664\n",
            "Iteration 212, loss = 0.59569711\n",
            "Iteration 213, loss = 0.59481455\n",
            "Iteration 214, loss = 0.59394281\n",
            "Iteration 215, loss = 0.59305719\n",
            "Iteration 216, loss = 0.59213965\n",
            "Iteration 217, loss = 0.59123633\n",
            "Iteration 218, loss = 0.59037114\n",
            "Iteration 219, loss = 0.58946783\n",
            "Iteration 220, loss = 0.58854220\n",
            "Iteration 221, loss = 0.58766797\n",
            "Iteration 222, loss = 0.58676713\n",
            "Iteration 223, loss = 0.58586051\n",
            "Iteration 224, loss = 0.58498688\n",
            "Iteration 225, loss = 0.58410385\n",
            "Iteration 226, loss = 0.58321822\n",
            "Iteration 227, loss = 0.58231147\n",
            "Iteration 228, loss = 0.58143130\n",
            "Iteration 229, loss = 0.58053365\n",
            "Iteration 230, loss = 0.57968059\n",
            "Iteration 231, loss = 0.57876645\n",
            "Iteration 232, loss = 0.57788535\n",
            "Iteration 233, loss = 0.57700369\n",
            "Iteration 234, loss = 0.57617680\n",
            "Iteration 235, loss = 0.57528239\n",
            "Iteration 236, loss = 0.57442176\n",
            "Iteration 237, loss = 0.57352341\n",
            "Iteration 238, loss = 0.57263842\n",
            "Iteration 239, loss = 0.57181086\n",
            "Iteration 240, loss = 0.57091816\n",
            "Iteration 241, loss = 0.57006018\n",
            "Iteration 242, loss = 0.56917559\n",
            "Iteration 243, loss = 0.56833993\n",
            "Iteration 244, loss = 0.56741230\n",
            "Iteration 245, loss = 0.56663379\n",
            "Iteration 246, loss = 0.56568455\n",
            "Iteration 247, loss = 0.56482685\n",
            "Iteration 248, loss = 0.56395782\n",
            "Iteration 249, loss = 0.56312993\n",
            "Iteration 250, loss = 0.56227387\n",
            "Iteration 251, loss = 0.56137619\n",
            "Iteration 252, loss = 0.56053952\n",
            "Iteration 253, loss = 0.55967608\n",
            "Iteration 254, loss = 0.55881461\n",
            "Iteration 255, loss = 0.55798706\n",
            "Iteration 256, loss = 0.55712510\n",
            "Iteration 257, loss = 0.55626217\n",
            "Iteration 258, loss = 0.55537891\n",
            "Iteration 259, loss = 0.55450875\n",
            "Iteration 260, loss = 0.55368645\n",
            "Iteration 261, loss = 0.55279099\n",
            "Iteration 262, loss = 0.55195966\n",
            "Iteration 263, loss = 0.55109566\n",
            "Iteration 264, loss = 0.55024773\n",
            "Iteration 265, loss = 0.54942545\n",
            "Iteration 266, loss = 0.54854202\n",
            "Iteration 267, loss = 0.54772156\n",
            "Iteration 268, loss = 0.54685457\n",
            "Iteration 269, loss = 0.54607653\n",
            "Iteration 270, loss = 0.54522324\n",
            "Iteration 271, loss = 0.54439303\n",
            "Iteration 272, loss = 0.54358078\n",
            "Iteration 273, loss = 0.54276572\n",
            "Iteration 274, loss = 0.54194403\n",
            "Iteration 275, loss = 0.54112599\n",
            "Iteration 276, loss = 0.54032116\n",
            "Iteration 277, loss = 0.53949669\n",
            "Iteration 278, loss = 0.53869134\n",
            "Iteration 279, loss = 0.53789176\n",
            "Iteration 280, loss = 0.53710199\n",
            "Iteration 281, loss = 0.53630349\n",
            "Iteration 282, loss = 0.53551088\n",
            "Iteration 283, loss = 0.53470970\n",
            "Iteration 284, loss = 0.53394482\n",
            "Iteration 285, loss = 0.53313305\n",
            "Iteration 286, loss = 0.53237015\n",
            "Iteration 287, loss = 0.53156384\n",
            "Iteration 288, loss = 0.53070389\n",
            "Iteration 289, loss = 0.52995927\n",
            "Iteration 290, loss = 0.52913145\n",
            "Iteration 291, loss = 0.52837975\n",
            "Iteration 292, loss = 0.52756771\n",
            "Iteration 293, loss = 0.52677386\n",
            "Iteration 294, loss = 0.52603554\n",
            "Iteration 295, loss = 0.52529177\n",
            "Iteration 296, loss = 0.52453135\n",
            "Iteration 297, loss = 0.52378941\n",
            "Iteration 298, loss = 0.52295923\n",
            "Iteration 299, loss = 0.52224072\n",
            "Iteration 300, loss = 0.52142376\n",
            "Iteration 301, loss = 0.52067379\n",
            "Iteration 302, loss = 0.51991986\n",
            "Iteration 303, loss = 0.51913311\n",
            "Iteration 304, loss = 0.51840972\n",
            "Iteration 305, loss = 0.51765907\n",
            "Iteration 306, loss = 0.51693316\n",
            "Iteration 307, loss = 0.51618028\n",
            "Iteration 308, loss = 0.51543164\n",
            "Iteration 309, loss = 0.51471161\n",
            "Iteration 310, loss = 0.51395949\n",
            "Iteration 311, loss = 0.51321627\n",
            "Iteration 312, loss = 0.51250217\n",
            "Iteration 313, loss = 0.51176608\n",
            "Iteration 314, loss = 0.51108138\n",
            "Iteration 315, loss = 0.51035097\n",
            "Iteration 316, loss = 0.50961932\n",
            "Iteration 317, loss = 0.50890840\n",
            "Iteration 318, loss = 0.50820916\n",
            "Iteration 319, loss = 0.50746643\n",
            "Iteration 320, loss = 0.50680012\n",
            "Iteration 321, loss = 0.50607879\n",
            "Iteration 322, loss = 0.50536683\n",
            "Iteration 323, loss = 0.50470905\n",
            "Iteration 324, loss = 0.50397965\n",
            "Iteration 325, loss = 0.50333668\n",
            "Iteration 326, loss = 0.50263530\n",
            "Iteration 327, loss = 0.50198934\n",
            "Iteration 328, loss = 0.50127951\n",
            "Iteration 329, loss = 0.50060640\n",
            "Iteration 330, loss = 0.49990596\n",
            "Iteration 331, loss = 0.49927314\n",
            "Iteration 332, loss = 0.49856473\n",
            "Iteration 333, loss = 0.49791442\n",
            "Iteration 334, loss = 0.49723200\n",
            "Iteration 335, loss = 0.49659431\n",
            "Iteration 336, loss = 0.49591391\n",
            "Iteration 337, loss = 0.49527163\n",
            "Iteration 338, loss = 0.49463815\n",
            "Iteration 339, loss = 0.49397721\n",
            "Iteration 340, loss = 0.49333324\n",
            "Iteration 341, loss = 0.49271633\n",
            "Iteration 342, loss = 0.49212569\n",
            "Iteration 343, loss = 0.49146711\n",
            "Iteration 344, loss = 0.49084026\n",
            "Iteration 345, loss = 0.49022211\n",
            "Iteration 346, loss = 0.48958634\n",
            "Iteration 347, loss = 0.48894561\n",
            "Iteration 348, loss = 0.48835912\n",
            "Iteration 349, loss = 0.48772414\n",
            "Iteration 350, loss = 0.48712196\n",
            "Iteration 351, loss = 0.48651767\n",
            "Iteration 352, loss = 0.48589496\n",
            "Iteration 353, loss = 0.48530371\n",
            "Iteration 354, loss = 0.48470805\n",
            "Iteration 355, loss = 0.48409671\n",
            "Iteration 356, loss = 0.48351152\n",
            "Iteration 357, loss = 0.48293750\n",
            "Iteration 358, loss = 0.48233338\n",
            "Iteration 359, loss = 0.48173840\n",
            "Iteration 360, loss = 0.48116090\n",
            "Iteration 361, loss = 0.48059271\n",
            "Iteration 362, loss = 0.48003386\n",
            "Iteration 363, loss = 0.47944753\n",
            "Iteration 364, loss = 0.47891202\n",
            "Iteration 365, loss = 0.47829992\n",
            "Iteration 366, loss = 0.47776106\n",
            "Iteration 367, loss = 0.47722244\n",
            "Iteration 368, loss = 0.47668113\n",
            "Iteration 369, loss = 0.47612833\n",
            "Iteration 370, loss = 0.47559067\n",
            "Iteration 371, loss = 0.47505225\n",
            "Iteration 372, loss = 0.47453301\n",
            "Iteration 373, loss = 0.47397126\n",
            "Iteration 374, loss = 0.47344651\n",
            "Iteration 375, loss = 0.47293531\n",
            "Iteration 376, loss = 0.47240017\n",
            "Iteration 377, loss = 0.47191337\n",
            "Iteration 378, loss = 0.47141315\n",
            "Iteration 379, loss = 0.47091706\n",
            "Iteration 380, loss = 0.47045241\n",
            "Iteration 381, loss = 0.46990156\n",
            "Iteration 382, loss = 0.46943476\n",
            "Iteration 383, loss = 0.46893737\n",
            "Iteration 384, loss = 0.46843580\n",
            "Iteration 385, loss = 0.46800439\n",
            "Iteration 386, loss = 0.46749148\n",
            "Iteration 387, loss = 0.46700122\n",
            "Iteration 388, loss = 0.46652215\n",
            "Iteration 389, loss = 0.46603129\n",
            "Iteration 390, loss = 0.46555472\n",
            "Iteration 391, loss = 0.46509456\n",
            "Iteration 392, loss = 0.46463147\n",
            "Iteration 393, loss = 0.46414303\n",
            "Iteration 394, loss = 0.46370220\n",
            "Iteration 395, loss = 0.46318486\n",
            "Iteration 396, loss = 0.46273935\n",
            "Iteration 397, loss = 0.46229937\n",
            "Iteration 398, loss = 0.46184563\n",
            "Iteration 399, loss = 0.46141374\n",
            "Iteration 400, loss = 0.46097701\n",
            "Iteration 401, loss = 0.46053844\n",
            "Iteration 402, loss = 0.46009522\n",
            "Iteration 403, loss = 0.45967929\n",
            "Iteration 404, loss = 0.45924684\n",
            "Iteration 405, loss = 0.45881756\n",
            "Iteration 406, loss = 0.45836335\n",
            "Iteration 407, loss = 0.45794526\n",
            "Iteration 408, loss = 0.45751291\n",
            "Iteration 409, loss = 0.45707484\n",
            "Iteration 410, loss = 0.45665993\n",
            "Iteration 411, loss = 0.45621172\n",
            "Iteration 412, loss = 0.45578335\n",
            "Iteration 413, loss = 0.45540987\n",
            "Iteration 414, loss = 0.45496978\n",
            "Iteration 415, loss = 0.45456977\n",
            "Iteration 416, loss = 0.45414836\n",
            "Iteration 417, loss = 0.45376474\n",
            "Iteration 418, loss = 0.45335136\n",
            "Iteration 419, loss = 0.45295938\n",
            "Iteration 420, loss = 0.45255137\n",
            "Iteration 421, loss = 0.45218395\n",
            "Iteration 422, loss = 0.45181134\n",
            "Iteration 423, loss = 0.45140244\n",
            "Iteration 424, loss = 0.45101135\n",
            "Iteration 425, loss = 0.45061665\n",
            "Iteration 426, loss = 0.45024753\n",
            "Iteration 427, loss = 0.44984449\n",
            "Iteration 428, loss = 0.44946064\n",
            "Iteration 429, loss = 0.44910271\n",
            "Iteration 430, loss = 0.44866966\n",
            "Iteration 431, loss = 0.44832844\n",
            "Iteration 432, loss = 0.44805452\n",
            "Iteration 433, loss = 0.44761816\n",
            "Iteration 434, loss = 0.44726279\n",
            "Iteration 435, loss = 0.44689721\n",
            "Iteration 436, loss = 0.44650959\n",
            "Iteration 437, loss = 0.44617627\n",
            "Iteration 438, loss = 0.44577931\n",
            "Iteration 439, loss = 0.44544237\n",
            "Iteration 440, loss = 0.44507934\n",
            "Iteration 441, loss = 0.44469266\n",
            "Iteration 442, loss = 0.44438284\n",
            "Iteration 443, loss = 0.44398971\n",
            "Iteration 444, loss = 0.44364989\n",
            "Iteration 445, loss = 0.44328165\n",
            "Iteration 446, loss = 0.44292670\n",
            "Iteration 447, loss = 0.44260721\n",
            "Iteration 448, loss = 0.44225233\n",
            "Iteration 449, loss = 0.44192809\n",
            "Iteration 450, loss = 0.44157709\n",
            "Iteration 451, loss = 0.44125605\n",
            "Iteration 452, loss = 0.44091151\n",
            "Iteration 453, loss = 0.44058161\n",
            "Iteration 454, loss = 0.44026185\n",
            "Iteration 455, loss = 0.43991278\n",
            "Iteration 456, loss = 0.43961282\n",
            "Iteration 457, loss = 0.43927543\n",
            "Iteration 458, loss = 0.43899239\n",
            "Iteration 459, loss = 0.43865054\n",
            "Iteration 460, loss = 0.43834857\n",
            "Iteration 461, loss = 0.43800019\n",
            "Iteration 462, loss = 0.43771701\n",
            "Iteration 463, loss = 0.43739674\n",
            "Iteration 464, loss = 0.43712233\n",
            "Iteration 465, loss = 0.43679713\n",
            "Iteration 466, loss = 0.43650598\n",
            "Iteration 467, loss = 0.43621720\n",
            "Iteration 468, loss = 0.43589617\n",
            "Iteration 469, loss = 0.43562203\n",
            "Iteration 470, loss = 0.43532363\n",
            "Iteration 471, loss = 0.43500586\n",
            "Iteration 472, loss = 0.43476809\n",
            "Iteration 473, loss = 0.43449804\n",
            "Iteration 474, loss = 0.43419391\n",
            "Iteration 475, loss = 0.43389784\n",
            "Iteration 476, loss = 0.43361503\n",
            "Iteration 477, loss = 0.43337257\n",
            "Iteration 478, loss = 0.43307996\n",
            "Iteration 479, loss = 0.43281828\n",
            "Iteration 480, loss = 0.43253066\n",
            "Iteration 481, loss = 0.43224713\n",
            "Iteration 482, loss = 0.43197910\n",
            "Iteration 483, loss = 0.43175652\n",
            "Iteration 484, loss = 0.43149052\n",
            "Iteration 485, loss = 0.43122417\n",
            "Iteration 486, loss = 0.43097234\n",
            "Iteration 487, loss = 0.43072187\n",
            "Iteration 488, loss = 0.43048642\n",
            "Iteration 489, loss = 0.43023985\n",
            "Iteration 490, loss = 0.43000016\n",
            "Iteration 491, loss = 0.42974511\n",
            "Iteration 492, loss = 0.42950932\n",
            "Iteration 493, loss = 0.42927653\n",
            "Iteration 494, loss = 0.42905080\n",
            "Iteration 495, loss = 0.42878358\n",
            "Iteration 496, loss = 0.42860431\n",
            "Iteration 497, loss = 0.42833581\n",
            "Iteration 498, loss = 0.42815388\n",
            "Iteration 499, loss = 0.42790877\n",
            "Iteration 500, loss = 0.42768449\n",
            "Iteration 501, loss = 0.42747878\n",
            "Iteration 502, loss = 0.42724679\n",
            "Iteration 503, loss = 0.42708564\n",
            "Iteration 504, loss = 0.42681994\n",
            "Iteration 505, loss = 0.42661918\n",
            "Iteration 506, loss = 0.42640524\n",
            "Iteration 507, loss = 0.42619434\n",
            "Iteration 508, loss = 0.42599111\n",
            "Iteration 509, loss = 0.42579365\n",
            "Iteration 510, loss = 0.42559497\n",
            "Iteration 511, loss = 0.42547201\n",
            "Iteration 512, loss = 0.42524735\n",
            "Iteration 513, loss = 0.42502746\n",
            "Iteration 514, loss = 0.42480268\n",
            "Iteration 515, loss = 0.42471015\n",
            "Iteration 516, loss = 0.42438708\n",
            "Iteration 517, loss = 0.42421229\n",
            "Iteration 518, loss = 0.42401302\n",
            "Iteration 519, loss = 0.42382292\n",
            "Iteration 520, loss = 0.42361410\n",
            "Iteration 521, loss = 0.42343774\n",
            "Iteration 522, loss = 0.42323565\n",
            "Iteration 523, loss = 0.42307595\n",
            "Iteration 524, loss = 0.42286665\n",
            "Iteration 525, loss = 0.42269534\n",
            "Iteration 526, loss = 0.42254980\n",
            "Iteration 527, loss = 0.42233968\n",
            "Iteration 528, loss = 0.42219485\n",
            "Iteration 529, loss = 0.42201495\n",
            "Iteration 530, loss = 0.42184610\n",
            "Iteration 531, loss = 0.42165799\n",
            "Iteration 532, loss = 0.42149925\n",
            "Iteration 533, loss = 0.42143622\n",
            "Iteration 534, loss = 0.42119797\n",
            "Iteration 535, loss = 0.42104474\n",
            "Iteration 536, loss = 0.42088357\n",
            "Iteration 537, loss = 0.42070964\n",
            "Iteration 538, loss = 0.42058108\n",
            "Iteration 539, loss = 0.42043401\n",
            "Iteration 540, loss = 0.42028139\n",
            "Iteration 541, loss = 0.42010939\n",
            "Iteration 542, loss = 0.41996158\n",
            "Iteration 543, loss = 0.41981768\n",
            "Iteration 544, loss = 0.41967754\n",
            "Iteration 545, loss = 0.41950353\n",
            "Iteration 546, loss = 0.41936365\n",
            "Iteration 547, loss = 0.41922238\n",
            "Iteration 548, loss = 0.41905034\n",
            "Iteration 549, loss = 0.41890961\n",
            "Iteration 550, loss = 0.41877528\n",
            "Iteration 551, loss = 0.41862114\n",
            "Iteration 552, loss = 0.41850753\n",
            "Iteration 553, loss = 0.41833811\n",
            "Iteration 554, loss = 0.41819678\n",
            "Iteration 555, loss = 0.41804833\n",
            "Iteration 556, loss = 0.41792335\n",
            "Iteration 557, loss = 0.41774995\n",
            "Iteration 558, loss = 0.41760067\n",
            "Iteration 559, loss = 0.41749084\n",
            "Iteration 560, loss = 0.41733588\n",
            "Iteration 561, loss = 0.41720151\n",
            "Iteration 562, loss = 0.41707791\n",
            "Iteration 563, loss = 0.41695345\n",
            "Iteration 564, loss = 0.41683683\n",
            "Iteration 565, loss = 0.41673865\n",
            "Iteration 566, loss = 0.41659004\n",
            "Iteration 567, loss = 0.41648106\n",
            "Iteration 568, loss = 0.41631069\n",
            "Iteration 569, loss = 0.41620171\n",
            "Iteration 570, loss = 0.41607062\n",
            "Iteration 571, loss = 0.41595540\n",
            "Iteration 572, loss = 0.41582655\n",
            "Iteration 573, loss = 0.41571928\n",
            "Iteration 574, loss = 0.41561548\n",
            "Iteration 575, loss = 0.41548583\n",
            "Iteration 576, loss = 0.41537788\n",
            "Iteration 577, loss = 0.41524806\n",
            "Iteration 578, loss = 0.41512492\n",
            "Iteration 579, loss = 0.41505832\n",
            "Iteration 580, loss = 0.41489001\n",
            "Iteration 581, loss = 0.41477837\n",
            "Iteration 582, loss = 0.41466041\n",
            "Iteration 583, loss = 0.41455979\n",
            "Iteration 584, loss = 0.41445568\n",
            "Iteration 585, loss = 0.41435599\n",
            "Iteration 586, loss = 0.41424261\n",
            "Iteration 587, loss = 0.41411644\n",
            "Iteration 588, loss = 0.41401356\n",
            "Iteration 589, loss = 0.41391388\n",
            "Iteration 590, loss = 0.41379639\n",
            "Iteration 591, loss = 0.41369627\n",
            "Iteration 592, loss = 0.41361978\n",
            "Iteration 593, loss = 0.41348978\n",
            "Iteration 594, loss = 0.41337731\n",
            "Iteration 595, loss = 0.41328208\n",
            "Iteration 596, loss = 0.41319175\n",
            "Iteration 597, loss = 0.41307592\n",
            "Iteration 598, loss = 0.41296100\n",
            "Iteration 599, loss = 0.41292608\n",
            "Iteration 600, loss = 0.41276807\n",
            "Iteration 601, loss = 0.41268326\n",
            "Iteration 602, loss = 0.41260652\n",
            "Iteration 603, loss = 0.41249898\n",
            "Iteration 604, loss = 0.41239147\n",
            "Iteration 605, loss = 0.41235130\n",
            "Iteration 606, loss = 0.41223988\n",
            "Iteration 607, loss = 0.41215838\n",
            "Iteration 608, loss = 0.41206588\n",
            "Iteration 609, loss = 0.41195761\n",
            "Iteration 610, loss = 0.41187695\n",
            "Iteration 611, loss = 0.41179821\n",
            "Iteration 612, loss = 0.41169823\n",
            "Iteration 613, loss = 0.41160568\n",
            "Iteration 614, loss = 0.41147464\n",
            "Iteration 615, loss = 0.41148732\n",
            "Iteration 616, loss = 0.41131241\n",
            "Iteration 617, loss = 0.41124152\n",
            "Iteration 618, loss = 0.41111616\n",
            "Iteration 619, loss = 0.41105169\n",
            "Iteration 620, loss = 0.41097267\n",
            "Iteration 621, loss = 0.41089990\n",
            "Iteration 622, loss = 0.41082419\n",
            "Iteration 623, loss = 0.41074081\n",
            "Iteration 624, loss = 0.41062218\n",
            "Iteration 625, loss = 0.41056615\n",
            "Iteration 626, loss = 0.41051910\n",
            "Iteration 627, loss = 0.41037141\n",
            "Iteration 628, loss = 0.41030850\n",
            "Iteration 629, loss = 0.41019628\n",
            "Iteration 630, loss = 0.41013051\n",
            "Iteration 631, loss = 0.41006607\n",
            "Iteration 632, loss = 0.40998638\n",
            "Iteration 633, loss = 0.40989593\n",
            "Iteration 634, loss = 0.40983259\n",
            "Iteration 635, loss = 0.40973041\n",
            "Iteration 636, loss = 0.40966136\n",
            "Iteration 637, loss = 0.40959786\n",
            "Iteration 638, loss = 0.40951100\n",
            "Iteration 639, loss = 0.40944634\n",
            "Iteration 640, loss = 0.40935264\n",
            "Iteration 641, loss = 0.40929560\n",
            "Iteration 642, loss = 0.40923995\n",
            "Iteration 643, loss = 0.40914034\n",
            "Iteration 644, loss = 0.40909452\n",
            "Iteration 645, loss = 0.40900399\n",
            "Iteration 646, loss = 0.40891770\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.71663960\n",
            "Iteration 2, loss = 0.71558295\n",
            "Iteration 3, loss = 0.71439681\n",
            "Iteration 4, loss = 0.71340010\n",
            "Iteration 5, loss = 0.71224782\n",
            "Iteration 6, loss = 0.71129121\n",
            "Iteration 7, loss = 0.71025197\n",
            "Iteration 8, loss = 0.70942711\n",
            "Iteration 9, loss = 0.70843241\n",
            "Iteration 10, loss = 0.70762628\n",
            "Iteration 11, loss = 0.70673531\n",
            "Iteration 12, loss = 0.70598992\n",
            "Iteration 13, loss = 0.70521057\n",
            "Iteration 14, loss = 0.70447081\n",
            "Iteration 15, loss = 0.70373762\n",
            "Iteration 16, loss = 0.70312063\n",
            "Iteration 17, loss = 0.70247136\n",
            "Iteration 18, loss = 0.70180330\n",
            "Iteration 19, loss = 0.70123416\n",
            "Iteration 20, loss = 0.70064211\n",
            "Iteration 21, loss = 0.70018730\n",
            "Iteration 22, loss = 0.69970382\n",
            "Iteration 23, loss = 0.69917576\n",
            "Iteration 24, loss = 0.69875620\n",
            "Iteration 25, loss = 0.69837386\n",
            "Iteration 26, loss = 0.69800183\n",
            "Iteration 27, loss = 0.69765923\n",
            "Iteration 28, loss = 0.69717628\n",
            "Iteration 29, loss = 0.69696248\n",
            "Iteration 30, loss = 0.69654140\n",
            "Iteration 31, loss = 0.69620516\n",
            "Iteration 32, loss = 0.69599326\n",
            "Iteration 33, loss = 0.69565572\n",
            "Iteration 34, loss = 0.69537288\n",
            "Iteration 35, loss = 0.69511665\n",
            "Iteration 36, loss = 0.69484623\n",
            "Iteration 37, loss = 0.69467666\n",
            "Iteration 38, loss = 0.69447184\n",
            "Iteration 39, loss = 0.69425576\n",
            "Iteration 40, loss = 0.69405760\n",
            "Iteration 41, loss = 0.69390891\n",
            "Iteration 42, loss = 0.69372604\n",
            "Iteration 43, loss = 0.69357573\n",
            "Iteration 44, loss = 0.69343025\n",
            "Iteration 45, loss = 0.69329795\n",
            "Iteration 46, loss = 0.69307816\n",
            "Iteration 47, loss = 0.69294167\n",
            "Iteration 48, loss = 0.69283517\n",
            "Iteration 49, loss = 0.69264860\n",
            "Iteration 50, loss = 0.69252494\n",
            "Iteration 51, loss = 0.69237296\n",
            "Iteration 52, loss = 0.69220712\n",
            "Iteration 53, loss = 0.69206897\n",
            "Iteration 54, loss = 0.69195903\n",
            "Iteration 55, loss = 0.69183286\n",
            "Iteration 56, loss = 0.69166555\n",
            "Iteration 57, loss = 0.69152538\n",
            "Iteration 58, loss = 0.69141829\n",
            "Iteration 59, loss = 0.69127121\n",
            "Iteration 60, loss = 0.69111474\n",
            "Iteration 61, loss = 0.69095923\n",
            "Iteration 62, loss = 0.69082719\n",
            "Iteration 63, loss = 0.69066072\n",
            "Iteration 64, loss = 0.69050289\n",
            "Iteration 65, loss = 0.69034280\n",
            "Iteration 66, loss = 0.69016960\n",
            "Iteration 67, loss = 0.68999983\n",
            "Iteration 68, loss = 0.68980973\n",
            "Iteration 69, loss = 0.68963788\n",
            "Iteration 70, loss = 0.68944100\n",
            "Iteration 71, loss = 0.68925058\n",
            "Iteration 72, loss = 0.68904542\n",
            "Iteration 73, loss = 0.68884716\n",
            "Iteration 74, loss = 0.68863406\n",
            "Iteration 75, loss = 0.68841230\n",
            "Iteration 76, loss = 0.68819261\n",
            "Iteration 77, loss = 0.68794130\n",
            "Iteration 78, loss = 0.68771153\n",
            "Iteration 79, loss = 0.68743907\n",
            "Iteration 80, loss = 0.68718842\n",
            "Iteration 81, loss = 0.68692041\n",
            "Iteration 82, loss = 0.68664753\n",
            "Iteration 83, loss = 0.68636365\n",
            "Iteration 84, loss = 0.68606956\n",
            "Iteration 85, loss = 0.68576771\n",
            "Iteration 86, loss = 0.68545731\n",
            "Iteration 87, loss = 0.68513914\n",
            "Iteration 88, loss = 0.68481603\n",
            "Iteration 89, loss = 0.68447347\n",
            "Iteration 90, loss = 0.68415091\n",
            "Iteration 91, loss = 0.68377652\n",
            "Iteration 92, loss = 0.68341590\n",
            "Iteration 93, loss = 0.68305066\n",
            "Iteration 94, loss = 0.68267716\n",
            "Iteration 95, loss = 0.68230199\n",
            "Iteration 96, loss = 0.68192764\n",
            "Iteration 97, loss = 0.68150158\n",
            "Iteration 98, loss = 0.68111385\n",
            "Iteration 99, loss = 0.68069576\n",
            "Iteration 100, loss = 0.68026924\n",
            "Iteration 101, loss = 0.67985600\n",
            "Iteration 102, loss = 0.67940576\n",
            "Iteration 103, loss = 0.67894969\n",
            "Iteration 104, loss = 0.67850792\n",
            "Iteration 105, loss = 0.67803722\n",
            "Iteration 106, loss = 0.67757480\n",
            "Iteration 107, loss = 0.67708717\n",
            "Iteration 108, loss = 0.67658124\n",
            "Iteration 109, loss = 0.67608582\n",
            "Iteration 110, loss = 0.67556359\n",
            "Iteration 111, loss = 0.67505295\n",
            "Iteration 112, loss = 0.67453027\n",
            "Iteration 113, loss = 0.67398335\n",
            "Iteration 114, loss = 0.67343902\n",
            "Iteration 115, loss = 0.67289546\n",
            "Iteration 116, loss = 0.67232257\n",
            "Iteration 117, loss = 0.67175585\n",
            "Iteration 118, loss = 0.67118954\n",
            "Iteration 119, loss = 0.67059919\n",
            "Iteration 120, loss = 0.67000532\n",
            "Iteration 121, loss = 0.66939984\n",
            "Iteration 122, loss = 0.66877543\n",
            "Iteration 123, loss = 0.66815755\n",
            "Iteration 124, loss = 0.66754056\n",
            "Iteration 125, loss = 0.66692504\n",
            "Iteration 126, loss = 0.66624383\n",
            "Iteration 127, loss = 0.66559648\n",
            "Iteration 128, loss = 0.66495725\n",
            "Iteration 129, loss = 0.66429165\n",
            "Iteration 130, loss = 0.66362190\n",
            "Iteration 131, loss = 0.66295124\n",
            "Iteration 132, loss = 0.66226067\n",
            "Iteration 133, loss = 0.66158123\n",
            "Iteration 134, loss = 0.66090084\n",
            "Iteration 135, loss = 0.66021373\n",
            "Iteration 136, loss = 0.65949440\n",
            "Iteration 137, loss = 0.65877389\n",
            "Iteration 138, loss = 0.65804564\n",
            "Iteration 139, loss = 0.65734943\n",
            "Iteration 140, loss = 0.65663164\n",
            "Iteration 141, loss = 0.65590165\n",
            "Iteration 142, loss = 0.65515807\n",
            "Iteration 143, loss = 0.65441679\n",
            "Iteration 144, loss = 0.65366920\n",
            "Iteration 145, loss = 0.65292584\n",
            "Iteration 146, loss = 0.65216180\n",
            "Iteration 147, loss = 0.65137923\n",
            "Iteration 148, loss = 0.65060084\n",
            "Iteration 149, loss = 0.64983322\n",
            "Iteration 150, loss = 0.64905350\n",
            "Iteration 151, loss = 0.64826252\n",
            "Iteration 152, loss = 0.64747468\n",
            "Iteration 153, loss = 0.64668785\n",
            "Iteration 154, loss = 0.64586822\n",
            "Iteration 155, loss = 0.64507280\n",
            "Iteration 156, loss = 0.64427013\n",
            "Iteration 157, loss = 0.64341739\n",
            "Iteration 158, loss = 0.64259447\n",
            "Iteration 159, loss = 0.64179674\n",
            "Iteration 160, loss = 0.64093411\n",
            "Iteration 161, loss = 0.64011276\n",
            "Iteration 162, loss = 0.63929109\n",
            "Iteration 163, loss = 0.63844490\n",
            "Iteration 164, loss = 0.63759717\n",
            "Iteration 165, loss = 0.63672869\n",
            "Iteration 166, loss = 0.63585565\n",
            "Iteration 167, loss = 0.63505835\n",
            "Iteration 168, loss = 0.63415170\n",
            "Iteration 169, loss = 0.63330042\n",
            "Iteration 170, loss = 0.63241511\n",
            "Iteration 171, loss = 0.63159251\n",
            "Iteration 172, loss = 0.63072124\n",
            "Iteration 173, loss = 0.62984361\n",
            "Iteration 174, loss = 0.62897181\n",
            "Iteration 175, loss = 0.62810127\n",
            "Iteration 176, loss = 0.62722794\n",
            "Iteration 177, loss = 0.62635997\n",
            "Iteration 178, loss = 0.62552222\n",
            "Iteration 179, loss = 0.62463431\n",
            "Iteration 180, loss = 0.62376134\n",
            "Iteration 181, loss = 0.62288785\n",
            "Iteration 182, loss = 0.62202670\n",
            "Iteration 183, loss = 0.62112073\n",
            "Iteration 184, loss = 0.62022802\n",
            "Iteration 185, loss = 0.61934482\n",
            "Iteration 186, loss = 0.61846993\n",
            "Iteration 187, loss = 0.61757819\n",
            "Iteration 188, loss = 0.61669764\n",
            "Iteration 189, loss = 0.61580427\n",
            "Iteration 190, loss = 0.61493044\n",
            "Iteration 191, loss = 0.61408368\n",
            "Iteration 192, loss = 0.61316349\n",
            "Iteration 193, loss = 0.61230671\n",
            "Iteration 194, loss = 0.61139337\n",
            "Iteration 195, loss = 0.61051801\n",
            "Iteration 196, loss = 0.60966075\n",
            "Iteration 197, loss = 0.60873950\n",
            "Iteration 198, loss = 0.60785334\n",
            "Iteration 199, loss = 0.60697560\n",
            "Iteration 200, loss = 0.60609961\n",
            "Iteration 201, loss = 0.60517637\n",
            "Iteration 202, loss = 0.60432746\n",
            "Iteration 203, loss = 0.60346142\n",
            "Iteration 204, loss = 0.60258042\n",
            "Iteration 205, loss = 0.60173566\n",
            "Iteration 206, loss = 0.60078956\n",
            "Iteration 207, loss = 0.59994339\n",
            "Iteration 208, loss = 0.59905569\n",
            "Iteration 209, loss = 0.59819833\n",
            "Iteration 210, loss = 0.59731946\n",
            "Iteration 211, loss = 0.59641909\n",
            "Iteration 212, loss = 0.59555355\n",
            "Iteration 213, loss = 0.59469149\n",
            "Iteration 214, loss = 0.59386057\n",
            "Iteration 215, loss = 0.59295877\n",
            "Iteration 216, loss = 0.59204695\n",
            "Iteration 217, loss = 0.59117201\n",
            "Iteration 218, loss = 0.59030311\n",
            "Iteration 219, loss = 0.58944506\n",
            "Iteration 220, loss = 0.58853134\n",
            "Iteration 221, loss = 0.58766797\n",
            "Iteration 222, loss = 0.58680757\n",
            "Iteration 223, loss = 0.58592346\n",
            "Iteration 224, loss = 0.58506644\n",
            "Iteration 225, loss = 0.58419881\n",
            "Iteration 226, loss = 0.58332920\n",
            "Iteration 227, loss = 0.58244115\n",
            "Iteration 228, loss = 0.58156092\n",
            "Iteration 229, loss = 0.58072448\n",
            "Iteration 230, loss = 0.57987052\n",
            "Iteration 231, loss = 0.57897267\n",
            "Iteration 232, loss = 0.57812936\n",
            "Iteration 233, loss = 0.57726964\n",
            "Iteration 234, loss = 0.57644637\n",
            "Iteration 235, loss = 0.57557237\n",
            "Iteration 236, loss = 0.57475083\n",
            "Iteration 237, loss = 0.57387985\n",
            "Iteration 238, loss = 0.57300720\n",
            "Iteration 239, loss = 0.57218662\n",
            "Iteration 240, loss = 0.57131550\n",
            "Iteration 241, loss = 0.57046837\n",
            "Iteration 242, loss = 0.56960813\n",
            "Iteration 243, loss = 0.56878260\n",
            "Iteration 244, loss = 0.56787582\n",
            "Iteration 245, loss = 0.56709401\n",
            "Iteration 246, loss = 0.56617176\n",
            "Iteration 247, loss = 0.56531634\n",
            "Iteration 248, loss = 0.56447043\n",
            "Iteration 249, loss = 0.56363643\n",
            "Iteration 250, loss = 0.56278920\n",
            "Iteration 251, loss = 0.56190310\n",
            "Iteration 252, loss = 0.56106860\n",
            "Iteration 253, loss = 0.56022432\n",
            "Iteration 254, loss = 0.55936860\n",
            "Iteration 255, loss = 0.55852930\n",
            "Iteration 256, loss = 0.55766402\n",
            "Iteration 257, loss = 0.55682007\n",
            "Iteration 258, loss = 0.55596326\n",
            "Iteration 259, loss = 0.55511334\n",
            "Iteration 260, loss = 0.55425807\n",
            "Iteration 261, loss = 0.55338454\n",
            "Iteration 262, loss = 0.55256034\n",
            "Iteration 263, loss = 0.55167707\n",
            "Iteration 264, loss = 0.55083304\n",
            "Iteration 265, loss = 0.54999282\n",
            "Iteration 266, loss = 0.54912212\n",
            "Iteration 267, loss = 0.54827145\n",
            "Iteration 268, loss = 0.54739250\n",
            "Iteration 269, loss = 0.54659156\n",
            "Iteration 270, loss = 0.54571764\n",
            "Iteration 271, loss = 0.54486772\n",
            "Iteration 272, loss = 0.54402365\n",
            "Iteration 273, loss = 0.54320353\n",
            "Iteration 274, loss = 0.54234686\n",
            "Iteration 275, loss = 0.54150458\n",
            "Iteration 276, loss = 0.54066333\n",
            "Iteration 277, loss = 0.53982393\n",
            "Iteration 278, loss = 0.53898853\n",
            "Iteration 279, loss = 0.53817443\n",
            "Iteration 280, loss = 0.53736037\n",
            "Iteration 281, loss = 0.53653479\n",
            "Iteration 282, loss = 0.53571039\n",
            "Iteration 283, loss = 0.53490032\n",
            "Iteration 284, loss = 0.53409045\n",
            "Iteration 285, loss = 0.53326106\n",
            "Iteration 286, loss = 0.53244946\n",
            "Iteration 287, loss = 0.53161223\n",
            "Iteration 288, loss = 0.53074791\n",
            "Iteration 289, loss = 0.52997846\n",
            "Iteration 290, loss = 0.52912979\n",
            "Iteration 291, loss = 0.52834500\n",
            "Iteration 292, loss = 0.52749281\n",
            "Iteration 293, loss = 0.52667611\n",
            "Iteration 294, loss = 0.52588701\n",
            "Iteration 295, loss = 0.52511697\n",
            "Iteration 296, loss = 0.52432622\n",
            "Iteration 297, loss = 0.52355142\n",
            "Iteration 298, loss = 0.52268158\n",
            "Iteration 299, loss = 0.52190875\n",
            "Iteration 300, loss = 0.52109138\n",
            "Iteration 301, loss = 0.52030133\n",
            "Iteration 302, loss = 0.51953950\n",
            "Iteration 303, loss = 0.51872619\n",
            "Iteration 304, loss = 0.51797237\n",
            "Iteration 305, loss = 0.51718460\n",
            "Iteration 306, loss = 0.51642227\n",
            "Iteration 307, loss = 0.51564721\n",
            "Iteration 308, loss = 0.51484473\n",
            "Iteration 309, loss = 0.51408676\n",
            "Iteration 310, loss = 0.51332685\n",
            "Iteration 311, loss = 0.51253479\n",
            "Iteration 312, loss = 0.51178127\n",
            "Iteration 313, loss = 0.51102073\n",
            "Iteration 314, loss = 0.51030734\n",
            "Iteration 315, loss = 0.50954609\n",
            "Iteration 316, loss = 0.50877287\n",
            "Iteration 317, loss = 0.50801305\n",
            "Iteration 318, loss = 0.50727714\n",
            "Iteration 319, loss = 0.50650765\n",
            "Iteration 320, loss = 0.50580260\n",
            "Iteration 321, loss = 0.50505099\n",
            "Iteration 322, loss = 0.50430468\n",
            "Iteration 323, loss = 0.50361769\n",
            "Iteration 324, loss = 0.50285604\n",
            "Iteration 325, loss = 0.50217745\n",
            "Iteration 326, loss = 0.50144849\n",
            "Iteration 327, loss = 0.50075135\n",
            "Iteration 328, loss = 0.50003884\n",
            "Iteration 329, loss = 0.49932800\n",
            "Iteration 330, loss = 0.49859901\n",
            "Iteration 331, loss = 0.49793321\n",
            "Iteration 332, loss = 0.49719833\n",
            "Iteration 333, loss = 0.49653226\n",
            "Iteration 334, loss = 0.49582341\n",
            "Iteration 335, loss = 0.49512854\n",
            "Iteration 336, loss = 0.49444610\n",
            "Iteration 337, loss = 0.49376421\n",
            "Iteration 338, loss = 0.49310453\n",
            "Iteration 339, loss = 0.49239436\n",
            "Iteration 340, loss = 0.49171652\n",
            "Iteration 341, loss = 0.49106061\n",
            "Iteration 342, loss = 0.49043811\n",
            "Iteration 343, loss = 0.48976671\n",
            "Iteration 344, loss = 0.48912426\n",
            "Iteration 345, loss = 0.48845115\n",
            "Iteration 346, loss = 0.48781807\n",
            "Iteration 347, loss = 0.48713832\n",
            "Iteration 348, loss = 0.48652667\n",
            "Iteration 349, loss = 0.48585530\n",
            "Iteration 350, loss = 0.48523569\n",
            "Iteration 351, loss = 0.48460128\n",
            "Iteration 352, loss = 0.48396499\n",
            "Iteration 353, loss = 0.48335255\n",
            "Iteration 354, loss = 0.48270199\n",
            "Iteration 355, loss = 0.48205821\n",
            "Iteration 356, loss = 0.48144656\n",
            "Iteration 357, loss = 0.48083587\n",
            "Iteration 358, loss = 0.48021317\n",
            "Iteration 359, loss = 0.47959348\n",
            "Iteration 360, loss = 0.47898564\n",
            "Iteration 361, loss = 0.47838568\n",
            "Iteration 362, loss = 0.47779529\n",
            "Iteration 363, loss = 0.47718017\n",
            "Iteration 364, loss = 0.47661332\n",
            "Iteration 365, loss = 0.47599848\n",
            "Iteration 366, loss = 0.47541831\n",
            "Iteration 367, loss = 0.47486823\n",
            "Iteration 368, loss = 0.47430376\n",
            "Iteration 369, loss = 0.47372545\n",
            "Iteration 370, loss = 0.47317588\n",
            "Iteration 371, loss = 0.47260767\n",
            "Iteration 372, loss = 0.47206223\n",
            "Iteration 373, loss = 0.47148992\n",
            "Iteration 374, loss = 0.47093998\n",
            "Iteration 375, loss = 0.47039889\n",
            "Iteration 376, loss = 0.46985303\n",
            "Iteration 377, loss = 0.46933324\n",
            "Iteration 378, loss = 0.46880542\n",
            "Iteration 379, loss = 0.46829148\n",
            "Iteration 380, loss = 0.46779461\n",
            "Iteration 381, loss = 0.46724444\n",
            "Iteration 382, loss = 0.46675658\n",
            "Iteration 383, loss = 0.46623085\n",
            "Iteration 384, loss = 0.46571361\n",
            "Iteration 385, loss = 0.46526684\n",
            "Iteration 386, loss = 0.46472931\n",
            "Iteration 387, loss = 0.46424058\n",
            "Iteration 388, loss = 0.46372646\n",
            "Iteration 389, loss = 0.46321603\n",
            "Iteration 390, loss = 0.46271041\n",
            "Iteration 391, loss = 0.46224006\n",
            "Iteration 392, loss = 0.46176542\n",
            "Iteration 393, loss = 0.46124801\n",
            "Iteration 394, loss = 0.46078873\n",
            "Iteration 395, loss = 0.46026841\n",
            "Iteration 396, loss = 0.45980438\n",
            "Iteration 397, loss = 0.45934260\n",
            "Iteration 398, loss = 0.45886330\n",
            "Iteration 399, loss = 0.45841308\n",
            "Iteration 400, loss = 0.45796644\n",
            "Iteration 401, loss = 0.45751379\n",
            "Iteration 402, loss = 0.45705094\n",
            "Iteration 403, loss = 0.45659735\n",
            "Iteration 404, loss = 0.45614672\n",
            "Iteration 405, loss = 0.45569690\n",
            "Iteration 406, loss = 0.45522743\n",
            "Iteration 407, loss = 0.45481465\n",
            "Iteration 408, loss = 0.45438077\n",
            "Iteration 409, loss = 0.45392253\n",
            "Iteration 410, loss = 0.45349483\n",
            "Iteration 411, loss = 0.45304417\n",
            "Iteration 412, loss = 0.45261617\n",
            "Iteration 413, loss = 0.45220136\n",
            "Iteration 414, loss = 0.45178916\n",
            "Iteration 415, loss = 0.45136883\n",
            "Iteration 416, loss = 0.45093286\n",
            "Iteration 417, loss = 0.45054312\n",
            "Iteration 418, loss = 0.45010649\n",
            "Iteration 419, loss = 0.44971841\n",
            "Iteration 420, loss = 0.44930947\n",
            "Iteration 421, loss = 0.44893640\n",
            "Iteration 422, loss = 0.44859467\n",
            "Iteration 423, loss = 0.44815815\n",
            "Iteration 424, loss = 0.44775932\n",
            "Iteration 425, loss = 0.44733889\n",
            "Iteration 426, loss = 0.44695855\n",
            "Iteration 427, loss = 0.44658401\n",
            "Iteration 428, loss = 0.44617646\n",
            "Iteration 429, loss = 0.44583751\n",
            "Iteration 430, loss = 0.44540666\n",
            "Iteration 431, loss = 0.44507600\n",
            "Iteration 432, loss = 0.44478797\n",
            "Iteration 433, loss = 0.44437824\n",
            "Iteration 434, loss = 0.44402020\n",
            "Iteration 435, loss = 0.44368696\n",
            "Iteration 436, loss = 0.44330265\n",
            "Iteration 437, loss = 0.44297158\n",
            "Iteration 438, loss = 0.44259971\n",
            "Iteration 439, loss = 0.44227923\n",
            "Iteration 440, loss = 0.44190848\n",
            "Iteration 441, loss = 0.44154423\n",
            "Iteration 442, loss = 0.44126930\n",
            "Iteration 443, loss = 0.44085794\n",
            "Iteration 444, loss = 0.44053394\n",
            "Iteration 445, loss = 0.44018825\n",
            "Iteration 446, loss = 0.43986662\n",
            "Iteration 447, loss = 0.43957433\n",
            "Iteration 448, loss = 0.43924220\n",
            "Iteration 449, loss = 0.43892851\n",
            "Iteration 450, loss = 0.43860614\n",
            "Iteration 451, loss = 0.43831621\n",
            "Iteration 452, loss = 0.43798415\n",
            "Iteration 453, loss = 0.43768057\n",
            "Iteration 454, loss = 0.43737651\n",
            "Iteration 455, loss = 0.43708026\n",
            "Iteration 456, loss = 0.43680590\n",
            "Iteration 457, loss = 0.43652132\n",
            "Iteration 458, loss = 0.43623077\n",
            "Iteration 459, loss = 0.43593891\n",
            "Iteration 460, loss = 0.43563684\n",
            "Iteration 461, loss = 0.43533681\n",
            "Iteration 462, loss = 0.43504921\n",
            "Iteration 463, loss = 0.43475436\n",
            "Iteration 464, loss = 0.43449851\n",
            "Iteration 465, loss = 0.43422440\n",
            "Iteration 466, loss = 0.43393448\n",
            "Iteration 467, loss = 0.43368439\n",
            "Iteration 468, loss = 0.43339720\n",
            "Iteration 469, loss = 0.43315119\n",
            "Iteration 470, loss = 0.43288602\n",
            "Iteration 471, loss = 0.43261155\n",
            "Iteration 472, loss = 0.43237193\n",
            "Iteration 473, loss = 0.43215995\n",
            "Iteration 474, loss = 0.43189491\n",
            "Iteration 475, loss = 0.43161873\n",
            "Iteration 476, loss = 0.43135179\n",
            "Iteration 477, loss = 0.43116676\n",
            "Iteration 478, loss = 0.43089240\n",
            "Iteration 479, loss = 0.43065890\n",
            "Iteration 480, loss = 0.43044831\n",
            "Iteration 481, loss = 0.43017834\n",
            "Iteration 482, loss = 0.42994157\n",
            "Iteration 483, loss = 0.42972010\n",
            "Iteration 484, loss = 0.42950932\n",
            "Iteration 485, loss = 0.42929920\n",
            "Iteration 486, loss = 0.42906105\n",
            "Iteration 487, loss = 0.42883230\n",
            "Iteration 488, loss = 0.42861817\n",
            "Iteration 489, loss = 0.42839090\n",
            "Iteration 490, loss = 0.42815100\n",
            "Iteration 491, loss = 0.42796515\n",
            "Iteration 492, loss = 0.42775170\n",
            "Iteration 493, loss = 0.42755366\n",
            "Iteration 494, loss = 0.42733764\n",
            "Iteration 495, loss = 0.42710389\n",
            "Iteration 496, loss = 0.42693752\n",
            "Iteration 497, loss = 0.42670915\n",
            "Iteration 498, loss = 0.42653089\n",
            "Iteration 499, loss = 0.42630718\n",
            "Iteration 500, loss = 0.42611423\n",
            "Iteration 501, loss = 0.42593492\n",
            "Iteration 502, loss = 0.42574057\n",
            "Iteration 503, loss = 0.42561524\n",
            "Iteration 504, loss = 0.42537171\n",
            "Iteration 505, loss = 0.42516867\n",
            "Iteration 506, loss = 0.42498623\n",
            "Iteration 507, loss = 0.42480953\n",
            "Iteration 508, loss = 0.42463221\n",
            "Iteration 509, loss = 0.42445996\n",
            "Iteration 510, loss = 0.42428898\n",
            "Iteration 511, loss = 0.42414889\n",
            "Iteration 512, loss = 0.42395468\n",
            "Iteration 513, loss = 0.42377419\n",
            "Iteration 514, loss = 0.42357377\n",
            "Iteration 515, loss = 0.42352395\n",
            "Iteration 516, loss = 0.42321935\n",
            "Iteration 517, loss = 0.42305115\n",
            "Iteration 518, loss = 0.42287913\n",
            "Iteration 519, loss = 0.42272038\n",
            "Iteration 520, loss = 0.42251937\n",
            "Iteration 521, loss = 0.42235858\n",
            "Iteration 522, loss = 0.42216810\n",
            "Iteration 523, loss = 0.42204440\n",
            "Iteration 524, loss = 0.42186108\n",
            "Iteration 525, loss = 0.42169613\n",
            "Iteration 526, loss = 0.42158705\n",
            "Iteration 527, loss = 0.42139149\n",
            "Iteration 528, loss = 0.42126037\n",
            "Iteration 529, loss = 0.42109944\n",
            "Iteration 530, loss = 0.42097961\n",
            "Iteration 531, loss = 0.42081447\n",
            "Iteration 532, loss = 0.42066152\n",
            "Iteration 533, loss = 0.42056740\n",
            "Iteration 534, loss = 0.42038461\n",
            "Iteration 535, loss = 0.42024153\n",
            "Iteration 536, loss = 0.42010705\n",
            "Iteration 537, loss = 0.41995907\n",
            "Iteration 538, loss = 0.41983153\n",
            "Iteration 539, loss = 0.41971070\n",
            "Iteration 540, loss = 0.41960055\n",
            "Iteration 541, loss = 0.41942839\n",
            "Iteration 542, loss = 0.41930080\n",
            "Iteration 543, loss = 0.41916268\n",
            "Iteration 544, loss = 0.41904433\n",
            "Iteration 545, loss = 0.41891843\n",
            "Iteration 546, loss = 0.41879240\n",
            "Iteration 547, loss = 0.41865060\n",
            "Iteration 548, loss = 0.41850292\n",
            "Iteration 549, loss = 0.41836841\n",
            "Iteration 550, loss = 0.41825377\n",
            "Iteration 551, loss = 0.41810840\n",
            "Iteration 552, loss = 0.41801517\n",
            "Iteration 553, loss = 0.41786674\n",
            "Iteration 554, loss = 0.41774331\n",
            "Iteration 555, loss = 0.41760643\n",
            "Iteration 556, loss = 0.41749239\n",
            "Iteration 557, loss = 0.41735667\n",
            "Iteration 558, loss = 0.41720836\n",
            "Iteration 559, loss = 0.41710707\n",
            "Iteration 560, loss = 0.41695880\n",
            "Iteration 561, loss = 0.41684364\n",
            "Iteration 562, loss = 0.41673045\n",
            "Iteration 563, loss = 0.41663188\n",
            "Iteration 564, loss = 0.41652636\n",
            "Iteration 565, loss = 0.41642048\n",
            "Iteration 566, loss = 0.41631003\n",
            "Iteration 567, loss = 0.41620590\n",
            "Iteration 568, loss = 0.41606941\n",
            "Iteration 569, loss = 0.41599627\n",
            "Iteration 570, loss = 0.41586985\n",
            "Iteration 571, loss = 0.41577502\n",
            "Iteration 572, loss = 0.41565499\n",
            "Iteration 573, loss = 0.41555319\n",
            "Iteration 574, loss = 0.41545807\n",
            "Iteration 575, loss = 0.41535649\n",
            "Iteration 576, loss = 0.41526935\n",
            "Iteration 577, loss = 0.41514213\n",
            "Iteration 578, loss = 0.41504193\n",
            "Iteration 579, loss = 0.41495049\n",
            "Iteration 580, loss = 0.41482883\n",
            "Iteration 581, loss = 0.41474238\n",
            "Iteration 582, loss = 0.41463616\n",
            "Iteration 583, loss = 0.41455727\n",
            "Iteration 584, loss = 0.41446529\n",
            "Iteration 585, loss = 0.41436399\n",
            "Iteration 586, loss = 0.41428477\n",
            "Iteration 587, loss = 0.41415867\n",
            "Iteration 588, loss = 0.41407341\n",
            "Iteration 589, loss = 0.41397089\n",
            "Iteration 590, loss = 0.41386826\n",
            "Iteration 591, loss = 0.41378698\n",
            "Iteration 592, loss = 0.41372682\n",
            "Iteration 593, loss = 0.41359791\n",
            "Iteration 594, loss = 0.41349841\n",
            "Iteration 595, loss = 0.41342027\n",
            "Iteration 596, loss = 0.41333475\n",
            "Iteration 597, loss = 0.41322813\n",
            "Iteration 598, loss = 0.41314835\n",
            "Iteration 599, loss = 0.41311032\n",
            "Iteration 600, loss = 0.41298874\n",
            "Iteration 601, loss = 0.41292212\n",
            "Iteration 602, loss = 0.41285547\n",
            "Iteration 603, loss = 0.41275021\n",
            "Iteration 604, loss = 0.41265961\n",
            "Iteration 605, loss = 0.41263510\n",
            "Iteration 606, loss = 0.41252608\n",
            "Iteration 607, loss = 0.41246057\n",
            "Iteration 608, loss = 0.41235926\n",
            "Iteration 609, loss = 0.41229219\n",
            "Iteration 610, loss = 0.41221469\n",
            "Iteration 611, loss = 0.41214339\n",
            "Iteration 612, loss = 0.41205651\n",
            "Iteration 613, loss = 0.41196859\n",
            "Iteration 614, loss = 0.41185260\n",
            "Iteration 615, loss = 0.41183520\n",
            "Iteration 616, loss = 0.41176137\n",
            "Iteration 617, loss = 0.41169320\n",
            "Iteration 618, loss = 0.41159188\n",
            "Iteration 619, loss = 0.41152950\n",
            "Iteration 620, loss = 0.41144603\n",
            "Iteration 621, loss = 0.41139469\n",
            "Iteration 622, loss = 0.41132690\n",
            "Iteration 623, loss = 0.41126176\n",
            "Iteration 624, loss = 0.41115090\n",
            "Iteration 625, loss = 0.41116175\n",
            "Iteration 626, loss = 0.41103335\n",
            "Iteration 627, loss = 0.41096255\n",
            "Iteration 628, loss = 0.41088969\n",
            "Iteration 629, loss = 0.41079094\n",
            "Iteration 630, loss = 0.41074366\n",
            "Iteration 631, loss = 0.41067304\n",
            "Iteration 632, loss = 0.41063745\n",
            "Iteration 633, loss = 0.41055543\n",
            "Iteration 634, loss = 0.41047573\n",
            "Iteration 635, loss = 0.41040488\n",
            "Iteration 636, loss = 0.41034047\n",
            "Iteration 637, loss = 0.41028715\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.71664906\n",
            "Iteration 2, loss = 0.71560193\n",
            "Iteration 3, loss = 0.71444780\n",
            "Iteration 4, loss = 0.71342039\n",
            "Iteration 5, loss = 0.71228185\n",
            "Iteration 6, loss = 0.71130386\n",
            "Iteration 7, loss = 0.71033805\n",
            "Iteration 8, loss = 0.70945952\n",
            "Iteration 9, loss = 0.70840546\n",
            "Iteration 10, loss = 0.70763717\n",
            "Iteration 11, loss = 0.70675532\n",
            "Iteration 12, loss = 0.70598041\n",
            "Iteration 13, loss = 0.70519632\n",
            "Iteration 14, loss = 0.70444607\n",
            "Iteration 15, loss = 0.70369516\n",
            "Iteration 16, loss = 0.70307803\n",
            "Iteration 17, loss = 0.70242416\n",
            "Iteration 18, loss = 0.70176272\n",
            "Iteration 19, loss = 0.70116749\n",
            "Iteration 20, loss = 0.70058482\n",
            "Iteration 21, loss = 0.70019403\n",
            "Iteration 22, loss = 0.69964170\n",
            "Iteration 23, loss = 0.69910742\n",
            "Iteration 24, loss = 0.69864776\n",
            "Iteration 25, loss = 0.69824840\n",
            "Iteration 26, loss = 0.69788448\n",
            "Iteration 27, loss = 0.69754097\n",
            "Iteration 28, loss = 0.69705436\n",
            "Iteration 29, loss = 0.69683572\n",
            "Iteration 30, loss = 0.69642424\n",
            "Iteration 31, loss = 0.69609211\n",
            "Iteration 32, loss = 0.69585708\n",
            "Iteration 33, loss = 0.69555838\n",
            "Iteration 34, loss = 0.69529173\n",
            "Iteration 35, loss = 0.69501270\n",
            "Iteration 36, loss = 0.69475941\n",
            "Iteration 37, loss = 0.69459134\n",
            "Iteration 38, loss = 0.69435712\n",
            "Iteration 39, loss = 0.69418497\n",
            "Iteration 40, loss = 0.69397259\n",
            "Iteration 41, loss = 0.69382713\n",
            "Iteration 42, loss = 0.69363110\n",
            "Iteration 43, loss = 0.69349620\n",
            "Iteration 44, loss = 0.69332809\n",
            "Iteration 45, loss = 0.69321021\n",
            "Iteration 46, loss = 0.69300804\n",
            "Iteration 47, loss = 0.69287826\n",
            "Iteration 48, loss = 0.69277764\n",
            "Iteration 49, loss = 0.69259396\n",
            "Iteration 50, loss = 0.69247147\n",
            "Iteration 51, loss = 0.69232769\n",
            "Iteration 52, loss = 0.69217057\n",
            "Iteration 53, loss = 0.69203538\n",
            "Iteration 54, loss = 0.69193100\n",
            "Iteration 55, loss = 0.69178921\n",
            "Iteration 56, loss = 0.69163007\n",
            "Iteration 57, loss = 0.69148734\n",
            "Iteration 58, loss = 0.69136491\n",
            "Iteration 59, loss = 0.69124685\n",
            "Iteration 60, loss = 0.69108035\n",
            "Iteration 61, loss = 0.69092454\n",
            "Iteration 62, loss = 0.69079865\n",
            "Iteration 63, loss = 0.69063346\n",
            "Iteration 64, loss = 0.69048264\n",
            "Iteration 65, loss = 0.69031343\n",
            "Iteration 66, loss = 0.69014666\n",
            "Iteration 67, loss = 0.68997749\n",
            "Iteration 68, loss = 0.68978881\n",
            "Iteration 69, loss = 0.68961366\n",
            "Iteration 70, loss = 0.68942559\n",
            "Iteration 71, loss = 0.68922664\n",
            "Iteration 72, loss = 0.68901990\n",
            "Iteration 73, loss = 0.68883186\n",
            "Iteration 74, loss = 0.68861724\n",
            "Iteration 75, loss = 0.68839926\n",
            "Iteration 76, loss = 0.68817361\n",
            "Iteration 77, loss = 0.68792838\n",
            "Iteration 78, loss = 0.68769169\n",
            "Iteration 79, loss = 0.68742246\n",
            "Iteration 80, loss = 0.68718386\n",
            "Iteration 81, loss = 0.68690685\n",
            "Iteration 82, loss = 0.68663654\n",
            "Iteration 83, loss = 0.68635964\n",
            "Iteration 84, loss = 0.68606708\n",
            "Iteration 85, loss = 0.68576716\n",
            "Iteration 86, loss = 0.68545461\n",
            "Iteration 87, loss = 0.68513945\n",
            "Iteration 88, loss = 0.68481565\n",
            "Iteration 89, loss = 0.68448413\n",
            "Iteration 90, loss = 0.68413613\n",
            "Iteration 91, loss = 0.68378370\n",
            "Iteration 92, loss = 0.68341419\n",
            "Iteration 93, loss = 0.68305180\n",
            "Iteration 94, loss = 0.68268103\n",
            "Iteration 95, loss = 0.68231022\n",
            "Iteration 96, loss = 0.68193521\n",
            "Iteration 97, loss = 0.68150883\n",
            "Iteration 98, loss = 0.68111920\n",
            "Iteration 99, loss = 0.68068922\n",
            "Iteration 100, loss = 0.68025787\n",
            "Iteration 101, loss = 0.67984589\n",
            "Iteration 102, loss = 0.67938847\n",
            "Iteration 103, loss = 0.67893610\n",
            "Iteration 104, loss = 0.67848964\n",
            "Iteration 105, loss = 0.67802040\n",
            "Iteration 106, loss = 0.67754356\n",
            "Iteration 107, loss = 0.67706298\n",
            "Iteration 108, loss = 0.67656151\n",
            "Iteration 109, loss = 0.67605444\n",
            "Iteration 110, loss = 0.67554119\n",
            "Iteration 111, loss = 0.67502920\n",
            "Iteration 112, loss = 0.67452418\n",
            "Iteration 113, loss = 0.67397106\n",
            "Iteration 114, loss = 0.67343249\n",
            "Iteration 115, loss = 0.67288989\n",
            "Iteration 116, loss = 0.67233304\n",
            "Iteration 117, loss = 0.67176565\n",
            "Iteration 118, loss = 0.67120115\n",
            "Iteration 119, loss = 0.67060596\n",
            "Iteration 120, loss = 0.67002360\n",
            "Iteration 121, loss = 0.66942652\n",
            "Iteration 122, loss = 0.66880644\n",
            "Iteration 123, loss = 0.66819352\n",
            "Iteration 124, loss = 0.66756716\n",
            "Iteration 125, loss = 0.66696262\n",
            "Iteration 126, loss = 0.66629313\n",
            "Iteration 127, loss = 0.66564483\n",
            "Iteration 128, loss = 0.66502231\n",
            "Iteration 129, loss = 0.66434570\n",
            "Iteration 130, loss = 0.66367568\n",
            "Iteration 131, loss = 0.66300666\n",
            "Iteration 132, loss = 0.66232483\n",
            "Iteration 133, loss = 0.66164489\n",
            "Iteration 134, loss = 0.66095266\n",
            "Iteration 135, loss = 0.66028136\n",
            "Iteration 136, loss = 0.65954585\n",
            "Iteration 137, loss = 0.65882524\n",
            "Iteration 138, loss = 0.65809529\n",
            "Iteration 139, loss = 0.65738586\n",
            "Iteration 140, loss = 0.65668824\n",
            "Iteration 141, loss = 0.65594515\n",
            "Iteration 142, loss = 0.65520160\n",
            "Iteration 143, loss = 0.65446480\n",
            "Iteration 144, loss = 0.65372515\n",
            "Iteration 145, loss = 0.65297371\n",
            "Iteration 146, loss = 0.65220774\n",
            "Iteration 147, loss = 0.65142824\n",
            "Iteration 148, loss = 0.65062841\n",
            "Iteration 149, loss = 0.64985757\n",
            "Iteration 150, loss = 0.64906985\n",
            "Iteration 151, loss = 0.64825943\n",
            "Iteration 152, loss = 0.64746777\n",
            "Iteration 153, loss = 0.64666465\n",
            "Iteration 154, loss = 0.64582565\n",
            "Iteration 155, loss = 0.64503070\n",
            "Iteration 156, loss = 0.64422473\n",
            "Iteration 157, loss = 0.64334623\n",
            "Iteration 158, loss = 0.64251383\n",
            "Iteration 159, loss = 0.64170109\n",
            "Iteration 160, loss = 0.64083993\n",
            "Iteration 161, loss = 0.64001466\n",
            "Iteration 162, loss = 0.63915657\n",
            "Iteration 163, loss = 0.63829862\n",
            "Iteration 164, loss = 0.63747140\n",
            "Iteration 165, loss = 0.63658941\n",
            "Iteration 166, loss = 0.63571221\n",
            "Iteration 167, loss = 0.63488170\n",
            "Iteration 168, loss = 0.63399973\n",
            "Iteration 169, loss = 0.63315218\n",
            "Iteration 170, loss = 0.63225998\n",
            "Iteration 171, loss = 0.63141083\n",
            "Iteration 172, loss = 0.63053692\n",
            "Iteration 173, loss = 0.62965397\n",
            "Iteration 174, loss = 0.62876004\n",
            "Iteration 175, loss = 0.62788162\n",
            "Iteration 176, loss = 0.62700746\n",
            "Iteration 177, loss = 0.62612912\n",
            "Iteration 178, loss = 0.62525878\n",
            "Iteration 179, loss = 0.62439963\n",
            "Iteration 180, loss = 0.62349747\n",
            "Iteration 181, loss = 0.62261517\n",
            "Iteration 182, loss = 0.62175538\n",
            "Iteration 183, loss = 0.62081221\n",
            "Iteration 184, loss = 0.61992840\n",
            "Iteration 185, loss = 0.61902991\n",
            "Iteration 186, loss = 0.61814020\n",
            "Iteration 187, loss = 0.61723468\n",
            "Iteration 188, loss = 0.61634089\n",
            "Iteration 189, loss = 0.61544476\n",
            "Iteration 190, loss = 0.61454069\n",
            "Iteration 191, loss = 0.61366459\n",
            "Iteration 192, loss = 0.61274468\n",
            "Iteration 193, loss = 0.61185112\n",
            "Iteration 194, loss = 0.61093818\n",
            "Iteration 195, loss = 0.61003355\n",
            "Iteration 196, loss = 0.60916327\n",
            "Iteration 197, loss = 0.60823240\n",
            "Iteration 198, loss = 0.60731594\n",
            "Iteration 199, loss = 0.60642155\n",
            "Iteration 200, loss = 0.60552924\n",
            "Iteration 201, loss = 0.60458549\n",
            "Iteration 202, loss = 0.60370616\n",
            "Iteration 203, loss = 0.60282687\n",
            "Iteration 204, loss = 0.60190639\n",
            "Iteration 205, loss = 0.60105054\n",
            "Iteration 206, loss = 0.60008742\n",
            "Iteration 207, loss = 0.59921837\n",
            "Iteration 208, loss = 0.59829845\n",
            "Iteration 209, loss = 0.59742492\n",
            "Iteration 210, loss = 0.59651645\n",
            "Iteration 211, loss = 0.59560362\n",
            "Iteration 212, loss = 0.59474636\n",
            "Iteration 213, loss = 0.59385909\n",
            "Iteration 214, loss = 0.59296739\n",
            "Iteration 215, loss = 0.59208803\n",
            "Iteration 216, loss = 0.59118614\n",
            "Iteration 217, loss = 0.59026328\n",
            "Iteration 218, loss = 0.58937401\n",
            "Iteration 219, loss = 0.58848940\n",
            "Iteration 220, loss = 0.58754035\n",
            "Iteration 221, loss = 0.58666330\n",
            "Iteration 222, loss = 0.58577572\n",
            "Iteration 223, loss = 0.58484487\n",
            "Iteration 224, loss = 0.58398813\n",
            "Iteration 225, loss = 0.58307065\n",
            "Iteration 226, loss = 0.58216182\n",
            "Iteration 227, loss = 0.58126889\n",
            "Iteration 228, loss = 0.58035144\n",
            "Iteration 229, loss = 0.57947316\n",
            "Iteration 230, loss = 0.57859582\n",
            "Iteration 231, loss = 0.57768340\n",
            "Iteration 232, loss = 0.57681575\n",
            "Iteration 233, loss = 0.57592255\n",
            "Iteration 234, loss = 0.57508633\n",
            "Iteration 235, loss = 0.57418397\n",
            "Iteration 236, loss = 0.57332642\n",
            "Iteration 237, loss = 0.57244248\n",
            "Iteration 238, loss = 0.57156706\n",
            "Iteration 239, loss = 0.57072281\n",
            "Iteration 240, loss = 0.56982635\n",
            "Iteration 241, loss = 0.56894641\n",
            "Iteration 242, loss = 0.56807845\n",
            "Iteration 243, loss = 0.56723411\n",
            "Iteration 244, loss = 0.56627191\n",
            "Iteration 245, loss = 0.56545812\n",
            "Iteration 246, loss = 0.56451828\n",
            "Iteration 247, loss = 0.56361582\n",
            "Iteration 248, loss = 0.56275663\n",
            "Iteration 249, loss = 0.56189015\n",
            "Iteration 250, loss = 0.56100816\n",
            "Iteration 251, loss = 0.56011019\n",
            "Iteration 252, loss = 0.55924763\n",
            "Iteration 253, loss = 0.55839407\n",
            "Iteration 254, loss = 0.55752140\n",
            "Iteration 255, loss = 0.55665112\n",
            "Iteration 256, loss = 0.55578499\n",
            "Iteration 257, loss = 0.55493176\n",
            "Iteration 258, loss = 0.55405567\n",
            "Iteration 259, loss = 0.55319624\n",
            "Iteration 260, loss = 0.55231678\n",
            "Iteration 261, loss = 0.55142665\n",
            "Iteration 262, loss = 0.55057532\n",
            "Iteration 263, loss = 0.54969967\n",
            "Iteration 264, loss = 0.54884535\n",
            "Iteration 265, loss = 0.54798264\n",
            "Iteration 266, loss = 0.54710412\n",
            "Iteration 267, loss = 0.54624441\n",
            "Iteration 268, loss = 0.54539135\n",
            "Iteration 269, loss = 0.54455134\n",
            "Iteration 270, loss = 0.54368149\n",
            "Iteration 271, loss = 0.54283004\n",
            "Iteration 272, loss = 0.54197496\n",
            "Iteration 273, loss = 0.54113787\n",
            "Iteration 274, loss = 0.54029299\n",
            "Iteration 275, loss = 0.53944038\n",
            "Iteration 276, loss = 0.53859923\n",
            "Iteration 277, loss = 0.53776906\n",
            "Iteration 278, loss = 0.53694434\n",
            "Iteration 279, loss = 0.53610884\n",
            "Iteration 280, loss = 0.53530415\n",
            "Iteration 281, loss = 0.53446171\n",
            "Iteration 282, loss = 0.53365489\n",
            "Iteration 283, loss = 0.53285886\n",
            "Iteration 284, loss = 0.53203773\n",
            "Iteration 285, loss = 0.53123042\n",
            "Iteration 286, loss = 0.53042281\n",
            "Iteration 287, loss = 0.52961268\n",
            "Iteration 288, loss = 0.52878996\n",
            "Iteration 289, loss = 0.52798208\n",
            "Iteration 290, loss = 0.52717100\n",
            "Iteration 291, loss = 0.52640198\n",
            "Iteration 292, loss = 0.52556474\n",
            "Iteration 293, loss = 0.52476084\n",
            "Iteration 294, loss = 0.52399592\n",
            "Iteration 295, loss = 0.52323147\n",
            "Iteration 296, loss = 0.52244003\n",
            "Iteration 297, loss = 0.52167498\n",
            "Iteration 298, loss = 0.52083755\n",
            "Iteration 299, loss = 0.52010612\n",
            "Iteration 300, loss = 0.51930618\n",
            "Iteration 301, loss = 0.51853640\n",
            "Iteration 302, loss = 0.51777304\n",
            "Iteration 303, loss = 0.51698859\n",
            "Iteration 304, loss = 0.51623750\n",
            "Iteration 305, loss = 0.51550211\n",
            "Iteration 306, loss = 0.51473650\n",
            "Iteration 307, loss = 0.51397838\n",
            "Iteration 308, loss = 0.51321141\n",
            "Iteration 309, loss = 0.51248105\n",
            "Iteration 310, loss = 0.51173934\n",
            "Iteration 311, loss = 0.51097153\n",
            "Iteration 312, loss = 0.51025664\n",
            "Iteration 313, loss = 0.50953403\n",
            "Iteration 314, loss = 0.50879714\n",
            "Iteration 315, loss = 0.50806359\n",
            "Iteration 316, loss = 0.50731680\n",
            "Iteration 317, loss = 0.50659582\n",
            "Iteration 318, loss = 0.50587519\n",
            "Iteration 319, loss = 0.50513824\n",
            "Iteration 320, loss = 0.50443422\n",
            "Iteration 321, loss = 0.50373566\n",
            "Iteration 322, loss = 0.50301700\n",
            "Iteration 323, loss = 0.50233485\n",
            "Iteration 324, loss = 0.50161748\n",
            "Iteration 325, loss = 0.50096457\n",
            "Iteration 326, loss = 0.50025761\n",
            "Iteration 327, loss = 0.49956867\n",
            "Iteration 328, loss = 0.49888005\n",
            "Iteration 329, loss = 0.49823375\n",
            "Iteration 330, loss = 0.49752961\n",
            "Iteration 331, loss = 0.49687979\n",
            "Iteration 332, loss = 0.49617320\n",
            "Iteration 333, loss = 0.49551579\n",
            "Iteration 334, loss = 0.49482694\n",
            "Iteration 335, loss = 0.49416065\n",
            "Iteration 336, loss = 0.49346880\n",
            "Iteration 337, loss = 0.49282895\n",
            "Iteration 338, loss = 0.49217469\n",
            "Iteration 339, loss = 0.49150165\n",
            "Iteration 340, loss = 0.49082904\n",
            "Iteration 341, loss = 0.49019214\n",
            "Iteration 342, loss = 0.48956388\n",
            "Iteration 343, loss = 0.48889798\n",
            "Iteration 344, loss = 0.48823641\n",
            "Iteration 345, loss = 0.48761149\n",
            "Iteration 346, loss = 0.48698708\n",
            "Iteration 347, loss = 0.48631043\n",
            "Iteration 348, loss = 0.48570995\n",
            "Iteration 349, loss = 0.48506761\n",
            "Iteration 350, loss = 0.48445436\n",
            "Iteration 351, loss = 0.48382889\n",
            "Iteration 352, loss = 0.48320172\n",
            "Iteration 353, loss = 0.48258867\n",
            "Iteration 354, loss = 0.48194729\n",
            "Iteration 355, loss = 0.48131933\n",
            "Iteration 356, loss = 0.48071298\n",
            "Iteration 357, loss = 0.48009074\n",
            "Iteration 358, loss = 0.47947449\n",
            "Iteration 359, loss = 0.47886454\n",
            "Iteration 360, loss = 0.47826126\n",
            "Iteration 361, loss = 0.47766036\n",
            "Iteration 362, loss = 0.47708316\n",
            "Iteration 363, loss = 0.47647333\n",
            "Iteration 364, loss = 0.47589891\n",
            "Iteration 365, loss = 0.47529906\n",
            "Iteration 366, loss = 0.47471948\n",
            "Iteration 367, loss = 0.47416397\n",
            "Iteration 368, loss = 0.47358777\n",
            "Iteration 369, loss = 0.47302787\n",
            "Iteration 370, loss = 0.47246377\n",
            "Iteration 371, loss = 0.47191288\n",
            "Iteration 372, loss = 0.47136550\n",
            "Iteration 373, loss = 0.47079540\n",
            "Iteration 374, loss = 0.47024534\n",
            "Iteration 375, loss = 0.46971206\n",
            "Iteration 376, loss = 0.46917191\n",
            "Iteration 377, loss = 0.46866189\n",
            "Iteration 378, loss = 0.46809876\n",
            "Iteration 379, loss = 0.46757928\n",
            "Iteration 380, loss = 0.46707114\n",
            "Iteration 381, loss = 0.46651874\n",
            "Iteration 382, loss = 0.46603720\n",
            "Iteration 383, loss = 0.46551056\n",
            "Iteration 384, loss = 0.46499971\n",
            "Iteration 385, loss = 0.46452831\n",
            "Iteration 386, loss = 0.46399210\n",
            "Iteration 387, loss = 0.46348585\n",
            "Iteration 388, loss = 0.46298221\n",
            "Iteration 389, loss = 0.46245163\n",
            "Iteration 390, loss = 0.46198154\n",
            "Iteration 391, loss = 0.46147665\n",
            "Iteration 392, loss = 0.46099167\n",
            "Iteration 393, loss = 0.46047424\n",
            "Iteration 394, loss = 0.46000739\n",
            "Iteration 395, loss = 0.45950732\n",
            "Iteration 396, loss = 0.45900934\n",
            "Iteration 397, loss = 0.45855574\n",
            "Iteration 398, loss = 0.45807754\n",
            "Iteration 399, loss = 0.45761239\n",
            "Iteration 400, loss = 0.45714537\n",
            "Iteration 401, loss = 0.45668479\n",
            "Iteration 402, loss = 0.45621371\n",
            "Iteration 403, loss = 0.45574683\n",
            "Iteration 404, loss = 0.45528897\n",
            "Iteration 405, loss = 0.45484018\n",
            "Iteration 406, loss = 0.45436085\n",
            "Iteration 407, loss = 0.45392834\n",
            "Iteration 408, loss = 0.45350951\n",
            "Iteration 409, loss = 0.45302780\n",
            "Iteration 410, loss = 0.45259636\n",
            "Iteration 411, loss = 0.45214992\n",
            "Iteration 412, loss = 0.45171207\n",
            "Iteration 413, loss = 0.45128779\n",
            "Iteration 414, loss = 0.45088306\n",
            "Iteration 415, loss = 0.45046140\n",
            "Iteration 416, loss = 0.45003594\n",
            "Iteration 417, loss = 0.44963040\n",
            "Iteration 418, loss = 0.44921237\n",
            "Iteration 419, loss = 0.44882036\n",
            "Iteration 420, loss = 0.44839702\n",
            "Iteration 421, loss = 0.44803519\n",
            "Iteration 422, loss = 0.44765864\n",
            "Iteration 423, loss = 0.44723566\n",
            "Iteration 424, loss = 0.44682859\n",
            "Iteration 425, loss = 0.44641953\n",
            "Iteration 426, loss = 0.44604286\n",
            "Iteration 427, loss = 0.44569000\n",
            "Iteration 428, loss = 0.44528363\n",
            "Iteration 429, loss = 0.44494091\n",
            "Iteration 430, loss = 0.44454463\n",
            "Iteration 431, loss = 0.44420162\n",
            "Iteration 432, loss = 0.44389787\n",
            "Iteration 433, loss = 0.44351647\n",
            "Iteration 434, loss = 0.44314808\n",
            "Iteration 435, loss = 0.44280954\n",
            "Iteration 436, loss = 0.44241793\n",
            "Iteration 437, loss = 0.44208900\n",
            "Iteration 438, loss = 0.44171345\n",
            "Iteration 439, loss = 0.44141701\n",
            "Iteration 440, loss = 0.44101518\n",
            "Iteration 441, loss = 0.44064983\n",
            "Iteration 442, loss = 0.44038673\n",
            "Iteration 443, loss = 0.43997575\n",
            "Iteration 444, loss = 0.43964495\n",
            "Iteration 445, loss = 0.43930073\n",
            "Iteration 446, loss = 0.43897709\n",
            "Iteration 447, loss = 0.43868417\n",
            "Iteration 448, loss = 0.43831325\n",
            "Iteration 449, loss = 0.43801837\n",
            "Iteration 450, loss = 0.43768194\n",
            "Iteration 451, loss = 0.43737774\n",
            "Iteration 452, loss = 0.43705358\n",
            "Iteration 453, loss = 0.43674724\n",
            "Iteration 454, loss = 0.43644624\n",
            "Iteration 455, loss = 0.43616209\n",
            "Iteration 456, loss = 0.43589565\n",
            "Iteration 457, loss = 0.43560987\n",
            "Iteration 458, loss = 0.43530443\n",
            "Iteration 459, loss = 0.43498756\n",
            "Iteration 460, loss = 0.43469131\n",
            "Iteration 461, loss = 0.43441630\n",
            "Iteration 462, loss = 0.43413944\n",
            "Iteration 463, loss = 0.43385621\n",
            "Iteration 464, loss = 0.43361369\n",
            "Iteration 465, loss = 0.43333672\n",
            "Iteration 466, loss = 0.43306575\n",
            "Iteration 467, loss = 0.43280457\n",
            "Iteration 468, loss = 0.43252141\n",
            "Iteration 469, loss = 0.43226290\n",
            "Iteration 470, loss = 0.43200073\n",
            "Iteration 471, loss = 0.43173899\n",
            "Iteration 472, loss = 0.43148866\n",
            "Iteration 473, loss = 0.43126011\n",
            "Iteration 474, loss = 0.43099611\n",
            "Iteration 475, loss = 0.43073259\n",
            "Iteration 476, loss = 0.43046613\n",
            "Iteration 477, loss = 0.43025902\n",
            "Iteration 478, loss = 0.42999987\n",
            "Iteration 479, loss = 0.42975211\n",
            "Iteration 480, loss = 0.42957693\n",
            "Iteration 481, loss = 0.42928083\n",
            "Iteration 482, loss = 0.42905206\n",
            "Iteration 483, loss = 0.42881391\n",
            "Iteration 484, loss = 0.42859784\n",
            "Iteration 485, loss = 0.42837663\n",
            "Iteration 486, loss = 0.42816710\n",
            "Iteration 487, loss = 0.42791011\n",
            "Iteration 488, loss = 0.42770099\n",
            "Iteration 489, loss = 0.42748477\n",
            "Iteration 490, loss = 0.42723841\n",
            "Iteration 491, loss = 0.42707220\n",
            "Iteration 492, loss = 0.42685658\n",
            "Iteration 493, loss = 0.42666293\n",
            "Iteration 494, loss = 0.42642680\n",
            "Iteration 495, loss = 0.42621174\n",
            "Iteration 496, loss = 0.42603311\n",
            "Iteration 497, loss = 0.42581363\n",
            "Iteration 498, loss = 0.42561488\n",
            "Iteration 499, loss = 0.42541076\n",
            "Iteration 500, loss = 0.42520118\n",
            "Iteration 501, loss = 0.42501554\n",
            "Iteration 502, loss = 0.42482823\n",
            "Iteration 503, loss = 0.42471804\n",
            "Iteration 504, loss = 0.42446370\n",
            "Iteration 505, loss = 0.42424321\n",
            "Iteration 506, loss = 0.42407360\n",
            "Iteration 507, loss = 0.42388588\n",
            "Iteration 508, loss = 0.42370797\n",
            "Iteration 509, loss = 0.42353064\n",
            "Iteration 510, loss = 0.42336437\n",
            "Iteration 511, loss = 0.42319784\n",
            "Iteration 512, loss = 0.42302983\n",
            "Iteration 513, loss = 0.42283772\n",
            "Iteration 514, loss = 0.42266012\n",
            "Iteration 515, loss = 0.42258560\n",
            "Iteration 516, loss = 0.42231728\n",
            "Iteration 517, loss = 0.42213539\n",
            "Iteration 518, loss = 0.42197891\n",
            "Iteration 519, loss = 0.42181197\n",
            "Iteration 520, loss = 0.42166657\n",
            "Iteration 521, loss = 0.42147500\n",
            "Iteration 522, loss = 0.42130100\n",
            "Iteration 523, loss = 0.42115186\n",
            "Iteration 524, loss = 0.42098382\n",
            "Iteration 525, loss = 0.42082181\n",
            "Iteration 526, loss = 0.42068461\n",
            "Iteration 527, loss = 0.42052196\n",
            "Iteration 528, loss = 0.42038884\n",
            "Iteration 529, loss = 0.42024704\n",
            "Iteration 530, loss = 0.42008909\n",
            "Iteration 531, loss = 0.41995010\n",
            "Iteration 532, loss = 0.41978702\n",
            "Iteration 533, loss = 0.41969423\n",
            "Iteration 534, loss = 0.41951808\n",
            "Iteration 535, loss = 0.41938927\n",
            "Iteration 536, loss = 0.41926113\n",
            "Iteration 537, loss = 0.41911129\n",
            "Iteration 538, loss = 0.41899516\n",
            "Iteration 539, loss = 0.41885814\n",
            "Iteration 540, loss = 0.41877324\n",
            "Iteration 541, loss = 0.41859109\n",
            "Iteration 542, loss = 0.41845375\n",
            "Iteration 543, loss = 0.41833311\n",
            "Iteration 544, loss = 0.41820117\n",
            "Iteration 545, loss = 0.41806594\n",
            "Iteration 546, loss = 0.41794562\n",
            "Iteration 547, loss = 0.41779365\n",
            "Iteration 548, loss = 0.41765634\n",
            "Iteration 549, loss = 0.41753041\n",
            "Iteration 550, loss = 0.41740434\n",
            "Iteration 551, loss = 0.41727573\n",
            "Iteration 552, loss = 0.41714953\n",
            "Iteration 553, loss = 0.41702003\n",
            "Iteration 554, loss = 0.41689598\n",
            "Iteration 555, loss = 0.41677209\n",
            "Iteration 556, loss = 0.41663919\n",
            "Iteration 557, loss = 0.41650977\n",
            "Iteration 558, loss = 0.41636174\n",
            "Iteration 559, loss = 0.41628210\n",
            "Iteration 560, loss = 0.41612669\n",
            "Iteration 561, loss = 0.41604133\n",
            "Iteration 562, loss = 0.41592613\n",
            "Iteration 563, loss = 0.41580488\n",
            "Iteration 564, loss = 0.41571551\n",
            "Iteration 565, loss = 0.41560685\n",
            "Iteration 566, loss = 0.41549406\n",
            "Iteration 567, loss = 0.41537973\n",
            "Iteration 568, loss = 0.41524983\n",
            "Iteration 569, loss = 0.41515742\n",
            "Iteration 570, loss = 0.41502550\n",
            "Iteration 571, loss = 0.41493217\n",
            "Iteration 572, loss = 0.41482291\n",
            "Iteration 573, loss = 0.41471311\n",
            "Iteration 574, loss = 0.41461065\n",
            "Iteration 575, loss = 0.41451726\n",
            "Iteration 576, loss = 0.41441235\n",
            "Iteration 577, loss = 0.41428931\n",
            "Iteration 578, loss = 0.41419628\n",
            "Iteration 579, loss = 0.41411642\n",
            "Iteration 580, loss = 0.41399080\n",
            "Iteration 581, loss = 0.41390558\n",
            "Iteration 582, loss = 0.41381191\n",
            "Iteration 583, loss = 0.41372732\n",
            "Iteration 584, loss = 0.41363551\n",
            "Iteration 585, loss = 0.41353704\n",
            "Iteration 586, loss = 0.41344318\n",
            "Iteration 587, loss = 0.41333475\n",
            "Iteration 588, loss = 0.41324406\n",
            "Iteration 589, loss = 0.41315436\n",
            "Iteration 590, loss = 0.41304554\n",
            "Iteration 591, loss = 0.41294789\n",
            "Iteration 592, loss = 0.41289585\n",
            "Iteration 593, loss = 0.41277304\n",
            "Iteration 594, loss = 0.41269396\n",
            "Iteration 595, loss = 0.41259699\n",
            "Iteration 596, loss = 0.41250731\n",
            "Iteration 597, loss = 0.41241275\n",
            "Iteration 598, loss = 0.41232461\n",
            "Iteration 599, loss = 0.41232341\n",
            "Iteration 600, loss = 0.41219065\n",
            "Iteration 601, loss = 0.41213627\n",
            "Iteration 602, loss = 0.41204820\n",
            "Iteration 603, loss = 0.41195505\n",
            "Iteration 604, loss = 0.41185687\n",
            "Iteration 605, loss = 0.41182704\n",
            "Iteration 606, loss = 0.41172576\n",
            "Iteration 607, loss = 0.41165613\n",
            "Iteration 608, loss = 0.41155778\n",
            "Iteration 609, loss = 0.41148128\n",
            "Iteration 610, loss = 0.41140185\n",
            "Iteration 611, loss = 0.41132913\n",
            "Iteration 612, loss = 0.41125785\n",
            "Iteration 613, loss = 0.41117306\n",
            "Iteration 614, loss = 0.41108256\n",
            "Iteration 615, loss = 0.41103716\n",
            "Iteration 616, loss = 0.41097819\n",
            "Iteration 617, loss = 0.41091920\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.71662457\n",
            "Iteration 2, loss = 0.71558327\n",
            "Iteration 3, loss = 0.71448400\n",
            "Iteration 4, loss = 0.71340703\n",
            "Iteration 5, loss = 0.71233674\n",
            "Iteration 6, loss = 0.71135586\n",
            "Iteration 7, loss = 0.71045913\n",
            "Iteration 8, loss = 0.70937382\n",
            "Iteration 9, loss = 0.70856079\n",
            "Iteration 10, loss = 0.70761950\n",
            "Iteration 11, loss = 0.70683213\n",
            "Iteration 12, loss = 0.70603446\n",
            "Iteration 13, loss = 0.70526426\n",
            "Iteration 14, loss = 0.70454846\n",
            "Iteration 15, loss = 0.70386894\n",
            "Iteration 16, loss = 0.70322523\n",
            "Iteration 17, loss = 0.70253326\n",
            "Iteration 18, loss = 0.70191051\n",
            "Iteration 19, loss = 0.70135830\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.71663585\n",
            "Iteration 2, loss = 0.71557175\n",
            "Iteration 3, loss = 0.71447967\n",
            "Iteration 4, loss = 0.71345146\n",
            "Iteration 5, loss = 0.71241309\n",
            "Iteration 6, loss = 0.71142451\n",
            "Iteration 7, loss = 0.71049352\n",
            "Iteration 8, loss = 0.70948338\n",
            "Iteration 9, loss = 0.70860640\n",
            "Iteration 10, loss = 0.70769722\n",
            "Iteration 11, loss = 0.70687110\n",
            "Iteration 12, loss = 0.70611620\n",
            "Iteration 13, loss = 0.70528787\n",
            "Iteration 14, loss = 0.70455687\n",
            "Iteration 15, loss = 0.70385817\n",
            "Iteration 16, loss = 0.70321043\n",
            "Iteration 17, loss = 0.70249926\n",
            "Iteration 18, loss = 0.70184018\n",
            "Iteration 19, loss = 0.70132572\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.71666287\n",
            "Iteration 2, loss = 0.71556160\n",
            "Iteration 3, loss = 0.71441904\n",
            "Iteration 4, loss = 0.71336538\n",
            "Iteration 5, loss = 0.71227163\n",
            "Iteration 6, loss = 0.71127277\n",
            "Iteration 7, loss = 0.71031681\n",
            "Iteration 8, loss = 0.70936848\n",
            "Iteration 9, loss = 0.70839615\n",
            "Iteration 10, loss = 0.70751833\n",
            "Iteration 11, loss = 0.70663513\n",
            "Iteration 12, loss = 0.70584712\n",
            "Iteration 13, loss = 0.70502856\n",
            "Iteration 14, loss = 0.70427203\n",
            "Iteration 15, loss = 0.70351577\n",
            "Iteration 16, loss = 0.70287164\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.71668601\n",
            "Iteration 2, loss = 0.71552948\n",
            "Iteration 3, loss = 0.71438543\n",
            "Iteration 4, loss = 0.71331898\n",
            "Iteration 5, loss = 0.71219458\n",
            "Iteration 6, loss = 0.71114544\n",
            "Iteration 7, loss = 0.71022420\n",
            "Iteration 8, loss = 0.70926848\n",
            "Iteration 9, loss = 0.70829431\n",
            "Iteration 10, loss = 0.70739169\n",
            "Iteration 11, loss = 0.70654392\n",
            "Iteration 12, loss = 0.70572835\n",
            "Iteration 13, loss = 0.70500687\n",
            "Iteration 14, loss = 0.70418685\n",
            "Iteration 15, loss = 0.70354299\n",
            "Iteration 16, loss = 0.70286694\n",
            "Iteration 17, loss = 0.70223884\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.71663519\n",
            "Iteration 2, loss = 0.71557696\n",
            "Iteration 3, loss = 0.71449648\n",
            "Iteration 4, loss = 0.71344198\n",
            "Iteration 5, loss = 0.71238511\n",
            "Iteration 6, loss = 0.71133791\n",
            "Iteration 7, loss = 0.71037898\n",
            "Iteration 8, loss = 0.70947064\n",
            "Iteration 9, loss = 0.70845961\n",
            "Iteration 10, loss = 0.70758910\n",
            "Iteration 11, loss = 0.70678246\n",
            "Iteration 12, loss = 0.70594977\n",
            "Iteration 13, loss = 0.70520965\n",
            "Iteration 14, loss = 0.70439865\n",
            "Iteration 15, loss = 0.70376180\n",
            "Iteration 16, loss = 0.70311275\n",
            "Iteration 17, loss = 0.70250225\n",
            "Iteration 18, loss = 0.70185428\n",
            "Iteration 19, loss = 0.70123690\n",
            "Iteration 20, loss = 0.70069557\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.71667603\n",
            "Iteration 2, loss = 0.71559423\n",
            "Iteration 3, loss = 0.71437674\n",
            "Iteration 4, loss = 0.71335734\n",
            "Iteration 5, loss = 0.71221318\n",
            "Iteration 6, loss = 0.71130978\n",
            "Iteration 7, loss = 0.71025995\n",
            "Iteration 8, loss = 0.70938996\n",
            "Iteration 9, loss = 0.70836877\n",
            "Iteration 10, loss = 0.70756772\n",
            "Iteration 11, loss = 0.70669141\n",
            "Iteration 12, loss = 0.70592366\n",
            "Iteration 13, loss = 0.70514384\n",
            "Iteration 14, loss = 0.70439348\n",
            "Iteration 15, loss = 0.70370222\n",
            "Iteration 16, loss = 0.70305212\n",
            "Iteration 17, loss = 0.70242707\n",
            "Iteration 18, loss = 0.70173405\n",
            "Iteration 19, loss = 0.70114060\n",
            "Iteration 20, loss = 0.70053021\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.71657573\n",
            "Iteration 2, loss = 0.71565874\n",
            "Iteration 3, loss = 0.71448115\n",
            "Iteration 4, loss = 0.71347398\n",
            "Iteration 5, loss = 0.71237619\n",
            "Iteration 6, loss = 0.71143606\n",
            "Iteration 7, loss = 0.71038825\n",
            "Iteration 8, loss = 0.70954211\n",
            "Iteration 9, loss = 0.70849149\n",
            "Iteration 10, loss = 0.70773278\n",
            "Iteration 11, loss = 0.70681970\n",
            "Iteration 12, loss = 0.70604967\n",
            "Iteration 13, loss = 0.70533603\n",
            "Iteration 14, loss = 0.70449123\n",
            "Iteration 15, loss = 0.70379363\n",
            "Iteration 16, loss = 0.70314333\n",
            "Iteration 17, loss = 0.70253867\n",
            "Iteration 18, loss = 0.70188220\n",
            "Iteration 19, loss = 0.70129819\n",
            "Iteration 20, loss = 0.70072890\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.71664268\n",
            "Iteration 2, loss = 0.71559298\n",
            "Iteration 3, loss = 0.71444289\n",
            "Iteration 4, loss = 0.71337931\n",
            "Iteration 5, loss = 0.71225573\n",
            "Iteration 6, loss = 0.71128171\n",
            "Iteration 7, loss = 0.71024947\n",
            "Iteration 8, loss = 0.70937098\n",
            "Iteration 9, loss = 0.70839796\n",
            "Iteration 10, loss = 0.70756519\n",
            "Iteration 11, loss = 0.70664584\n",
            "Iteration 12, loss = 0.70592521\n",
            "Iteration 13, loss = 0.70519011\n",
            "Iteration 14, loss = 0.70440313\n",
            "Iteration 15, loss = 0.70367570\n",
            "Iteration 16, loss = 0.70306183\n",
            "Iteration 17, loss = 0.70242838\n",
            "Iteration 18, loss = 0.70176612\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.71663960\n",
            "Iteration 2, loss = 0.71558295\n",
            "Iteration 3, loss = 0.71439681\n",
            "Iteration 4, loss = 0.71340010\n",
            "Iteration 5, loss = 0.71224782\n",
            "Iteration 6, loss = 0.71129121\n",
            "Iteration 7, loss = 0.71025197\n",
            "Iteration 8, loss = 0.70942711\n",
            "Iteration 9, loss = 0.70843241\n",
            "Iteration 10, loss = 0.70762628\n",
            "Iteration 11, loss = 0.70673531\n",
            "Iteration 12, loss = 0.70598992\n",
            "Iteration 13, loss = 0.70521057\n",
            "Iteration 14, loss = 0.70447081\n",
            "Iteration 15, loss = 0.70373762\n",
            "Iteration 16, loss = 0.70312063\n",
            "Iteration 17, loss = 0.70247136\n",
            "Iteration 18, loss = 0.70180330\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.71664906\n",
            "Iteration 2, loss = 0.71560193\n",
            "Iteration 3, loss = 0.71444780\n",
            "Iteration 4, loss = 0.71342039\n",
            "Iteration 5, loss = 0.71228185\n",
            "Iteration 6, loss = 0.71130386\n",
            "Iteration 7, loss = 0.71033805\n",
            "Iteration 8, loss = 0.70945952\n",
            "Iteration 9, loss = 0.70840546\n",
            "Iteration 10, loss = 0.70763717\n",
            "Iteration 11, loss = 0.70675532\n",
            "Iteration 12, loss = 0.70598041\n",
            "Iteration 13, loss = 0.70519632\n",
            "Iteration 14, loss = 0.70444607\n",
            "Iteration 15, loss = 0.70369516\n",
            "Iteration 16, loss = 0.70307803\n",
            "Iteration 17, loss = 0.70242416\n",
            "Iteration 18, loss = 0.70176272\n",
            "Iteration 19, loss = 0.70116749\n",
            "Iteration 20, loss = 0.70058482\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.88736743\n",
            "Iteration 2, loss = 0.88235151\n",
            "Iteration 3, loss = 0.87737107\n",
            "Iteration 4, loss = 0.87262263\n",
            "Iteration 5, loss = 0.86780539\n",
            "Iteration 6, loss = 0.86316987\n",
            "Iteration 7, loss = 0.85863161\n",
            "Iteration 8, loss = 0.85433307\n",
            "Iteration 9, loss = 0.85010014\n",
            "Iteration 10, loss = 0.84585866\n",
            "Iteration 11, loss = 0.84166943\n",
            "Iteration 12, loss = 0.83750662\n",
            "Iteration 13, loss = 0.83359658\n",
            "Iteration 14, loss = 0.82977218\n",
            "Iteration 15, loss = 0.82585888\n",
            "Iteration 16, loss = 0.82181269\n",
            "Iteration 17, loss = 0.81809385\n",
            "Iteration 18, loss = 0.81420008\n",
            "Iteration 19, loss = 0.81036193\n",
            "Iteration 20, loss = 0.80657033\n",
            "Iteration 21, loss = 0.80267679\n",
            "Iteration 22, loss = 0.79903060\n",
            "Iteration 23, loss = 0.79515809\n",
            "Iteration 24, loss = 0.79114745\n",
            "Iteration 25, loss = 0.78734695\n",
            "Iteration 26, loss = 0.78370396\n",
            "Iteration 27, loss = 0.77967517\n",
            "Iteration 28, loss = 0.77586700\n",
            "Iteration 29, loss = 0.77194068\n",
            "Iteration 30, loss = 0.76805334\n",
            "Iteration 31, loss = 0.76421680\n",
            "Iteration 32, loss = 0.76033287\n",
            "Iteration 33, loss = 0.75655665\n",
            "Iteration 34, loss = 0.75270872\n",
            "Iteration 35, loss = 0.74893593\n",
            "Iteration 36, loss = 0.74517253\n",
            "Iteration 37, loss = 0.74138385\n",
            "Iteration 38, loss = 0.73770966\n",
            "Iteration 39, loss = 0.73430414\n",
            "Iteration 40, loss = 0.73067919\n",
            "Iteration 41, loss = 0.72733264\n",
            "Iteration 42, loss = 0.72390303\n",
            "Iteration 43, loss = 0.72054882\n",
            "Iteration 44, loss = 0.71743380\n",
            "Iteration 45, loss = 0.71451951\n",
            "Iteration 46, loss = 0.71136407\n",
            "Iteration 47, loss = 0.70857483\n",
            "Iteration 48, loss = 0.70564205\n",
            "Iteration 49, loss = 0.70312952\n",
            "Iteration 50, loss = 0.70039840\n",
            "Iteration 51, loss = 0.69772912\n",
            "Iteration 52, loss = 0.69534710\n",
            "Iteration 53, loss = 0.69287630\n",
            "Iteration 54, loss = 0.69064439\n",
            "Iteration 55, loss = 0.68822562\n",
            "Iteration 56, loss = 0.68604193\n",
            "Iteration 57, loss = 0.68381469\n",
            "Iteration 58, loss = 0.68163453\n",
            "Iteration 59, loss = 0.67952471\n",
            "Iteration 60, loss = 0.67728788\n",
            "Iteration 61, loss = 0.67523035\n",
            "Iteration 62, loss = 0.67315497\n",
            "Iteration 63, loss = 0.67099820\n",
            "Iteration 64, loss = 0.66902389\n",
            "Iteration 65, loss = 0.66685950\n",
            "Iteration 66, loss = 0.66472340\n",
            "Iteration 67, loss = 0.66265728\n",
            "Iteration 68, loss = 0.66051258\n",
            "Iteration 69, loss = 0.65829251\n",
            "Iteration 70, loss = 0.65620183\n",
            "Iteration 71, loss = 0.65406169\n",
            "Iteration 72, loss = 0.65193517\n",
            "Iteration 73, loss = 0.64970122\n",
            "Iteration 74, loss = 0.64748963\n",
            "Iteration 75, loss = 0.64541345\n",
            "Iteration 76, loss = 0.64315211\n",
            "Iteration 77, loss = 0.64106225\n",
            "Iteration 78, loss = 0.63879388\n",
            "Iteration 79, loss = 0.63660583\n",
            "Iteration 80, loss = 0.63448996\n",
            "Iteration 81, loss = 0.63226256\n",
            "Iteration 82, loss = 0.63015951\n",
            "Iteration 83, loss = 0.62791230\n",
            "Iteration 84, loss = 0.62573140\n",
            "Iteration 85, loss = 0.62348018\n",
            "Iteration 86, loss = 0.62122615\n",
            "Iteration 87, loss = 0.61907918\n",
            "Iteration 88, loss = 0.61691279\n",
            "Iteration 89, loss = 0.61463324\n",
            "Iteration 90, loss = 0.61250685\n",
            "Iteration 91, loss = 0.61023572\n",
            "Iteration 92, loss = 0.60793514\n",
            "Iteration 93, loss = 0.60574367\n",
            "Iteration 94, loss = 0.60339491\n",
            "Iteration 95, loss = 0.60109649\n",
            "Iteration 96, loss = 0.59882432\n",
            "Iteration 97, loss = 0.59653688\n",
            "Iteration 98, loss = 0.59415471\n",
            "Iteration 99, loss = 0.59189558\n",
            "Iteration 100, loss = 0.58953310\n",
            "Iteration 101, loss = 0.58731663\n",
            "Iteration 102, loss = 0.58485993\n",
            "Iteration 103, loss = 0.58248609\n",
            "Iteration 104, loss = 0.58014631\n",
            "Iteration 105, loss = 0.57777666\n",
            "Iteration 106, loss = 0.57541156\n",
            "Iteration 107, loss = 0.57318585\n",
            "Iteration 108, loss = 0.57080097\n",
            "Iteration 109, loss = 0.56844951\n",
            "Iteration 110, loss = 0.56611438\n",
            "Iteration 111, loss = 0.56371231\n",
            "Iteration 112, loss = 0.56139332\n",
            "Iteration 113, loss = 0.55904167\n",
            "Iteration 114, loss = 0.55679914\n",
            "Iteration 115, loss = 0.55438792\n",
            "Iteration 116, loss = 0.55211020\n",
            "Iteration 117, loss = 0.54974357\n",
            "Iteration 118, loss = 0.54749563\n",
            "Iteration 119, loss = 0.54516930\n",
            "Iteration 120, loss = 0.54284215\n",
            "Iteration 121, loss = 0.54066200\n",
            "Iteration 122, loss = 0.53835628\n",
            "Iteration 123, loss = 0.53597859\n",
            "Iteration 124, loss = 0.53374317\n",
            "Iteration 125, loss = 0.53144677\n",
            "Iteration 126, loss = 0.52913799\n",
            "Iteration 127, loss = 0.52691342\n",
            "Iteration 128, loss = 0.52469992\n",
            "Iteration 129, loss = 0.52249909\n",
            "Iteration 130, loss = 0.52034188\n",
            "Iteration 131, loss = 0.51806544\n",
            "Iteration 132, loss = 0.51586126\n",
            "Iteration 133, loss = 0.51380687\n",
            "Iteration 134, loss = 0.51167285\n",
            "Iteration 135, loss = 0.50962701\n",
            "Iteration 136, loss = 0.50762594\n",
            "Iteration 137, loss = 0.50555416\n",
            "Iteration 138, loss = 0.50354872\n",
            "Iteration 139, loss = 0.50167321\n",
            "Iteration 140, loss = 0.49966996\n",
            "Iteration 141, loss = 0.49779591\n",
            "Iteration 142, loss = 0.49591783\n",
            "Iteration 143, loss = 0.49412814\n",
            "Iteration 144, loss = 0.49232142\n",
            "Iteration 145, loss = 0.49055445\n",
            "Iteration 146, loss = 0.48888060\n",
            "Iteration 147, loss = 0.48711258\n",
            "Iteration 148, loss = 0.48555211\n",
            "Iteration 149, loss = 0.48392715\n",
            "Iteration 150, loss = 0.48238371\n",
            "Iteration 151, loss = 0.48082929\n",
            "Iteration 152, loss = 0.47929527\n",
            "Iteration 153, loss = 0.47795850\n",
            "Iteration 154, loss = 0.47637744\n",
            "Iteration 155, loss = 0.47495540\n",
            "Iteration 156, loss = 0.47366131\n",
            "Iteration 157, loss = 0.47234153\n",
            "Iteration 158, loss = 0.47102631\n",
            "Iteration 159, loss = 0.46986568\n",
            "Iteration 160, loss = 0.46850652\n",
            "Iteration 161, loss = 0.46739906\n",
            "Iteration 162, loss = 0.46617411\n",
            "Iteration 163, loss = 0.46504773\n",
            "Iteration 164, loss = 0.46400598\n",
            "Iteration 165, loss = 0.46286254\n",
            "Iteration 166, loss = 0.46190615\n",
            "Iteration 167, loss = 0.46086102\n",
            "Iteration 168, loss = 0.45984918\n",
            "Iteration 169, loss = 0.45892022\n",
            "Iteration 170, loss = 0.45795134\n",
            "Iteration 171, loss = 0.45701732\n",
            "Iteration 172, loss = 0.45609618\n",
            "Iteration 173, loss = 0.45523878\n",
            "Iteration 174, loss = 0.45443736\n",
            "Iteration 175, loss = 0.45367201\n",
            "Iteration 176, loss = 0.45282068\n",
            "Iteration 177, loss = 0.45215267\n",
            "Iteration 178, loss = 0.45129668\n",
            "Iteration 179, loss = 0.45056037\n",
            "Iteration 180, loss = 0.44989207\n",
            "Iteration 181, loss = 0.44919334\n",
            "Iteration 182, loss = 0.44848540\n",
            "Iteration 183, loss = 0.44787490\n",
            "Iteration 184, loss = 0.44727313\n",
            "Iteration 185, loss = 0.44662589\n",
            "Iteration 186, loss = 0.44600036\n",
            "Iteration 187, loss = 0.44546576\n",
            "Iteration 188, loss = 0.44486291\n",
            "Iteration 189, loss = 0.44430205\n",
            "Iteration 190, loss = 0.44380415\n",
            "Iteration 191, loss = 0.44325749\n",
            "Iteration 192, loss = 0.44274298\n",
            "Iteration 193, loss = 0.44222605\n",
            "Iteration 194, loss = 0.44177090\n",
            "Iteration 195, loss = 0.44138606\n",
            "Iteration 196, loss = 0.44080489\n",
            "Iteration 197, loss = 0.44036086\n",
            "Iteration 198, loss = 0.44002866\n",
            "Iteration 199, loss = 0.43957342\n",
            "Iteration 200, loss = 0.43911181\n",
            "Iteration 201, loss = 0.43866524\n",
            "Iteration 202, loss = 0.43830058\n",
            "Iteration 203, loss = 0.43788268\n",
            "Iteration 204, loss = 0.43746851\n",
            "Iteration 205, loss = 0.43715143\n",
            "Iteration 206, loss = 0.43672769\n",
            "Iteration 207, loss = 0.43634208\n",
            "Iteration 208, loss = 0.43602588\n",
            "Iteration 209, loss = 0.43563965\n",
            "Iteration 210, loss = 0.43530936\n",
            "Iteration 211, loss = 0.43490232\n",
            "Iteration 212, loss = 0.43459637\n",
            "Iteration 213, loss = 0.43424552\n",
            "Iteration 214, loss = 0.43394281\n",
            "Iteration 215, loss = 0.43353629\n",
            "Iteration 216, loss = 0.43327247\n",
            "Iteration 217, loss = 0.43292309\n",
            "Iteration 218, loss = 0.43258651\n",
            "Iteration 219, loss = 0.43235286\n",
            "Iteration 220, loss = 0.43193249\n",
            "Iteration 221, loss = 0.43164342\n",
            "Iteration 222, loss = 0.43130925\n",
            "Iteration 223, loss = 0.43106967\n",
            "Iteration 224, loss = 0.43074272\n",
            "Iteration 225, loss = 0.43040409\n",
            "Iteration 226, loss = 0.43011081\n",
            "Iteration 227, loss = 0.42987218\n",
            "Iteration 228, loss = 0.42953521\n",
            "Iteration 229, loss = 0.42931606\n",
            "Iteration 230, loss = 0.42892780\n",
            "Iteration 231, loss = 0.42866369\n",
            "Iteration 232, loss = 0.42834392\n",
            "Iteration 233, loss = 0.42803916\n",
            "Iteration 234, loss = 0.42777740\n",
            "Iteration 235, loss = 0.42746116\n",
            "Iteration 236, loss = 0.42718029\n",
            "Iteration 237, loss = 0.42691856\n",
            "Iteration 238, loss = 0.42661676\n",
            "Iteration 239, loss = 0.42644427\n",
            "Iteration 240, loss = 0.42608574\n",
            "Iteration 241, loss = 0.42570148\n",
            "Iteration 242, loss = 0.42539593\n",
            "Iteration 243, loss = 0.42510304\n",
            "Iteration 244, loss = 0.42485064\n",
            "Iteration 245, loss = 0.42459929\n",
            "Iteration 246, loss = 0.42425051\n",
            "Iteration 247, loss = 0.42390450\n",
            "Iteration 248, loss = 0.42364818\n",
            "Iteration 249, loss = 0.42337942\n",
            "Iteration 250, loss = 0.42301973\n",
            "Iteration 251, loss = 0.42278555\n",
            "Iteration 252, loss = 0.42245754\n",
            "Iteration 253, loss = 0.42214886\n",
            "Iteration 254, loss = 0.42182942\n",
            "Iteration 255, loss = 0.42152511\n",
            "Iteration 256, loss = 0.42124492\n",
            "Iteration 257, loss = 0.42092589\n",
            "Iteration 258, loss = 0.42066462\n",
            "Iteration 259, loss = 0.42034120\n",
            "Iteration 260, loss = 0.42009489\n",
            "Iteration 261, loss = 0.41976059\n",
            "Iteration 262, loss = 0.41950314\n",
            "Iteration 263, loss = 0.41915774\n",
            "Iteration 264, loss = 0.41883818\n",
            "Iteration 265, loss = 0.41861300\n",
            "Iteration 266, loss = 0.41825887\n",
            "Iteration 267, loss = 0.41801747\n",
            "Iteration 268, loss = 0.41773162\n",
            "Iteration 269, loss = 0.41738582\n",
            "Iteration 270, loss = 0.41714401\n",
            "Iteration 271, loss = 0.41676500\n",
            "Iteration 272, loss = 0.41670662\n",
            "Iteration 273, loss = 0.41621524\n",
            "Iteration 274, loss = 0.41590888\n",
            "Iteration 275, loss = 0.41565753\n",
            "Iteration 276, loss = 0.41528597\n",
            "Iteration 277, loss = 0.41505093\n",
            "Iteration 278, loss = 0.41478643\n",
            "Iteration 279, loss = 0.41447702\n",
            "Iteration 280, loss = 0.41420943\n",
            "Iteration 281, loss = 0.41392067\n",
            "Iteration 282, loss = 0.41361741\n",
            "Iteration 283, loss = 0.41337568\n",
            "Iteration 284, loss = 0.41312274\n",
            "Iteration 285, loss = 0.41286421\n",
            "Iteration 286, loss = 0.41255153\n",
            "Iteration 287, loss = 0.41229218\n",
            "Iteration 288, loss = 0.41195582\n",
            "Iteration 289, loss = 0.41181468\n",
            "Iteration 290, loss = 0.41143512\n",
            "Iteration 291, loss = 0.41125030\n",
            "Iteration 292, loss = 0.41093436\n",
            "Iteration 293, loss = 0.41063515\n",
            "Iteration 294, loss = 0.41037535\n",
            "Iteration 295, loss = 0.41017100\n",
            "Iteration 296, loss = 0.40983612\n",
            "Iteration 297, loss = 0.40966268\n",
            "Iteration 298, loss = 0.40934857\n",
            "Iteration 299, loss = 0.40917310\n",
            "Iteration 300, loss = 0.40880222\n",
            "Iteration 301, loss = 0.40852110\n",
            "Iteration 302, loss = 0.40824827\n",
            "Iteration 303, loss = 0.40804868\n",
            "Iteration 304, loss = 0.40775217\n",
            "Iteration 305, loss = 0.40749645\n",
            "Iteration 306, loss = 0.40727134\n",
            "Iteration 307, loss = 0.40701992\n",
            "Iteration 308, loss = 0.40676258\n",
            "Iteration 309, loss = 0.40651520\n",
            "Iteration 310, loss = 0.40631161\n",
            "Iteration 311, loss = 0.40603625\n",
            "Iteration 312, loss = 0.40581487\n",
            "Iteration 313, loss = 0.40554160\n",
            "Iteration 314, loss = 0.40530041\n",
            "Iteration 315, loss = 0.40505698\n",
            "Iteration 316, loss = 0.40483083\n",
            "Iteration 317, loss = 0.40466115\n",
            "Iteration 318, loss = 0.40439413\n",
            "Iteration 319, loss = 0.40422276\n",
            "Iteration 320, loss = 0.40388969\n",
            "Iteration 321, loss = 0.40366511\n",
            "Iteration 322, loss = 0.40344139\n",
            "Iteration 323, loss = 0.40319692\n",
            "Iteration 324, loss = 0.40297627\n",
            "Iteration 325, loss = 0.40283221\n",
            "Iteration 326, loss = 0.40255282\n",
            "Iteration 327, loss = 0.40227889\n",
            "Iteration 328, loss = 0.40212229\n",
            "Iteration 329, loss = 0.40191726\n",
            "Iteration 330, loss = 0.40169621\n",
            "Iteration 331, loss = 0.40150463\n",
            "Iteration 332, loss = 0.40134001\n",
            "Iteration 333, loss = 0.40101174\n",
            "Iteration 334, loss = 0.40091229\n",
            "Iteration 335, loss = 0.40070174\n",
            "Iteration 336, loss = 0.40046962\n",
            "Iteration 337, loss = 0.40016768\n",
            "Iteration 338, loss = 0.39998494\n",
            "Iteration 339, loss = 0.39987204\n",
            "Iteration 340, loss = 0.39952935\n",
            "Iteration 341, loss = 0.39936852\n",
            "Iteration 342, loss = 0.39913556\n",
            "Iteration 343, loss = 0.39893025\n",
            "Iteration 344, loss = 0.39879437\n",
            "Iteration 345, loss = 0.39858822\n",
            "Iteration 346, loss = 0.39843192\n",
            "Iteration 347, loss = 0.39821120\n",
            "Iteration 348, loss = 0.39795986\n",
            "Iteration 349, loss = 0.39784549\n",
            "Iteration 350, loss = 0.39758949\n",
            "Iteration 351, loss = 0.39742489\n",
            "Iteration 352, loss = 0.39727142\n",
            "Iteration 353, loss = 0.39707299\n",
            "Iteration 354, loss = 0.39689868\n",
            "Iteration 355, loss = 0.39670185\n",
            "Iteration 356, loss = 0.39650523\n",
            "Iteration 357, loss = 0.39631080\n",
            "Iteration 358, loss = 0.39613924\n",
            "Iteration 359, loss = 0.39604892\n",
            "Iteration 360, loss = 0.39590894\n",
            "Iteration 361, loss = 0.39559833\n",
            "Iteration 362, loss = 0.39542003\n",
            "Iteration 363, loss = 0.39538867\n",
            "Iteration 364, loss = 0.39510309\n",
            "Iteration 365, loss = 0.39491368\n",
            "Iteration 366, loss = 0.39493357\n",
            "Iteration 367, loss = 0.39458689\n",
            "Iteration 368, loss = 0.39449519\n",
            "Iteration 369, loss = 0.39423026\n",
            "Iteration 370, loss = 0.39403974\n",
            "Iteration 371, loss = 0.39392273\n",
            "Iteration 372, loss = 0.39381355\n",
            "Iteration 373, loss = 0.39360404\n",
            "Iteration 374, loss = 0.39340572\n",
            "Iteration 375, loss = 0.39341495\n",
            "Iteration 376, loss = 0.39309876\n",
            "Iteration 377, loss = 0.39293746\n",
            "Iteration 378, loss = 0.39276582\n",
            "Iteration 379, loss = 0.39263285\n",
            "Iteration 380, loss = 0.39244316\n",
            "Iteration 381, loss = 0.39231406\n",
            "Iteration 382, loss = 0.39217639\n",
            "Iteration 383, loss = 0.39212700\n",
            "Iteration 384, loss = 0.39188799\n",
            "Iteration 385, loss = 0.39172005\n",
            "Iteration 386, loss = 0.39163114\n",
            "Iteration 387, loss = 0.39141959\n",
            "Iteration 388, loss = 0.39126264\n",
            "Iteration 389, loss = 0.39115301\n",
            "Iteration 390, loss = 0.39094784\n",
            "Iteration 391, loss = 0.39083312\n",
            "Iteration 392, loss = 0.39070180\n",
            "Iteration 393, loss = 0.39053051\n",
            "Iteration 394, loss = 0.39038353\n",
            "Iteration 395, loss = 0.39029885\n",
            "Iteration 396, loss = 0.39011692\n",
            "Iteration 397, loss = 0.38997689\n",
            "Iteration 398, loss = 0.38986068\n",
            "Iteration 399, loss = 0.38974750\n",
            "Iteration 400, loss = 0.38958420\n",
            "Iteration 401, loss = 0.38943918\n",
            "Iteration 402, loss = 0.38932416\n",
            "Iteration 403, loss = 0.38918195\n",
            "Iteration 404, loss = 0.38911093\n",
            "Iteration 405, loss = 0.38888136\n",
            "Iteration 406, loss = 0.38886538\n",
            "Iteration 407, loss = 0.38865520\n",
            "Iteration 408, loss = 0.38855090\n",
            "Iteration 409, loss = 0.38838917\n",
            "Iteration 410, loss = 0.38828601\n",
            "Iteration 411, loss = 0.38823328\n",
            "Iteration 412, loss = 0.38797700\n",
            "Iteration 413, loss = 0.38789441\n",
            "Iteration 414, loss = 0.38784739\n",
            "Iteration 415, loss = 0.38763237\n",
            "Iteration 416, loss = 0.38750307\n",
            "Iteration 417, loss = 0.38747317\n",
            "Iteration 418, loss = 0.38724484\n",
            "Iteration 419, loss = 0.38711764\n",
            "Iteration 420, loss = 0.38701887\n",
            "Iteration 421, loss = 0.38697815\n",
            "Iteration 422, loss = 0.38686172\n",
            "Iteration 423, loss = 0.38665189\n",
            "Iteration 424, loss = 0.38658423\n",
            "Iteration 425, loss = 0.38653714\n",
            "Iteration 426, loss = 0.38633374\n",
            "Iteration 427, loss = 0.38621244\n",
            "Iteration 428, loss = 0.38610033\n",
            "Iteration 429, loss = 0.38596285\n",
            "Iteration 430, loss = 0.38586642\n",
            "Iteration 431, loss = 0.38584093\n",
            "Iteration 432, loss = 0.38570555\n",
            "Iteration 433, loss = 0.38554840\n",
            "Iteration 434, loss = 0.38545481\n",
            "Iteration 435, loss = 0.38532805\n",
            "Iteration 436, loss = 0.38531191\n",
            "Iteration 437, loss = 0.38517250\n",
            "Iteration 438, loss = 0.38500082\n",
            "Iteration 439, loss = 0.38495654\n",
            "Iteration 440, loss = 0.38475438\n",
            "Iteration 441, loss = 0.38465848\n",
            "Iteration 442, loss = 0.38487715\n",
            "Iteration 443, loss = 0.38449576\n",
            "Iteration 444, loss = 0.38440502\n",
            "Iteration 445, loss = 0.38449874\n",
            "Iteration 446, loss = 0.38433669\n",
            "Iteration 447, loss = 0.38404902\n",
            "Iteration 448, loss = 0.38395301\n",
            "Iteration 449, loss = 0.38385327\n",
            "Iteration 450, loss = 0.38377244\n",
            "Iteration 451, loss = 0.38363978\n",
            "Iteration 452, loss = 0.38357051\n",
            "Iteration 453, loss = 0.38364168\n",
            "Iteration 454, loss = 0.38347133\n",
            "Iteration 455, loss = 0.38332235\n",
            "Iteration 456, loss = 0.38319286\n",
            "Iteration 457, loss = 0.38305200\n",
            "Iteration 458, loss = 0.38300343\n",
            "Iteration 459, loss = 0.38291340\n",
            "Iteration 460, loss = 0.38284480\n",
            "Iteration 461, loss = 0.38272239\n",
            "Iteration 462, loss = 0.38264041\n",
            "Iteration 463, loss = 0.38254592\n",
            "Iteration 464, loss = 0.38244392\n",
            "Iteration 465, loss = 0.38236231\n",
            "Iteration 466, loss = 0.38231692\n",
            "Iteration 467, loss = 0.38226799\n",
            "Iteration 468, loss = 0.38211719\n",
            "Iteration 469, loss = 0.38202186\n",
            "Iteration 470, loss = 0.38192473\n",
            "Iteration 471, loss = 0.38181281\n",
            "Iteration 472, loss = 0.38177200\n",
            "Iteration 473, loss = 0.38168768\n",
            "Iteration 474, loss = 0.38156232\n",
            "Iteration 475, loss = 0.38146653\n",
            "Iteration 476, loss = 0.38138172\n",
            "Iteration 477, loss = 0.38137378\n",
            "Iteration 478, loss = 0.38120951\n",
            "Iteration 479, loss = 0.38114096\n",
            "Iteration 480, loss = 0.38104089\n",
            "Iteration 481, loss = 0.38096526\n",
            "Iteration 482, loss = 0.38089281\n",
            "Iteration 483, loss = 0.38087969\n",
            "Iteration 484, loss = 0.38082513\n",
            "Iteration 485, loss = 0.38068814\n",
            "Iteration 486, loss = 0.38057861\n",
            "Iteration 487, loss = 0.38060908\n",
            "Iteration 488, loss = 0.38046404\n",
            "Iteration 489, loss = 0.38034637\n",
            "Iteration 490, loss = 0.38038762\n",
            "Iteration 491, loss = 0.38017755\n",
            "Iteration 492, loss = 0.38014890\n",
            "Iteration 493, loss = 0.38005751\n",
            "Iteration 494, loss = 0.38006308\n",
            "Iteration 495, loss = 0.37985431\n",
            "Iteration 496, loss = 0.37977169\n",
            "Iteration 497, loss = 0.37966724\n",
            "Iteration 498, loss = 0.37972884\n",
            "Iteration 499, loss = 0.37967734\n",
            "Iteration 500, loss = 0.37956632\n",
            "Iteration 501, loss = 0.37949213\n",
            "Iteration 502, loss = 0.37947405\n",
            "Iteration 503, loss = 0.37934565\n",
            "Iteration 504, loss = 0.37922340\n",
            "Iteration 505, loss = 0.37915470\n",
            "Iteration 506, loss = 0.37909663\n",
            "Iteration 507, loss = 0.37914490\n",
            "Iteration 508, loss = 0.37892838\n",
            "Iteration 509, loss = 0.37911588\n",
            "Iteration 510, loss = 0.37878721\n",
            "Iteration 511, loss = 0.37875883\n",
            "Iteration 512, loss = 0.37865619\n",
            "Iteration 513, loss = 0.37870705\n",
            "Iteration 514, loss = 0.37855426\n",
            "Iteration 515, loss = 0.37850564\n",
            "Iteration 516, loss = 0.37844389\n",
            "Iteration 517, loss = 0.37836515\n",
            "Iteration 518, loss = 0.37824326\n",
            "Iteration 519, loss = 0.37823938\n",
            "Iteration 520, loss = 0.37813740\n",
            "Iteration 521, loss = 0.37806509\n",
            "Iteration 522, loss = 0.37819322\n",
            "Iteration 523, loss = 0.37799362\n",
            "Iteration 524, loss = 0.37788132\n",
            "Iteration 525, loss = 0.37783534\n",
            "Iteration 526, loss = 0.37776956\n",
            "Iteration 527, loss = 0.37769271\n",
            "Iteration 528, loss = 0.37764481\n",
            "Iteration 529, loss = 0.37753747\n",
            "Iteration 530, loss = 0.37750371\n",
            "Iteration 531, loss = 0.37746232\n",
            "Iteration 532, loss = 0.37761008\n",
            "Iteration 533, loss = 0.37735483\n",
            "Iteration 534, loss = 0.37730741\n",
            "Iteration 535, loss = 0.37725909\n",
            "Iteration 536, loss = 0.37713215\n",
            "Iteration 537, loss = 0.37714777\n",
            "Iteration 538, loss = 0.37705187\n",
            "Iteration 539, loss = 0.37697086\n",
            "Iteration 540, loss = 0.37697227\n",
            "Iteration 541, loss = 0.37685535\n",
            "Iteration 542, loss = 0.37689754\n",
            "Iteration 543, loss = 0.37674571\n",
            "Iteration 544, loss = 0.37673440\n",
            "Iteration 545, loss = 0.37666843\n",
            "Iteration 546, loss = 0.37653848\n",
            "Iteration 547, loss = 0.37654294\n",
            "Iteration 548, loss = 0.37653781\n",
            "Iteration 549, loss = 0.37641361\n",
            "Iteration 550, loss = 0.37639090\n",
            "Iteration 551, loss = 0.37628629\n",
            "Iteration 552, loss = 0.37625317\n",
            "Iteration 553, loss = 0.37620301\n",
            "Iteration 554, loss = 0.37622564\n",
            "Iteration 555, loss = 0.37609168\n",
            "Iteration 556, loss = 0.37600929\n",
            "Iteration 557, loss = 0.37603498\n",
            "Iteration 558, loss = 0.37601897\n",
            "Iteration 559, loss = 0.37593892\n",
            "Iteration 560, loss = 0.37584076\n",
            "Iteration 561, loss = 0.37580247\n",
            "Iteration 562, loss = 0.37573084\n",
            "Iteration 563, loss = 0.37577979\n",
            "Iteration 564, loss = 0.37570812\n",
            "Iteration 565, loss = 0.37558252\n",
            "Iteration 566, loss = 0.37560579\n",
            "Iteration 567, loss = 0.37546290\n",
            "Iteration 568, loss = 0.37552440\n",
            "Iteration 569, loss = 0.37536871\n",
            "Iteration 570, loss = 0.37543170\n",
            "Iteration 571, loss = 0.37527549\n",
            "Iteration 572, loss = 0.37532844\n",
            "Iteration 573, loss = 0.37519027\n",
            "Iteration 574, loss = 0.37531602\n",
            "Iteration 575, loss = 0.37514892\n",
            "Iteration 576, loss = 0.37507890\n",
            "Iteration 577, loss = 0.37499235\n",
            "Iteration 578, loss = 0.37499715\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Melhores hiperparâmetros encontrados através do Grid Search:\n",
            "{'activation': 'tanh', 'max_iter': 1000, 'solver': 'adam', 'tol': 0.0001}\n",
            "Melhor pontuação (acurácia) encontrada através do Grid Search: 0.8210526315789475\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ajuste do modelo MLP aos dados de treinamento\n",
        "rna2_best_grid = MLPClassifier(**best_params_grid2)\n",
        "rna2_best_grid.fit(X_train_oversampled, y_train_oversampled)\n",
        "\n",
        "# Predições nos dados de teste usando o modelo com melhores hiperparâmetros encontrados pelo Grid Search\n",
        "pred_grid2 = rna2_best_grid.predict(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXsJI0soU5-3",
        "outputId": "cf96575d-68fb-47be-e3bd-11e2b64b8350"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cálculo e impressão da acurácia nos dados de teste\n",
        "accuracy_grid = accuracy_score(y_test, pred_grid2)\n",
        "print(\"Acurácia do modelo MLP com melhores hiperparâmetros pelo Grid Search:\", accuracy_grid)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc907a3c-02c4-49b6-fa2f-a0978e0287ba",
        "id": "-8YBrDTUU5-3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acurácia do modelo MLP com melhores hiperparâmetros pelo Grid Search: 0.75\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Matriz de confusão para o modelo com melhores hiperparâmetros pelo Grid Search\n",
        "print(\"Matriz de Confusão - Grid Search\")\n",
        "cm_rna2_grid = confusion_matrix(y_test, pred_grid2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8a2f2eb-1ab1-43c0-c2fc-5a045c823a1d",
        "id": "42z_1Wm2U5-4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matriz de Confusão - Grid Search\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtém a média das acurácias (10 folds) referente ao conjunto treino\n",
        "rna2_grid = g_results.loc[g_search.best_index_,'mean_test_score']"
      ],
      "metadata": {
        "id": "okIABVhGU5-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RandomSearch"
      ],
      "metadata": {
        "id": "28DqdXxLU5-4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Random Search\n",
        "param_dist = {\n",
        "    'activation': ['relu', 'tanh', 'logistic'],\n",
        "    'solver': ['sgd', 'adam'],\n",
        "    'max_iter': [1000, 2000],\n",
        "    'tol': [0.0001, 0.001],\n",
        "}\n",
        "\n",
        "r_search = RandomizedSearchCV(estimator=rna2, param_distributions=param_dist, cv=10)\n",
        "r_search.fit(X_train_oversampled, y_train_oversampled)\n",
        "\n",
        "best_params_random = r_search.best_params_\n",
        "best_score_random = r_search.best_score_\n",
        "\n",
        "print(\"Melhores hiperparâmetros encontrados através do Random Search:\")\n",
        "print(best_params_random)\n",
        "print(\"Melhor pontuação (acurácia) encontrada através do Random Search:\", best_score_random)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92eaa9b6-ce2d-470b-d60e-77c44102b8f5",
        "id": "p67MdHUmU5-4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mA saída de streaming foi truncada nas últimas 5000 linhas.\u001b[0m\n",
            "Iteration 113, loss = 0.50057150\n",
            "Iteration 114, loss = 0.49909149\n",
            "Iteration 115, loss = 0.49768672\n",
            "Iteration 116, loss = 0.49630148\n",
            "Iteration 117, loss = 0.49499132\n",
            "Iteration 118, loss = 0.49364595\n",
            "Iteration 119, loss = 0.49234540\n",
            "Iteration 120, loss = 0.49102917\n",
            "Iteration 121, loss = 0.48972598\n",
            "Iteration 122, loss = 0.48848662\n",
            "Iteration 123, loss = 0.48724503\n",
            "Iteration 124, loss = 0.48594885\n",
            "Iteration 125, loss = 0.48476129\n",
            "Iteration 126, loss = 0.48355621\n",
            "Iteration 127, loss = 0.48239961\n",
            "Iteration 128, loss = 0.48126069\n",
            "Iteration 129, loss = 0.48017980\n",
            "Iteration 130, loss = 0.47906952\n",
            "Iteration 131, loss = 0.47793834\n",
            "Iteration 132, loss = 0.47688976\n",
            "Iteration 133, loss = 0.47587132\n",
            "Iteration 134, loss = 0.47488583\n",
            "Iteration 135, loss = 0.47376425\n",
            "Iteration 136, loss = 0.47271411\n",
            "Iteration 137, loss = 0.47176556\n",
            "Iteration 138, loss = 0.47074834\n",
            "Iteration 139, loss = 0.46981088\n",
            "Iteration 140, loss = 0.46880300\n",
            "Iteration 141, loss = 0.46777997\n",
            "Iteration 142, loss = 0.46687962\n",
            "Iteration 143, loss = 0.46585035\n",
            "Iteration 144, loss = 0.46486864\n",
            "Iteration 145, loss = 0.46396298\n",
            "Iteration 146, loss = 0.46296090\n",
            "Iteration 147, loss = 0.46200593\n",
            "Iteration 148, loss = 0.46106001\n",
            "Iteration 149, loss = 0.46023413\n",
            "Iteration 150, loss = 0.45929680\n",
            "Iteration 151, loss = 0.45834833\n",
            "Iteration 152, loss = 0.45746402\n",
            "Iteration 153, loss = 0.45659077\n",
            "Iteration 154, loss = 0.45571053\n",
            "Iteration 155, loss = 0.45474687\n",
            "Iteration 156, loss = 0.45387903\n",
            "Iteration 157, loss = 0.45286458\n",
            "Iteration 158, loss = 0.45200067\n",
            "Iteration 159, loss = 0.45122135\n",
            "Iteration 160, loss = 0.45035287\n",
            "Iteration 161, loss = 0.44946407\n",
            "Iteration 162, loss = 0.44858558\n",
            "Iteration 163, loss = 0.44777084\n",
            "Iteration 164, loss = 0.44699901\n",
            "Iteration 165, loss = 0.44620900\n",
            "Iteration 166, loss = 0.44540371\n",
            "Iteration 167, loss = 0.44459624\n",
            "Iteration 168, loss = 0.44385151\n",
            "Iteration 169, loss = 0.44306561\n",
            "Iteration 170, loss = 0.44233585\n",
            "Iteration 171, loss = 0.44156599\n",
            "Iteration 172, loss = 0.44086069\n",
            "Iteration 173, loss = 0.44010685\n",
            "Iteration 174, loss = 0.43940183\n",
            "Iteration 175, loss = 0.43884224\n",
            "Iteration 176, loss = 0.43820021\n",
            "Iteration 177, loss = 0.43737264\n",
            "Iteration 178, loss = 0.43672641\n",
            "Iteration 179, loss = 0.43614426\n",
            "Iteration 180, loss = 0.43544029\n",
            "Iteration 181, loss = 0.43491791\n",
            "Iteration 182, loss = 0.43431878\n",
            "Iteration 183, loss = 0.43370358\n",
            "Iteration 184, loss = 0.43316519\n",
            "Iteration 185, loss = 0.43262514\n",
            "Iteration 186, loss = 0.43198280\n",
            "Iteration 187, loss = 0.43142892\n",
            "Iteration 188, loss = 0.43085765\n",
            "Iteration 189, loss = 0.43033601\n",
            "Iteration 190, loss = 0.42988266\n",
            "Iteration 191, loss = 0.42932528\n",
            "Iteration 192, loss = 0.42870966\n",
            "Iteration 193, loss = 0.42834092\n",
            "Iteration 194, loss = 0.42778299\n",
            "Iteration 195, loss = 0.42718569\n",
            "Iteration 196, loss = 0.42682615\n",
            "Iteration 197, loss = 0.42641234\n",
            "Iteration 198, loss = 0.42589253\n",
            "Iteration 199, loss = 0.42548866\n",
            "Iteration 200, loss = 0.42504451\n",
            "Iteration 201, loss = 0.42460298\n",
            "Iteration 202, loss = 0.42414438\n",
            "Iteration 203, loss = 0.42378484\n",
            "Iteration 204, loss = 0.42332137\n",
            "Iteration 205, loss = 0.42296501\n",
            "Iteration 206, loss = 0.42260523\n",
            "Iteration 207, loss = 0.42215540\n",
            "Iteration 208, loss = 0.42179882\n",
            "Iteration 209, loss = 0.42144945\n",
            "Iteration 210, loss = 0.42109140\n",
            "Iteration 211, loss = 0.42071480\n",
            "Iteration 212, loss = 0.42031638\n",
            "Iteration 213, loss = 0.42005365\n",
            "Iteration 214, loss = 0.41961969\n",
            "Iteration 215, loss = 0.41927947\n",
            "Iteration 216, loss = 0.41898036\n",
            "Iteration 217, loss = 0.41862243\n",
            "Iteration 218, loss = 0.41835548\n",
            "Iteration 219, loss = 0.41795916\n",
            "Iteration 220, loss = 0.41769926\n",
            "Iteration 221, loss = 0.41739498\n",
            "Iteration 222, loss = 0.41707664\n",
            "Iteration 223, loss = 0.41675819\n",
            "Iteration 224, loss = 0.41653224\n",
            "Iteration 225, loss = 0.41627373\n",
            "Iteration 226, loss = 0.41590819\n",
            "Iteration 227, loss = 0.41569517\n",
            "Iteration 228, loss = 0.41544925\n",
            "Iteration 229, loss = 0.41516023\n",
            "Iteration 230, loss = 0.41486922\n",
            "Iteration 231, loss = 0.41467920\n",
            "Iteration 232, loss = 0.41445535\n",
            "Iteration 233, loss = 0.41415717\n",
            "Iteration 234, loss = 0.41396670\n",
            "Iteration 235, loss = 0.41367769\n",
            "Iteration 236, loss = 0.41342363\n",
            "Iteration 237, loss = 0.41322858\n",
            "Iteration 238, loss = 0.41297739\n",
            "Iteration 239, loss = 0.41273501\n",
            "Iteration 240, loss = 0.41252853\n",
            "Iteration 241, loss = 0.41231896\n",
            "Iteration 242, loss = 0.41208982\n",
            "Iteration 243, loss = 0.41195164\n",
            "Iteration 244, loss = 0.41176543\n",
            "Iteration 245, loss = 0.41155055\n",
            "Iteration 246, loss = 0.41129277\n",
            "Iteration 247, loss = 0.41102582\n",
            "Iteration 248, loss = 0.41107786\n",
            "Iteration 249, loss = 0.41078843\n",
            "Iteration 250, loss = 0.41047494\n",
            "Iteration 251, loss = 0.41043293\n",
            "Iteration 252, loss = 0.41013474\n",
            "Iteration 253, loss = 0.40991799\n",
            "Iteration 254, loss = 0.40968596\n",
            "Iteration 255, loss = 0.40952523\n",
            "Iteration 256, loss = 0.40941476\n",
            "Iteration 257, loss = 0.40924273\n",
            "Iteration 258, loss = 0.40900374\n",
            "Iteration 259, loss = 0.40890376\n",
            "Iteration 260, loss = 0.40875095\n",
            "Iteration 261, loss = 0.40860864\n",
            "Iteration 262, loss = 0.40830075\n",
            "Iteration 263, loss = 0.40818148\n",
            "Iteration 264, loss = 0.40803799\n",
            "Iteration 265, loss = 0.40802025\n",
            "Iteration 266, loss = 0.40787682\n",
            "Iteration 267, loss = 0.40771643\n",
            "Iteration 268, loss = 0.40749809\n",
            "Iteration 269, loss = 0.40736285\n",
            "Iteration 270, loss = 0.40717954\n",
            "Iteration 271, loss = 0.40702648\n",
            "Iteration 272, loss = 0.40691842\n",
            "Iteration 273, loss = 0.40671532\n",
            "Iteration 274, loss = 0.40654292\n",
            "Iteration 275, loss = 0.40642765\n",
            "Iteration 276, loss = 0.40625917\n",
            "Iteration 277, loss = 0.40617208\n",
            "Iteration 278, loss = 0.40607465\n",
            "Iteration 279, loss = 0.40601200\n",
            "Iteration 280, loss = 0.40578768\n",
            "Iteration 281, loss = 0.40566472\n",
            "Iteration 282, loss = 0.40547369\n",
            "Iteration 283, loss = 0.40534669\n",
            "Iteration 284, loss = 0.40534306\n",
            "Iteration 285, loss = 0.40524944\n",
            "Iteration 286, loss = 0.40509488\n",
            "Iteration 287, loss = 0.40493282\n",
            "Iteration 288, loss = 0.40477626\n",
            "Iteration 289, loss = 0.40468787\n",
            "Iteration 290, loss = 0.40452440\n",
            "Iteration 291, loss = 0.40452977\n",
            "Iteration 292, loss = 0.40448307\n",
            "Iteration 293, loss = 0.40429352\n",
            "Iteration 294, loss = 0.40418541\n",
            "Iteration 295, loss = 0.40392828\n",
            "Iteration 296, loss = 0.40380604\n",
            "Iteration 297, loss = 0.40371167\n",
            "Iteration 298, loss = 0.40364947\n",
            "Iteration 299, loss = 0.40349374\n",
            "Iteration 300, loss = 0.40342054\n",
            "Iteration 301, loss = 0.40330341\n",
            "Iteration 302, loss = 0.40312038\n",
            "Iteration 303, loss = 0.40322320\n",
            "Iteration 304, loss = 0.40301548\n",
            "Iteration 305, loss = 0.40290176\n",
            "Iteration 306, loss = 0.40281917\n",
            "Iteration 307, loss = 0.40259212\n",
            "Iteration 308, loss = 0.40246210\n",
            "Iteration 309, loss = 0.40245027\n",
            "Iteration 310, loss = 0.40238475\n",
            "Iteration 311, loss = 0.40225390\n",
            "Iteration 312, loss = 0.40214320\n",
            "Iteration 313, loss = 0.40201887\n",
            "Iteration 314, loss = 0.40193302\n",
            "Iteration 315, loss = 0.40176512\n",
            "Iteration 316, loss = 0.40186496\n",
            "Iteration 317, loss = 0.40176365\n",
            "Iteration 318, loss = 0.40162866\n",
            "Iteration 319, loss = 0.40144663\n",
            "Iteration 320, loss = 0.40131401\n",
            "Iteration 321, loss = 0.40120964\n",
            "Iteration 322, loss = 0.40112630\n",
            "Iteration 323, loss = 0.40111737\n",
            "Iteration 324, loss = 0.40102932\n",
            "Iteration 325, loss = 0.40096466\n",
            "Iteration 326, loss = 0.40079673\n",
            "Iteration 327, loss = 0.40067720\n",
            "Iteration 328, loss = 0.40068405\n",
            "Iteration 329, loss = 0.40054716\n",
            "Iteration 330, loss = 0.40040965\n",
            "Iteration 331, loss = 0.40038283\n",
            "Iteration 332, loss = 0.40024010\n",
            "Iteration 333, loss = 0.40019890\n",
            "Iteration 334, loss = 0.40015486\n",
            "Iteration 335, loss = 0.40010822\n",
            "Iteration 336, loss = 0.40000607\n",
            "Iteration 337, loss = 0.39989345\n",
            "Iteration 338, loss = 0.39975519\n",
            "Iteration 339, loss = 0.39968962\n",
            "Iteration 340, loss = 0.39959949\n",
            "Iteration 341, loss = 0.39953565\n",
            "Iteration 342, loss = 0.39954672\n",
            "Iteration 343, loss = 0.39933448\n",
            "Iteration 344, loss = 0.39926203\n",
            "Iteration 345, loss = 0.39918827\n",
            "Iteration 346, loss = 0.39911085\n",
            "Iteration 347, loss = 0.39903901\n",
            "Iteration 348, loss = 0.39901422\n",
            "Iteration 349, loss = 0.39894864\n",
            "Iteration 350, loss = 0.39891612\n",
            "Iteration 351, loss = 0.39870140\n",
            "Iteration 352, loss = 0.39887387\n",
            "Iteration 353, loss = 0.39867482\n",
            "Iteration 354, loss = 0.39851424\n",
            "Iteration 355, loss = 0.39860326\n",
            "Iteration 356, loss = 0.39833669\n",
            "Iteration 357, loss = 0.39828009\n",
            "Iteration 358, loss = 0.39823266\n",
            "Iteration 359, loss = 0.39824448\n",
            "Iteration 360, loss = 0.39809501\n",
            "Iteration 361, loss = 0.39803828\n",
            "Iteration 362, loss = 0.39798283\n",
            "Iteration 363, loss = 0.39793365\n",
            "Iteration 364, loss = 0.39781716\n",
            "Iteration 365, loss = 0.39777205\n",
            "Iteration 366, loss = 0.39762794\n",
            "Iteration 367, loss = 0.39772562\n",
            "Iteration 368, loss = 0.39765313\n",
            "Iteration 369, loss = 0.39757097\n",
            "Iteration 370, loss = 0.39751800\n",
            "Iteration 371, loss = 0.39740561\n",
            "Iteration 372, loss = 0.39738490\n",
            "Iteration 373, loss = 0.39728741\n",
            "Iteration 374, loss = 0.39723291\n",
            "Iteration 375, loss = 0.39718506\n",
            "Iteration 376, loss = 0.39718016\n",
            "Iteration 377, loss = 0.39708656\n",
            "Iteration 378, loss = 0.39699862\n",
            "Iteration 379, loss = 0.39699659\n",
            "Iteration 380, loss = 0.39690916\n",
            "Iteration 381, loss = 0.39686573\n",
            "Iteration 382, loss = 0.39672978\n",
            "Iteration 383, loss = 0.39680437\n",
            "Iteration 384, loss = 0.39671832\n",
            "Iteration 385, loss = 0.39663034\n",
            "Iteration 386, loss = 0.39659467\n",
            "Iteration 387, loss = 0.39644066\n",
            "Iteration 388, loss = 0.39635255\n",
            "Iteration 389, loss = 0.39627197\n",
            "Iteration 390, loss = 0.39621810\n",
            "Iteration 391, loss = 0.39613107\n",
            "Iteration 392, loss = 0.39605931\n",
            "Iteration 393, loss = 0.39601637\n",
            "Iteration 394, loss = 0.39588067\n",
            "Iteration 395, loss = 0.39583631\n",
            "Iteration 396, loss = 0.39585086\n",
            "Iteration 397, loss = 0.39585362\n",
            "Iteration 398, loss = 0.39567737\n",
            "Iteration 399, loss = 0.39557895\n",
            "Iteration 400, loss = 0.39551264\n",
            "Iteration 401, loss = 0.39551186\n",
            "Iteration 402, loss = 0.39542870\n",
            "Iteration 403, loss = 0.39555628\n",
            "Iteration 404, loss = 0.39527582\n",
            "Iteration 405, loss = 0.39522806\n",
            "Iteration 406, loss = 0.39516617\n",
            "Iteration 407, loss = 0.39535934\n",
            "Iteration 408, loss = 0.39514997\n",
            "Iteration 409, loss = 0.39492471\n",
            "Iteration 410, loss = 0.39495038\n",
            "Iteration 411, loss = 0.39490407\n",
            "Iteration 412, loss = 0.39485834\n",
            "Iteration 413, loss = 0.39474905\n",
            "Iteration 414, loss = 0.39466505\n",
            "Iteration 415, loss = 0.39459366\n",
            "Iteration 416, loss = 0.39454430\n",
            "Iteration 417, loss = 0.39449075\n",
            "Iteration 418, loss = 0.39441216\n",
            "Iteration 419, loss = 0.39440341\n",
            "Iteration 420, loss = 0.39431044\n",
            "Iteration 421, loss = 0.39436096\n",
            "Iteration 422, loss = 0.39435841\n",
            "Iteration 423, loss = 0.39423342\n",
            "Iteration 424, loss = 0.39410100\n",
            "Iteration 425, loss = 0.39394414\n",
            "Iteration 426, loss = 0.39402204\n",
            "Iteration 427, loss = 0.39413462\n",
            "Iteration 428, loss = 0.39406360\n",
            "Iteration 429, loss = 0.39409473\n",
            "Iteration 430, loss = 0.39385850\n",
            "Iteration 431, loss = 0.39382975\n",
            "Iteration 432, loss = 0.39370920\n",
            "Iteration 433, loss = 0.39367674\n",
            "Iteration 434, loss = 0.39362098\n",
            "Iteration 435, loss = 0.39360872\n",
            "Iteration 436, loss = 0.39363616\n",
            "Iteration 437, loss = 0.39353494\n",
            "Iteration 438, loss = 0.39352848\n",
            "Iteration 439, loss = 0.39350860\n",
            "Iteration 440, loss = 0.39343167\n",
            "Iteration 441, loss = 0.39333499\n",
            "Iteration 442, loss = 0.39330612\n",
            "Iteration 443, loss = 0.39323709\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.80981066\n",
            "Iteration 2, loss = 0.80719831\n",
            "Iteration 3, loss = 0.80448492\n",
            "Iteration 4, loss = 0.80185394\n",
            "Iteration 5, loss = 0.79913225\n",
            "Iteration 6, loss = 0.79643592\n",
            "Iteration 7, loss = 0.79377806\n",
            "Iteration 8, loss = 0.79106197\n",
            "Iteration 9, loss = 0.78826740\n",
            "Iteration 10, loss = 0.78551509\n",
            "Iteration 11, loss = 0.78270282\n",
            "Iteration 12, loss = 0.77998596\n",
            "Iteration 13, loss = 0.77711771\n",
            "Iteration 14, loss = 0.77432380\n",
            "Iteration 15, loss = 0.77142640\n",
            "Iteration 16, loss = 0.76861856\n",
            "Iteration 17, loss = 0.76562020\n",
            "Iteration 18, loss = 0.76263396\n",
            "Iteration 19, loss = 0.75980123\n",
            "Iteration 20, loss = 0.75666498\n",
            "Iteration 21, loss = 0.75380178\n",
            "Iteration 22, loss = 0.75058745\n",
            "Iteration 23, loss = 0.74744810\n",
            "Iteration 24, loss = 0.74453613\n",
            "Iteration 25, loss = 0.74122845\n",
            "Iteration 26, loss = 0.73815007\n",
            "Iteration 27, loss = 0.73486323\n",
            "Iteration 28, loss = 0.73166820\n",
            "Iteration 29, loss = 0.72849218\n",
            "Iteration 30, loss = 0.72521482\n",
            "Iteration 31, loss = 0.72193544\n",
            "Iteration 32, loss = 0.71864199\n",
            "Iteration 33, loss = 0.71521222\n",
            "Iteration 34, loss = 0.71197771\n",
            "Iteration 35, loss = 0.70846411\n",
            "Iteration 36, loss = 0.70505099\n",
            "Iteration 37, loss = 0.70163385\n",
            "Iteration 38, loss = 0.69808385\n",
            "Iteration 39, loss = 0.69471401\n",
            "Iteration 40, loss = 0.69113145\n",
            "Iteration 41, loss = 0.68753067\n",
            "Iteration 42, loss = 0.68393761\n",
            "Iteration 43, loss = 0.68026450\n",
            "Iteration 44, loss = 0.67666041\n",
            "Iteration 45, loss = 0.67304838\n",
            "Iteration 46, loss = 0.66948457\n",
            "Iteration 47, loss = 0.66592495\n",
            "Iteration 48, loss = 0.66235999\n",
            "Iteration 49, loss = 0.65887601\n",
            "Iteration 50, loss = 0.65545773\n",
            "Iteration 51, loss = 0.65200894\n",
            "Iteration 52, loss = 0.64848011\n",
            "Iteration 53, loss = 0.64506609\n",
            "Iteration 54, loss = 0.64165147\n",
            "Iteration 55, loss = 0.63838138\n",
            "Iteration 56, loss = 0.63495989\n",
            "Iteration 57, loss = 0.63134446\n",
            "Iteration 58, loss = 0.62818850\n",
            "Iteration 59, loss = 0.62471200\n",
            "Iteration 60, loss = 0.62123968\n",
            "Iteration 61, loss = 0.61778840\n",
            "Iteration 62, loss = 0.61430619\n",
            "Iteration 63, loss = 0.61085626\n",
            "Iteration 64, loss = 0.60712373\n",
            "Iteration 65, loss = 0.60386655\n",
            "Iteration 66, loss = 0.60030222\n",
            "Iteration 67, loss = 0.59667721\n",
            "Iteration 68, loss = 0.59334088\n",
            "Iteration 69, loss = 0.58982606\n",
            "Iteration 70, loss = 0.58667236\n",
            "Iteration 71, loss = 0.58370766\n",
            "Iteration 72, loss = 0.58102622\n",
            "Iteration 73, loss = 0.57816939\n",
            "Iteration 74, loss = 0.57557192\n",
            "Iteration 75, loss = 0.57291749\n",
            "Iteration 76, loss = 0.57027079\n",
            "Iteration 77, loss = 0.56769350\n",
            "Iteration 78, loss = 0.56531475\n",
            "Iteration 79, loss = 0.56280315\n",
            "Iteration 80, loss = 0.56040971\n",
            "Iteration 81, loss = 0.55834630\n",
            "Iteration 82, loss = 0.55612468\n",
            "Iteration 83, loss = 0.55398835\n",
            "Iteration 84, loss = 0.55198526\n",
            "Iteration 85, loss = 0.54991953\n",
            "Iteration 86, loss = 0.54798223\n",
            "Iteration 87, loss = 0.54609777\n",
            "Iteration 88, loss = 0.54414465\n",
            "Iteration 89, loss = 0.54232705\n",
            "Iteration 90, loss = 0.54053588\n",
            "Iteration 91, loss = 0.53874940\n",
            "Iteration 92, loss = 0.53691362\n",
            "Iteration 93, loss = 0.53493186\n",
            "Iteration 94, loss = 0.53322962\n",
            "Iteration 95, loss = 0.53140502\n",
            "Iteration 96, loss = 0.52973558\n",
            "Iteration 97, loss = 0.52803245\n",
            "Iteration 98, loss = 0.52642328\n",
            "Iteration 99, loss = 0.52478482\n",
            "Iteration 100, loss = 0.52315473\n",
            "Iteration 101, loss = 0.52158369\n",
            "Iteration 102, loss = 0.52004166\n",
            "Iteration 103, loss = 0.51853107\n",
            "Iteration 104, loss = 0.51702115\n",
            "Iteration 105, loss = 0.51551047\n",
            "Iteration 106, loss = 0.51410814\n",
            "Iteration 107, loss = 0.51255915\n",
            "Iteration 108, loss = 0.51110934\n",
            "Iteration 109, loss = 0.50974502\n",
            "Iteration 110, loss = 0.50834695\n",
            "Iteration 111, loss = 0.50695238\n",
            "Iteration 112, loss = 0.50571048\n",
            "Iteration 113, loss = 0.50437838\n",
            "Iteration 114, loss = 0.50299944\n",
            "Iteration 115, loss = 0.50171796\n",
            "Iteration 116, loss = 0.50034899\n",
            "Iteration 117, loss = 0.49913495\n",
            "Iteration 118, loss = 0.49789369\n",
            "Iteration 119, loss = 0.49667392\n",
            "Iteration 120, loss = 0.49544393\n",
            "Iteration 121, loss = 0.49432103\n",
            "Iteration 122, loss = 0.49308485\n",
            "Iteration 123, loss = 0.49198598\n",
            "Iteration 124, loss = 0.49085701\n",
            "Iteration 125, loss = 0.48982364\n",
            "Iteration 126, loss = 0.48864773\n",
            "Iteration 127, loss = 0.48761725\n",
            "Iteration 128, loss = 0.48652270\n",
            "Iteration 129, loss = 0.48552175\n",
            "Iteration 130, loss = 0.48453616\n",
            "Iteration 131, loss = 0.48357756\n",
            "Iteration 132, loss = 0.48258643\n",
            "Iteration 133, loss = 0.48164033\n",
            "Iteration 134, loss = 0.48075795\n",
            "Iteration 135, loss = 0.47977633\n",
            "Iteration 136, loss = 0.47883022\n",
            "Iteration 137, loss = 0.47793379\n",
            "Iteration 138, loss = 0.47704540\n",
            "Iteration 139, loss = 0.47619125\n",
            "Iteration 140, loss = 0.47530326\n",
            "Iteration 141, loss = 0.47438402\n",
            "Iteration 142, loss = 0.47352402\n",
            "Iteration 143, loss = 0.47275795\n",
            "Iteration 144, loss = 0.47194799\n",
            "Iteration 145, loss = 0.47124711\n",
            "Iteration 146, loss = 0.47046364\n",
            "Iteration 147, loss = 0.46978079\n",
            "Iteration 148, loss = 0.46900750\n",
            "Iteration 149, loss = 0.46841846\n",
            "Iteration 150, loss = 0.46771260\n",
            "Iteration 151, loss = 0.46701785\n",
            "Iteration 152, loss = 0.46635356\n",
            "Iteration 153, loss = 0.46578608\n",
            "Iteration 154, loss = 0.46510732\n",
            "Iteration 155, loss = 0.46442821\n",
            "Iteration 156, loss = 0.46371901\n",
            "Iteration 157, loss = 0.46300966\n",
            "Iteration 158, loss = 0.46235655\n",
            "Iteration 159, loss = 0.46163623\n",
            "Iteration 160, loss = 0.46102022\n",
            "Iteration 161, loss = 0.46033448\n",
            "Iteration 162, loss = 0.45961364\n",
            "Iteration 163, loss = 0.45903818\n",
            "Iteration 164, loss = 0.45827002\n",
            "Iteration 165, loss = 0.45759112\n",
            "Iteration 166, loss = 0.45677041\n",
            "Iteration 167, loss = 0.45608901\n",
            "Iteration 168, loss = 0.45541888\n",
            "Iteration 169, loss = 0.45462560\n",
            "Iteration 170, loss = 0.45394889\n",
            "Iteration 171, loss = 0.45324518\n",
            "Iteration 172, loss = 0.45250132\n",
            "Iteration 173, loss = 0.45181896\n",
            "Iteration 174, loss = 0.45114473\n",
            "Iteration 175, loss = 0.45057584\n",
            "Iteration 176, loss = 0.44993079\n",
            "Iteration 177, loss = 0.44929836\n",
            "Iteration 178, loss = 0.44864558\n",
            "Iteration 179, loss = 0.44803743\n",
            "Iteration 180, loss = 0.44743361\n",
            "Iteration 181, loss = 0.44682444\n",
            "Iteration 182, loss = 0.44620799\n",
            "Iteration 183, loss = 0.44562539\n",
            "Iteration 184, loss = 0.44504295\n",
            "Iteration 185, loss = 0.44446736\n",
            "Iteration 186, loss = 0.44390602\n",
            "Iteration 187, loss = 0.44334625\n",
            "Iteration 188, loss = 0.44281915\n",
            "Iteration 189, loss = 0.44233310\n",
            "Iteration 190, loss = 0.44177488\n",
            "Iteration 191, loss = 0.44129169\n",
            "Iteration 192, loss = 0.44076308\n",
            "Iteration 193, loss = 0.44038641\n",
            "Iteration 194, loss = 0.43983070\n",
            "Iteration 195, loss = 0.43927941\n",
            "Iteration 196, loss = 0.43888067\n",
            "Iteration 197, loss = 0.43851141\n",
            "Iteration 198, loss = 0.43802156\n",
            "Iteration 199, loss = 0.43762391\n",
            "Iteration 200, loss = 0.43715033\n",
            "Iteration 201, loss = 0.43676212\n",
            "Iteration 202, loss = 0.43629404\n",
            "Iteration 203, loss = 0.43591913\n",
            "Iteration 204, loss = 0.43560093\n",
            "Iteration 205, loss = 0.43515562\n",
            "Iteration 206, loss = 0.43472512\n",
            "Iteration 207, loss = 0.43432926\n",
            "Iteration 208, loss = 0.43393056\n",
            "Iteration 209, loss = 0.43363062\n",
            "Iteration 210, loss = 0.43324534\n",
            "Iteration 211, loss = 0.43285745\n",
            "Iteration 212, loss = 0.43253127\n",
            "Iteration 213, loss = 0.43224893\n",
            "Iteration 214, loss = 0.43180765\n",
            "Iteration 215, loss = 0.43139915\n",
            "Iteration 216, loss = 0.43122442\n",
            "Iteration 217, loss = 0.43087098\n",
            "Iteration 218, loss = 0.43050994\n",
            "Iteration 219, loss = 0.43021322\n",
            "Iteration 220, loss = 0.42993767\n",
            "Iteration 221, loss = 0.42971865\n",
            "Iteration 222, loss = 0.42936288\n",
            "Iteration 223, loss = 0.42910808\n",
            "Iteration 224, loss = 0.42880869\n",
            "Iteration 225, loss = 0.42858205\n",
            "Iteration 226, loss = 0.42819864\n",
            "Iteration 227, loss = 0.42793119\n",
            "Iteration 228, loss = 0.42765624\n",
            "Iteration 229, loss = 0.42738370\n",
            "Iteration 230, loss = 0.42703062\n",
            "Iteration 231, loss = 0.42697026\n",
            "Iteration 232, loss = 0.42662957\n",
            "Iteration 233, loss = 0.42626663\n",
            "Iteration 234, loss = 0.42607156\n",
            "Iteration 235, loss = 0.42576452\n",
            "Iteration 236, loss = 0.42548413\n",
            "Iteration 237, loss = 0.42538711\n",
            "Iteration 238, loss = 0.42501296\n",
            "Iteration 239, loss = 0.42476991\n",
            "Iteration 240, loss = 0.42458751\n",
            "Iteration 241, loss = 0.42436749\n",
            "Iteration 242, loss = 0.42404171\n",
            "Iteration 243, loss = 0.42385208\n",
            "Iteration 244, loss = 0.42369573\n",
            "Iteration 245, loss = 0.42341271\n",
            "Iteration 246, loss = 0.42317642\n",
            "Iteration 247, loss = 0.42291621\n",
            "Iteration 248, loss = 0.42280126\n",
            "Iteration 249, loss = 0.42254256\n",
            "Iteration 250, loss = 0.42226786\n",
            "Iteration 251, loss = 0.42212859\n",
            "Iteration 252, loss = 0.42189127\n",
            "Iteration 253, loss = 0.42158574\n",
            "Iteration 254, loss = 0.42136576\n",
            "Iteration 255, loss = 0.42117711\n",
            "Iteration 256, loss = 0.42104275\n",
            "Iteration 257, loss = 0.42084333\n",
            "Iteration 258, loss = 0.42060734\n",
            "Iteration 259, loss = 0.42046781\n",
            "Iteration 260, loss = 0.42019702\n",
            "Iteration 261, loss = 0.42001241\n",
            "Iteration 262, loss = 0.41988280\n",
            "Iteration 263, loss = 0.41964600\n",
            "Iteration 264, loss = 0.41957019\n",
            "Iteration 265, loss = 0.41937931\n",
            "Iteration 266, loss = 0.41921372\n",
            "Iteration 267, loss = 0.41907762\n",
            "Iteration 268, loss = 0.41883464\n",
            "Iteration 269, loss = 0.41869972\n",
            "Iteration 270, loss = 0.41854353\n",
            "Iteration 271, loss = 0.41834175\n",
            "Iteration 272, loss = 0.41827082\n",
            "Iteration 273, loss = 0.41794877\n",
            "Iteration 274, loss = 0.41782758\n",
            "Iteration 275, loss = 0.41760157\n",
            "Iteration 276, loss = 0.41744389\n",
            "Iteration 277, loss = 0.41731355\n",
            "Iteration 278, loss = 0.41715511\n",
            "Iteration 279, loss = 0.41702772\n",
            "Iteration 280, loss = 0.41687751\n",
            "Iteration 281, loss = 0.41667674\n",
            "Iteration 282, loss = 0.41652473\n",
            "Iteration 283, loss = 0.41642199\n",
            "Iteration 284, loss = 0.41625174\n",
            "Iteration 285, loss = 0.41625143\n",
            "Iteration 286, loss = 0.41610994\n",
            "Iteration 287, loss = 0.41589434\n",
            "Iteration 288, loss = 0.41575849\n",
            "Iteration 289, loss = 0.41563270\n",
            "Iteration 290, loss = 0.41543661\n",
            "Iteration 291, loss = 0.41533482\n",
            "Iteration 292, loss = 0.41516853\n",
            "Iteration 293, loss = 0.41508629\n",
            "Iteration 294, loss = 0.41491016\n",
            "Iteration 295, loss = 0.41474623\n",
            "Iteration 296, loss = 0.41470092\n",
            "Iteration 297, loss = 0.41453866\n",
            "Iteration 298, loss = 0.41439980\n",
            "Iteration 299, loss = 0.41429344\n",
            "Iteration 300, loss = 0.41415920\n",
            "Iteration 301, loss = 0.41404588\n",
            "Iteration 302, loss = 0.41396011\n",
            "Iteration 303, loss = 0.41374720\n",
            "Iteration 304, loss = 0.41364788\n",
            "Iteration 305, loss = 0.41355096\n",
            "Iteration 306, loss = 0.41342529\n",
            "Iteration 307, loss = 0.41326496\n",
            "Iteration 308, loss = 0.41315300\n",
            "Iteration 309, loss = 0.41309367\n",
            "Iteration 310, loss = 0.41300889\n",
            "Iteration 311, loss = 0.41283752\n",
            "Iteration 312, loss = 0.41272206\n",
            "Iteration 313, loss = 0.41263525\n",
            "Iteration 314, loss = 0.41273158\n",
            "Iteration 315, loss = 0.41239349\n",
            "Iteration 316, loss = 0.41236461\n",
            "Iteration 317, loss = 0.41234215\n",
            "Iteration 318, loss = 0.41214261\n",
            "Iteration 319, loss = 0.41199675\n",
            "Iteration 320, loss = 0.41188595\n",
            "Iteration 321, loss = 0.41183884\n",
            "Iteration 322, loss = 0.41162375\n",
            "Iteration 323, loss = 0.41153092\n",
            "Iteration 324, loss = 0.41139671\n",
            "Iteration 325, loss = 0.41132981\n",
            "Iteration 326, loss = 0.41115318\n",
            "Iteration 327, loss = 0.41110119\n",
            "Iteration 328, loss = 0.41100904\n",
            "Iteration 329, loss = 0.41095212\n",
            "Iteration 330, loss = 0.41076940\n",
            "Iteration 331, loss = 0.41075580\n",
            "Iteration 332, loss = 0.41064576\n",
            "Iteration 333, loss = 0.41050549\n",
            "Iteration 334, loss = 0.41039480\n",
            "Iteration 335, loss = 0.41033797\n",
            "Iteration 336, loss = 0.41025416\n",
            "Iteration 337, loss = 0.41021328\n",
            "Iteration 338, loss = 0.41008649\n",
            "Iteration 339, loss = 0.40996840\n",
            "Iteration 340, loss = 0.40989434\n",
            "Iteration 341, loss = 0.40975458\n",
            "Iteration 342, loss = 0.40970062\n",
            "Iteration 343, loss = 0.40956730\n",
            "Iteration 344, loss = 0.40948790\n",
            "Iteration 345, loss = 0.40940111\n",
            "Iteration 346, loss = 0.40931083\n",
            "Iteration 347, loss = 0.40927234\n",
            "Iteration 348, loss = 0.40915831\n",
            "Iteration 349, loss = 0.40904193\n",
            "Iteration 350, loss = 0.40894909\n",
            "Iteration 351, loss = 0.40882109\n",
            "Iteration 352, loss = 0.40886603\n",
            "Iteration 353, loss = 0.40864700\n",
            "Iteration 354, loss = 0.40855245\n",
            "Iteration 355, loss = 0.40866077\n",
            "Iteration 356, loss = 0.40837614\n",
            "Iteration 357, loss = 0.40831165\n",
            "Iteration 358, loss = 0.40823843\n",
            "Iteration 359, loss = 0.40815874\n",
            "Iteration 360, loss = 0.40816203\n",
            "Iteration 361, loss = 0.40812506\n",
            "Iteration 362, loss = 0.40794422\n",
            "Iteration 363, loss = 0.40785265\n",
            "Iteration 364, loss = 0.40786980\n",
            "Iteration 365, loss = 0.40770022\n",
            "Iteration 366, loss = 0.40758041\n",
            "Iteration 367, loss = 0.40745449\n",
            "Iteration 368, loss = 0.40748404\n",
            "Iteration 369, loss = 0.40748467\n",
            "Iteration 370, loss = 0.40746954\n",
            "Iteration 371, loss = 0.40728979\n",
            "Iteration 372, loss = 0.40720643\n",
            "Iteration 373, loss = 0.40719540\n",
            "Iteration 374, loss = 0.40700689\n",
            "Iteration 375, loss = 0.40696441\n",
            "Iteration 376, loss = 0.40690338\n",
            "Iteration 377, loss = 0.40695993\n",
            "Iteration 378, loss = 0.40676379\n",
            "Iteration 379, loss = 0.40668073\n",
            "Iteration 380, loss = 0.40664348\n",
            "Iteration 381, loss = 0.40653704\n",
            "Iteration 382, loss = 0.40648189\n",
            "Iteration 383, loss = 0.40641346\n",
            "Iteration 384, loss = 0.40637880\n",
            "Iteration 385, loss = 0.40627080\n",
            "Iteration 386, loss = 0.40624972\n",
            "Iteration 387, loss = 0.40624221\n",
            "Iteration 388, loss = 0.40613172\n",
            "Iteration 389, loss = 0.40612529\n",
            "Iteration 390, loss = 0.40599232\n",
            "Iteration 391, loss = 0.40596819\n",
            "Iteration 392, loss = 0.40582111\n",
            "Iteration 393, loss = 0.40574015\n",
            "Iteration 394, loss = 0.40582806\n",
            "Iteration 395, loss = 0.40570885\n",
            "Iteration 396, loss = 0.40560951\n",
            "Iteration 397, loss = 0.40558240\n",
            "Iteration 398, loss = 0.40546757\n",
            "Iteration 399, loss = 0.40536513\n",
            "Iteration 400, loss = 0.40533948\n",
            "Iteration 401, loss = 0.40531897\n",
            "Iteration 402, loss = 0.40521359\n",
            "Iteration 403, loss = 0.40544998\n",
            "Iteration 404, loss = 0.40511625\n",
            "Iteration 405, loss = 0.40507491\n",
            "Iteration 406, loss = 0.40498277\n",
            "Iteration 407, loss = 0.40497995\n",
            "Iteration 408, loss = 0.40486570\n",
            "Iteration 409, loss = 0.40486281\n",
            "Iteration 410, loss = 0.40479932\n",
            "Iteration 411, loss = 0.40481397\n",
            "Iteration 412, loss = 0.40475821\n",
            "Iteration 413, loss = 0.40461301\n",
            "Iteration 414, loss = 0.40469430\n",
            "Iteration 415, loss = 0.40454548\n",
            "Iteration 416, loss = 0.40441490\n",
            "Iteration 417, loss = 0.40438577\n",
            "Iteration 418, loss = 0.40434137\n",
            "Iteration 419, loss = 0.40428460\n",
            "Iteration 420, loss = 0.40426648\n",
            "Iteration 421, loss = 0.40422073\n",
            "Iteration 422, loss = 0.40418017\n",
            "Iteration 423, loss = 0.40415289\n",
            "Iteration 424, loss = 0.40409256\n",
            "Iteration 425, loss = 0.40394535\n",
            "Iteration 426, loss = 0.40388502\n",
            "Iteration 427, loss = 0.40404440\n",
            "Iteration 428, loss = 0.40410144\n",
            "Iteration 429, loss = 0.40400104\n",
            "Iteration 430, loss = 0.40393489\n",
            "Iteration 431, loss = 0.40374689\n",
            "Iteration 432, loss = 0.40370455\n",
            "Iteration 433, loss = 0.40373510\n",
            "Iteration 434, loss = 0.40362199\n",
            "Iteration 435, loss = 0.40357971\n",
            "Iteration 436, loss = 0.40354771\n",
            "Iteration 437, loss = 0.40343554\n",
            "Iteration 438, loss = 0.40342762\n",
            "Iteration 439, loss = 0.40343538\n",
            "Iteration 440, loss = 0.40337814\n",
            "Iteration 441, loss = 0.40326938\n",
            "Iteration 442, loss = 0.40326427\n",
            "Iteration 443, loss = 0.40320874\n",
            "Iteration 444, loss = 0.40326069\n",
            "Iteration 445, loss = 0.40323623\n",
            "Iteration 446, loss = 0.40311442\n",
            "Iteration 447, loss = 0.40304713\n",
            "Iteration 448, loss = 0.40293136\n",
            "Iteration 449, loss = 0.40280839\n",
            "Iteration 450, loss = 0.40294117\n",
            "Iteration 451, loss = 0.40300656\n",
            "Iteration 452, loss = 0.40293040\n",
            "Iteration 453, loss = 0.40283257\n",
            "Iteration 454, loss = 0.40270667\n",
            "Iteration 455, loss = 0.40266334\n",
            "Iteration 456, loss = 0.40264684\n",
            "Iteration 457, loss = 0.40255469\n",
            "Iteration 458, loss = 0.40253581\n",
            "Iteration 459, loss = 0.40244095\n",
            "Iteration 460, loss = 0.40239745\n",
            "Iteration 461, loss = 0.40232865\n",
            "Iteration 462, loss = 0.40232507\n",
            "Iteration 463, loss = 0.40234435\n",
            "Iteration 464, loss = 0.40227268\n",
            "Iteration 465, loss = 0.40218532\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.81028592\n",
            "Iteration 2, loss = 0.80758422\n",
            "Iteration 3, loss = 0.80488720\n",
            "Iteration 4, loss = 0.80219346\n",
            "Iteration 5, loss = 0.79942851\n",
            "Iteration 6, loss = 0.79670811\n",
            "Iteration 7, loss = 0.79405735\n",
            "Iteration 8, loss = 0.79128154\n",
            "Iteration 9, loss = 0.78846427\n",
            "Iteration 10, loss = 0.78562731\n",
            "Iteration 11, loss = 0.78282494\n",
            "Iteration 12, loss = 0.78001734\n",
            "Iteration 13, loss = 0.77722583\n",
            "Iteration 14, loss = 0.77428812\n",
            "Iteration 15, loss = 0.77147696\n",
            "Iteration 16, loss = 0.76862222\n",
            "Iteration 17, loss = 0.76573997\n",
            "Iteration 18, loss = 0.76278219\n",
            "Iteration 19, loss = 0.75996105\n",
            "Iteration 20, loss = 0.75687243\n",
            "Iteration 21, loss = 0.75390968\n",
            "Iteration 22, loss = 0.75081479\n",
            "Iteration 23, loss = 0.74773724\n",
            "Iteration 24, loss = 0.74460874\n",
            "Iteration 25, loss = 0.74139996\n",
            "Iteration 26, loss = 0.73814375\n",
            "Iteration 27, loss = 0.73495921\n",
            "Iteration 28, loss = 0.73153643\n",
            "Iteration 29, loss = 0.72822999\n",
            "Iteration 30, loss = 0.72492636\n",
            "Iteration 31, loss = 0.72140384\n",
            "Iteration 32, loss = 0.71790664\n",
            "Iteration 33, loss = 0.71437493\n",
            "Iteration 34, loss = 0.71077340\n",
            "Iteration 35, loss = 0.70726333\n",
            "Iteration 36, loss = 0.70355585\n",
            "Iteration 37, loss = 0.69977786\n",
            "Iteration 38, loss = 0.69632337\n",
            "Iteration 39, loss = 0.69252410\n",
            "Iteration 40, loss = 0.68877496\n",
            "Iteration 41, loss = 0.68503449\n",
            "Iteration 42, loss = 0.68120957\n",
            "Iteration 43, loss = 0.67754998\n",
            "Iteration 44, loss = 0.67371705\n",
            "Iteration 45, loss = 0.66994347\n",
            "Iteration 46, loss = 0.66610020\n",
            "Iteration 47, loss = 0.66245689\n",
            "Iteration 48, loss = 0.65870098\n",
            "Iteration 49, loss = 0.65511561\n",
            "Iteration 50, loss = 0.65145841\n",
            "Iteration 51, loss = 0.64794551\n",
            "Iteration 52, loss = 0.64419529\n",
            "Iteration 53, loss = 0.64070492\n",
            "Iteration 54, loss = 0.63718047\n",
            "Iteration 55, loss = 0.63366228\n",
            "Iteration 56, loss = 0.63016672\n",
            "Iteration 57, loss = 0.62650428\n",
            "Iteration 58, loss = 0.62316015\n",
            "Iteration 59, loss = 0.61971486\n",
            "Iteration 60, loss = 0.61617306\n",
            "Iteration 61, loss = 0.61270721\n",
            "Iteration 62, loss = 0.60933683\n",
            "Iteration 63, loss = 0.60574465\n",
            "Iteration 64, loss = 0.60190848\n",
            "Iteration 65, loss = 0.59841831\n",
            "Iteration 66, loss = 0.59472826\n",
            "Iteration 67, loss = 0.59104682\n",
            "Iteration 68, loss = 0.58755863\n",
            "Iteration 69, loss = 0.58407688\n",
            "Iteration 70, loss = 0.58091705\n",
            "Iteration 71, loss = 0.57790424\n",
            "Iteration 72, loss = 0.57505249\n",
            "Iteration 73, loss = 0.57222639\n",
            "Iteration 74, loss = 0.56950464\n",
            "Iteration 75, loss = 0.56676732\n",
            "Iteration 76, loss = 0.56397704\n",
            "Iteration 77, loss = 0.56146840\n",
            "Iteration 78, loss = 0.55886126\n",
            "Iteration 79, loss = 0.55630719\n",
            "Iteration 80, loss = 0.55392350\n",
            "Iteration 81, loss = 0.55170483\n",
            "Iteration 82, loss = 0.54937283\n",
            "Iteration 83, loss = 0.54720406\n",
            "Iteration 84, loss = 0.54513132\n",
            "Iteration 85, loss = 0.54300462\n",
            "Iteration 86, loss = 0.54089360\n",
            "Iteration 87, loss = 0.53889659\n",
            "Iteration 88, loss = 0.53687301\n",
            "Iteration 89, loss = 0.53475334\n",
            "Iteration 90, loss = 0.53281999\n",
            "Iteration 91, loss = 0.53076890\n",
            "Iteration 92, loss = 0.52884771\n",
            "Iteration 93, loss = 0.52676910\n",
            "Iteration 94, loss = 0.52495743\n",
            "Iteration 95, loss = 0.52293628\n",
            "Iteration 96, loss = 0.52116190\n",
            "Iteration 97, loss = 0.51924987\n",
            "Iteration 98, loss = 0.51744568\n",
            "Iteration 99, loss = 0.51567906\n",
            "Iteration 100, loss = 0.51387303\n",
            "Iteration 101, loss = 0.51211972\n",
            "Iteration 102, loss = 0.51044965\n",
            "Iteration 103, loss = 0.50876940\n",
            "Iteration 104, loss = 0.50707210\n",
            "Iteration 105, loss = 0.50540801\n",
            "Iteration 106, loss = 0.50386161\n",
            "Iteration 107, loss = 0.50218450\n",
            "Iteration 108, loss = 0.50058715\n",
            "Iteration 109, loss = 0.49903048\n",
            "Iteration 110, loss = 0.49751431\n",
            "Iteration 111, loss = 0.49601868\n",
            "Iteration 112, loss = 0.49462424\n",
            "Iteration 113, loss = 0.49309315\n",
            "Iteration 114, loss = 0.49165796\n",
            "Iteration 115, loss = 0.49020824\n",
            "Iteration 116, loss = 0.48878754\n",
            "Iteration 117, loss = 0.48745049\n",
            "Iteration 118, loss = 0.48606669\n",
            "Iteration 119, loss = 0.48474722\n",
            "Iteration 120, loss = 0.48344563\n",
            "Iteration 121, loss = 0.48214850\n",
            "Iteration 122, loss = 0.48075561\n",
            "Iteration 123, loss = 0.47958224\n",
            "Iteration 124, loss = 0.47828383\n",
            "Iteration 125, loss = 0.47708308\n",
            "Iteration 126, loss = 0.47584725\n",
            "Iteration 127, loss = 0.47472308\n",
            "Iteration 128, loss = 0.47354735\n",
            "Iteration 129, loss = 0.47252071\n",
            "Iteration 130, loss = 0.47136118\n",
            "Iteration 131, loss = 0.47031559\n",
            "Iteration 132, loss = 0.46915206\n",
            "Iteration 133, loss = 0.46811033\n",
            "Iteration 134, loss = 0.46715597\n",
            "Iteration 135, loss = 0.46609547\n",
            "Iteration 136, loss = 0.46509069\n",
            "Iteration 137, loss = 0.46410741\n",
            "Iteration 138, loss = 0.46326097\n",
            "Iteration 139, loss = 0.46227123\n",
            "Iteration 140, loss = 0.46145514\n",
            "Iteration 141, loss = 0.46044864\n",
            "Iteration 142, loss = 0.45957563\n",
            "Iteration 143, loss = 0.45867218\n",
            "Iteration 144, loss = 0.45780661\n",
            "Iteration 145, loss = 0.45706309\n",
            "Iteration 146, loss = 0.45618845\n",
            "Iteration 147, loss = 0.45539584\n",
            "Iteration 148, loss = 0.45457759\n",
            "Iteration 149, loss = 0.45389180\n",
            "Iteration 150, loss = 0.45313981\n",
            "Iteration 151, loss = 0.45243366\n",
            "Iteration 152, loss = 0.45173085\n",
            "Iteration 153, loss = 0.45112297\n",
            "Iteration 154, loss = 0.45046693\n",
            "Iteration 155, loss = 0.44983659\n",
            "Iteration 156, loss = 0.44907624\n",
            "Iteration 157, loss = 0.44837334\n",
            "Iteration 158, loss = 0.44780080\n",
            "Iteration 159, loss = 0.44711130\n",
            "Iteration 160, loss = 0.44641276\n",
            "Iteration 161, loss = 0.44576153\n",
            "Iteration 162, loss = 0.44508772\n",
            "Iteration 163, loss = 0.44440546\n",
            "Iteration 164, loss = 0.44371931\n",
            "Iteration 165, loss = 0.44304381\n",
            "Iteration 166, loss = 0.44235141\n",
            "Iteration 167, loss = 0.44166112\n",
            "Iteration 168, loss = 0.44098738\n",
            "Iteration 169, loss = 0.44032680\n",
            "Iteration 170, loss = 0.43962330\n",
            "Iteration 171, loss = 0.43896254\n",
            "Iteration 172, loss = 0.43842608\n",
            "Iteration 173, loss = 0.43766572\n",
            "Iteration 174, loss = 0.43706263\n",
            "Iteration 175, loss = 0.43643168\n",
            "Iteration 176, loss = 0.43581411\n",
            "Iteration 177, loss = 0.43519007\n",
            "Iteration 178, loss = 0.43457301\n",
            "Iteration 179, loss = 0.43405260\n",
            "Iteration 180, loss = 0.43345593\n",
            "Iteration 181, loss = 0.43292016\n",
            "Iteration 182, loss = 0.43225511\n",
            "Iteration 183, loss = 0.43168975\n",
            "Iteration 184, loss = 0.43113324\n",
            "Iteration 185, loss = 0.43060594\n",
            "Iteration 186, loss = 0.43004650\n",
            "Iteration 187, loss = 0.42946672\n",
            "Iteration 188, loss = 0.42888836\n",
            "Iteration 189, loss = 0.42835186\n",
            "Iteration 190, loss = 0.42795623\n",
            "Iteration 191, loss = 0.42733525\n",
            "Iteration 192, loss = 0.42668582\n",
            "Iteration 193, loss = 0.42620200\n",
            "Iteration 194, loss = 0.42576182\n",
            "Iteration 195, loss = 0.42522807\n",
            "Iteration 196, loss = 0.42477831\n",
            "Iteration 197, loss = 0.42433371\n",
            "Iteration 198, loss = 0.42389572\n",
            "Iteration 199, loss = 0.42346590\n",
            "Iteration 200, loss = 0.42304868\n",
            "Iteration 201, loss = 0.42255652\n",
            "Iteration 202, loss = 0.42214344\n",
            "Iteration 203, loss = 0.42174740\n",
            "Iteration 204, loss = 0.42130208\n",
            "Iteration 205, loss = 0.42086119\n",
            "Iteration 206, loss = 0.42046801\n",
            "Iteration 207, loss = 0.42002522\n",
            "Iteration 208, loss = 0.41971795\n",
            "Iteration 209, loss = 0.41923070\n",
            "Iteration 210, loss = 0.41890004\n",
            "Iteration 211, loss = 0.41852843\n",
            "Iteration 212, loss = 0.41815911\n",
            "Iteration 213, loss = 0.41786563\n",
            "Iteration 214, loss = 0.41762018\n",
            "Iteration 215, loss = 0.41715979\n",
            "Iteration 216, loss = 0.41693242\n",
            "Iteration 217, loss = 0.41658829\n",
            "Iteration 218, loss = 0.41625242\n",
            "Iteration 219, loss = 0.41595381\n",
            "Iteration 220, loss = 0.41568692\n",
            "Iteration 221, loss = 0.41540781\n",
            "Iteration 222, loss = 0.41503425\n",
            "Iteration 223, loss = 0.41483287\n",
            "Iteration 224, loss = 0.41463883\n",
            "Iteration 225, loss = 0.41428276\n",
            "Iteration 226, loss = 0.41396704\n",
            "Iteration 227, loss = 0.41371982\n",
            "Iteration 228, loss = 0.41343255\n",
            "Iteration 229, loss = 0.41320096\n",
            "Iteration 230, loss = 0.41292391\n",
            "Iteration 231, loss = 0.41262476\n",
            "Iteration 232, loss = 0.41255883\n",
            "Iteration 233, loss = 0.41222800\n",
            "Iteration 234, loss = 0.41190865\n",
            "Iteration 235, loss = 0.41162064\n",
            "Iteration 236, loss = 0.41150651\n",
            "Iteration 237, loss = 0.41135583\n",
            "Iteration 238, loss = 0.41104043\n",
            "Iteration 239, loss = 0.41070310\n",
            "Iteration 240, loss = 0.41047513\n",
            "Iteration 241, loss = 0.41021073\n",
            "Iteration 242, loss = 0.40997307\n",
            "Iteration 243, loss = 0.40980825\n",
            "Iteration 244, loss = 0.40955977\n",
            "Iteration 245, loss = 0.40936921\n",
            "Iteration 246, loss = 0.40911292\n",
            "Iteration 247, loss = 0.40890017\n",
            "Iteration 248, loss = 0.40878706\n",
            "Iteration 249, loss = 0.40854788\n",
            "Iteration 250, loss = 0.40825627\n",
            "Iteration 251, loss = 0.40805845\n",
            "Iteration 252, loss = 0.40784982\n",
            "Iteration 253, loss = 0.40761375\n",
            "Iteration 254, loss = 0.40736501\n",
            "Iteration 255, loss = 0.40725016\n",
            "Iteration 256, loss = 0.40718956\n",
            "Iteration 257, loss = 0.40685180\n",
            "Iteration 258, loss = 0.40656715\n",
            "Iteration 259, loss = 0.40653489\n",
            "Iteration 260, loss = 0.40632275\n",
            "Iteration 261, loss = 0.40606864\n",
            "Iteration 262, loss = 0.40599306\n",
            "Iteration 263, loss = 0.40576121\n",
            "Iteration 264, loss = 0.40557303\n",
            "Iteration 265, loss = 0.40545927\n",
            "Iteration 266, loss = 0.40526674\n",
            "Iteration 267, loss = 0.40514945\n",
            "Iteration 268, loss = 0.40489039\n",
            "Iteration 269, loss = 0.40480453\n",
            "Iteration 270, loss = 0.40464324\n",
            "Iteration 271, loss = 0.40445924\n",
            "Iteration 272, loss = 0.40449243\n",
            "Iteration 273, loss = 0.40412925\n",
            "Iteration 274, loss = 0.40401553\n",
            "Iteration 275, loss = 0.40381214\n",
            "Iteration 276, loss = 0.40368007\n",
            "Iteration 277, loss = 0.40357101\n",
            "Iteration 278, loss = 0.40336539\n",
            "Iteration 279, loss = 0.40326553\n",
            "Iteration 280, loss = 0.40310244\n",
            "Iteration 281, loss = 0.40297724\n",
            "Iteration 282, loss = 0.40276197\n",
            "Iteration 283, loss = 0.40265668\n",
            "Iteration 284, loss = 0.40256470\n",
            "Iteration 285, loss = 0.40250345\n",
            "Iteration 286, loss = 0.40245741\n",
            "Iteration 287, loss = 0.40213245\n",
            "Iteration 288, loss = 0.40198485\n",
            "Iteration 289, loss = 0.40196792\n",
            "Iteration 290, loss = 0.40172673\n",
            "Iteration 291, loss = 0.40162247\n",
            "Iteration 292, loss = 0.40155350\n",
            "Iteration 293, loss = 0.40145354\n",
            "Iteration 294, loss = 0.40130668\n",
            "Iteration 295, loss = 0.40111408\n",
            "Iteration 296, loss = 0.40107901\n",
            "Iteration 297, loss = 0.40092722\n",
            "Iteration 298, loss = 0.40078083\n",
            "Iteration 299, loss = 0.40068413\n",
            "Iteration 300, loss = 0.40056725\n",
            "Iteration 301, loss = 0.40044640\n",
            "Iteration 302, loss = 0.40031522\n",
            "Iteration 303, loss = 0.40025584\n",
            "Iteration 304, loss = 0.40018353\n",
            "Iteration 305, loss = 0.40008040\n",
            "Iteration 306, loss = 0.39989880\n",
            "Iteration 307, loss = 0.39967843\n",
            "Iteration 308, loss = 0.39958839\n",
            "Iteration 309, loss = 0.39963981\n",
            "Iteration 310, loss = 0.39945581\n",
            "Iteration 311, loss = 0.39929035\n",
            "Iteration 312, loss = 0.39917724\n",
            "Iteration 313, loss = 0.39907008\n",
            "Iteration 314, loss = 0.39909852\n",
            "Iteration 315, loss = 0.39884229\n",
            "Iteration 316, loss = 0.39875621\n",
            "Iteration 317, loss = 0.39866930\n",
            "Iteration 318, loss = 0.39859421\n",
            "Iteration 319, loss = 0.39846209\n",
            "Iteration 320, loss = 0.39840588\n",
            "Iteration 321, loss = 0.39826346\n",
            "Iteration 322, loss = 0.39815141\n",
            "Iteration 323, loss = 0.39805419\n",
            "Iteration 324, loss = 0.39794977\n",
            "Iteration 325, loss = 0.39784930\n",
            "Iteration 326, loss = 0.39776990\n",
            "Iteration 327, loss = 0.39774016\n",
            "Iteration 328, loss = 0.39756664\n",
            "Iteration 329, loss = 0.39747883\n",
            "Iteration 330, loss = 0.39742152\n",
            "Iteration 331, loss = 0.39738018\n",
            "Iteration 332, loss = 0.39735842\n",
            "Iteration 333, loss = 0.39715584\n",
            "Iteration 334, loss = 0.39706163\n",
            "Iteration 335, loss = 0.39691686\n",
            "Iteration 336, loss = 0.39688143\n",
            "Iteration 337, loss = 0.39679918\n",
            "Iteration 338, loss = 0.39668945\n",
            "Iteration 339, loss = 0.39656457\n",
            "Iteration 340, loss = 0.39649200\n",
            "Iteration 341, loss = 0.39641962\n",
            "Iteration 342, loss = 0.39631583\n",
            "Iteration 343, loss = 0.39627612\n",
            "Iteration 344, loss = 0.39612829\n",
            "Iteration 345, loss = 0.39595673\n",
            "Iteration 346, loss = 0.39590092\n",
            "Iteration 347, loss = 0.39591500\n",
            "Iteration 348, loss = 0.39583593\n",
            "Iteration 349, loss = 0.39566783\n",
            "Iteration 350, loss = 0.39565461\n",
            "Iteration 351, loss = 0.39537626\n",
            "Iteration 352, loss = 0.39540713\n",
            "Iteration 353, loss = 0.39526129\n",
            "Iteration 354, loss = 0.39515607\n",
            "Iteration 355, loss = 0.39516258\n",
            "Iteration 356, loss = 0.39489465\n",
            "Iteration 357, loss = 0.39485717\n",
            "Iteration 358, loss = 0.39473535\n",
            "Iteration 359, loss = 0.39469708\n",
            "Iteration 360, loss = 0.39465281\n",
            "Iteration 361, loss = 0.39457623\n",
            "Iteration 362, loss = 0.39447598\n",
            "Iteration 363, loss = 0.39438087\n",
            "Iteration 364, loss = 0.39427304\n",
            "Iteration 365, loss = 0.39410265\n",
            "Iteration 366, loss = 0.39402193\n",
            "Iteration 367, loss = 0.39401636\n",
            "Iteration 368, loss = 0.39393748\n",
            "Iteration 369, loss = 0.39384908\n",
            "Iteration 370, loss = 0.39381820\n",
            "Iteration 371, loss = 0.39366089\n",
            "Iteration 372, loss = 0.39361417\n",
            "Iteration 373, loss = 0.39355803\n",
            "Iteration 374, loss = 0.39346745\n",
            "Iteration 375, loss = 0.39337403\n",
            "Iteration 376, loss = 0.39331068\n",
            "Iteration 377, loss = 0.39338957\n",
            "Iteration 378, loss = 0.39327499\n",
            "Iteration 379, loss = 0.39315378\n",
            "Iteration 380, loss = 0.39324138\n",
            "Iteration 381, loss = 0.39300249\n",
            "Iteration 382, loss = 0.39288606\n",
            "Iteration 383, loss = 0.39285431\n",
            "Iteration 384, loss = 0.39282707\n",
            "Iteration 385, loss = 0.39272648\n",
            "Iteration 386, loss = 0.39265315\n",
            "Iteration 387, loss = 0.39259211\n",
            "Iteration 388, loss = 0.39249526\n",
            "Iteration 389, loss = 0.39243503\n",
            "Iteration 390, loss = 0.39235288\n",
            "Iteration 391, loss = 0.39241074\n",
            "Iteration 392, loss = 0.39226056\n",
            "Iteration 393, loss = 0.39218830\n",
            "Iteration 394, loss = 0.39218145\n",
            "Iteration 395, loss = 0.39213810\n",
            "Iteration 396, loss = 0.39202472\n",
            "Iteration 397, loss = 0.39194864\n",
            "Iteration 398, loss = 0.39191531\n",
            "Iteration 399, loss = 0.39185568\n",
            "Iteration 400, loss = 0.39173676\n",
            "Iteration 401, loss = 0.39170774\n",
            "Iteration 402, loss = 0.39161354\n",
            "Iteration 403, loss = 0.39169369\n",
            "Iteration 404, loss = 0.39157202\n",
            "Iteration 405, loss = 0.39146766\n",
            "Iteration 406, loss = 0.39136560\n",
            "Iteration 407, loss = 0.39139462\n",
            "Iteration 408, loss = 0.39133603\n",
            "Iteration 409, loss = 0.39121700\n",
            "Iteration 410, loss = 0.39116912\n",
            "Iteration 411, loss = 0.39120663\n",
            "Iteration 412, loss = 0.39118360\n",
            "Iteration 413, loss = 0.39096326\n",
            "Iteration 414, loss = 0.39102695\n",
            "Iteration 415, loss = 0.39086637\n",
            "Iteration 416, loss = 0.39079280\n",
            "Iteration 417, loss = 0.39075958\n",
            "Iteration 418, loss = 0.39068328\n",
            "Iteration 419, loss = 0.39065077\n",
            "Iteration 420, loss = 0.39061348\n",
            "Iteration 421, loss = 0.39061193\n",
            "Iteration 422, loss = 0.39054648\n",
            "Iteration 423, loss = 0.39047665\n",
            "Iteration 424, loss = 0.39034629\n",
            "Iteration 425, loss = 0.39029225\n",
            "Iteration 426, loss = 0.39035186\n",
            "Iteration 427, loss = 0.39047818\n",
            "Iteration 428, loss = 0.39043488\n",
            "Iteration 429, loss = 0.39030860\n",
            "Iteration 430, loss = 0.39022132\n",
            "Iteration 431, loss = 0.39005831\n",
            "Iteration 432, loss = 0.39014336\n",
            "Iteration 433, loss = 0.39004441\n",
            "Iteration 434, loss = 0.38991743\n",
            "Iteration 435, loss = 0.38984989\n",
            "Iteration 436, loss = 0.38975976\n",
            "Iteration 437, loss = 0.38970547\n",
            "Iteration 438, loss = 0.38979192\n",
            "Iteration 439, loss = 0.38981408\n",
            "Iteration 440, loss = 0.38961644\n",
            "Iteration 441, loss = 0.38952976\n",
            "Iteration 442, loss = 0.38954660\n",
            "Iteration 443, loss = 0.38938969\n",
            "Iteration 444, loss = 0.38951247\n",
            "Iteration 445, loss = 0.38939912\n",
            "Iteration 446, loss = 0.38939619\n",
            "Iteration 447, loss = 0.38934365\n",
            "Iteration 448, loss = 0.38925568\n",
            "Iteration 449, loss = 0.38912151\n",
            "Iteration 450, loss = 0.38914170\n",
            "Iteration 451, loss = 0.38910552\n",
            "Iteration 452, loss = 0.38915940\n",
            "Iteration 453, loss = 0.38916174\n",
            "Iteration 454, loss = 0.38913118\n",
            "Iteration 455, loss = 0.38903891\n",
            "Iteration 456, loss = 0.38888355\n",
            "Iteration 457, loss = 0.38879516\n",
            "Iteration 458, loss = 0.38880704\n",
            "Iteration 459, loss = 0.38886491\n",
            "Iteration 460, loss = 0.38879900\n",
            "Iteration 461, loss = 0.38865544\n",
            "Iteration 462, loss = 0.38867202\n",
            "Iteration 463, loss = 0.38864734\n",
            "Iteration 464, loss = 0.38858454\n",
            "Iteration 465, loss = 0.38852864\n",
            "Iteration 466, loss = 0.38851949\n",
            "Iteration 467, loss = 0.38847429\n",
            "Iteration 468, loss = 0.38850150\n",
            "Iteration 469, loss = 0.38836264\n",
            "Iteration 470, loss = 0.38829422\n",
            "Iteration 471, loss = 0.38824588\n",
            "Iteration 472, loss = 0.38832381\n",
            "Iteration 473, loss = 0.38832169\n",
            "Iteration 474, loss = 0.38822755\n",
            "Iteration 475, loss = 0.38822733\n",
            "Iteration 476, loss = 0.38810356\n",
            "Iteration 477, loss = 0.38809505\n",
            "Iteration 478, loss = 0.38802235\n",
            "Iteration 479, loss = 0.38798394\n",
            "Iteration 480, loss = 0.38803418\n",
            "Iteration 481, loss = 0.38793587\n",
            "Iteration 482, loss = 0.38789927\n",
            "Iteration 483, loss = 0.38783513\n",
            "Iteration 484, loss = 0.38777443\n",
            "Iteration 485, loss = 0.38774009\n",
            "Iteration 486, loss = 0.38769881\n",
            "Iteration 487, loss = 0.38768444\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.80965269\n",
            "Iteration 2, loss = 0.80703593\n",
            "Iteration 3, loss = 0.80435516\n",
            "Iteration 4, loss = 0.80166999\n",
            "Iteration 5, loss = 0.79895310\n",
            "Iteration 6, loss = 0.79618509\n",
            "Iteration 7, loss = 0.79350033\n",
            "Iteration 8, loss = 0.79080190\n",
            "Iteration 9, loss = 0.78794785\n",
            "Iteration 10, loss = 0.78518685\n",
            "Iteration 11, loss = 0.78245754\n",
            "Iteration 12, loss = 0.77967632\n",
            "Iteration 13, loss = 0.77689630\n",
            "Iteration 14, loss = 0.77398614\n",
            "Iteration 15, loss = 0.77127890\n",
            "Iteration 16, loss = 0.76850222\n",
            "Iteration 17, loss = 0.76567153\n",
            "Iteration 18, loss = 0.76275749\n",
            "Iteration 19, loss = 0.75986821\n",
            "Iteration 20, loss = 0.75690908\n",
            "Iteration 21, loss = 0.75409540\n",
            "Iteration 22, loss = 0.75098542\n",
            "Iteration 23, loss = 0.74805019\n",
            "Iteration 24, loss = 0.74495416\n",
            "Iteration 25, loss = 0.74175019\n",
            "Iteration 26, loss = 0.73872263\n",
            "Iteration 27, loss = 0.73541645\n",
            "Iteration 28, loss = 0.73222518\n",
            "Iteration 29, loss = 0.72899709\n",
            "Iteration 30, loss = 0.72567880\n",
            "Iteration 31, loss = 0.72227532\n",
            "Iteration 32, loss = 0.71892514\n",
            "Iteration 33, loss = 0.71539925\n",
            "Iteration 34, loss = 0.71198571\n",
            "Iteration 35, loss = 0.70847392\n",
            "Iteration 36, loss = 0.70495211\n",
            "Iteration 37, loss = 0.70127444\n",
            "Iteration 38, loss = 0.69787244\n",
            "Iteration 39, loss = 0.69428119\n",
            "Iteration 40, loss = 0.69055007\n",
            "Iteration 41, loss = 0.68697378\n",
            "Iteration 42, loss = 0.68318139\n",
            "Iteration 43, loss = 0.67971845\n",
            "Iteration 44, loss = 0.67591221\n",
            "Iteration 45, loss = 0.67230291\n",
            "Iteration 46, loss = 0.66848036\n",
            "Iteration 47, loss = 0.66497870\n",
            "Iteration 48, loss = 0.66140408\n",
            "Iteration 49, loss = 0.65787388\n",
            "Iteration 50, loss = 0.65431660\n",
            "Iteration 51, loss = 0.65083814\n",
            "Iteration 52, loss = 0.64730158\n",
            "Iteration 53, loss = 0.64378227\n",
            "Iteration 54, loss = 0.64053228\n",
            "Iteration 55, loss = 0.63714642\n",
            "Iteration 56, loss = 0.63376399\n",
            "Iteration 57, loss = 0.63015064\n",
            "Iteration 58, loss = 0.62683984\n",
            "Iteration 59, loss = 0.62337432\n",
            "Iteration 60, loss = 0.61983938\n",
            "Iteration 61, loss = 0.61628977\n",
            "Iteration 62, loss = 0.61285100\n",
            "Iteration 63, loss = 0.60939487\n",
            "Iteration 64, loss = 0.60578711\n",
            "Iteration 65, loss = 0.60265853\n",
            "Iteration 66, loss = 0.59915133\n",
            "Iteration 67, loss = 0.59574864\n",
            "Iteration 68, loss = 0.59244552\n",
            "Iteration 69, loss = 0.58921422\n",
            "Iteration 70, loss = 0.58608045\n",
            "Iteration 71, loss = 0.58332078\n",
            "Iteration 72, loss = 0.58056487\n",
            "Iteration 73, loss = 0.57781823\n",
            "Iteration 74, loss = 0.57526908\n",
            "Iteration 75, loss = 0.57270759\n",
            "Iteration 76, loss = 0.57017896\n",
            "Iteration 77, loss = 0.56772233\n",
            "Iteration 78, loss = 0.56539655\n",
            "Iteration 79, loss = 0.56304158\n",
            "Iteration 80, loss = 0.56085567\n",
            "Iteration 81, loss = 0.55878303\n",
            "Iteration 82, loss = 0.55655449\n",
            "Iteration 83, loss = 0.55454501\n",
            "Iteration 84, loss = 0.55254884\n",
            "Iteration 85, loss = 0.55049171\n",
            "Iteration 86, loss = 0.54845347\n",
            "Iteration 87, loss = 0.54645046\n",
            "Iteration 88, loss = 0.54442071\n",
            "Iteration 89, loss = 0.54235333\n",
            "Iteration 90, loss = 0.54044855\n",
            "Iteration 91, loss = 0.53846108\n",
            "Iteration 92, loss = 0.53652412\n",
            "Iteration 93, loss = 0.53461741\n",
            "Iteration 94, loss = 0.53288510\n",
            "Iteration 95, loss = 0.53105990\n",
            "Iteration 96, loss = 0.52937787\n",
            "Iteration 97, loss = 0.52762628\n",
            "Iteration 98, loss = 0.52599151\n",
            "Iteration 99, loss = 0.52434394\n",
            "Iteration 100, loss = 0.52267404\n",
            "Iteration 101, loss = 0.52112784\n",
            "Iteration 102, loss = 0.51965480\n",
            "Iteration 103, loss = 0.51815726\n",
            "Iteration 104, loss = 0.51667115\n",
            "Iteration 105, loss = 0.51521287\n",
            "Iteration 106, loss = 0.51386435\n",
            "Iteration 107, loss = 0.51244683\n",
            "Iteration 108, loss = 0.51101694\n",
            "Iteration 109, loss = 0.50976468\n",
            "Iteration 110, loss = 0.50840505\n",
            "Iteration 111, loss = 0.50713960\n",
            "Iteration 112, loss = 0.50593411\n",
            "Iteration 113, loss = 0.50464732\n",
            "Iteration 114, loss = 0.50339265\n",
            "Iteration 115, loss = 0.50217580\n",
            "Iteration 116, loss = 0.50097180\n",
            "Iteration 117, loss = 0.49983931\n",
            "Iteration 118, loss = 0.49869733\n",
            "Iteration 119, loss = 0.49755497\n",
            "Iteration 120, loss = 0.49637662\n",
            "Iteration 121, loss = 0.49532786\n",
            "Iteration 122, loss = 0.49413817\n",
            "Iteration 123, loss = 0.49312039\n",
            "Iteration 124, loss = 0.49196352\n",
            "Iteration 125, loss = 0.49091806\n",
            "Iteration 126, loss = 0.48982093\n",
            "Iteration 127, loss = 0.48875738\n",
            "Iteration 128, loss = 0.48773318\n",
            "Iteration 129, loss = 0.48673168\n",
            "Iteration 130, loss = 0.48570691\n",
            "Iteration 131, loss = 0.48474416\n",
            "Iteration 132, loss = 0.48362343\n",
            "Iteration 133, loss = 0.48265907\n",
            "Iteration 134, loss = 0.48173692\n",
            "Iteration 135, loss = 0.48081954\n",
            "Iteration 136, loss = 0.47985436\n",
            "Iteration 137, loss = 0.47902002\n",
            "Iteration 138, loss = 0.47810734\n",
            "Iteration 139, loss = 0.47723108\n",
            "Iteration 140, loss = 0.47644013\n",
            "Iteration 141, loss = 0.47555034\n",
            "Iteration 142, loss = 0.47480363\n",
            "Iteration 143, loss = 0.47396375\n",
            "Iteration 144, loss = 0.47323320\n",
            "Iteration 145, loss = 0.47250257\n",
            "Iteration 146, loss = 0.47179105\n",
            "Iteration 147, loss = 0.47098568\n",
            "Iteration 148, loss = 0.47020988\n",
            "Iteration 149, loss = 0.46969190\n",
            "Iteration 150, loss = 0.46885575\n",
            "Iteration 151, loss = 0.46819335\n",
            "Iteration 152, loss = 0.46755535\n",
            "Iteration 153, loss = 0.46689257\n",
            "Iteration 154, loss = 0.46622535\n",
            "Iteration 155, loss = 0.46560707\n",
            "Iteration 156, loss = 0.46497245\n",
            "Iteration 157, loss = 0.46423610\n",
            "Iteration 158, loss = 0.46360191\n",
            "Iteration 159, loss = 0.46306502\n",
            "Iteration 160, loss = 0.46250309\n",
            "Iteration 161, loss = 0.46192855\n",
            "Iteration 162, loss = 0.46128723\n",
            "Iteration 163, loss = 0.46080888\n",
            "Iteration 164, loss = 0.46023954\n",
            "Iteration 165, loss = 0.45967076\n",
            "Iteration 166, loss = 0.45921447\n",
            "Iteration 167, loss = 0.45865105\n",
            "Iteration 168, loss = 0.45814520\n",
            "Iteration 169, loss = 0.45756954\n",
            "Iteration 170, loss = 0.45705327\n",
            "Iteration 171, loss = 0.45642295\n",
            "Iteration 172, loss = 0.45595835\n",
            "Iteration 173, loss = 0.45539382\n",
            "Iteration 174, loss = 0.45480888\n",
            "Iteration 175, loss = 0.45425049\n",
            "Iteration 176, loss = 0.45373895\n",
            "Iteration 177, loss = 0.45317573\n",
            "Iteration 178, loss = 0.45264166\n",
            "Iteration 179, loss = 0.45217097\n",
            "Iteration 180, loss = 0.45168455\n",
            "Iteration 181, loss = 0.45119755\n",
            "Iteration 182, loss = 0.45062023\n",
            "Iteration 183, loss = 0.45011183\n",
            "Iteration 184, loss = 0.44958090\n",
            "Iteration 185, loss = 0.44907297\n",
            "Iteration 186, loss = 0.44854552\n",
            "Iteration 187, loss = 0.44802718\n",
            "Iteration 188, loss = 0.44752599\n",
            "Iteration 189, loss = 0.44704754\n",
            "Iteration 190, loss = 0.44669267\n",
            "Iteration 191, loss = 0.44613177\n",
            "Iteration 192, loss = 0.44558521\n",
            "Iteration 193, loss = 0.44529165\n",
            "Iteration 194, loss = 0.44479417\n",
            "Iteration 195, loss = 0.44437017\n",
            "Iteration 196, loss = 0.44381512\n",
            "Iteration 197, loss = 0.44343077\n",
            "Iteration 198, loss = 0.44304915\n",
            "Iteration 199, loss = 0.44265555\n",
            "Iteration 200, loss = 0.44217873\n",
            "Iteration 201, loss = 0.44177157\n",
            "Iteration 202, loss = 0.44132244\n",
            "Iteration 203, loss = 0.44097039\n",
            "Iteration 204, loss = 0.44055801\n",
            "Iteration 205, loss = 0.44013088\n",
            "Iteration 206, loss = 0.43974039\n",
            "Iteration 207, loss = 0.43932907\n",
            "Iteration 208, loss = 0.43898344\n",
            "Iteration 209, loss = 0.43857554\n",
            "Iteration 210, loss = 0.43824384\n",
            "Iteration 211, loss = 0.43791389\n",
            "Iteration 212, loss = 0.43757367\n",
            "Iteration 213, loss = 0.43721039\n",
            "Iteration 214, loss = 0.43696212\n",
            "Iteration 215, loss = 0.43654148\n",
            "Iteration 216, loss = 0.43627737\n",
            "Iteration 217, loss = 0.43591179\n",
            "Iteration 218, loss = 0.43566037\n",
            "Iteration 219, loss = 0.43527809\n",
            "Iteration 220, loss = 0.43508567\n",
            "Iteration 221, loss = 0.43468828\n",
            "Iteration 222, loss = 0.43437728\n",
            "Iteration 223, loss = 0.43408532\n",
            "Iteration 224, loss = 0.43378311\n",
            "Iteration 225, loss = 0.43352651\n",
            "Iteration 226, loss = 0.43322210\n",
            "Iteration 227, loss = 0.43294751\n",
            "Iteration 228, loss = 0.43262327\n",
            "Iteration 229, loss = 0.43238452\n",
            "Iteration 230, loss = 0.43211368\n",
            "Iteration 231, loss = 0.43183904\n",
            "Iteration 232, loss = 0.43165789\n",
            "Iteration 233, loss = 0.43131327\n",
            "Iteration 234, loss = 0.43104280\n",
            "Iteration 235, loss = 0.43077624\n",
            "Iteration 236, loss = 0.43053081\n",
            "Iteration 237, loss = 0.43038447\n",
            "Iteration 238, loss = 0.43012243\n",
            "Iteration 239, loss = 0.42973403\n",
            "Iteration 240, loss = 0.42952056\n",
            "Iteration 241, loss = 0.42922373\n",
            "Iteration 242, loss = 0.42897671\n",
            "Iteration 243, loss = 0.42879941\n",
            "Iteration 244, loss = 0.42850892\n",
            "Iteration 245, loss = 0.42844570\n",
            "Iteration 246, loss = 0.42802328\n",
            "Iteration 247, loss = 0.42774801\n",
            "Iteration 248, loss = 0.42755687\n",
            "Iteration 249, loss = 0.42731143\n",
            "Iteration 250, loss = 0.42705242\n",
            "Iteration 251, loss = 0.42684221\n",
            "Iteration 252, loss = 0.42669035\n",
            "Iteration 253, loss = 0.42638208\n",
            "Iteration 254, loss = 0.42612463\n",
            "Iteration 255, loss = 0.42597026\n",
            "Iteration 256, loss = 0.42591537\n",
            "Iteration 257, loss = 0.42564345\n",
            "Iteration 258, loss = 0.42535014\n",
            "Iteration 259, loss = 0.42525602\n",
            "Iteration 260, loss = 0.42493725\n",
            "Iteration 261, loss = 0.42473609\n",
            "Iteration 262, loss = 0.42450219\n",
            "Iteration 263, loss = 0.42436502\n",
            "Iteration 264, loss = 0.42411028\n",
            "Iteration 265, loss = 0.42396065\n",
            "Iteration 266, loss = 0.42376293\n",
            "Iteration 267, loss = 0.42358522\n",
            "Iteration 268, loss = 0.42333529\n",
            "Iteration 269, loss = 0.42326143\n",
            "Iteration 270, loss = 0.42302430\n",
            "Iteration 271, loss = 0.42279538\n",
            "Iteration 272, loss = 0.42273940\n",
            "Iteration 273, loss = 0.42244593\n",
            "Iteration 274, loss = 0.42232958\n",
            "Iteration 275, loss = 0.42212212\n",
            "Iteration 276, loss = 0.42195891\n",
            "Iteration 277, loss = 0.42175040\n",
            "Iteration 278, loss = 0.42152285\n",
            "Iteration 279, loss = 0.42141367\n",
            "Iteration 280, loss = 0.42121822\n",
            "Iteration 281, loss = 0.42104188\n",
            "Iteration 282, loss = 0.42083923\n",
            "Iteration 283, loss = 0.42066763\n",
            "Iteration 284, loss = 0.42063774\n",
            "Iteration 285, loss = 0.42045493\n",
            "Iteration 286, loss = 0.42031126\n",
            "Iteration 287, loss = 0.42009295\n",
            "Iteration 288, loss = 0.41987760\n",
            "Iteration 289, loss = 0.41985356\n",
            "Iteration 290, loss = 0.41956320\n",
            "Iteration 291, loss = 0.41947811\n",
            "Iteration 292, loss = 0.41928080\n",
            "Iteration 293, loss = 0.41924251\n",
            "Iteration 294, loss = 0.41895927\n",
            "Iteration 295, loss = 0.41879291\n",
            "Iteration 296, loss = 0.41869924\n",
            "Iteration 297, loss = 0.41854259\n",
            "Iteration 298, loss = 0.41834176\n",
            "Iteration 299, loss = 0.41823381\n",
            "Iteration 300, loss = 0.41817777\n",
            "Iteration 301, loss = 0.41797961\n",
            "Iteration 302, loss = 0.41782025\n",
            "Iteration 303, loss = 0.41766708\n",
            "Iteration 304, loss = 0.41741795\n",
            "Iteration 305, loss = 0.41752756\n",
            "Iteration 306, loss = 0.41726599\n",
            "Iteration 307, loss = 0.41710564\n",
            "Iteration 308, loss = 0.41696589\n",
            "Iteration 309, loss = 0.41680663\n",
            "Iteration 310, loss = 0.41670746\n",
            "Iteration 311, loss = 0.41650075\n",
            "Iteration 312, loss = 0.41634780\n",
            "Iteration 313, loss = 0.41620814\n",
            "Iteration 314, loss = 0.41625892\n",
            "Iteration 315, loss = 0.41610458\n",
            "Iteration 316, loss = 0.41597762\n",
            "Iteration 317, loss = 0.41580179\n",
            "Iteration 318, loss = 0.41563318\n",
            "Iteration 319, loss = 0.41554950\n",
            "Iteration 320, loss = 0.41539053\n",
            "Iteration 321, loss = 0.41526559\n",
            "Iteration 322, loss = 0.41507679\n",
            "Iteration 323, loss = 0.41501119\n",
            "Iteration 324, loss = 0.41484431\n",
            "Iteration 325, loss = 0.41471762\n",
            "Iteration 326, loss = 0.41455213\n",
            "Iteration 327, loss = 0.41442073\n",
            "Iteration 328, loss = 0.41433020\n",
            "Iteration 329, loss = 0.41417291\n",
            "Iteration 330, loss = 0.41406798\n",
            "Iteration 331, loss = 0.41405959\n",
            "Iteration 332, loss = 0.41384950\n",
            "Iteration 333, loss = 0.41363740\n",
            "Iteration 334, loss = 0.41360854\n",
            "Iteration 335, loss = 0.41344512\n",
            "Iteration 336, loss = 0.41338817\n",
            "Iteration 337, loss = 0.41326901\n",
            "Iteration 338, loss = 0.41304797\n",
            "Iteration 339, loss = 0.41296617\n",
            "Iteration 340, loss = 0.41276705\n",
            "Iteration 341, loss = 0.41262793\n",
            "Iteration 342, loss = 0.41254329\n",
            "Iteration 343, loss = 0.41241958\n",
            "Iteration 344, loss = 0.41224557\n",
            "Iteration 345, loss = 0.41209007\n",
            "Iteration 346, loss = 0.41196382\n",
            "Iteration 347, loss = 0.41190631\n",
            "Iteration 348, loss = 0.41182704\n",
            "Iteration 349, loss = 0.41166909\n",
            "Iteration 350, loss = 0.41156757\n",
            "Iteration 351, loss = 0.41139353\n",
            "Iteration 352, loss = 0.41139129\n",
            "Iteration 353, loss = 0.41118465\n",
            "Iteration 354, loss = 0.41105963\n",
            "Iteration 355, loss = 0.41098958\n",
            "Iteration 356, loss = 0.41080129\n",
            "Iteration 357, loss = 0.41072878\n",
            "Iteration 358, loss = 0.41059979\n",
            "Iteration 359, loss = 0.41054505\n",
            "Iteration 360, loss = 0.41041500\n",
            "Iteration 361, loss = 0.41033527\n",
            "Iteration 362, loss = 0.41021792\n",
            "Iteration 363, loss = 0.41011602\n",
            "Iteration 364, loss = 0.40996684\n",
            "Iteration 365, loss = 0.40983182\n",
            "Iteration 366, loss = 0.40974288\n",
            "Iteration 367, loss = 0.40969364\n",
            "Iteration 368, loss = 0.40950447\n",
            "Iteration 369, loss = 0.40943377\n",
            "Iteration 370, loss = 0.40932570\n",
            "Iteration 371, loss = 0.40913581\n",
            "Iteration 372, loss = 0.40905632\n",
            "Iteration 373, loss = 0.40896679\n",
            "Iteration 374, loss = 0.40884409\n",
            "Iteration 375, loss = 0.40876430\n",
            "Iteration 376, loss = 0.40864904\n",
            "Iteration 377, loss = 0.40857237\n",
            "Iteration 378, loss = 0.40842581\n",
            "Iteration 379, loss = 0.40828268\n",
            "Iteration 380, loss = 0.40838838\n",
            "Iteration 381, loss = 0.40815612\n",
            "Iteration 382, loss = 0.40807781\n",
            "Iteration 383, loss = 0.40795720\n",
            "Iteration 384, loss = 0.40790949\n",
            "Iteration 385, loss = 0.40777372\n",
            "Iteration 386, loss = 0.40761943\n",
            "Iteration 387, loss = 0.40751392\n",
            "Iteration 388, loss = 0.40740805\n",
            "Iteration 389, loss = 0.40744612\n",
            "Iteration 390, loss = 0.40729689\n",
            "Iteration 391, loss = 0.40718151\n",
            "Iteration 392, loss = 0.40707455\n",
            "Iteration 393, loss = 0.40706371\n",
            "Iteration 394, loss = 0.40697586\n",
            "Iteration 395, loss = 0.40684439\n",
            "Iteration 396, loss = 0.40671099\n",
            "Iteration 397, loss = 0.40660161\n",
            "Iteration 398, loss = 0.40654347\n",
            "Iteration 399, loss = 0.40642085\n",
            "Iteration 400, loss = 0.40647513\n",
            "Iteration 401, loss = 0.40624091\n",
            "Iteration 402, loss = 0.40618498\n",
            "Iteration 403, loss = 0.40630151\n",
            "Iteration 404, loss = 0.40610106\n",
            "Iteration 405, loss = 0.40595845\n",
            "Iteration 406, loss = 0.40582769\n",
            "Iteration 407, loss = 0.40577426\n",
            "Iteration 408, loss = 0.40569704\n",
            "Iteration 409, loss = 0.40558659\n",
            "Iteration 410, loss = 0.40547941\n",
            "Iteration 411, loss = 0.40544547\n",
            "Iteration 412, loss = 0.40538696\n",
            "Iteration 413, loss = 0.40520374\n",
            "Iteration 414, loss = 0.40518937\n",
            "Iteration 415, loss = 0.40504138\n",
            "Iteration 416, loss = 0.40499461\n",
            "Iteration 417, loss = 0.40492458\n",
            "Iteration 418, loss = 0.40480755\n",
            "Iteration 419, loss = 0.40472034\n",
            "Iteration 420, loss = 0.40473003\n",
            "Iteration 421, loss = 0.40452867\n",
            "Iteration 422, loss = 0.40452018\n",
            "Iteration 423, loss = 0.40436316\n",
            "Iteration 424, loss = 0.40420697\n",
            "Iteration 425, loss = 0.40401829\n",
            "Iteration 426, loss = 0.40410565\n",
            "Iteration 427, loss = 0.40426595\n",
            "Iteration 428, loss = 0.40430262\n",
            "Iteration 429, loss = 0.40415909\n",
            "Iteration 430, loss = 0.40399231\n",
            "Iteration 431, loss = 0.40372432\n",
            "Iteration 432, loss = 0.40359971\n",
            "Iteration 433, loss = 0.40348653\n",
            "Iteration 434, loss = 0.40342665\n",
            "Iteration 435, loss = 0.40347408\n",
            "Iteration 436, loss = 0.40325884\n",
            "Iteration 437, loss = 0.40311750\n",
            "Iteration 438, loss = 0.40303765\n",
            "Iteration 439, loss = 0.40308254\n",
            "Iteration 440, loss = 0.40297510\n",
            "Iteration 441, loss = 0.40289787\n",
            "Iteration 442, loss = 0.40289053\n",
            "Iteration 443, loss = 0.40279237\n",
            "Iteration 444, loss = 0.40280056\n",
            "Iteration 445, loss = 0.40261243\n",
            "Iteration 446, loss = 0.40251489\n",
            "Iteration 447, loss = 0.40243514\n",
            "Iteration 448, loss = 0.40227408\n",
            "Iteration 449, loss = 0.40211219\n",
            "Iteration 450, loss = 0.40204489\n",
            "Iteration 451, loss = 0.40199427\n",
            "Iteration 452, loss = 0.40183976\n",
            "Iteration 453, loss = 0.40170538\n",
            "Iteration 454, loss = 0.40163216\n",
            "Iteration 455, loss = 0.40146837\n",
            "Iteration 456, loss = 0.40138088\n",
            "Iteration 457, loss = 0.40121926\n",
            "Iteration 458, loss = 0.40111099\n",
            "Iteration 459, loss = 0.40104213\n",
            "Iteration 460, loss = 0.40091949\n",
            "Iteration 461, loss = 0.40085569\n",
            "Iteration 462, loss = 0.40074439\n",
            "Iteration 463, loss = 0.40064779\n",
            "Iteration 464, loss = 0.40053424\n",
            "Iteration 465, loss = 0.40047362\n",
            "Iteration 466, loss = 0.40043553\n",
            "Iteration 467, loss = 0.40032845\n",
            "Iteration 468, loss = 0.40021003\n",
            "Iteration 469, loss = 0.40009325\n",
            "Iteration 470, loss = 0.40002685\n",
            "Iteration 471, loss = 0.39997303\n",
            "Iteration 472, loss = 0.39993117\n",
            "Iteration 473, loss = 0.39983574\n",
            "Iteration 474, loss = 0.39966671\n",
            "Iteration 475, loss = 0.39978204\n",
            "Iteration 476, loss = 0.39960730\n",
            "Iteration 477, loss = 0.39949979\n",
            "Iteration 478, loss = 0.39941460\n",
            "Iteration 479, loss = 0.39930635\n",
            "Iteration 480, loss = 0.39921797\n",
            "Iteration 481, loss = 0.39914177\n",
            "Iteration 482, loss = 0.39907874\n",
            "Iteration 483, loss = 0.39895583\n",
            "Iteration 484, loss = 0.39901785\n",
            "Iteration 485, loss = 0.39881618\n",
            "Iteration 486, loss = 0.39874526\n",
            "Iteration 487, loss = 0.39860942\n",
            "Iteration 488, loss = 0.39865068\n",
            "Iteration 489, loss = 0.39853111\n",
            "Iteration 490, loss = 0.39844566\n",
            "Iteration 491, loss = 0.39834587\n",
            "Iteration 492, loss = 0.39828172\n",
            "Iteration 493, loss = 0.39824767\n",
            "Iteration 494, loss = 0.39830141\n",
            "Iteration 495, loss = 0.39824239\n",
            "Iteration 496, loss = 0.39807284\n",
            "Iteration 497, loss = 0.39793791\n",
            "Iteration 498, loss = 0.39789486\n",
            "Iteration 499, loss = 0.39796064\n",
            "Iteration 500, loss = 0.39786349\n",
            "Iteration 501, loss = 0.39778377\n",
            "Iteration 502, loss = 0.39768185\n",
            "Iteration 503, loss = 0.39760545\n",
            "Iteration 504, loss = 0.39749991\n",
            "Iteration 505, loss = 0.39743057\n",
            "Iteration 506, loss = 0.39737609\n",
            "Iteration 507, loss = 0.39736024\n",
            "Iteration 508, loss = 0.39740670\n",
            "Iteration 509, loss = 0.39738966\n",
            "Iteration 510, loss = 0.39720424\n",
            "Iteration 511, loss = 0.39712400\n",
            "Iteration 512, loss = 0.39712779\n",
            "Iteration 513, loss = 0.39704034\n",
            "Iteration 514, loss = 0.39698275\n",
            "Iteration 515, loss = 0.39689924\n",
            "Iteration 516, loss = 0.39684980\n",
            "Iteration 517, loss = 0.39682001\n",
            "Iteration 518, loss = 0.39677943\n",
            "Iteration 519, loss = 0.39668540\n",
            "Iteration 520, loss = 0.39658726\n",
            "Iteration 521, loss = 0.39651186\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.81076682\n",
            "Iteration 2, loss = 0.80818715\n",
            "Iteration 3, loss = 0.80540680\n",
            "Iteration 4, loss = 0.80280873\n",
            "Iteration 5, loss = 0.80004553\n",
            "Iteration 6, loss = 0.79746264\n",
            "Iteration 7, loss = 0.79468699\n",
            "Iteration 8, loss = 0.79209523\n",
            "Iteration 9, loss = 0.78927565\n",
            "Iteration 10, loss = 0.78664102\n",
            "Iteration 11, loss = 0.78388108\n",
            "Iteration 12, loss = 0.78121272\n",
            "Iteration 13, loss = 0.77844487\n",
            "Iteration 14, loss = 0.77567003\n",
            "Iteration 15, loss = 0.77292566\n",
            "Iteration 16, loss = 0.77017608\n",
            "Iteration 17, loss = 0.76738421\n",
            "Iteration 18, loss = 0.76445701\n",
            "Iteration 19, loss = 0.76159857\n",
            "Iteration 20, loss = 0.75854016\n",
            "Iteration 21, loss = 0.75570751\n",
            "Iteration 22, loss = 0.75258739\n",
            "Iteration 23, loss = 0.74948820\n",
            "Iteration 24, loss = 0.74634792\n",
            "Iteration 25, loss = 0.74310522\n",
            "Iteration 26, loss = 0.74010068\n",
            "Iteration 27, loss = 0.73670811\n",
            "Iteration 28, loss = 0.73350253\n",
            "Iteration 29, loss = 0.73033395\n",
            "Iteration 30, loss = 0.72692153\n",
            "Iteration 31, loss = 0.72360408\n",
            "Iteration 32, loss = 0.72025887\n",
            "Iteration 33, loss = 0.71682994\n",
            "Iteration 34, loss = 0.71336566\n",
            "Iteration 35, loss = 0.70982920\n",
            "Iteration 36, loss = 0.70642143\n",
            "Iteration 37, loss = 0.70281163\n",
            "Iteration 38, loss = 0.69929622\n",
            "Iteration 39, loss = 0.69573189\n",
            "Iteration 40, loss = 0.69202064\n",
            "Iteration 41, loss = 0.68846881\n",
            "Iteration 42, loss = 0.68470015\n",
            "Iteration 43, loss = 0.68123556\n",
            "Iteration 44, loss = 0.67734633\n",
            "Iteration 45, loss = 0.67390020\n",
            "Iteration 46, loss = 0.66999843\n",
            "Iteration 47, loss = 0.66653329\n",
            "Iteration 48, loss = 0.66283903\n",
            "Iteration 49, loss = 0.65943144\n",
            "Iteration 50, loss = 0.65597005\n",
            "Iteration 51, loss = 0.65248297\n",
            "Iteration 52, loss = 0.64898004\n",
            "Iteration 53, loss = 0.64561601\n",
            "Iteration 54, loss = 0.64234251\n",
            "Iteration 55, loss = 0.63896174\n",
            "Iteration 56, loss = 0.63559383\n",
            "Iteration 57, loss = 0.63213712\n",
            "Iteration 58, loss = 0.62890882\n",
            "Iteration 59, loss = 0.62557968\n",
            "Iteration 60, loss = 0.62228878\n",
            "Iteration 61, loss = 0.61881985\n",
            "Iteration 62, loss = 0.61544016\n",
            "Iteration 63, loss = 0.61186880\n",
            "Iteration 64, loss = 0.60830476\n",
            "Iteration 65, loss = 0.60487492\n",
            "Iteration 66, loss = 0.60133733\n",
            "Iteration 67, loss = 0.59798465\n",
            "Iteration 68, loss = 0.59440699\n",
            "Iteration 69, loss = 0.59113554\n",
            "Iteration 70, loss = 0.58792068\n",
            "Iteration 71, loss = 0.58494629\n",
            "Iteration 72, loss = 0.58221968\n",
            "Iteration 73, loss = 0.57933029\n",
            "Iteration 74, loss = 0.57671884\n",
            "Iteration 75, loss = 0.57426424\n",
            "Iteration 76, loss = 0.57165162\n",
            "Iteration 77, loss = 0.56912870\n",
            "Iteration 78, loss = 0.56685784\n",
            "Iteration 79, loss = 0.56450066\n",
            "Iteration 80, loss = 0.56235558\n",
            "Iteration 81, loss = 0.56030103\n",
            "Iteration 82, loss = 0.55816688\n",
            "Iteration 83, loss = 0.55617555\n",
            "Iteration 84, loss = 0.55422795\n",
            "Iteration 85, loss = 0.55239381\n",
            "Iteration 86, loss = 0.55048665\n",
            "Iteration 87, loss = 0.54872881\n",
            "Iteration 88, loss = 0.54699562\n",
            "Iteration 89, loss = 0.54522072\n",
            "Iteration 90, loss = 0.54350161\n",
            "Iteration 91, loss = 0.54167157\n",
            "Iteration 92, loss = 0.53981327\n",
            "Iteration 93, loss = 0.53802554\n",
            "Iteration 94, loss = 0.53636742\n",
            "Iteration 95, loss = 0.53449754\n",
            "Iteration 96, loss = 0.53287333\n",
            "Iteration 97, loss = 0.53112131\n",
            "Iteration 98, loss = 0.52951913\n",
            "Iteration 99, loss = 0.52790119\n",
            "Iteration 100, loss = 0.52627891\n",
            "Iteration 101, loss = 0.52466779\n",
            "Iteration 102, loss = 0.52314621\n",
            "Iteration 103, loss = 0.52160823\n",
            "Iteration 104, loss = 0.52001094\n",
            "Iteration 105, loss = 0.51855689\n",
            "Iteration 106, loss = 0.51706994\n",
            "Iteration 107, loss = 0.51558700\n",
            "Iteration 108, loss = 0.51400223\n",
            "Iteration 109, loss = 0.51255252\n",
            "Iteration 110, loss = 0.51112188\n",
            "Iteration 111, loss = 0.50981166\n",
            "Iteration 112, loss = 0.50840614\n",
            "Iteration 113, loss = 0.50699034\n",
            "Iteration 114, loss = 0.50569066\n",
            "Iteration 115, loss = 0.50432883\n",
            "Iteration 116, loss = 0.50297528\n",
            "Iteration 117, loss = 0.50166998\n",
            "Iteration 118, loss = 0.50040236\n",
            "Iteration 119, loss = 0.49909092\n",
            "Iteration 120, loss = 0.49785116\n",
            "Iteration 121, loss = 0.49663427\n",
            "Iteration 122, loss = 0.49527442\n",
            "Iteration 123, loss = 0.49415686\n",
            "Iteration 124, loss = 0.49293612\n",
            "Iteration 125, loss = 0.49190705\n",
            "Iteration 126, loss = 0.49069785\n",
            "Iteration 127, loss = 0.48960006\n",
            "Iteration 128, loss = 0.48854387\n",
            "Iteration 129, loss = 0.48748551\n",
            "Iteration 130, loss = 0.48641747\n",
            "Iteration 131, loss = 0.48537791\n",
            "Iteration 132, loss = 0.48426609\n",
            "Iteration 133, loss = 0.48319129\n",
            "Iteration 134, loss = 0.48221250\n",
            "Iteration 135, loss = 0.48122019\n",
            "Iteration 136, loss = 0.48013082\n",
            "Iteration 137, loss = 0.47912429\n",
            "Iteration 138, loss = 0.47803691\n",
            "Iteration 139, loss = 0.47706018\n",
            "Iteration 140, loss = 0.47610814\n",
            "Iteration 141, loss = 0.47500628\n",
            "Iteration 142, loss = 0.47406351\n",
            "Iteration 143, loss = 0.47303584\n",
            "Iteration 144, loss = 0.47207065\n",
            "Iteration 145, loss = 0.47120756\n",
            "Iteration 146, loss = 0.47020614\n",
            "Iteration 147, loss = 0.46927687\n",
            "Iteration 148, loss = 0.46831690\n",
            "Iteration 149, loss = 0.46750559\n",
            "Iteration 150, loss = 0.46660194\n",
            "Iteration 151, loss = 0.46563601\n",
            "Iteration 152, loss = 0.46478194\n",
            "Iteration 153, loss = 0.46381664\n",
            "Iteration 154, loss = 0.46295587\n",
            "Iteration 155, loss = 0.46208828\n",
            "Iteration 156, loss = 0.46121248\n",
            "Iteration 157, loss = 0.46035085\n",
            "Iteration 158, loss = 0.45951835\n",
            "Iteration 159, loss = 0.45861355\n",
            "Iteration 160, loss = 0.45785305\n",
            "Iteration 161, loss = 0.45701634\n",
            "Iteration 162, loss = 0.45610964\n",
            "Iteration 163, loss = 0.45525820\n",
            "Iteration 164, loss = 0.45447801\n",
            "Iteration 165, loss = 0.45363875\n",
            "Iteration 166, loss = 0.45282181\n",
            "Iteration 167, loss = 0.45208443\n",
            "Iteration 168, loss = 0.45141693\n",
            "Iteration 169, loss = 0.45063129\n",
            "Iteration 170, loss = 0.44992527\n",
            "Iteration 171, loss = 0.44928413\n",
            "Iteration 172, loss = 0.44854017\n",
            "Iteration 173, loss = 0.44787784\n",
            "Iteration 174, loss = 0.44725913\n",
            "Iteration 175, loss = 0.44662959\n",
            "Iteration 176, loss = 0.44597587\n",
            "Iteration 177, loss = 0.44541783\n",
            "Iteration 178, loss = 0.44476548\n",
            "Iteration 179, loss = 0.44414102\n",
            "Iteration 180, loss = 0.44363812\n",
            "Iteration 181, loss = 0.44305953\n",
            "Iteration 182, loss = 0.44246498\n",
            "Iteration 183, loss = 0.44198580\n",
            "Iteration 184, loss = 0.44144664\n",
            "Iteration 185, loss = 0.44095483\n",
            "Iteration 186, loss = 0.44036281\n",
            "Iteration 187, loss = 0.43982519\n",
            "Iteration 188, loss = 0.43930459\n",
            "Iteration 189, loss = 0.43882502\n",
            "Iteration 190, loss = 0.43846185\n",
            "Iteration 191, loss = 0.43798357\n",
            "Iteration 192, loss = 0.43736065\n",
            "Iteration 193, loss = 0.43704053\n",
            "Iteration 194, loss = 0.43658442\n",
            "Iteration 195, loss = 0.43621236\n",
            "Iteration 196, loss = 0.43562303\n",
            "Iteration 197, loss = 0.43522772\n",
            "Iteration 198, loss = 0.43488605\n",
            "Iteration 199, loss = 0.43454855\n",
            "Iteration 200, loss = 0.43409720\n",
            "Iteration 201, loss = 0.43366791\n",
            "Iteration 202, loss = 0.43320351\n",
            "Iteration 203, loss = 0.43288390\n",
            "Iteration 204, loss = 0.43249227\n",
            "Iteration 205, loss = 0.43209622\n",
            "Iteration 206, loss = 0.43170479\n",
            "Iteration 207, loss = 0.43134348\n",
            "Iteration 208, loss = 0.43105195\n",
            "Iteration 209, loss = 0.43064097\n",
            "Iteration 210, loss = 0.43029783\n",
            "Iteration 211, loss = 0.43000706\n",
            "Iteration 212, loss = 0.42963334\n",
            "Iteration 213, loss = 0.42934542\n",
            "Iteration 214, loss = 0.42898993\n",
            "Iteration 215, loss = 0.42868351\n",
            "Iteration 216, loss = 0.42843819\n",
            "Iteration 217, loss = 0.42808580\n",
            "Iteration 218, loss = 0.42787553\n",
            "Iteration 219, loss = 0.42754948\n",
            "Iteration 220, loss = 0.42731013\n",
            "Iteration 221, loss = 0.42698352\n",
            "Iteration 222, loss = 0.42671288\n",
            "Iteration 223, loss = 0.42651948\n",
            "Iteration 224, loss = 0.42619304\n",
            "Iteration 225, loss = 0.42596906\n",
            "Iteration 226, loss = 0.42564355\n",
            "Iteration 227, loss = 0.42544715\n",
            "Iteration 228, loss = 0.42514717\n",
            "Iteration 229, loss = 0.42493040\n",
            "Iteration 230, loss = 0.42472014\n",
            "Iteration 231, loss = 0.42446837\n",
            "Iteration 232, loss = 0.42423820\n",
            "Iteration 233, loss = 0.42401783\n",
            "Iteration 234, loss = 0.42377977\n",
            "Iteration 235, loss = 0.42358825\n",
            "Iteration 236, loss = 0.42333788\n",
            "Iteration 237, loss = 0.42317450\n",
            "Iteration 238, loss = 0.42302476\n",
            "Iteration 239, loss = 0.42268054\n",
            "Iteration 240, loss = 0.42242072\n",
            "Iteration 241, loss = 0.42223000\n",
            "Iteration 242, loss = 0.42201796\n",
            "Iteration 243, loss = 0.42179983\n",
            "Iteration 244, loss = 0.42159039\n",
            "Iteration 245, loss = 0.42149046\n",
            "Iteration 246, loss = 0.42120262\n",
            "Iteration 247, loss = 0.42095329\n",
            "Iteration 248, loss = 0.42073410\n",
            "Iteration 249, loss = 0.42058250\n",
            "Iteration 250, loss = 0.42035869\n",
            "Iteration 251, loss = 0.42024360\n",
            "Iteration 252, loss = 0.41998760\n",
            "Iteration 253, loss = 0.41974658\n",
            "Iteration 254, loss = 0.41957698\n",
            "Iteration 255, loss = 0.41937900\n",
            "Iteration 256, loss = 0.41917551\n",
            "Iteration 257, loss = 0.41895195\n",
            "Iteration 258, loss = 0.41879453\n",
            "Iteration 259, loss = 0.41863047\n",
            "Iteration 260, loss = 0.41841937\n",
            "Iteration 261, loss = 0.41825180\n",
            "Iteration 262, loss = 0.41806803\n",
            "Iteration 263, loss = 0.41789177\n",
            "Iteration 264, loss = 0.41774168\n",
            "Iteration 265, loss = 0.41762271\n",
            "Iteration 266, loss = 0.41741103\n",
            "Iteration 267, loss = 0.41727440\n",
            "Iteration 268, loss = 0.41707058\n",
            "Iteration 269, loss = 0.41697148\n",
            "Iteration 270, loss = 0.41676149\n",
            "Iteration 271, loss = 0.41658215\n",
            "Iteration 272, loss = 0.41650108\n",
            "Iteration 273, loss = 0.41631237\n",
            "Iteration 274, loss = 0.41620211\n",
            "Iteration 275, loss = 0.41609144\n",
            "Iteration 276, loss = 0.41590189\n",
            "Iteration 277, loss = 0.41569515\n",
            "Iteration 278, loss = 0.41552970\n",
            "Iteration 279, loss = 0.41537704\n",
            "Iteration 280, loss = 0.41527298\n",
            "Iteration 281, loss = 0.41518184\n",
            "Iteration 282, loss = 0.41503357\n",
            "Iteration 283, loss = 0.41488853\n",
            "Iteration 284, loss = 0.41489609\n",
            "Iteration 285, loss = 0.41466541\n",
            "Iteration 286, loss = 0.41448913\n",
            "Iteration 287, loss = 0.41435429\n",
            "Iteration 288, loss = 0.41418735\n",
            "Iteration 289, loss = 0.41416396\n",
            "Iteration 290, loss = 0.41387785\n",
            "Iteration 291, loss = 0.41373660\n",
            "Iteration 292, loss = 0.41352622\n",
            "Iteration 293, loss = 0.41356256\n",
            "Iteration 294, loss = 0.41333531\n",
            "Iteration 295, loss = 0.41311679\n",
            "Iteration 296, loss = 0.41299478\n",
            "Iteration 297, loss = 0.41277878\n",
            "Iteration 298, loss = 0.41262171\n",
            "Iteration 299, loss = 0.41261578\n",
            "Iteration 300, loss = 0.41243007\n",
            "Iteration 301, loss = 0.41235215\n",
            "Iteration 302, loss = 0.41217390\n",
            "Iteration 303, loss = 0.41197296\n",
            "Iteration 304, loss = 0.41175402\n",
            "Iteration 305, loss = 0.41190493\n",
            "Iteration 306, loss = 0.41165631\n",
            "Iteration 307, loss = 0.41146911\n",
            "Iteration 308, loss = 0.41135544\n",
            "Iteration 309, loss = 0.41116580\n",
            "Iteration 310, loss = 0.41120951\n",
            "Iteration 311, loss = 0.41093393\n",
            "Iteration 312, loss = 0.41086247\n",
            "Iteration 313, loss = 0.41065976\n",
            "Iteration 314, loss = 0.41063733\n",
            "Iteration 315, loss = 0.41059744\n",
            "Iteration 316, loss = 0.41040374\n",
            "Iteration 317, loss = 0.41027326\n",
            "Iteration 318, loss = 0.41012612\n",
            "Iteration 319, loss = 0.41007010\n",
            "Iteration 320, loss = 0.41002698\n",
            "Iteration 321, loss = 0.40991842\n",
            "Iteration 322, loss = 0.40970949\n",
            "Iteration 323, loss = 0.40964384\n",
            "Iteration 324, loss = 0.40959847\n",
            "Iteration 325, loss = 0.40939651\n",
            "Iteration 326, loss = 0.40927432\n",
            "Iteration 327, loss = 0.40912260\n",
            "Iteration 328, loss = 0.40915154\n",
            "Iteration 329, loss = 0.40917116\n",
            "Iteration 330, loss = 0.40887283\n",
            "Iteration 331, loss = 0.40881810\n",
            "Iteration 332, loss = 0.40866485\n",
            "Iteration 333, loss = 0.40850477\n",
            "Iteration 334, loss = 0.40852847\n",
            "Iteration 335, loss = 0.40835880\n",
            "Iteration 336, loss = 0.40841637\n",
            "Iteration 337, loss = 0.40817719\n",
            "Iteration 338, loss = 0.40802678\n",
            "Iteration 339, loss = 0.40792610\n",
            "Iteration 340, loss = 0.40787774\n",
            "Iteration 341, loss = 0.40788391\n",
            "Iteration 342, loss = 0.40779476\n",
            "Iteration 343, loss = 0.40760060\n",
            "Iteration 344, loss = 0.40741995\n",
            "Iteration 345, loss = 0.40729174\n",
            "Iteration 346, loss = 0.40728338\n",
            "Iteration 347, loss = 0.40717251\n",
            "Iteration 348, loss = 0.40704451\n",
            "Iteration 349, loss = 0.40695184\n",
            "Iteration 350, loss = 0.40697748\n",
            "Iteration 351, loss = 0.40674132\n",
            "Iteration 352, loss = 0.40676168\n",
            "Iteration 353, loss = 0.40656366\n",
            "Iteration 354, loss = 0.40643408\n",
            "Iteration 355, loss = 0.40642653\n",
            "Iteration 356, loss = 0.40628138\n",
            "Iteration 357, loss = 0.40626593\n",
            "Iteration 358, loss = 0.40608982\n",
            "Iteration 359, loss = 0.40601584\n",
            "Iteration 360, loss = 0.40597737\n",
            "Iteration 361, loss = 0.40597112\n",
            "Iteration 362, loss = 0.40578481\n",
            "Iteration 363, loss = 0.40579405\n",
            "Iteration 364, loss = 0.40560961\n",
            "Iteration 365, loss = 0.40548337\n",
            "Iteration 366, loss = 0.40540385\n",
            "Iteration 367, loss = 0.40542540\n",
            "Iteration 368, loss = 0.40523902\n",
            "Iteration 369, loss = 0.40519886\n",
            "Iteration 370, loss = 0.40512664\n",
            "Iteration 371, loss = 0.40500123\n",
            "Iteration 372, loss = 0.40497618\n",
            "Iteration 373, loss = 0.40483736\n",
            "Iteration 374, loss = 0.40484554\n",
            "Iteration 375, loss = 0.40473262\n",
            "Iteration 376, loss = 0.40469169\n",
            "Iteration 377, loss = 0.40456089\n",
            "Iteration 378, loss = 0.40449099\n",
            "Iteration 379, loss = 0.40446577\n",
            "Iteration 380, loss = 0.40453918\n",
            "Iteration 381, loss = 0.40435412\n",
            "Iteration 382, loss = 0.40429037\n",
            "Iteration 383, loss = 0.40414919\n",
            "Iteration 384, loss = 0.40409603\n",
            "Iteration 385, loss = 0.40402781\n",
            "Iteration 386, loss = 0.40391467\n",
            "Iteration 387, loss = 0.40392803\n",
            "Iteration 388, loss = 0.40377898\n",
            "Iteration 389, loss = 0.40387090\n",
            "Iteration 390, loss = 0.40370429\n",
            "Iteration 391, loss = 0.40364683\n",
            "Iteration 392, loss = 0.40356379\n",
            "Iteration 393, loss = 0.40344252\n",
            "Iteration 394, loss = 0.40359535\n",
            "Iteration 395, loss = 0.40333462\n",
            "Iteration 396, loss = 0.40324097\n",
            "Iteration 397, loss = 0.40315011\n",
            "Iteration 398, loss = 0.40313303\n",
            "Iteration 399, loss = 0.40306097\n",
            "Iteration 400, loss = 0.40306492\n",
            "Iteration 401, loss = 0.40294130\n",
            "Iteration 402, loss = 0.40283901\n",
            "Iteration 403, loss = 0.40295838\n",
            "Iteration 404, loss = 0.40280062\n",
            "Iteration 405, loss = 0.40272368\n",
            "Iteration 406, loss = 0.40260402\n",
            "Iteration 407, loss = 0.40253143\n",
            "Iteration 408, loss = 0.40252700\n",
            "Iteration 409, loss = 0.40243759\n",
            "Iteration 410, loss = 0.40239118\n",
            "Iteration 411, loss = 0.40235388\n",
            "Iteration 412, loss = 0.40230151\n",
            "Iteration 413, loss = 0.40219164\n",
            "Iteration 414, loss = 0.40217393\n",
            "Iteration 415, loss = 0.40206895\n",
            "Iteration 416, loss = 0.40205130\n",
            "Iteration 417, loss = 0.40198934\n",
            "Iteration 418, loss = 0.40184079\n",
            "Iteration 419, loss = 0.40183626\n",
            "Iteration 420, loss = 0.40181359\n",
            "Iteration 421, loss = 0.40178381\n",
            "Iteration 422, loss = 0.40179877\n",
            "Iteration 423, loss = 0.40170460\n",
            "Iteration 424, loss = 0.40147194\n",
            "Iteration 425, loss = 0.40139987\n",
            "Iteration 426, loss = 0.40150822\n",
            "Iteration 427, loss = 0.40176206\n",
            "Iteration 428, loss = 0.40163380\n",
            "Iteration 429, loss = 0.40139487\n",
            "Iteration 430, loss = 0.40137567\n",
            "Iteration 431, loss = 0.40125120\n",
            "Iteration 432, loss = 0.40124566\n",
            "Iteration 433, loss = 0.40114405\n",
            "Iteration 434, loss = 0.40117380\n",
            "Iteration 435, loss = 0.40111552\n",
            "Iteration 436, loss = 0.40096750\n",
            "Iteration 437, loss = 0.40090879\n",
            "Iteration 438, loss = 0.40092280\n",
            "Iteration 439, loss = 0.40099132\n",
            "Iteration 440, loss = 0.40090682\n",
            "Iteration 441, loss = 0.40084483\n",
            "Iteration 442, loss = 0.40091896\n",
            "Iteration 443, loss = 0.40068640\n",
            "Iteration 444, loss = 0.40071970\n",
            "Iteration 445, loss = 0.40059342\n",
            "Iteration 446, loss = 0.40056164\n",
            "Iteration 447, loss = 0.40048221\n",
            "Iteration 448, loss = 0.40048344\n",
            "Iteration 449, loss = 0.40033399\n",
            "Iteration 450, loss = 0.40037688\n",
            "Iteration 451, loss = 0.40034567\n",
            "Iteration 452, loss = 0.40025977\n",
            "Iteration 453, loss = 0.40020206\n",
            "Iteration 454, loss = 0.40028148\n",
            "Iteration 455, loss = 0.40021629\n",
            "Iteration 456, loss = 0.40005315\n",
            "Iteration 457, loss = 0.39998750\n",
            "Iteration 458, loss = 0.40005106\n",
            "Iteration 459, loss = 0.39986859\n",
            "Iteration 460, loss = 0.39989341\n",
            "Iteration 461, loss = 0.39985092\n",
            "Iteration 462, loss = 0.39976789\n",
            "Iteration 463, loss = 0.39973569\n",
            "Iteration 464, loss = 0.39975439\n",
            "Iteration 465, loss = 0.39968560\n",
            "Iteration 466, loss = 0.39962526\n",
            "Iteration 467, loss = 0.39962497\n",
            "Iteration 468, loss = 0.39966374\n",
            "Iteration 469, loss = 0.39948823\n",
            "Iteration 470, loss = 0.39949117\n",
            "Iteration 471, loss = 0.39945674\n",
            "Iteration 472, loss = 0.39940398\n",
            "Iteration 473, loss = 0.39938965\n",
            "Iteration 474, loss = 0.39933229\n",
            "Iteration 475, loss = 0.39927272\n",
            "Iteration 476, loss = 0.39927622\n",
            "Iteration 477, loss = 0.39926187\n",
            "Iteration 478, loss = 0.39919982\n",
            "Iteration 479, loss = 0.39913892\n",
            "Iteration 480, loss = 0.39909944\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.81052704\n",
            "Iteration 2, loss = 0.80810046\n",
            "Iteration 3, loss = 0.80534221\n",
            "Iteration 4, loss = 0.80276273\n",
            "Iteration 5, loss = 0.80004245\n",
            "Iteration 6, loss = 0.79747167\n",
            "Iteration 7, loss = 0.79469720\n",
            "Iteration 8, loss = 0.79211015\n",
            "Iteration 9, loss = 0.78925409\n",
            "Iteration 10, loss = 0.78664792\n",
            "Iteration 11, loss = 0.78381236\n",
            "Iteration 12, loss = 0.78109929\n",
            "Iteration 13, loss = 0.77835964\n",
            "Iteration 14, loss = 0.77544280\n",
            "Iteration 15, loss = 0.77264238\n",
            "Iteration 16, loss = 0.76986795\n",
            "Iteration 17, loss = 0.76703933\n",
            "Iteration 18, loss = 0.76410804\n",
            "Iteration 19, loss = 0.76119698\n",
            "Iteration 20, loss = 0.75816241\n",
            "Iteration 21, loss = 0.75534219\n",
            "Iteration 22, loss = 0.75225866\n",
            "Iteration 23, loss = 0.74905702\n",
            "Iteration 24, loss = 0.74596359\n",
            "Iteration 25, loss = 0.74274747\n",
            "Iteration 26, loss = 0.73966966\n",
            "Iteration 27, loss = 0.73637222\n",
            "Iteration 28, loss = 0.73290061\n",
            "Iteration 29, loss = 0.72972224\n",
            "Iteration 30, loss = 0.72624609\n",
            "Iteration 31, loss = 0.72266922\n",
            "Iteration 32, loss = 0.71930084\n",
            "Iteration 33, loss = 0.71578088\n",
            "Iteration 34, loss = 0.71215772\n",
            "Iteration 35, loss = 0.70865843\n",
            "Iteration 36, loss = 0.70501071\n",
            "Iteration 37, loss = 0.70135691\n",
            "Iteration 38, loss = 0.69753124\n",
            "Iteration 39, loss = 0.69388327\n",
            "Iteration 40, loss = 0.69003829\n",
            "Iteration 41, loss = 0.68630863\n",
            "Iteration 42, loss = 0.68244757\n",
            "Iteration 43, loss = 0.67858792\n",
            "Iteration 44, loss = 0.67481751\n",
            "Iteration 45, loss = 0.67111607\n",
            "Iteration 46, loss = 0.66710823\n",
            "Iteration 47, loss = 0.66340551\n",
            "Iteration 48, loss = 0.65957831\n",
            "Iteration 49, loss = 0.65577403\n",
            "Iteration 50, loss = 0.65213073\n",
            "Iteration 51, loss = 0.64840211\n",
            "Iteration 52, loss = 0.64449817\n",
            "Iteration 53, loss = 0.64088484\n",
            "Iteration 54, loss = 0.63728584\n",
            "Iteration 55, loss = 0.63370479\n",
            "Iteration 56, loss = 0.63012087\n",
            "Iteration 57, loss = 0.62644709\n",
            "Iteration 58, loss = 0.62297917\n",
            "Iteration 59, loss = 0.61928714\n",
            "Iteration 60, loss = 0.61575148\n",
            "Iteration 61, loss = 0.61194651\n",
            "Iteration 62, loss = 0.60846652\n",
            "Iteration 63, loss = 0.60461996\n",
            "Iteration 64, loss = 0.60106690\n",
            "Iteration 65, loss = 0.59749627\n",
            "Iteration 66, loss = 0.59389300\n",
            "Iteration 67, loss = 0.59031363\n",
            "Iteration 68, loss = 0.58674450\n",
            "Iteration 69, loss = 0.58349943\n",
            "Iteration 70, loss = 0.58044767\n",
            "Iteration 71, loss = 0.57752944\n",
            "Iteration 72, loss = 0.57478193\n",
            "Iteration 73, loss = 0.57191832\n",
            "Iteration 74, loss = 0.56934218\n",
            "Iteration 75, loss = 0.56689568\n",
            "Iteration 76, loss = 0.56434078\n",
            "Iteration 77, loss = 0.56190293\n",
            "Iteration 78, loss = 0.55960588\n",
            "Iteration 79, loss = 0.55731844\n",
            "Iteration 80, loss = 0.55514579\n",
            "Iteration 81, loss = 0.55298481\n",
            "Iteration 82, loss = 0.55080880\n",
            "Iteration 83, loss = 0.54857251\n",
            "Iteration 84, loss = 0.54638551\n",
            "Iteration 85, loss = 0.54433858\n",
            "Iteration 86, loss = 0.54216205\n",
            "Iteration 87, loss = 0.54003340\n",
            "Iteration 88, loss = 0.53807225\n",
            "Iteration 89, loss = 0.53596444\n",
            "Iteration 90, loss = 0.53411778\n",
            "Iteration 91, loss = 0.53212979\n",
            "Iteration 92, loss = 0.53029902\n",
            "Iteration 93, loss = 0.52842487\n",
            "Iteration 94, loss = 0.52672038\n",
            "Iteration 95, loss = 0.52484955\n",
            "Iteration 96, loss = 0.52322558\n",
            "Iteration 97, loss = 0.52144284\n",
            "Iteration 98, loss = 0.51978164\n",
            "Iteration 99, loss = 0.51818168\n",
            "Iteration 100, loss = 0.51655704\n",
            "Iteration 101, loss = 0.51502817\n",
            "Iteration 102, loss = 0.51349963\n",
            "Iteration 103, loss = 0.51206561\n",
            "Iteration 104, loss = 0.51053677\n",
            "Iteration 105, loss = 0.50916619\n",
            "Iteration 106, loss = 0.50777726\n",
            "Iteration 107, loss = 0.50635564\n",
            "Iteration 108, loss = 0.50494987\n",
            "Iteration 109, loss = 0.50361935\n",
            "Iteration 110, loss = 0.50224789\n",
            "Iteration 111, loss = 0.50109282\n",
            "Iteration 112, loss = 0.49974582\n",
            "Iteration 113, loss = 0.49848298\n",
            "Iteration 114, loss = 0.49720186\n",
            "Iteration 115, loss = 0.49591985\n",
            "Iteration 116, loss = 0.49476409\n",
            "Iteration 117, loss = 0.49357680\n",
            "Iteration 118, loss = 0.49237234\n",
            "Iteration 119, loss = 0.49121678\n",
            "Iteration 120, loss = 0.49010839\n",
            "Iteration 121, loss = 0.48894458\n",
            "Iteration 122, loss = 0.48778672\n",
            "Iteration 123, loss = 0.48672705\n",
            "Iteration 124, loss = 0.48549250\n",
            "Iteration 125, loss = 0.48458460\n",
            "Iteration 126, loss = 0.48338368\n",
            "Iteration 127, loss = 0.48228674\n",
            "Iteration 128, loss = 0.48126295\n",
            "Iteration 129, loss = 0.48030542\n",
            "Iteration 130, loss = 0.47930273\n",
            "Iteration 131, loss = 0.47835890\n",
            "Iteration 132, loss = 0.47733139\n",
            "Iteration 133, loss = 0.47638025\n",
            "Iteration 134, loss = 0.47547999\n",
            "Iteration 135, loss = 0.47458155\n",
            "Iteration 136, loss = 0.47366328\n",
            "Iteration 137, loss = 0.47278975\n",
            "Iteration 138, loss = 0.47186920\n",
            "Iteration 139, loss = 0.47103620\n",
            "Iteration 140, loss = 0.47025582\n",
            "Iteration 141, loss = 0.46932688\n",
            "Iteration 142, loss = 0.46856586\n",
            "Iteration 143, loss = 0.46780772\n",
            "Iteration 144, loss = 0.46694827\n",
            "Iteration 145, loss = 0.46610618\n",
            "Iteration 146, loss = 0.46529376\n",
            "Iteration 147, loss = 0.46455277\n",
            "Iteration 148, loss = 0.46372398\n",
            "Iteration 149, loss = 0.46313506\n",
            "Iteration 150, loss = 0.46233381\n",
            "Iteration 151, loss = 0.46165093\n",
            "Iteration 152, loss = 0.46097067\n",
            "Iteration 153, loss = 0.46028148\n",
            "Iteration 154, loss = 0.45963881\n",
            "Iteration 155, loss = 0.45897362\n",
            "Iteration 156, loss = 0.45834995\n",
            "Iteration 157, loss = 0.45773087\n",
            "Iteration 158, loss = 0.45707164\n",
            "Iteration 159, loss = 0.45638004\n",
            "Iteration 160, loss = 0.45574628\n",
            "Iteration 161, loss = 0.45510215\n",
            "Iteration 162, loss = 0.45446228\n",
            "Iteration 163, loss = 0.45379242\n",
            "Iteration 164, loss = 0.45316989\n",
            "Iteration 165, loss = 0.45250133\n",
            "Iteration 166, loss = 0.45190810\n",
            "Iteration 167, loss = 0.45126621\n",
            "Iteration 168, loss = 0.45067224\n",
            "Iteration 169, loss = 0.45007003\n",
            "Iteration 170, loss = 0.44943739\n",
            "Iteration 171, loss = 0.44899242\n",
            "Iteration 172, loss = 0.44830975\n",
            "Iteration 173, loss = 0.44762613\n",
            "Iteration 174, loss = 0.44710127\n",
            "Iteration 175, loss = 0.44652170\n",
            "Iteration 176, loss = 0.44594760\n",
            "Iteration 177, loss = 0.44533453\n",
            "Iteration 178, loss = 0.44468775\n",
            "Iteration 179, loss = 0.44417920\n",
            "Iteration 180, loss = 0.44363932\n",
            "Iteration 181, loss = 0.44308838\n",
            "Iteration 182, loss = 0.44259398\n",
            "Iteration 183, loss = 0.44208923\n",
            "Iteration 184, loss = 0.44146632\n",
            "Iteration 185, loss = 0.44100697\n",
            "Iteration 186, loss = 0.44045842\n",
            "Iteration 187, loss = 0.43994735\n",
            "Iteration 188, loss = 0.43949589\n",
            "Iteration 189, loss = 0.43900160\n",
            "Iteration 190, loss = 0.43868693\n",
            "Iteration 191, loss = 0.43823684\n",
            "Iteration 192, loss = 0.43766489\n",
            "Iteration 193, loss = 0.43719641\n",
            "Iteration 194, loss = 0.43677996\n",
            "Iteration 195, loss = 0.43644086\n",
            "Iteration 196, loss = 0.43591585\n",
            "Iteration 197, loss = 0.43548011\n",
            "Iteration 198, loss = 0.43501999\n",
            "Iteration 199, loss = 0.43462688\n",
            "Iteration 200, loss = 0.43420011\n",
            "Iteration 201, loss = 0.43376457\n",
            "Iteration 202, loss = 0.43331938\n",
            "Iteration 203, loss = 0.43290490\n",
            "Iteration 204, loss = 0.43247849\n",
            "Iteration 205, loss = 0.43206862\n",
            "Iteration 206, loss = 0.43168152\n",
            "Iteration 207, loss = 0.43132620\n",
            "Iteration 208, loss = 0.43093206\n",
            "Iteration 209, loss = 0.43055501\n",
            "Iteration 210, loss = 0.43018238\n",
            "Iteration 211, loss = 0.42983626\n",
            "Iteration 212, loss = 0.42945146\n",
            "Iteration 213, loss = 0.42912182\n",
            "Iteration 214, loss = 0.42874734\n",
            "Iteration 215, loss = 0.42842252\n",
            "Iteration 216, loss = 0.42807848\n",
            "Iteration 217, loss = 0.42770556\n",
            "Iteration 218, loss = 0.42742230\n",
            "Iteration 219, loss = 0.42722133\n",
            "Iteration 220, loss = 0.42682037\n",
            "Iteration 221, loss = 0.42644914\n",
            "Iteration 222, loss = 0.42617186\n",
            "Iteration 223, loss = 0.42590252\n",
            "Iteration 224, loss = 0.42554389\n",
            "Iteration 225, loss = 0.42528915\n",
            "Iteration 226, loss = 0.42495180\n",
            "Iteration 227, loss = 0.42464365\n",
            "Iteration 228, loss = 0.42437722\n",
            "Iteration 229, loss = 0.42408431\n",
            "Iteration 230, loss = 0.42386386\n",
            "Iteration 231, loss = 0.42358882\n",
            "Iteration 232, loss = 0.42327993\n",
            "Iteration 233, loss = 0.42311796\n",
            "Iteration 234, loss = 0.42279761\n",
            "Iteration 235, loss = 0.42249675\n",
            "Iteration 236, loss = 0.42221004\n",
            "Iteration 237, loss = 0.42202015\n",
            "Iteration 238, loss = 0.42172232\n",
            "Iteration 239, loss = 0.42143756\n",
            "Iteration 240, loss = 0.42117251\n",
            "Iteration 241, loss = 0.42092953\n",
            "Iteration 242, loss = 0.42065231\n",
            "Iteration 243, loss = 0.42040743\n",
            "Iteration 244, loss = 0.42011752\n",
            "Iteration 245, loss = 0.41991176\n",
            "Iteration 246, loss = 0.41958413\n",
            "Iteration 247, loss = 0.41938519\n",
            "Iteration 248, loss = 0.41913147\n",
            "Iteration 249, loss = 0.41885014\n",
            "Iteration 250, loss = 0.41861702\n",
            "Iteration 251, loss = 0.41842841\n",
            "Iteration 252, loss = 0.41815548\n",
            "Iteration 253, loss = 0.41792757\n",
            "Iteration 254, loss = 0.41768618\n",
            "Iteration 255, loss = 0.41745911\n",
            "Iteration 256, loss = 0.41722174\n",
            "Iteration 257, loss = 0.41694833\n",
            "Iteration 258, loss = 0.41671559\n",
            "Iteration 259, loss = 0.41650391\n",
            "Iteration 260, loss = 0.41627190\n",
            "Iteration 261, loss = 0.41604244\n",
            "Iteration 262, loss = 0.41580938\n",
            "Iteration 263, loss = 0.41565294\n",
            "Iteration 264, loss = 0.41542551\n",
            "Iteration 265, loss = 0.41522429\n",
            "Iteration 266, loss = 0.41498397\n",
            "Iteration 267, loss = 0.41474342\n",
            "Iteration 268, loss = 0.41453955\n",
            "Iteration 269, loss = 0.41434957\n",
            "Iteration 270, loss = 0.41409470\n",
            "Iteration 271, loss = 0.41387731\n",
            "Iteration 272, loss = 0.41370568\n",
            "Iteration 273, loss = 0.41346557\n",
            "Iteration 274, loss = 0.41331237\n",
            "Iteration 275, loss = 0.41309413\n",
            "Iteration 276, loss = 0.41288113\n",
            "Iteration 277, loss = 0.41273446\n",
            "Iteration 278, loss = 0.41246194\n",
            "Iteration 279, loss = 0.41231243\n",
            "Iteration 280, loss = 0.41210497\n",
            "Iteration 281, loss = 0.41192682\n",
            "Iteration 282, loss = 0.41172730\n",
            "Iteration 283, loss = 0.41158102\n",
            "Iteration 284, loss = 0.41137703\n",
            "Iteration 285, loss = 0.41117176\n",
            "Iteration 286, loss = 0.41101697\n",
            "Iteration 287, loss = 0.41087620\n",
            "Iteration 288, loss = 0.41063083\n",
            "Iteration 289, loss = 0.41051110\n",
            "Iteration 290, loss = 0.41030054\n",
            "Iteration 291, loss = 0.41019658\n",
            "Iteration 292, loss = 0.40994287\n",
            "Iteration 293, loss = 0.40978360\n",
            "Iteration 294, loss = 0.40969614\n",
            "Iteration 295, loss = 0.40940477\n",
            "Iteration 296, loss = 0.40933732\n",
            "Iteration 297, loss = 0.40915345\n",
            "Iteration 298, loss = 0.40895377\n",
            "Iteration 299, loss = 0.40884818\n",
            "Iteration 300, loss = 0.40862862\n",
            "Iteration 301, loss = 0.40854630\n",
            "Iteration 302, loss = 0.40841946\n",
            "Iteration 303, loss = 0.40826194\n",
            "Iteration 304, loss = 0.40800890\n",
            "Iteration 305, loss = 0.40794499\n",
            "Iteration 306, loss = 0.40766622\n",
            "Iteration 307, loss = 0.40753724\n",
            "Iteration 308, loss = 0.40738415\n",
            "Iteration 309, loss = 0.40722896\n",
            "Iteration 310, loss = 0.40707636\n",
            "Iteration 311, loss = 0.40694260\n",
            "Iteration 312, loss = 0.40686422\n",
            "Iteration 313, loss = 0.40656424\n",
            "Iteration 314, loss = 0.40652591\n",
            "Iteration 315, loss = 0.40639542\n",
            "Iteration 316, loss = 0.40617639\n",
            "Iteration 317, loss = 0.40601805\n",
            "Iteration 318, loss = 0.40584166\n",
            "Iteration 319, loss = 0.40572849\n",
            "Iteration 320, loss = 0.40559208\n",
            "Iteration 321, loss = 0.40549475\n",
            "Iteration 322, loss = 0.40526237\n",
            "Iteration 323, loss = 0.40523995\n",
            "Iteration 324, loss = 0.40498091\n",
            "Iteration 325, loss = 0.40488237\n",
            "Iteration 326, loss = 0.40470355\n",
            "Iteration 327, loss = 0.40458624\n",
            "Iteration 328, loss = 0.40450596\n",
            "Iteration 329, loss = 0.40431487\n",
            "Iteration 330, loss = 0.40415440\n",
            "Iteration 331, loss = 0.40406390\n",
            "Iteration 332, loss = 0.40386852\n",
            "Iteration 333, loss = 0.40383499\n",
            "Iteration 334, loss = 0.40367565\n",
            "Iteration 335, loss = 0.40353554\n",
            "Iteration 336, loss = 0.40335577\n",
            "Iteration 337, loss = 0.40318898\n",
            "Iteration 338, loss = 0.40305090\n",
            "Iteration 339, loss = 0.40289908\n",
            "Iteration 340, loss = 0.40279206\n",
            "Iteration 341, loss = 0.40267231\n",
            "Iteration 342, loss = 0.40257699\n",
            "Iteration 343, loss = 0.40247029\n",
            "Iteration 344, loss = 0.40229172\n",
            "Iteration 345, loss = 0.40211482\n",
            "Iteration 346, loss = 0.40205274\n",
            "Iteration 347, loss = 0.40186864\n",
            "Iteration 348, loss = 0.40178676\n",
            "Iteration 349, loss = 0.40164562\n",
            "Iteration 350, loss = 0.40153116\n",
            "Iteration 351, loss = 0.40141336\n",
            "Iteration 352, loss = 0.40133306\n",
            "Iteration 353, loss = 0.40117404\n",
            "Iteration 354, loss = 0.40110420\n",
            "Iteration 355, loss = 0.40091918\n",
            "Iteration 356, loss = 0.40082003\n",
            "Iteration 357, loss = 0.40083886\n",
            "Iteration 358, loss = 0.40061094\n",
            "Iteration 359, loss = 0.40046035\n",
            "Iteration 360, loss = 0.40037058\n",
            "Iteration 361, loss = 0.40033824\n",
            "Iteration 362, loss = 0.40017697\n",
            "Iteration 363, loss = 0.40004591\n",
            "Iteration 364, loss = 0.39996522\n",
            "Iteration 365, loss = 0.39980215\n",
            "Iteration 366, loss = 0.39973705\n",
            "Iteration 367, loss = 0.39964247\n",
            "Iteration 368, loss = 0.39961956\n",
            "Iteration 369, loss = 0.39942216\n",
            "Iteration 370, loss = 0.39930059\n",
            "Iteration 371, loss = 0.39920709\n",
            "Iteration 372, loss = 0.39916135\n",
            "Iteration 373, loss = 0.39900961\n",
            "Iteration 374, loss = 0.39890380\n",
            "Iteration 375, loss = 0.39881437\n",
            "Iteration 376, loss = 0.39873146\n",
            "Iteration 377, loss = 0.39862399\n",
            "Iteration 378, loss = 0.39858450\n",
            "Iteration 379, loss = 0.39846879\n",
            "Iteration 380, loss = 0.39863365\n",
            "Iteration 381, loss = 0.39842541\n",
            "Iteration 382, loss = 0.39831760\n",
            "Iteration 383, loss = 0.39816580\n",
            "Iteration 384, loss = 0.39801065\n",
            "Iteration 385, loss = 0.39794620\n",
            "Iteration 386, loss = 0.39785033\n",
            "Iteration 387, loss = 0.39774647\n",
            "Iteration 388, loss = 0.39765613\n",
            "Iteration 389, loss = 0.39758566\n",
            "Iteration 390, loss = 0.39746729\n",
            "Iteration 391, loss = 0.39739605\n",
            "Iteration 392, loss = 0.39727266\n",
            "Iteration 393, loss = 0.39716418\n",
            "Iteration 394, loss = 0.39709076\n",
            "Iteration 395, loss = 0.39695891\n",
            "Iteration 396, loss = 0.39690086\n",
            "Iteration 397, loss = 0.39674829\n",
            "Iteration 398, loss = 0.39669780\n",
            "Iteration 399, loss = 0.39663138\n",
            "Iteration 400, loss = 0.39662752\n",
            "Iteration 401, loss = 0.39645339\n",
            "Iteration 402, loss = 0.39638891\n",
            "Iteration 403, loss = 0.39637011\n",
            "Iteration 404, loss = 0.39626900\n",
            "Iteration 405, loss = 0.39617101\n",
            "Iteration 406, loss = 0.39604864\n",
            "Iteration 407, loss = 0.39592085\n",
            "Iteration 408, loss = 0.39584743\n",
            "Iteration 409, loss = 0.39576400\n",
            "Iteration 410, loss = 0.39568156\n",
            "Iteration 411, loss = 0.39561516\n",
            "Iteration 412, loss = 0.39569449\n",
            "Iteration 413, loss = 0.39559448\n",
            "Iteration 414, loss = 0.39547901\n",
            "Iteration 415, loss = 0.39523911\n",
            "Iteration 416, loss = 0.39520273\n",
            "Iteration 417, loss = 0.39528336\n",
            "Iteration 418, loss = 0.39517178\n",
            "Iteration 419, loss = 0.39494347\n",
            "Iteration 420, loss = 0.39494875\n",
            "Iteration 421, loss = 0.39492376\n",
            "Iteration 422, loss = 0.39493545\n",
            "Iteration 423, loss = 0.39485971\n",
            "Iteration 424, loss = 0.39468021\n",
            "Iteration 425, loss = 0.39446486\n",
            "Iteration 426, loss = 0.39438640\n",
            "Iteration 427, loss = 0.39447257\n",
            "Iteration 428, loss = 0.39446458\n",
            "Iteration 429, loss = 0.39441136\n",
            "Iteration 430, loss = 0.39430367\n",
            "Iteration 431, loss = 0.39419344\n",
            "Iteration 432, loss = 0.39415361\n",
            "Iteration 433, loss = 0.39401684\n",
            "Iteration 434, loss = 0.39410772\n",
            "Iteration 435, loss = 0.39390955\n",
            "Iteration 436, loss = 0.39375073\n",
            "Iteration 437, loss = 0.39372410\n",
            "Iteration 438, loss = 0.39375299\n",
            "Iteration 439, loss = 0.39373315\n",
            "Iteration 440, loss = 0.39360346\n",
            "Iteration 441, loss = 0.39351465\n",
            "Iteration 442, loss = 0.39348919\n",
            "Iteration 443, loss = 0.39332372\n",
            "Iteration 444, loss = 0.39328948\n",
            "Iteration 445, loss = 0.39318484\n",
            "Iteration 446, loss = 0.39311870\n",
            "Iteration 447, loss = 0.39305650\n",
            "Iteration 448, loss = 0.39301533\n",
            "Iteration 449, loss = 0.39292829\n",
            "Iteration 450, loss = 0.39286606\n",
            "Iteration 451, loss = 0.39280922\n",
            "Iteration 452, loss = 0.39274775\n",
            "Iteration 453, loss = 0.39267194\n",
            "Iteration 454, loss = 0.39255237\n",
            "Iteration 455, loss = 0.39243233\n",
            "Iteration 456, loss = 0.39250472\n",
            "Iteration 457, loss = 0.39245105\n",
            "Iteration 458, loss = 0.39254094\n",
            "Iteration 459, loss = 0.39222175\n",
            "Iteration 460, loss = 0.39215304\n",
            "Iteration 461, loss = 0.39210089\n",
            "Iteration 462, loss = 0.39199390\n",
            "Iteration 463, loss = 0.39198470\n",
            "Iteration 464, loss = 0.39189198\n",
            "Iteration 465, loss = 0.39182423\n",
            "Iteration 466, loss = 0.39174466\n",
            "Iteration 467, loss = 0.39163224\n",
            "Iteration 468, loss = 0.39153986\n",
            "Iteration 469, loss = 0.39143591\n",
            "Iteration 470, loss = 0.39139825\n",
            "Iteration 471, loss = 0.39137201\n",
            "Iteration 472, loss = 0.39128567\n",
            "Iteration 473, loss = 0.39128957\n",
            "Iteration 474, loss = 0.39118455\n",
            "Iteration 475, loss = 0.39106707\n",
            "Iteration 476, loss = 0.39093247\n",
            "Iteration 477, loss = 0.39091068\n",
            "Iteration 478, loss = 0.39077013\n",
            "Iteration 479, loss = 0.39076158\n",
            "Iteration 480, loss = 0.39074296\n",
            "Iteration 481, loss = 0.39067374\n",
            "Iteration 482, loss = 0.39053590\n",
            "Iteration 483, loss = 0.39042514\n",
            "Iteration 484, loss = 0.39038542\n",
            "Iteration 485, loss = 0.39026293\n",
            "Iteration 486, loss = 0.39019594\n",
            "Iteration 487, loss = 0.39011444\n",
            "Iteration 488, loss = 0.39006787\n",
            "Iteration 489, loss = 0.39006412\n",
            "Iteration 490, loss = 0.38997821\n",
            "Iteration 491, loss = 0.38991712\n",
            "Iteration 492, loss = 0.38975390\n",
            "Iteration 493, loss = 0.38969980\n",
            "Iteration 494, loss = 0.38974879\n",
            "Iteration 495, loss = 0.38977325\n",
            "Iteration 496, loss = 0.38973225\n",
            "Iteration 497, loss = 0.38958003\n",
            "Iteration 498, loss = 0.38947551\n",
            "Iteration 499, loss = 0.38938798\n",
            "Iteration 500, loss = 0.38929416\n",
            "Iteration 501, loss = 0.38925283\n",
            "Iteration 502, loss = 0.38924381\n",
            "Iteration 503, loss = 0.38921873\n",
            "Iteration 504, loss = 0.38917917\n",
            "Iteration 505, loss = 0.38903533\n",
            "Iteration 506, loss = 0.38894229\n",
            "Iteration 507, loss = 0.38890202\n",
            "Iteration 508, loss = 0.38881402\n",
            "Iteration 509, loss = 0.38877704\n",
            "Iteration 510, loss = 0.38876203\n",
            "Iteration 511, loss = 0.38866198\n",
            "Iteration 512, loss = 0.38864840\n",
            "Iteration 513, loss = 0.38852590\n",
            "Iteration 514, loss = 0.38856038\n",
            "Iteration 515, loss = 0.38853801\n",
            "Iteration 516, loss = 0.38841035\n",
            "Iteration 517, loss = 0.38838195\n",
            "Iteration 518, loss = 0.38827883\n",
            "Iteration 519, loss = 0.38827826\n",
            "Iteration 520, loss = 0.38818322\n",
            "Iteration 521, loss = 0.38811133\n",
            "Iteration 522, loss = 0.38805425\n",
            "Iteration 523, loss = 0.38804650\n",
            "Iteration 524, loss = 0.38797783\n",
            "Iteration 525, loss = 0.38789850\n",
            "Iteration 526, loss = 0.38787602\n",
            "Iteration 527, loss = 0.38790585\n",
            "Iteration 528, loss = 0.38777452\n",
            "Iteration 529, loss = 0.38774537\n",
            "Iteration 530, loss = 0.38766820\n",
            "Iteration 531, loss = 0.38759746\n",
            "Iteration 532, loss = 0.38756493\n",
            "Iteration 533, loss = 0.38758041\n",
            "Iteration 534, loss = 0.38753758\n",
            "Iteration 535, loss = 0.38752798\n",
            "Iteration 536, loss = 0.38736116\n",
            "Iteration 537, loss = 0.38729118\n",
            "Iteration 538, loss = 0.38725948\n",
            "Iteration 539, loss = 0.38730990\n",
            "Iteration 540, loss = 0.38722642\n",
            "Iteration 541, loss = 0.38716197\n",
            "Iteration 542, loss = 0.38710123\n",
            "Iteration 543, loss = 0.38705305\n",
            "Iteration 544, loss = 0.38696402\n",
            "Iteration 545, loss = 0.38687871\n",
            "Iteration 546, loss = 0.38686467\n",
            "Iteration 547, loss = 0.38686700\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.81081366\n",
            "Iteration 2, loss = 0.80829132\n",
            "Iteration 3, loss = 0.80558473\n",
            "Iteration 4, loss = 0.80296843\n",
            "Iteration 5, loss = 0.80026327\n",
            "Iteration 6, loss = 0.79767311\n",
            "Iteration 7, loss = 0.79492642\n",
            "Iteration 8, loss = 0.79232344\n",
            "Iteration 9, loss = 0.78960128\n",
            "Iteration 10, loss = 0.78694591\n",
            "Iteration 11, loss = 0.78411974\n",
            "Iteration 12, loss = 0.78149821\n",
            "Iteration 13, loss = 0.77879579\n",
            "Iteration 14, loss = 0.77594556\n",
            "Iteration 15, loss = 0.77311085\n",
            "Iteration 16, loss = 0.77040166\n",
            "Iteration 17, loss = 0.76751956\n",
            "Iteration 18, loss = 0.76458998\n",
            "Iteration 19, loss = 0.76172618\n",
            "Iteration 20, loss = 0.75874986\n",
            "Iteration 21, loss = 0.75584362\n",
            "Iteration 22, loss = 0.75281825\n",
            "Iteration 23, loss = 0.74969026\n",
            "Iteration 24, loss = 0.74658567\n",
            "Iteration 25, loss = 0.74347194\n",
            "Iteration 26, loss = 0.74036602\n",
            "Iteration 27, loss = 0.73711574\n",
            "Iteration 28, loss = 0.73361999\n",
            "Iteration 29, loss = 0.73050293\n",
            "Iteration 30, loss = 0.72703262\n",
            "Iteration 31, loss = 0.72351845\n",
            "Iteration 32, loss = 0.72018594\n",
            "Iteration 33, loss = 0.71661236\n",
            "Iteration 34, loss = 0.71299134\n",
            "Iteration 35, loss = 0.70954806\n",
            "Iteration 36, loss = 0.70582610\n",
            "Iteration 37, loss = 0.70235415\n",
            "Iteration 38, loss = 0.69873403\n",
            "Iteration 39, loss = 0.69505115\n",
            "Iteration 40, loss = 0.69142578\n",
            "Iteration 41, loss = 0.68773247\n",
            "Iteration 42, loss = 0.68395643\n",
            "Iteration 43, loss = 0.68041366\n",
            "Iteration 44, loss = 0.67666639\n",
            "Iteration 45, loss = 0.67311151\n",
            "Iteration 46, loss = 0.66926091\n",
            "Iteration 47, loss = 0.66562146\n",
            "Iteration 48, loss = 0.66207540\n",
            "Iteration 49, loss = 0.65832184\n",
            "Iteration 50, loss = 0.65485372\n",
            "Iteration 51, loss = 0.65125775\n",
            "Iteration 52, loss = 0.64759866\n",
            "Iteration 53, loss = 0.64405107\n",
            "Iteration 54, loss = 0.64071628\n",
            "Iteration 55, loss = 0.63731046\n",
            "Iteration 56, loss = 0.63371852\n",
            "Iteration 57, loss = 0.63028750\n",
            "Iteration 58, loss = 0.62695099\n",
            "Iteration 59, loss = 0.62358378\n",
            "Iteration 60, loss = 0.62029628\n",
            "Iteration 61, loss = 0.61687503\n",
            "Iteration 62, loss = 0.61349241\n",
            "Iteration 63, loss = 0.60989681\n",
            "Iteration 64, loss = 0.60641647\n",
            "Iteration 65, loss = 0.60282866\n",
            "Iteration 66, loss = 0.59936176\n",
            "Iteration 67, loss = 0.59584157\n",
            "Iteration 68, loss = 0.59228164\n",
            "Iteration 69, loss = 0.58905967\n",
            "Iteration 70, loss = 0.58586842\n",
            "Iteration 71, loss = 0.58308986\n",
            "Iteration 72, loss = 0.58046393\n",
            "Iteration 73, loss = 0.57771304\n",
            "Iteration 74, loss = 0.57529600\n",
            "Iteration 75, loss = 0.57277436\n",
            "Iteration 76, loss = 0.57025684\n",
            "Iteration 77, loss = 0.56791548\n",
            "Iteration 78, loss = 0.56569975\n",
            "Iteration 79, loss = 0.56339669\n",
            "Iteration 80, loss = 0.56130564\n",
            "Iteration 81, loss = 0.55913754\n",
            "Iteration 82, loss = 0.55703159\n",
            "Iteration 83, loss = 0.55490272\n",
            "Iteration 84, loss = 0.55280901\n",
            "Iteration 85, loss = 0.55088570\n",
            "Iteration 86, loss = 0.54892518\n",
            "Iteration 87, loss = 0.54694715\n",
            "Iteration 88, loss = 0.54511632\n",
            "Iteration 89, loss = 0.54311390\n",
            "Iteration 90, loss = 0.54134148\n",
            "Iteration 91, loss = 0.53946254\n",
            "Iteration 92, loss = 0.53763419\n",
            "Iteration 93, loss = 0.53598253\n",
            "Iteration 94, loss = 0.53423924\n",
            "Iteration 95, loss = 0.53245485\n",
            "Iteration 96, loss = 0.53080083\n",
            "Iteration 97, loss = 0.52899965\n",
            "Iteration 98, loss = 0.52722550\n",
            "Iteration 99, loss = 0.52554746\n",
            "Iteration 100, loss = 0.52388927\n",
            "Iteration 101, loss = 0.52223555\n",
            "Iteration 102, loss = 0.52071704\n",
            "Iteration 103, loss = 0.51913694\n",
            "Iteration 104, loss = 0.51753046\n",
            "Iteration 105, loss = 0.51598661\n",
            "Iteration 106, loss = 0.51447743\n",
            "Iteration 107, loss = 0.51291672\n",
            "Iteration 108, loss = 0.51140828\n",
            "Iteration 109, loss = 0.50999038\n",
            "Iteration 110, loss = 0.50855301\n",
            "Iteration 111, loss = 0.50725627\n",
            "Iteration 112, loss = 0.50587507\n",
            "Iteration 113, loss = 0.50447560\n",
            "Iteration 114, loss = 0.50309961\n",
            "Iteration 115, loss = 0.50175124\n",
            "Iteration 116, loss = 0.50041573\n",
            "Iteration 117, loss = 0.49914900\n",
            "Iteration 118, loss = 0.49786686\n",
            "Iteration 119, loss = 0.49658878\n",
            "Iteration 120, loss = 0.49546207\n",
            "Iteration 121, loss = 0.49415381\n",
            "Iteration 122, loss = 0.49289767\n",
            "Iteration 123, loss = 0.49173754\n",
            "Iteration 124, loss = 0.49059930\n",
            "Iteration 125, loss = 0.48952013\n",
            "Iteration 126, loss = 0.48828538\n",
            "Iteration 127, loss = 0.48716626\n",
            "Iteration 128, loss = 0.48609799\n",
            "Iteration 129, loss = 0.48503857\n",
            "Iteration 130, loss = 0.48397398\n",
            "Iteration 131, loss = 0.48300536\n",
            "Iteration 132, loss = 0.48187351\n",
            "Iteration 133, loss = 0.48089354\n",
            "Iteration 134, loss = 0.47998047\n",
            "Iteration 135, loss = 0.47909525\n",
            "Iteration 136, loss = 0.47807563\n",
            "Iteration 137, loss = 0.47716788\n",
            "Iteration 138, loss = 0.47630565\n",
            "Iteration 139, loss = 0.47541950\n",
            "Iteration 140, loss = 0.47460185\n",
            "Iteration 141, loss = 0.47371003\n",
            "Iteration 142, loss = 0.47293919\n",
            "Iteration 143, loss = 0.47216801\n",
            "Iteration 144, loss = 0.47137016\n",
            "Iteration 145, loss = 0.47059428\n",
            "Iteration 146, loss = 0.46978008\n",
            "Iteration 147, loss = 0.46900667\n",
            "Iteration 148, loss = 0.46823376\n",
            "Iteration 149, loss = 0.46762863\n",
            "Iteration 150, loss = 0.46684419\n",
            "Iteration 151, loss = 0.46611691\n",
            "Iteration 152, loss = 0.46543865\n",
            "Iteration 153, loss = 0.46473928\n",
            "Iteration 154, loss = 0.46403478\n",
            "Iteration 155, loss = 0.46325489\n",
            "Iteration 156, loss = 0.46255930\n",
            "Iteration 157, loss = 0.46188871\n",
            "Iteration 158, loss = 0.46112109\n",
            "Iteration 159, loss = 0.46033264\n",
            "Iteration 160, loss = 0.45959682\n",
            "Iteration 161, loss = 0.45888343\n",
            "Iteration 162, loss = 0.45813848\n",
            "Iteration 163, loss = 0.45746845\n",
            "Iteration 164, loss = 0.45674676\n",
            "Iteration 165, loss = 0.45602343\n",
            "Iteration 166, loss = 0.45536626\n",
            "Iteration 167, loss = 0.45471746\n",
            "Iteration 168, loss = 0.45407175\n",
            "Iteration 169, loss = 0.45343109\n",
            "Iteration 170, loss = 0.45281035\n",
            "Iteration 171, loss = 0.45225553\n",
            "Iteration 172, loss = 0.45161917\n",
            "Iteration 173, loss = 0.45094181\n",
            "Iteration 174, loss = 0.45033373\n",
            "Iteration 175, loss = 0.44977137\n",
            "Iteration 176, loss = 0.44908716\n",
            "Iteration 177, loss = 0.44844785\n",
            "Iteration 178, loss = 0.44786644\n",
            "Iteration 179, loss = 0.44722226\n",
            "Iteration 180, loss = 0.44664831\n",
            "Iteration 181, loss = 0.44600465\n",
            "Iteration 182, loss = 0.44548265\n",
            "Iteration 183, loss = 0.44485463\n",
            "Iteration 184, loss = 0.44418709\n",
            "Iteration 185, loss = 0.44362718\n",
            "Iteration 186, loss = 0.44309215\n",
            "Iteration 187, loss = 0.44252819\n",
            "Iteration 188, loss = 0.44198007\n",
            "Iteration 189, loss = 0.44139694\n",
            "Iteration 190, loss = 0.44098145\n",
            "Iteration 191, loss = 0.44049632\n",
            "Iteration 192, loss = 0.43992091\n",
            "Iteration 193, loss = 0.43946000\n",
            "Iteration 194, loss = 0.43898344\n",
            "Iteration 195, loss = 0.43847322\n",
            "Iteration 196, loss = 0.43796676\n",
            "Iteration 197, loss = 0.43752480\n",
            "Iteration 198, loss = 0.43706468\n",
            "Iteration 199, loss = 0.43662637\n",
            "Iteration 200, loss = 0.43610940\n",
            "Iteration 201, loss = 0.43568284\n",
            "Iteration 202, loss = 0.43520339\n",
            "Iteration 203, loss = 0.43474164\n",
            "Iteration 204, loss = 0.43438789\n",
            "Iteration 205, loss = 0.43386874\n",
            "Iteration 206, loss = 0.43339598\n",
            "Iteration 207, loss = 0.43301328\n",
            "Iteration 208, loss = 0.43260928\n",
            "Iteration 209, loss = 0.43213741\n",
            "Iteration 210, loss = 0.43175378\n",
            "Iteration 211, loss = 0.43138409\n",
            "Iteration 212, loss = 0.43099339\n",
            "Iteration 213, loss = 0.43065849\n",
            "Iteration 214, loss = 0.43036427\n",
            "Iteration 215, loss = 0.42989742\n",
            "Iteration 216, loss = 0.42953034\n",
            "Iteration 217, loss = 0.42909693\n",
            "Iteration 218, loss = 0.42879093\n",
            "Iteration 219, loss = 0.42847198\n",
            "Iteration 220, loss = 0.42806667\n",
            "Iteration 221, loss = 0.42769476\n",
            "Iteration 222, loss = 0.42736760\n",
            "Iteration 223, loss = 0.42702030\n",
            "Iteration 224, loss = 0.42666927\n",
            "Iteration 225, loss = 0.42639409\n",
            "Iteration 226, loss = 0.42599360\n",
            "Iteration 227, loss = 0.42563195\n",
            "Iteration 228, loss = 0.42531453\n",
            "Iteration 229, loss = 0.42497797\n",
            "Iteration 230, loss = 0.42470956\n",
            "Iteration 231, loss = 0.42432021\n",
            "Iteration 232, loss = 0.42398385\n",
            "Iteration 233, loss = 0.42378777\n",
            "Iteration 234, loss = 0.42350731\n",
            "Iteration 235, loss = 0.42307207\n",
            "Iteration 236, loss = 0.42267997\n",
            "Iteration 237, loss = 0.42249317\n",
            "Iteration 238, loss = 0.42222624\n",
            "Iteration 239, loss = 0.42196828\n",
            "Iteration 240, loss = 0.42166740\n",
            "Iteration 241, loss = 0.42126035\n",
            "Iteration 242, loss = 0.42091869\n",
            "Iteration 243, loss = 0.42064751\n",
            "Iteration 244, loss = 0.42033726\n",
            "Iteration 245, loss = 0.42022425\n",
            "Iteration 246, loss = 0.41983554\n",
            "Iteration 247, loss = 0.41949639\n",
            "Iteration 248, loss = 0.41924224\n",
            "Iteration 249, loss = 0.41894729\n",
            "Iteration 250, loss = 0.41865827\n",
            "Iteration 251, loss = 0.41840661\n",
            "Iteration 252, loss = 0.41817781\n",
            "Iteration 253, loss = 0.41786714\n",
            "Iteration 254, loss = 0.41763980\n",
            "Iteration 255, loss = 0.41740927\n",
            "Iteration 256, loss = 0.41721697\n",
            "Iteration 257, loss = 0.41687688\n",
            "Iteration 258, loss = 0.41656989\n",
            "Iteration 259, loss = 0.41639382\n",
            "Iteration 260, loss = 0.41615618\n",
            "Iteration 261, loss = 0.41589892\n",
            "Iteration 262, loss = 0.41572579\n",
            "Iteration 263, loss = 0.41547323\n",
            "Iteration 264, loss = 0.41522480\n",
            "Iteration 265, loss = 0.41500023\n",
            "Iteration 266, loss = 0.41476209\n",
            "Iteration 267, loss = 0.41451866\n",
            "Iteration 268, loss = 0.41428391\n",
            "Iteration 269, loss = 0.41414023\n",
            "Iteration 270, loss = 0.41384866\n",
            "Iteration 271, loss = 0.41361048\n",
            "Iteration 272, loss = 0.41344328\n",
            "Iteration 273, loss = 0.41322354\n",
            "Iteration 274, loss = 0.41303591\n",
            "Iteration 275, loss = 0.41276789\n",
            "Iteration 276, loss = 0.41252821\n",
            "Iteration 277, loss = 0.41246073\n",
            "Iteration 278, loss = 0.41217921\n",
            "Iteration 279, loss = 0.41195865\n",
            "Iteration 280, loss = 0.41178191\n",
            "Iteration 281, loss = 0.41159849\n",
            "Iteration 282, loss = 0.41137120\n",
            "Iteration 283, loss = 0.41116786\n",
            "Iteration 284, loss = 0.41097968\n",
            "Iteration 285, loss = 0.41079580\n",
            "Iteration 286, loss = 0.41062118\n",
            "Iteration 287, loss = 0.41041711\n",
            "Iteration 288, loss = 0.41021403\n",
            "Iteration 289, loss = 0.41006340\n",
            "Iteration 290, loss = 0.40988892\n",
            "Iteration 291, loss = 0.40970039\n",
            "Iteration 292, loss = 0.40947200\n",
            "Iteration 293, loss = 0.40928893\n",
            "Iteration 294, loss = 0.40912828\n",
            "Iteration 295, loss = 0.40897260\n",
            "Iteration 296, loss = 0.40887123\n",
            "Iteration 297, loss = 0.40882666\n",
            "Iteration 298, loss = 0.40849289\n",
            "Iteration 299, loss = 0.40840579\n",
            "Iteration 300, loss = 0.40817171\n",
            "Iteration 301, loss = 0.40801314\n",
            "Iteration 302, loss = 0.40789054\n",
            "Iteration 303, loss = 0.40770646\n",
            "Iteration 304, loss = 0.40754797\n",
            "Iteration 305, loss = 0.40748866\n",
            "Iteration 306, loss = 0.40728138\n",
            "Iteration 307, loss = 0.40712075\n",
            "Iteration 308, loss = 0.40704632\n",
            "Iteration 309, loss = 0.40685435\n",
            "Iteration 310, loss = 0.40670829\n",
            "Iteration 311, loss = 0.40654565\n",
            "Iteration 312, loss = 0.40651843\n",
            "Iteration 313, loss = 0.40633445\n",
            "Iteration 314, loss = 0.40621267\n",
            "Iteration 315, loss = 0.40617954\n",
            "Iteration 316, loss = 0.40588838\n",
            "Iteration 317, loss = 0.40576605\n",
            "Iteration 318, loss = 0.40564078\n",
            "Iteration 319, loss = 0.40551258\n",
            "Iteration 320, loss = 0.40542833\n",
            "Iteration 321, loss = 0.40529756\n",
            "Iteration 322, loss = 0.40507463\n",
            "Iteration 323, loss = 0.40499361\n",
            "Iteration 324, loss = 0.40482550\n",
            "Iteration 325, loss = 0.40469630\n",
            "Iteration 326, loss = 0.40461313\n",
            "Iteration 327, loss = 0.40460049\n",
            "Iteration 328, loss = 0.40443122\n",
            "Iteration 329, loss = 0.40419270\n",
            "Iteration 330, loss = 0.40407050\n",
            "Iteration 331, loss = 0.40394491\n",
            "Iteration 332, loss = 0.40381095\n",
            "Iteration 333, loss = 0.40374906\n",
            "Iteration 334, loss = 0.40356444\n",
            "Iteration 335, loss = 0.40347907\n",
            "Iteration 336, loss = 0.40330106\n",
            "Iteration 337, loss = 0.40319602\n",
            "Iteration 338, loss = 0.40306407\n",
            "Iteration 339, loss = 0.40293487\n",
            "Iteration 340, loss = 0.40288055\n",
            "Iteration 341, loss = 0.40277784\n",
            "Iteration 342, loss = 0.40267732\n",
            "Iteration 343, loss = 0.40252443\n",
            "Iteration 344, loss = 0.40242122\n",
            "Iteration 345, loss = 0.40221348\n",
            "Iteration 346, loss = 0.40208646\n",
            "Iteration 347, loss = 0.40198207\n",
            "Iteration 348, loss = 0.40190811\n",
            "Iteration 349, loss = 0.40177380\n",
            "Iteration 350, loss = 0.40167862\n",
            "Iteration 351, loss = 0.40153625\n",
            "Iteration 352, loss = 0.40145214\n",
            "Iteration 353, loss = 0.40136852\n",
            "Iteration 354, loss = 0.40123027\n",
            "Iteration 355, loss = 0.40109990\n",
            "Iteration 356, loss = 0.40099281\n",
            "Iteration 357, loss = 0.40097023\n",
            "Iteration 358, loss = 0.40090440\n",
            "Iteration 359, loss = 0.40076905\n",
            "Iteration 360, loss = 0.40065096\n",
            "Iteration 361, loss = 0.40064490\n",
            "Iteration 362, loss = 0.40044691\n",
            "Iteration 363, loss = 0.40024580\n",
            "Iteration 364, loss = 0.40022465\n",
            "Iteration 365, loss = 0.39997729\n",
            "Iteration 366, loss = 0.39986298\n",
            "Iteration 367, loss = 0.39979454\n",
            "Iteration 368, loss = 0.39965553\n",
            "Iteration 369, loss = 0.39950174\n",
            "Iteration 370, loss = 0.39933556\n",
            "Iteration 371, loss = 0.39920788\n",
            "Iteration 372, loss = 0.39910865\n",
            "Iteration 373, loss = 0.39896032\n",
            "Iteration 374, loss = 0.39883537\n",
            "Iteration 375, loss = 0.39874265\n",
            "Iteration 376, loss = 0.39861779\n",
            "Iteration 377, loss = 0.39849632\n",
            "Iteration 378, loss = 0.39842033\n",
            "Iteration 379, loss = 0.39829782\n",
            "Iteration 380, loss = 0.39820673\n",
            "Iteration 381, loss = 0.39804922\n",
            "Iteration 382, loss = 0.39788603\n",
            "Iteration 383, loss = 0.39769568\n",
            "Iteration 384, loss = 0.39761476\n",
            "Iteration 385, loss = 0.39746809\n",
            "Iteration 386, loss = 0.39735318\n",
            "Iteration 387, loss = 0.39722776\n",
            "Iteration 388, loss = 0.39711505\n",
            "Iteration 389, loss = 0.39696890\n",
            "Iteration 390, loss = 0.39691184\n",
            "Iteration 391, loss = 0.39673326\n",
            "Iteration 392, loss = 0.39656794\n",
            "Iteration 393, loss = 0.39648142\n",
            "Iteration 394, loss = 0.39648099\n",
            "Iteration 395, loss = 0.39619336\n",
            "Iteration 396, loss = 0.39612541\n",
            "Iteration 397, loss = 0.39599519\n",
            "Iteration 398, loss = 0.39587972\n",
            "Iteration 399, loss = 0.39580662\n",
            "Iteration 400, loss = 0.39569749\n",
            "Iteration 401, loss = 0.39562437\n",
            "Iteration 402, loss = 0.39552110\n",
            "Iteration 403, loss = 0.39547069\n",
            "Iteration 404, loss = 0.39530076\n",
            "Iteration 405, loss = 0.39522239\n",
            "Iteration 406, loss = 0.39504254\n",
            "Iteration 407, loss = 0.39494783\n",
            "Iteration 408, loss = 0.39488531\n",
            "Iteration 409, loss = 0.39473250\n",
            "Iteration 410, loss = 0.39467032\n",
            "Iteration 411, loss = 0.39455613\n",
            "Iteration 412, loss = 0.39443800\n",
            "Iteration 413, loss = 0.39448149\n",
            "Iteration 414, loss = 0.39441181\n",
            "Iteration 415, loss = 0.39411930\n",
            "Iteration 416, loss = 0.39411794\n",
            "Iteration 417, loss = 0.39403220\n",
            "Iteration 418, loss = 0.39390992\n",
            "Iteration 419, loss = 0.39373532\n",
            "Iteration 420, loss = 0.39369757\n",
            "Iteration 421, loss = 0.39372020\n",
            "Iteration 422, loss = 0.39369235\n",
            "Iteration 423, loss = 0.39361710\n",
            "Iteration 424, loss = 0.39335770\n",
            "Iteration 425, loss = 0.39319817\n",
            "Iteration 426, loss = 0.39312293\n",
            "Iteration 427, loss = 0.39308449\n",
            "Iteration 428, loss = 0.39305316\n",
            "Iteration 429, loss = 0.39299541\n",
            "Iteration 430, loss = 0.39288212\n",
            "Iteration 431, loss = 0.39288770\n",
            "Iteration 432, loss = 0.39288317\n",
            "Iteration 433, loss = 0.39263888\n",
            "Iteration 434, loss = 0.39257903\n",
            "Iteration 435, loss = 0.39242666\n",
            "Iteration 436, loss = 0.39227625\n",
            "Iteration 437, loss = 0.39216699\n",
            "Iteration 438, loss = 0.39205762\n",
            "Iteration 439, loss = 0.39204665\n",
            "Iteration 440, loss = 0.39195039\n",
            "Iteration 441, loss = 0.39185678\n",
            "Iteration 442, loss = 0.39179219\n",
            "Iteration 443, loss = 0.39167778\n",
            "Iteration 444, loss = 0.39156898\n",
            "Iteration 445, loss = 0.39150633\n",
            "Iteration 446, loss = 0.39137117\n",
            "Iteration 447, loss = 0.39129916\n",
            "Iteration 448, loss = 0.39121982\n",
            "Iteration 449, loss = 0.39111241\n",
            "Iteration 450, loss = 0.39103963\n",
            "Iteration 451, loss = 0.39098268\n",
            "Iteration 452, loss = 0.39098361\n",
            "Iteration 453, loss = 0.39084983\n",
            "Iteration 454, loss = 0.39082596\n",
            "Iteration 455, loss = 0.39075876\n",
            "Iteration 456, loss = 0.39070422\n",
            "Iteration 457, loss = 0.39060074\n",
            "Iteration 458, loss = 0.39061517\n",
            "Iteration 459, loss = 0.39040541\n",
            "Iteration 460, loss = 0.39026553\n",
            "Iteration 461, loss = 0.39020107\n",
            "Iteration 462, loss = 0.39012833\n",
            "Iteration 463, loss = 0.39008145\n",
            "Iteration 464, loss = 0.39000822\n",
            "Iteration 465, loss = 0.38993102\n",
            "Iteration 466, loss = 0.38983908\n",
            "Iteration 467, loss = 0.38973975\n",
            "Iteration 468, loss = 0.38963064\n",
            "Iteration 469, loss = 0.38957123\n",
            "Iteration 470, loss = 0.38955249\n",
            "Iteration 471, loss = 0.38954304\n",
            "Iteration 472, loss = 0.38954443\n",
            "Iteration 473, loss = 0.38949897\n",
            "Iteration 474, loss = 0.38930472\n",
            "Iteration 475, loss = 0.38919944\n",
            "Iteration 476, loss = 0.38901152\n",
            "Iteration 477, loss = 0.38906122\n",
            "Iteration 478, loss = 0.38893090\n",
            "Iteration 479, loss = 0.38887691\n",
            "Iteration 480, loss = 0.38879675\n",
            "Iteration 481, loss = 0.38872743\n",
            "Iteration 482, loss = 0.38862343\n",
            "Iteration 483, loss = 0.38859335\n",
            "Iteration 484, loss = 0.38844640\n",
            "Iteration 485, loss = 0.38832727\n",
            "Iteration 486, loss = 0.38825853\n",
            "Iteration 487, loss = 0.38816755\n",
            "Iteration 488, loss = 0.38810097\n",
            "Iteration 489, loss = 0.38810928\n",
            "Iteration 490, loss = 0.38796656\n",
            "Iteration 491, loss = 0.38791191\n",
            "Iteration 492, loss = 0.38775465\n",
            "Iteration 493, loss = 0.38772084\n",
            "Iteration 494, loss = 0.38783280\n",
            "Iteration 495, loss = 0.38770394\n",
            "Iteration 496, loss = 0.38765356\n",
            "Iteration 497, loss = 0.38751135\n",
            "Iteration 498, loss = 0.38746633\n",
            "Iteration 499, loss = 0.38729976\n",
            "Iteration 500, loss = 0.38716565\n",
            "Iteration 501, loss = 0.38706746\n",
            "Iteration 502, loss = 0.38702680\n",
            "Iteration 503, loss = 0.38704940\n",
            "Iteration 504, loss = 0.38696655\n",
            "Iteration 505, loss = 0.38684042\n",
            "Iteration 506, loss = 0.38669512\n",
            "Iteration 507, loss = 0.38665573\n",
            "Iteration 508, loss = 0.38665915\n",
            "Iteration 509, loss = 0.38662950\n",
            "Iteration 510, loss = 0.38654585\n",
            "Iteration 511, loss = 0.38649086\n",
            "Iteration 512, loss = 0.38650321\n",
            "Iteration 513, loss = 0.38636861\n",
            "Iteration 514, loss = 0.38624876\n",
            "Iteration 515, loss = 0.38630848\n",
            "Iteration 516, loss = 0.38620253\n",
            "Iteration 517, loss = 0.38608857\n",
            "Iteration 518, loss = 0.38596876\n",
            "Iteration 519, loss = 0.38594200\n",
            "Iteration 520, loss = 0.38583370\n",
            "Iteration 521, loss = 0.38577641\n",
            "Iteration 522, loss = 0.38570708\n",
            "Iteration 523, loss = 0.38569028\n",
            "Iteration 524, loss = 0.38556312\n",
            "Iteration 525, loss = 0.38559911\n",
            "Iteration 526, loss = 0.38557126\n",
            "Iteration 527, loss = 0.38548953\n",
            "Iteration 528, loss = 0.38543418\n",
            "Iteration 529, loss = 0.38537350\n",
            "Iteration 530, loss = 0.38538097\n",
            "Iteration 531, loss = 0.38520772\n",
            "Iteration 532, loss = 0.38506150\n",
            "Iteration 533, loss = 0.38546665\n",
            "Iteration 534, loss = 0.38532040\n",
            "Iteration 535, loss = 0.38526321\n",
            "Iteration 536, loss = 0.38498618\n",
            "Iteration 537, loss = 0.38494514\n",
            "Iteration 538, loss = 0.38486685\n",
            "Iteration 539, loss = 0.38494819\n",
            "Iteration 540, loss = 0.38488533\n",
            "Iteration 541, loss = 0.38479932\n",
            "Iteration 542, loss = 0.38471505\n",
            "Iteration 543, loss = 0.38470987\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.81061088\n",
            "Iteration 2, loss = 0.80805747\n",
            "Iteration 3, loss = 0.80530594\n",
            "Iteration 4, loss = 0.80274460\n",
            "Iteration 5, loss = 0.79997205\n",
            "Iteration 6, loss = 0.79737683\n",
            "Iteration 7, loss = 0.79460204\n",
            "Iteration 8, loss = 0.79203625\n",
            "Iteration 9, loss = 0.78923375\n",
            "Iteration 10, loss = 0.78659228\n",
            "Iteration 11, loss = 0.78376114\n",
            "Iteration 12, loss = 0.78109194\n",
            "Iteration 13, loss = 0.77829478\n",
            "Iteration 14, loss = 0.77547421\n",
            "Iteration 15, loss = 0.77263790\n",
            "Iteration 16, loss = 0.76993434\n",
            "Iteration 17, loss = 0.76701492\n",
            "Iteration 18, loss = 0.76406898\n",
            "Iteration 19, loss = 0.76119477\n",
            "Iteration 20, loss = 0.75815973\n",
            "Iteration 21, loss = 0.75528164\n",
            "Iteration 22, loss = 0.75228634\n",
            "Iteration 23, loss = 0.74912714\n",
            "Iteration 24, loss = 0.74604647\n",
            "Iteration 25, loss = 0.74293668\n",
            "Iteration 26, loss = 0.73978344\n",
            "Iteration 27, loss = 0.73658862\n",
            "Iteration 28, loss = 0.73303148\n",
            "Iteration 29, loss = 0.72994430\n",
            "Iteration 30, loss = 0.72637434\n",
            "Iteration 31, loss = 0.72280794\n",
            "Iteration 32, loss = 0.71949250\n",
            "Iteration 33, loss = 0.71587850\n",
            "Iteration 34, loss = 0.71217810\n",
            "Iteration 35, loss = 0.70863549\n",
            "Iteration 36, loss = 0.70482195\n",
            "Iteration 37, loss = 0.70123888\n",
            "Iteration 38, loss = 0.69751427\n",
            "Iteration 39, loss = 0.69371344\n",
            "Iteration 40, loss = 0.68984471\n",
            "Iteration 41, loss = 0.68613644\n",
            "Iteration 42, loss = 0.68217482\n",
            "Iteration 43, loss = 0.67840966\n",
            "Iteration 44, loss = 0.67455572\n",
            "Iteration 45, loss = 0.67074388\n",
            "Iteration 46, loss = 0.66668595\n",
            "Iteration 47, loss = 0.66291411\n",
            "Iteration 48, loss = 0.65925986\n",
            "Iteration 49, loss = 0.65544919\n",
            "Iteration 50, loss = 0.65181786\n",
            "Iteration 51, loss = 0.64802857\n",
            "Iteration 52, loss = 0.64443640\n",
            "Iteration 53, loss = 0.64075802\n",
            "Iteration 54, loss = 0.63736668\n",
            "Iteration 55, loss = 0.63385282\n",
            "Iteration 56, loss = 0.63029795\n",
            "Iteration 57, loss = 0.62664296\n",
            "Iteration 58, loss = 0.62321089\n",
            "Iteration 59, loss = 0.61959894\n",
            "Iteration 60, loss = 0.61605458\n",
            "Iteration 61, loss = 0.61248181\n",
            "Iteration 62, loss = 0.60900359\n",
            "Iteration 63, loss = 0.60556275\n",
            "Iteration 64, loss = 0.60202538\n",
            "Iteration 65, loss = 0.59863733\n",
            "Iteration 66, loss = 0.59526905\n",
            "Iteration 67, loss = 0.59193184\n",
            "Iteration 68, loss = 0.58860576\n",
            "Iteration 69, loss = 0.58595464\n",
            "Iteration 70, loss = 0.58300891\n",
            "Iteration 71, loss = 0.58043008\n",
            "Iteration 72, loss = 0.57796553\n",
            "Iteration 73, loss = 0.57535153\n",
            "Iteration 74, loss = 0.57308259\n",
            "Iteration 75, loss = 0.57074641\n",
            "Iteration 76, loss = 0.56842286\n",
            "Iteration 77, loss = 0.56620935\n",
            "Iteration 78, loss = 0.56419550\n",
            "Iteration 79, loss = 0.56201541\n",
            "Iteration 80, loss = 0.56011640\n",
            "Iteration 81, loss = 0.55810465\n",
            "Iteration 82, loss = 0.55624258\n",
            "Iteration 83, loss = 0.55434363\n",
            "Iteration 84, loss = 0.55241273\n",
            "Iteration 85, loss = 0.55060543\n",
            "Iteration 86, loss = 0.54875452\n",
            "Iteration 87, loss = 0.54684598\n",
            "Iteration 88, loss = 0.54498336\n",
            "Iteration 89, loss = 0.54300522\n",
            "Iteration 90, loss = 0.54112761\n",
            "Iteration 91, loss = 0.53918955\n",
            "Iteration 92, loss = 0.53726179\n",
            "Iteration 93, loss = 0.53548099\n",
            "Iteration 94, loss = 0.53373369\n",
            "Iteration 95, loss = 0.53202147\n",
            "Iteration 96, loss = 0.53039970\n",
            "Iteration 97, loss = 0.52863113\n",
            "Iteration 98, loss = 0.52694040\n",
            "Iteration 99, loss = 0.52526697\n",
            "Iteration 100, loss = 0.52357154\n",
            "Iteration 101, loss = 0.52184889\n",
            "Iteration 102, loss = 0.52021471\n",
            "Iteration 103, loss = 0.51861252\n",
            "Iteration 104, loss = 0.51698059\n",
            "Iteration 105, loss = 0.51549237\n",
            "Iteration 106, loss = 0.51405690\n",
            "Iteration 107, loss = 0.51253468\n",
            "Iteration 108, loss = 0.51106155\n",
            "Iteration 109, loss = 0.50969095\n",
            "Iteration 110, loss = 0.50832294\n",
            "Iteration 111, loss = 0.50704675\n",
            "Iteration 112, loss = 0.50574059\n",
            "Iteration 113, loss = 0.50440364\n",
            "Iteration 114, loss = 0.50321170\n",
            "Iteration 115, loss = 0.50181116\n",
            "Iteration 116, loss = 0.50054439\n",
            "Iteration 117, loss = 0.49931797\n",
            "Iteration 118, loss = 0.49806362\n",
            "Iteration 119, loss = 0.49682553\n",
            "Iteration 120, loss = 0.49572589\n",
            "Iteration 121, loss = 0.49449437\n",
            "Iteration 122, loss = 0.49327935\n",
            "Iteration 123, loss = 0.49215497\n",
            "Iteration 124, loss = 0.49103990\n",
            "Iteration 125, loss = 0.49004437\n",
            "Iteration 126, loss = 0.48884041\n",
            "Iteration 127, loss = 0.48778327\n",
            "Iteration 128, loss = 0.48677771\n",
            "Iteration 129, loss = 0.48574607\n",
            "Iteration 130, loss = 0.48471390\n",
            "Iteration 131, loss = 0.48369887\n",
            "Iteration 132, loss = 0.48271478\n",
            "Iteration 133, loss = 0.48165818\n",
            "Iteration 134, loss = 0.48076419\n",
            "Iteration 135, loss = 0.47978928\n",
            "Iteration 136, loss = 0.47878066\n",
            "Iteration 137, loss = 0.47784908\n",
            "Iteration 138, loss = 0.47692317\n",
            "Iteration 139, loss = 0.47609402\n",
            "Iteration 140, loss = 0.47523048\n",
            "Iteration 141, loss = 0.47430832\n",
            "Iteration 142, loss = 0.47358986\n",
            "Iteration 143, loss = 0.47277613\n",
            "Iteration 144, loss = 0.47203102\n",
            "Iteration 145, loss = 0.47123732\n",
            "Iteration 146, loss = 0.47045203\n",
            "Iteration 147, loss = 0.46973216\n",
            "Iteration 148, loss = 0.46897662\n",
            "Iteration 149, loss = 0.46836467\n",
            "Iteration 150, loss = 0.46766961\n",
            "Iteration 151, loss = 0.46695206\n",
            "Iteration 152, loss = 0.46621349\n",
            "Iteration 153, loss = 0.46550727\n",
            "Iteration 154, loss = 0.46486959\n",
            "Iteration 155, loss = 0.46413001\n",
            "Iteration 156, loss = 0.46345902\n",
            "Iteration 157, loss = 0.46278184\n",
            "Iteration 158, loss = 0.46207044\n",
            "Iteration 159, loss = 0.46136359\n",
            "Iteration 160, loss = 0.46069129\n",
            "Iteration 161, loss = 0.46003925\n",
            "Iteration 162, loss = 0.45935307\n",
            "Iteration 163, loss = 0.45875748\n",
            "Iteration 164, loss = 0.45809028\n",
            "Iteration 165, loss = 0.45739568\n",
            "Iteration 166, loss = 0.45677136\n",
            "Iteration 167, loss = 0.45611873\n",
            "Iteration 168, loss = 0.45542516\n",
            "Iteration 169, loss = 0.45472974\n",
            "Iteration 170, loss = 0.45407634\n",
            "Iteration 171, loss = 0.45343919\n",
            "Iteration 172, loss = 0.45280827\n",
            "Iteration 173, loss = 0.45217638\n",
            "Iteration 174, loss = 0.45153043\n",
            "Iteration 175, loss = 0.45096115\n",
            "Iteration 176, loss = 0.45032800\n",
            "Iteration 177, loss = 0.44969385\n",
            "Iteration 178, loss = 0.44915189\n",
            "Iteration 179, loss = 0.44853109\n",
            "Iteration 180, loss = 0.44797870\n",
            "Iteration 181, loss = 0.44736561\n",
            "Iteration 182, loss = 0.44688268\n",
            "Iteration 183, loss = 0.44625940\n",
            "Iteration 184, loss = 0.44563247\n",
            "Iteration 185, loss = 0.44509428\n",
            "Iteration 186, loss = 0.44455241\n",
            "Iteration 187, loss = 0.44401259\n",
            "Iteration 188, loss = 0.44350564\n",
            "Iteration 189, loss = 0.44291658\n",
            "Iteration 190, loss = 0.44256108\n",
            "Iteration 191, loss = 0.44210426\n",
            "Iteration 192, loss = 0.44153608\n",
            "Iteration 193, loss = 0.44101489\n",
            "Iteration 194, loss = 0.44052597\n",
            "Iteration 195, loss = 0.44009922\n",
            "Iteration 196, loss = 0.43958621\n",
            "Iteration 197, loss = 0.43916371\n",
            "Iteration 198, loss = 0.43868390\n",
            "Iteration 199, loss = 0.43823085\n",
            "Iteration 200, loss = 0.43776275\n",
            "Iteration 201, loss = 0.43736378\n",
            "Iteration 202, loss = 0.43692815\n",
            "Iteration 203, loss = 0.43649578\n",
            "Iteration 204, loss = 0.43615608\n",
            "Iteration 205, loss = 0.43569405\n",
            "Iteration 206, loss = 0.43524148\n",
            "Iteration 207, loss = 0.43481893\n",
            "Iteration 208, loss = 0.43443320\n",
            "Iteration 209, loss = 0.43401972\n",
            "Iteration 210, loss = 0.43366839\n",
            "Iteration 211, loss = 0.43323556\n",
            "Iteration 212, loss = 0.43293934\n",
            "Iteration 213, loss = 0.43261201\n",
            "Iteration 214, loss = 0.43238023\n",
            "Iteration 215, loss = 0.43189022\n",
            "Iteration 216, loss = 0.43150748\n",
            "Iteration 217, loss = 0.43119876\n",
            "Iteration 218, loss = 0.43092444\n",
            "Iteration 219, loss = 0.43060373\n",
            "Iteration 220, loss = 0.43023672\n",
            "Iteration 221, loss = 0.42985305\n",
            "Iteration 222, loss = 0.42958745\n",
            "Iteration 223, loss = 0.42931023\n",
            "Iteration 224, loss = 0.42897117\n",
            "Iteration 225, loss = 0.42874672\n",
            "Iteration 226, loss = 0.42836491\n",
            "Iteration 227, loss = 0.42805515\n",
            "Iteration 228, loss = 0.42779830\n",
            "Iteration 229, loss = 0.42755604\n",
            "Iteration 230, loss = 0.42738095\n",
            "Iteration 231, loss = 0.42701109\n",
            "Iteration 232, loss = 0.42679568\n",
            "Iteration 233, loss = 0.42657222\n",
            "Iteration 234, loss = 0.42629315\n",
            "Iteration 235, loss = 0.42597336\n",
            "Iteration 236, loss = 0.42566114\n",
            "Iteration 237, loss = 0.42557110\n",
            "Iteration 238, loss = 0.42533482\n",
            "Iteration 239, loss = 0.42508783\n",
            "Iteration 240, loss = 0.42486277\n",
            "Iteration 241, loss = 0.42452764\n",
            "Iteration 242, loss = 0.42426361\n",
            "Iteration 243, loss = 0.42396117\n",
            "Iteration 244, loss = 0.42365558\n",
            "Iteration 245, loss = 0.42362821\n",
            "Iteration 246, loss = 0.42323831\n",
            "Iteration 247, loss = 0.42298769\n",
            "Iteration 248, loss = 0.42270294\n",
            "Iteration 249, loss = 0.42247997\n",
            "Iteration 250, loss = 0.42219917\n",
            "Iteration 251, loss = 0.42202189\n",
            "Iteration 252, loss = 0.42180498\n",
            "Iteration 253, loss = 0.42160044\n",
            "Iteration 254, loss = 0.42131437\n",
            "Iteration 255, loss = 0.42110058\n",
            "Iteration 256, loss = 0.42087815\n",
            "Iteration 257, loss = 0.42060276\n",
            "Iteration 258, loss = 0.42039615\n",
            "Iteration 259, loss = 0.42021532\n",
            "Iteration 260, loss = 0.42000484\n",
            "Iteration 261, loss = 0.41979758\n",
            "Iteration 262, loss = 0.41963551\n",
            "Iteration 263, loss = 0.41940604\n",
            "Iteration 264, loss = 0.41916177\n",
            "Iteration 265, loss = 0.41901680\n",
            "Iteration 266, loss = 0.41876736\n",
            "Iteration 267, loss = 0.41855821\n",
            "Iteration 268, loss = 0.41831742\n",
            "Iteration 269, loss = 0.41815032\n",
            "Iteration 270, loss = 0.41793767\n",
            "Iteration 271, loss = 0.41775024\n",
            "Iteration 272, loss = 0.41754602\n",
            "Iteration 273, loss = 0.41735891\n",
            "Iteration 274, loss = 0.41717138\n",
            "Iteration 275, loss = 0.41694314\n",
            "Iteration 276, loss = 0.41680026\n",
            "Iteration 277, loss = 0.41664236\n",
            "Iteration 278, loss = 0.41646633\n",
            "Iteration 279, loss = 0.41626597\n",
            "Iteration 280, loss = 0.41610787\n",
            "Iteration 281, loss = 0.41594657\n",
            "Iteration 282, loss = 0.41575978\n",
            "Iteration 283, loss = 0.41560475\n",
            "Iteration 284, loss = 0.41538711\n",
            "Iteration 285, loss = 0.41525043\n",
            "Iteration 286, loss = 0.41510309\n",
            "Iteration 287, loss = 0.41491094\n",
            "Iteration 288, loss = 0.41476673\n",
            "Iteration 289, loss = 0.41465837\n",
            "Iteration 290, loss = 0.41446359\n",
            "Iteration 291, loss = 0.41431267\n",
            "Iteration 292, loss = 0.41413381\n",
            "Iteration 293, loss = 0.41396419\n",
            "Iteration 294, loss = 0.41381397\n",
            "Iteration 295, loss = 0.41364623\n",
            "Iteration 296, loss = 0.41358042\n",
            "Iteration 297, loss = 0.41353616\n",
            "Iteration 298, loss = 0.41325662\n",
            "Iteration 299, loss = 0.41313391\n",
            "Iteration 300, loss = 0.41290739\n",
            "Iteration 301, loss = 0.41282302\n",
            "Iteration 302, loss = 0.41270214\n",
            "Iteration 303, loss = 0.41253542\n",
            "Iteration 304, loss = 0.41240867\n",
            "Iteration 305, loss = 0.41232926\n",
            "Iteration 306, loss = 0.41209877\n",
            "Iteration 307, loss = 0.41194294\n",
            "Iteration 308, loss = 0.41192759\n",
            "Iteration 309, loss = 0.41171271\n",
            "Iteration 310, loss = 0.41158437\n",
            "Iteration 311, loss = 0.41139425\n",
            "Iteration 312, loss = 0.41132494\n",
            "Iteration 313, loss = 0.41125525\n",
            "Iteration 314, loss = 0.41107694\n",
            "Iteration 315, loss = 0.41106758\n",
            "Iteration 316, loss = 0.41074597\n",
            "Iteration 317, loss = 0.41061886\n",
            "Iteration 318, loss = 0.41048095\n",
            "Iteration 319, loss = 0.41037330\n",
            "Iteration 320, loss = 0.41031205\n",
            "Iteration 321, loss = 0.41017190\n",
            "Iteration 322, loss = 0.40993396\n",
            "Iteration 323, loss = 0.40984838\n",
            "Iteration 324, loss = 0.40971335\n",
            "Iteration 325, loss = 0.40968346\n",
            "Iteration 326, loss = 0.40956834\n",
            "Iteration 327, loss = 0.40943276\n",
            "Iteration 328, loss = 0.40942716\n",
            "Iteration 329, loss = 0.40911476\n",
            "Iteration 330, loss = 0.40901945\n",
            "Iteration 331, loss = 0.40889569\n",
            "Iteration 332, loss = 0.40873975\n",
            "Iteration 333, loss = 0.40870143\n",
            "Iteration 334, loss = 0.40846151\n",
            "Iteration 335, loss = 0.40837460\n",
            "Iteration 336, loss = 0.40827823\n",
            "Iteration 337, loss = 0.40818639\n",
            "Iteration 338, loss = 0.40798705\n",
            "Iteration 339, loss = 0.40780736\n",
            "Iteration 340, loss = 0.40774301\n",
            "Iteration 341, loss = 0.40765290\n",
            "Iteration 342, loss = 0.40756705\n",
            "Iteration 343, loss = 0.40744062\n",
            "Iteration 344, loss = 0.40727715\n",
            "Iteration 345, loss = 0.40719566\n",
            "Iteration 346, loss = 0.40711070\n",
            "Iteration 347, loss = 0.40699878\n",
            "Iteration 348, loss = 0.40688901\n",
            "Iteration 349, loss = 0.40675858\n",
            "Iteration 350, loss = 0.40668775\n",
            "Iteration 351, loss = 0.40655117\n",
            "Iteration 352, loss = 0.40651317\n",
            "Iteration 353, loss = 0.40643438\n",
            "Iteration 354, loss = 0.40622701\n",
            "Iteration 355, loss = 0.40608022\n",
            "Iteration 356, loss = 0.40601184\n",
            "Iteration 357, loss = 0.40600696\n",
            "Iteration 358, loss = 0.40590400\n",
            "Iteration 359, loss = 0.40581604\n",
            "Iteration 360, loss = 0.40568522\n",
            "Iteration 361, loss = 0.40564625\n",
            "Iteration 362, loss = 0.40548136\n",
            "Iteration 363, loss = 0.40535399\n",
            "Iteration 364, loss = 0.40534609\n",
            "Iteration 365, loss = 0.40520058\n",
            "Iteration 366, loss = 0.40508438\n",
            "Iteration 367, loss = 0.40506226\n",
            "Iteration 368, loss = 0.40488399\n",
            "Iteration 369, loss = 0.40476269\n",
            "Iteration 370, loss = 0.40470469\n",
            "Iteration 371, loss = 0.40465098\n",
            "Iteration 372, loss = 0.40464805\n",
            "Iteration 373, loss = 0.40455654\n",
            "Iteration 374, loss = 0.40445022\n",
            "Iteration 375, loss = 0.40437988\n",
            "Iteration 376, loss = 0.40431451\n",
            "Iteration 377, loss = 0.40419431\n",
            "Iteration 378, loss = 0.40412001\n",
            "Iteration 379, loss = 0.40404624\n",
            "Iteration 380, loss = 0.40393918\n",
            "Iteration 381, loss = 0.40387939\n",
            "Iteration 382, loss = 0.40378678\n",
            "Iteration 383, loss = 0.40363953\n",
            "Iteration 384, loss = 0.40359574\n",
            "Iteration 385, loss = 0.40351739\n",
            "Iteration 386, loss = 0.40344323\n",
            "Iteration 387, loss = 0.40335580\n",
            "Iteration 388, loss = 0.40325870\n",
            "Iteration 389, loss = 0.40316790\n",
            "Iteration 390, loss = 0.40313955\n",
            "Iteration 391, loss = 0.40302652\n",
            "Iteration 392, loss = 0.40293764\n",
            "Iteration 393, loss = 0.40288912\n",
            "Iteration 394, loss = 0.40288461\n",
            "Iteration 395, loss = 0.40269614\n",
            "Iteration 396, loss = 0.40263240\n",
            "Iteration 397, loss = 0.40260229\n",
            "Iteration 398, loss = 0.40255961\n",
            "Iteration 399, loss = 0.40243958\n",
            "Iteration 400, loss = 0.40238715\n",
            "Iteration 401, loss = 0.40239412\n",
            "Iteration 402, loss = 0.40230977\n",
            "Iteration 403, loss = 0.40223002\n",
            "Iteration 404, loss = 0.40217117\n",
            "Iteration 405, loss = 0.40210183\n",
            "Iteration 406, loss = 0.40191910\n",
            "Iteration 407, loss = 0.40186733\n",
            "Iteration 408, loss = 0.40183136\n",
            "Iteration 409, loss = 0.40173680\n",
            "Iteration 410, loss = 0.40166805\n",
            "Iteration 411, loss = 0.40158359\n",
            "Iteration 412, loss = 0.40150612\n",
            "Iteration 413, loss = 0.40147044\n",
            "Iteration 414, loss = 0.40156037\n",
            "Iteration 415, loss = 0.40131271\n",
            "Iteration 416, loss = 0.40126074\n",
            "Iteration 417, loss = 0.40118060\n",
            "Iteration 418, loss = 0.40106928\n",
            "Iteration 419, loss = 0.40103549\n",
            "Iteration 420, loss = 0.40100653\n",
            "Iteration 421, loss = 0.40102763\n",
            "Iteration 422, loss = 0.40106623\n",
            "Iteration 423, loss = 0.40098325\n",
            "Iteration 424, loss = 0.40082153\n",
            "Iteration 425, loss = 0.40069323\n",
            "Iteration 426, loss = 0.40059310\n",
            "Iteration 427, loss = 0.40064890\n",
            "Iteration 428, loss = 0.40062791\n",
            "Iteration 429, loss = 0.40065130\n",
            "Iteration 430, loss = 0.40053803\n",
            "Iteration 431, loss = 0.40055254\n",
            "Iteration 432, loss = 0.40053403\n",
            "Iteration 433, loss = 0.40038902\n",
            "Iteration 434, loss = 0.40029342\n",
            "Iteration 435, loss = 0.40025039\n",
            "Iteration 436, loss = 0.40018459\n",
            "Iteration 437, loss = 0.40009052\n",
            "Iteration 438, loss = 0.40005158\n",
            "Iteration 439, loss = 0.40002176\n",
            "Iteration 440, loss = 0.39994832\n",
            "Iteration 441, loss = 0.39991782\n",
            "Iteration 442, loss = 0.39997097\n",
            "Iteration 443, loss = 0.39976225\n",
            "Iteration 444, loss = 0.39972773\n",
            "Iteration 445, loss = 0.39966102\n",
            "Iteration 446, loss = 0.39962672\n",
            "Iteration 447, loss = 0.39959259\n",
            "Iteration 448, loss = 0.39952949\n",
            "Iteration 449, loss = 0.39941549\n",
            "Iteration 450, loss = 0.39936986\n",
            "Iteration 451, loss = 0.39933810\n",
            "Iteration 452, loss = 0.39932824\n",
            "Iteration 453, loss = 0.39922480\n",
            "Iteration 454, loss = 0.39920290\n",
            "Iteration 455, loss = 0.39920155\n",
            "Iteration 456, loss = 0.39921236\n",
            "Iteration 457, loss = 0.39917728\n",
            "Iteration 458, loss = 0.39909360\n",
            "Iteration 459, loss = 0.39890166\n",
            "Iteration 460, loss = 0.39882365\n",
            "Iteration 461, loss = 0.39878768\n",
            "Iteration 462, loss = 0.39867697\n",
            "Iteration 463, loss = 0.39863908\n",
            "Iteration 464, loss = 0.39858432\n",
            "Iteration 465, loss = 0.39859267\n",
            "Iteration 466, loss = 0.39847321\n",
            "Iteration 467, loss = 0.39842082\n",
            "Iteration 468, loss = 0.39838055\n",
            "Iteration 469, loss = 0.39827541\n",
            "Iteration 470, loss = 0.39818986\n",
            "Iteration 471, loss = 0.39822217\n",
            "Iteration 472, loss = 0.39821616\n",
            "Iteration 473, loss = 0.39827163\n",
            "Iteration 474, loss = 0.39809763\n",
            "Iteration 475, loss = 0.39799732\n",
            "Iteration 476, loss = 0.39777971\n",
            "Iteration 477, loss = 0.39784408\n",
            "Iteration 478, loss = 0.39775735\n",
            "Iteration 479, loss = 0.39773002\n",
            "Iteration 480, loss = 0.39764125\n",
            "Iteration 481, loss = 0.39760438\n",
            "Iteration 482, loss = 0.39743445\n",
            "Iteration 483, loss = 0.39736214\n",
            "Iteration 484, loss = 0.39732989\n",
            "Iteration 485, loss = 0.39725495\n",
            "Iteration 486, loss = 0.39721162\n",
            "Iteration 487, loss = 0.39714955\n",
            "Iteration 488, loss = 0.39707096\n",
            "Iteration 489, loss = 0.39703796\n",
            "Iteration 490, loss = 0.39694896\n",
            "Iteration 491, loss = 0.39701767\n",
            "Iteration 492, loss = 0.39685279\n",
            "Iteration 493, loss = 0.39683841\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.81043191\n",
            "Iteration 2, loss = 0.80787277\n",
            "Iteration 3, loss = 0.80511892\n",
            "Iteration 4, loss = 0.80252813\n",
            "Iteration 5, loss = 0.79974791\n",
            "Iteration 6, loss = 0.79711421\n",
            "Iteration 7, loss = 0.79443495\n",
            "Iteration 8, loss = 0.79179386\n",
            "Iteration 9, loss = 0.78891020\n",
            "Iteration 10, loss = 0.78632165\n",
            "Iteration 11, loss = 0.78350817\n",
            "Iteration 12, loss = 0.78079756\n",
            "Iteration 13, loss = 0.77802178\n",
            "Iteration 14, loss = 0.77518948\n",
            "Iteration 15, loss = 0.77234240\n",
            "Iteration 16, loss = 0.76962204\n",
            "Iteration 17, loss = 0.76673772\n",
            "Iteration 18, loss = 0.76381586\n",
            "Iteration 19, loss = 0.76090142\n",
            "Iteration 20, loss = 0.75792286\n",
            "Iteration 21, loss = 0.75513457\n",
            "Iteration 22, loss = 0.75204178\n",
            "Iteration 23, loss = 0.74888112\n",
            "Iteration 24, loss = 0.74576016\n",
            "Iteration 25, loss = 0.74263343\n",
            "Iteration 26, loss = 0.73952575\n",
            "Iteration 27, loss = 0.73635763\n",
            "Iteration 28, loss = 0.73284949\n",
            "Iteration 29, loss = 0.72981600\n",
            "Iteration 30, loss = 0.72631294\n",
            "Iteration 31, loss = 0.72282656\n",
            "Iteration 32, loss = 0.71953125\n",
            "Iteration 33, loss = 0.71604566\n",
            "Iteration 34, loss = 0.71251912\n",
            "Iteration 35, loss = 0.70895186\n",
            "Iteration 36, loss = 0.70531938\n",
            "Iteration 37, loss = 0.70185370\n",
            "Iteration 38, loss = 0.69818340\n",
            "Iteration 39, loss = 0.69459111\n",
            "Iteration 40, loss = 0.69082376\n",
            "Iteration 41, loss = 0.68717271\n",
            "Iteration 42, loss = 0.68330349\n",
            "Iteration 43, loss = 0.67967989\n",
            "Iteration 44, loss = 0.67588142\n",
            "Iteration 45, loss = 0.67224894\n",
            "Iteration 46, loss = 0.66833943\n",
            "Iteration 47, loss = 0.66475853\n",
            "Iteration 48, loss = 0.66119204\n",
            "Iteration 49, loss = 0.65748737\n",
            "Iteration 50, loss = 0.65406343\n",
            "Iteration 51, loss = 0.65038126\n",
            "Iteration 52, loss = 0.64688956\n",
            "Iteration 53, loss = 0.64331756\n",
            "Iteration 54, loss = 0.64004011\n",
            "Iteration 55, loss = 0.63659247\n",
            "Iteration 56, loss = 0.63314708\n",
            "Iteration 57, loss = 0.62965243\n",
            "Iteration 58, loss = 0.62635934\n",
            "Iteration 59, loss = 0.62305199\n",
            "Iteration 60, loss = 0.61970027\n",
            "Iteration 61, loss = 0.61620822\n",
            "Iteration 62, loss = 0.61287804\n",
            "Iteration 63, loss = 0.60948062\n",
            "Iteration 64, loss = 0.60605617\n",
            "Iteration 65, loss = 0.60268698\n",
            "Iteration 66, loss = 0.59940413\n",
            "Iteration 67, loss = 0.59611577\n",
            "Iteration 68, loss = 0.59265074\n",
            "Iteration 69, loss = 0.58953658\n",
            "Iteration 70, loss = 0.58633475\n",
            "Iteration 71, loss = 0.58367019\n",
            "Iteration 72, loss = 0.58103471\n",
            "Iteration 73, loss = 0.57838731\n",
            "Iteration 74, loss = 0.57595472\n",
            "Iteration 75, loss = 0.57343942\n",
            "Iteration 76, loss = 0.57092283\n",
            "Iteration 77, loss = 0.56852531\n",
            "Iteration 78, loss = 0.56629552\n",
            "Iteration 79, loss = 0.56391499\n",
            "Iteration 80, loss = 0.56188506\n",
            "Iteration 81, loss = 0.55968097\n",
            "Iteration 82, loss = 0.55766107\n",
            "Iteration 83, loss = 0.55560312\n",
            "Iteration 84, loss = 0.55351728\n",
            "Iteration 85, loss = 0.55160898\n",
            "Iteration 86, loss = 0.54964306\n",
            "Iteration 87, loss = 0.54762869\n",
            "Iteration 88, loss = 0.54565377\n",
            "Iteration 89, loss = 0.54359649\n",
            "Iteration 90, loss = 0.54160050\n",
            "Iteration 91, loss = 0.53962496\n",
            "Iteration 92, loss = 0.53758546\n",
            "Iteration 93, loss = 0.53573139\n",
            "Iteration 94, loss = 0.53384449\n",
            "Iteration 95, loss = 0.53199818\n",
            "Iteration 96, loss = 0.53024855\n",
            "Iteration 97, loss = 0.52842502\n",
            "Iteration 98, loss = 0.52665881\n",
            "Iteration 99, loss = 0.52497609\n",
            "Iteration 100, loss = 0.52328491\n",
            "Iteration 101, loss = 0.52161628\n",
            "Iteration 102, loss = 0.51998903\n",
            "Iteration 103, loss = 0.51836226\n",
            "Iteration 104, loss = 0.51676945\n",
            "Iteration 105, loss = 0.51526196\n",
            "Iteration 106, loss = 0.51377042\n",
            "Iteration 107, loss = 0.51223689\n",
            "Iteration 108, loss = 0.51072508\n",
            "Iteration 109, loss = 0.50930201\n",
            "Iteration 110, loss = 0.50789933\n",
            "Iteration 111, loss = 0.50651816\n",
            "Iteration 112, loss = 0.50525610\n",
            "Iteration 113, loss = 0.50385818\n",
            "Iteration 114, loss = 0.50257825\n",
            "Iteration 115, loss = 0.50123626\n",
            "Iteration 116, loss = 0.49994543\n",
            "Iteration 117, loss = 0.49871896\n",
            "Iteration 118, loss = 0.49750495\n",
            "Iteration 119, loss = 0.49621771\n",
            "Iteration 120, loss = 0.49510673\n",
            "Iteration 121, loss = 0.49383240\n",
            "Iteration 122, loss = 0.49261372\n",
            "Iteration 123, loss = 0.49144665\n",
            "Iteration 124, loss = 0.49032153\n",
            "Iteration 125, loss = 0.48931298\n",
            "Iteration 126, loss = 0.48808304\n",
            "Iteration 127, loss = 0.48704893\n",
            "Iteration 128, loss = 0.48604030\n",
            "Iteration 129, loss = 0.48503582\n",
            "Iteration 130, loss = 0.48406184\n",
            "Iteration 131, loss = 0.48304539\n",
            "Iteration 132, loss = 0.48211743\n",
            "Iteration 133, loss = 0.48111320\n",
            "Iteration 134, loss = 0.48017091\n",
            "Iteration 135, loss = 0.47931840\n",
            "Iteration 136, loss = 0.47827894\n",
            "Iteration 137, loss = 0.47743021\n",
            "Iteration 138, loss = 0.47648766\n",
            "Iteration 139, loss = 0.47570558\n",
            "Iteration 140, loss = 0.47484432\n",
            "Iteration 141, loss = 0.47391258\n",
            "Iteration 142, loss = 0.47316071\n",
            "Iteration 143, loss = 0.47235710\n",
            "Iteration 144, loss = 0.47163405\n",
            "Iteration 145, loss = 0.47085143\n",
            "Iteration 146, loss = 0.47006516\n",
            "Iteration 147, loss = 0.46935854\n",
            "Iteration 148, loss = 0.46856773\n",
            "Iteration 149, loss = 0.46791903\n",
            "Iteration 150, loss = 0.46717419\n",
            "Iteration 151, loss = 0.46643910\n",
            "Iteration 152, loss = 0.46571019\n",
            "Iteration 153, loss = 0.46502301\n",
            "Iteration 154, loss = 0.46437711\n",
            "Iteration 155, loss = 0.46363820\n",
            "Iteration 156, loss = 0.46300452\n",
            "Iteration 157, loss = 0.46231853\n",
            "Iteration 158, loss = 0.46163847\n",
            "Iteration 159, loss = 0.46093719\n",
            "Iteration 160, loss = 0.46021733\n",
            "Iteration 161, loss = 0.45956779\n",
            "Iteration 162, loss = 0.45883748\n",
            "Iteration 163, loss = 0.45821086\n",
            "Iteration 164, loss = 0.45755887\n",
            "Iteration 165, loss = 0.45685724\n",
            "Iteration 166, loss = 0.45625562\n",
            "Iteration 167, loss = 0.45560923\n",
            "Iteration 168, loss = 0.45501369\n",
            "Iteration 169, loss = 0.45436872\n",
            "Iteration 170, loss = 0.45376377\n",
            "Iteration 171, loss = 0.45318008\n",
            "Iteration 172, loss = 0.45260786\n",
            "Iteration 173, loss = 0.45194638\n",
            "Iteration 174, loss = 0.45130341\n",
            "Iteration 175, loss = 0.45071034\n",
            "Iteration 176, loss = 0.45004452\n",
            "Iteration 177, loss = 0.44939236\n",
            "Iteration 178, loss = 0.44877225\n",
            "Iteration 179, loss = 0.44817150\n",
            "Iteration 180, loss = 0.44755677\n",
            "Iteration 181, loss = 0.44692692\n",
            "Iteration 182, loss = 0.44635485\n",
            "Iteration 183, loss = 0.44573298\n",
            "Iteration 184, loss = 0.44510085\n",
            "Iteration 185, loss = 0.44454497\n",
            "Iteration 186, loss = 0.44399154\n",
            "Iteration 187, loss = 0.44344611\n",
            "Iteration 188, loss = 0.44285583\n",
            "Iteration 189, loss = 0.44233406\n",
            "Iteration 190, loss = 0.44190308\n",
            "Iteration 191, loss = 0.44143100\n",
            "Iteration 192, loss = 0.44087738\n",
            "Iteration 193, loss = 0.44038505\n",
            "Iteration 194, loss = 0.43981568\n",
            "Iteration 195, loss = 0.43936421\n",
            "Iteration 196, loss = 0.43890422\n",
            "Iteration 197, loss = 0.43848047\n",
            "Iteration 198, loss = 0.43798292\n",
            "Iteration 199, loss = 0.43753279\n",
            "Iteration 200, loss = 0.43706746\n",
            "Iteration 201, loss = 0.43665216\n",
            "Iteration 202, loss = 0.43622592\n",
            "Iteration 203, loss = 0.43580498\n",
            "Iteration 204, loss = 0.43550651\n",
            "Iteration 205, loss = 0.43502089\n",
            "Iteration 206, loss = 0.43453670\n",
            "Iteration 207, loss = 0.43413086\n",
            "Iteration 208, loss = 0.43377273\n",
            "Iteration 209, loss = 0.43335122\n",
            "Iteration 210, loss = 0.43302598\n",
            "Iteration 211, loss = 0.43265021\n",
            "Iteration 212, loss = 0.43233131\n",
            "Iteration 213, loss = 0.43207936\n",
            "Iteration 214, loss = 0.43176284\n",
            "Iteration 215, loss = 0.43130259\n",
            "Iteration 216, loss = 0.43099271\n",
            "Iteration 217, loss = 0.43064136\n",
            "Iteration 218, loss = 0.43032389\n",
            "Iteration 219, loss = 0.43007028\n",
            "Iteration 220, loss = 0.42971039\n",
            "Iteration 221, loss = 0.42935876\n",
            "Iteration 222, loss = 0.42911360\n",
            "Iteration 223, loss = 0.42882938\n",
            "Iteration 224, loss = 0.42850180\n",
            "Iteration 225, loss = 0.42823843\n",
            "Iteration 226, loss = 0.42792468\n",
            "Iteration 227, loss = 0.42762815\n",
            "Iteration 228, loss = 0.42735207\n",
            "Iteration 229, loss = 0.42710010\n",
            "Iteration 230, loss = 0.42689875\n",
            "Iteration 231, loss = 0.42657107\n",
            "Iteration 232, loss = 0.42635910\n",
            "Iteration 233, loss = 0.42612623\n",
            "Iteration 234, loss = 0.42579777\n",
            "Iteration 235, loss = 0.42550374\n",
            "Iteration 236, loss = 0.42519989\n",
            "Iteration 237, loss = 0.42503064\n",
            "Iteration 238, loss = 0.42483474\n",
            "Iteration 239, loss = 0.42460713\n",
            "Iteration 240, loss = 0.42431940\n",
            "Iteration 241, loss = 0.42401214\n",
            "Iteration 242, loss = 0.42375127\n",
            "Iteration 243, loss = 0.42350220\n",
            "Iteration 244, loss = 0.42321791\n",
            "Iteration 245, loss = 0.42306614\n",
            "Iteration 246, loss = 0.42274422\n",
            "Iteration 247, loss = 0.42255270\n",
            "Iteration 248, loss = 0.42228049\n",
            "Iteration 249, loss = 0.42210625\n",
            "Iteration 250, loss = 0.42184983\n",
            "Iteration 251, loss = 0.42160796\n",
            "Iteration 252, loss = 0.42143102\n",
            "Iteration 253, loss = 0.42103414\n",
            "Iteration 254, loss = 0.42080965\n",
            "Iteration 255, loss = 0.42054886\n",
            "Iteration 256, loss = 0.42033186\n",
            "Iteration 257, loss = 0.42007669\n",
            "Iteration 258, loss = 0.41986274\n",
            "Iteration 259, loss = 0.41966621\n",
            "Iteration 260, loss = 0.41947591\n",
            "Iteration 261, loss = 0.41925325\n",
            "Iteration 262, loss = 0.41904934\n",
            "Iteration 263, loss = 0.41880338\n",
            "Iteration 264, loss = 0.41856560\n",
            "Iteration 265, loss = 0.41837686\n",
            "Iteration 266, loss = 0.41812616\n",
            "Iteration 267, loss = 0.41790512\n",
            "Iteration 268, loss = 0.41770327\n",
            "Iteration 269, loss = 0.41749524\n",
            "Iteration 270, loss = 0.41722043\n",
            "Iteration 271, loss = 0.41700663\n",
            "Iteration 272, loss = 0.41685416\n",
            "Iteration 273, loss = 0.41671981\n",
            "Iteration 274, loss = 0.41651954\n",
            "Iteration 275, loss = 0.41627607\n",
            "Iteration 276, loss = 0.41609492\n",
            "Iteration 277, loss = 0.41589843\n",
            "Iteration 278, loss = 0.41573008\n",
            "Iteration 279, loss = 0.41549718\n",
            "Iteration 280, loss = 0.41540126\n",
            "Iteration 281, loss = 0.41520406\n",
            "Iteration 282, loss = 0.41507497\n",
            "Iteration 283, loss = 0.41487317\n",
            "Iteration 284, loss = 0.41468401\n",
            "Iteration 285, loss = 0.41448653\n",
            "Iteration 286, loss = 0.41435743\n",
            "Iteration 287, loss = 0.41416997\n",
            "Iteration 288, loss = 0.41405927\n",
            "Iteration 289, loss = 0.41383354\n",
            "Iteration 290, loss = 0.41365941\n",
            "Iteration 291, loss = 0.41351930\n",
            "Iteration 292, loss = 0.41335756\n",
            "Iteration 293, loss = 0.41317673\n",
            "Iteration 294, loss = 0.41306399\n",
            "Iteration 295, loss = 0.41286553\n",
            "Iteration 296, loss = 0.41272570\n",
            "Iteration 297, loss = 0.41263064\n",
            "Iteration 298, loss = 0.41241990\n",
            "Iteration 299, loss = 0.41228794\n",
            "Iteration 300, loss = 0.41209338\n",
            "Iteration 301, loss = 0.41196470\n",
            "Iteration 302, loss = 0.41181778\n",
            "Iteration 303, loss = 0.41164583\n",
            "Iteration 304, loss = 0.41146127\n",
            "Iteration 305, loss = 0.41146489\n",
            "Iteration 306, loss = 0.41122280\n",
            "Iteration 307, loss = 0.41106947\n",
            "Iteration 308, loss = 0.41096402\n",
            "Iteration 309, loss = 0.41080594\n",
            "Iteration 310, loss = 0.41073936\n",
            "Iteration 311, loss = 0.41050325\n",
            "Iteration 312, loss = 0.41046367\n",
            "Iteration 313, loss = 0.41033773\n",
            "Iteration 314, loss = 0.41018382\n",
            "Iteration 315, loss = 0.41011753\n",
            "Iteration 316, loss = 0.40989770\n",
            "Iteration 317, loss = 0.40977634\n",
            "Iteration 318, loss = 0.40964551\n",
            "Iteration 319, loss = 0.40953640\n",
            "Iteration 320, loss = 0.40945560\n",
            "Iteration 321, loss = 0.40929056\n",
            "Iteration 322, loss = 0.40914228\n",
            "Iteration 323, loss = 0.40902715\n",
            "Iteration 324, loss = 0.40888413\n",
            "Iteration 325, loss = 0.40876685\n",
            "Iteration 326, loss = 0.40868022\n",
            "Iteration 327, loss = 0.40858533\n",
            "Iteration 328, loss = 0.40852823\n",
            "Iteration 329, loss = 0.40838167\n",
            "Iteration 330, loss = 0.40822245\n",
            "Iteration 331, loss = 0.40809415\n",
            "Iteration 332, loss = 0.40797885\n",
            "Iteration 333, loss = 0.40788437\n",
            "Iteration 334, loss = 0.40775691\n",
            "Iteration 335, loss = 0.40768498\n",
            "Iteration 336, loss = 0.40752424\n",
            "Iteration 337, loss = 0.40744525\n",
            "Iteration 338, loss = 0.40726072\n",
            "Iteration 339, loss = 0.40718437\n",
            "Iteration 340, loss = 0.40710928\n",
            "Iteration 341, loss = 0.40706407\n",
            "Iteration 342, loss = 0.40695309\n",
            "Iteration 343, loss = 0.40681087\n",
            "Iteration 344, loss = 0.40667762\n",
            "Iteration 345, loss = 0.40655684\n",
            "Iteration 346, loss = 0.40640932\n",
            "Iteration 347, loss = 0.40630311\n",
            "Iteration 348, loss = 0.40623095\n",
            "Iteration 349, loss = 0.40609425\n",
            "Iteration 350, loss = 0.40603306\n",
            "Iteration 351, loss = 0.40588311\n",
            "Iteration 352, loss = 0.40586183\n",
            "Iteration 353, loss = 0.40575970\n",
            "Iteration 354, loss = 0.40559470\n",
            "Iteration 355, loss = 0.40546194\n",
            "Iteration 356, loss = 0.40538561\n",
            "Iteration 357, loss = 0.40532801\n",
            "Iteration 358, loss = 0.40523396\n",
            "Iteration 359, loss = 0.40515553\n",
            "Iteration 360, loss = 0.40503949\n",
            "Iteration 361, loss = 0.40496593\n",
            "Iteration 362, loss = 0.40484889\n",
            "Iteration 363, loss = 0.40478285\n",
            "Iteration 364, loss = 0.40464005\n",
            "Iteration 365, loss = 0.40455737\n",
            "Iteration 366, loss = 0.40445780\n",
            "Iteration 367, loss = 0.40439251\n",
            "Iteration 368, loss = 0.40426918\n",
            "Iteration 369, loss = 0.40413571\n",
            "Iteration 370, loss = 0.40401596\n",
            "Iteration 371, loss = 0.40397845\n",
            "Iteration 372, loss = 0.40399465\n",
            "Iteration 373, loss = 0.40394448\n",
            "Iteration 374, loss = 0.40386472\n",
            "Iteration 375, loss = 0.40383276\n",
            "Iteration 376, loss = 0.40368437\n",
            "Iteration 377, loss = 0.40359030\n",
            "Iteration 378, loss = 0.40348762\n",
            "Iteration 379, loss = 0.40338798\n",
            "Iteration 380, loss = 0.40325998\n",
            "Iteration 381, loss = 0.40323435\n",
            "Iteration 382, loss = 0.40309730\n",
            "Iteration 383, loss = 0.40297993\n",
            "Iteration 384, loss = 0.40294532\n",
            "Iteration 385, loss = 0.40288487\n",
            "Iteration 386, loss = 0.40279295\n",
            "Iteration 387, loss = 0.40271146\n",
            "Iteration 388, loss = 0.40258087\n",
            "Iteration 389, loss = 0.40253055\n",
            "Iteration 390, loss = 0.40253022\n",
            "Iteration 391, loss = 0.40239236\n",
            "Iteration 392, loss = 0.40228983\n",
            "Iteration 393, loss = 0.40223071\n",
            "Iteration 394, loss = 0.40215054\n",
            "Iteration 395, loss = 0.40204644\n",
            "Iteration 396, loss = 0.40196605\n",
            "Iteration 397, loss = 0.40187638\n",
            "Iteration 398, loss = 0.40182556\n",
            "Iteration 399, loss = 0.40175324\n",
            "Iteration 400, loss = 0.40171560\n",
            "Iteration 401, loss = 0.40169705\n",
            "Iteration 402, loss = 0.40159610\n",
            "Iteration 403, loss = 0.40152327\n",
            "Iteration 404, loss = 0.40140958\n",
            "Iteration 405, loss = 0.40137341\n",
            "Iteration 406, loss = 0.40123341\n",
            "Iteration 407, loss = 0.40114836\n",
            "Iteration 408, loss = 0.40123754\n",
            "Iteration 409, loss = 0.40097254\n",
            "Iteration 410, loss = 0.40089479\n",
            "Iteration 411, loss = 0.40085836\n",
            "Iteration 412, loss = 0.40076602\n",
            "Iteration 413, loss = 0.40072942\n",
            "Iteration 414, loss = 0.40075378\n",
            "Iteration 415, loss = 0.40059323\n",
            "Iteration 416, loss = 0.40045983\n",
            "Iteration 417, loss = 0.40036329\n",
            "Iteration 418, loss = 0.40027457\n",
            "Iteration 419, loss = 0.40021535\n",
            "Iteration 420, loss = 0.40017788\n",
            "Iteration 421, loss = 0.40015621\n",
            "Iteration 422, loss = 0.40004237\n",
            "Iteration 423, loss = 0.40001159\n",
            "Iteration 424, loss = 0.39979787\n",
            "Iteration 425, loss = 0.39970584\n",
            "Iteration 426, loss = 0.39963435\n",
            "Iteration 427, loss = 0.39959047\n",
            "Iteration 428, loss = 0.39953618\n",
            "Iteration 429, loss = 0.39947288\n",
            "Iteration 430, loss = 0.39940498\n",
            "Iteration 431, loss = 0.39931232\n",
            "Iteration 432, loss = 0.39936111\n",
            "Iteration 433, loss = 0.39922369\n",
            "Iteration 434, loss = 0.39915663\n",
            "Iteration 435, loss = 0.39905136\n",
            "Iteration 436, loss = 0.39896352\n",
            "Iteration 437, loss = 0.39884599\n",
            "Iteration 438, loss = 0.39877353\n",
            "Iteration 439, loss = 0.39872638\n",
            "Iteration 440, loss = 0.39863060\n",
            "Iteration 441, loss = 0.39851886\n",
            "Iteration 442, loss = 0.39859172\n",
            "Iteration 443, loss = 0.39843901\n",
            "Iteration 444, loss = 0.39839735\n",
            "Iteration 445, loss = 0.39828550\n",
            "Iteration 446, loss = 0.39822222\n",
            "Iteration 447, loss = 0.39814576\n",
            "Iteration 448, loss = 0.39803045\n",
            "Iteration 449, loss = 0.39796234\n",
            "Iteration 450, loss = 0.39795091\n",
            "Iteration 451, loss = 0.39786920\n",
            "Iteration 452, loss = 0.39775155\n",
            "Iteration 453, loss = 0.39769286\n",
            "Iteration 454, loss = 0.39769268\n",
            "Iteration 455, loss = 0.39765846\n",
            "Iteration 456, loss = 0.39766522\n",
            "Iteration 457, loss = 0.39758487\n",
            "Iteration 458, loss = 0.39742341\n",
            "Iteration 459, loss = 0.39725416\n",
            "Iteration 460, loss = 0.39721550\n",
            "Iteration 461, loss = 0.39724418\n",
            "Iteration 462, loss = 0.39715906\n",
            "Iteration 463, loss = 0.39711090\n",
            "Iteration 464, loss = 0.39699333\n",
            "Iteration 465, loss = 0.39695433\n",
            "Iteration 466, loss = 0.39688415\n",
            "Iteration 467, loss = 0.39687329\n",
            "Iteration 468, loss = 0.39682022\n",
            "Iteration 469, loss = 0.39671584\n",
            "Iteration 470, loss = 0.39670500\n",
            "Iteration 471, loss = 0.39668932\n",
            "Iteration 472, loss = 0.39660034\n",
            "Iteration 473, loss = 0.39652984\n",
            "Iteration 474, loss = 0.39641485\n",
            "Iteration 475, loss = 0.39641896\n",
            "Iteration 476, loss = 0.39634002\n",
            "Iteration 477, loss = 0.39637158\n",
            "Iteration 478, loss = 0.39623968\n",
            "Iteration 479, loss = 0.39616023\n",
            "Iteration 480, loss = 0.39609277\n",
            "Iteration 481, loss = 0.39598564\n",
            "Iteration 482, loss = 0.39590144\n",
            "Iteration 483, loss = 0.39579975\n",
            "Iteration 484, loss = 0.39581457\n",
            "Iteration 485, loss = 0.39571815\n",
            "Iteration 486, loss = 0.39570725\n",
            "Iteration 487, loss = 0.39560236\n",
            "Iteration 488, loss = 0.39552444\n",
            "Iteration 489, loss = 0.39546953\n",
            "Iteration 490, loss = 0.39539608\n",
            "Iteration 491, loss = 0.39542143\n",
            "Iteration 492, loss = 0.39526725\n",
            "Iteration 493, loss = 0.39529177\n",
            "Iteration 494, loss = 0.39512759\n",
            "Iteration 495, loss = 0.39517945\n",
            "Iteration 496, loss = 0.39514761\n",
            "Iteration 497, loss = 0.39503947\n",
            "Iteration 498, loss = 0.39489943\n",
            "Iteration 499, loss = 0.39483816\n",
            "Iteration 500, loss = 0.39475699\n",
            "Iteration 501, loss = 0.39472148\n",
            "Iteration 502, loss = 0.39474137\n",
            "Iteration 503, loss = 0.39476303\n",
            "Iteration 504, loss = 0.39467436\n",
            "Iteration 505, loss = 0.39449967\n",
            "Iteration 506, loss = 0.39443735\n",
            "Iteration 507, loss = 0.39445470\n",
            "Iteration 508, loss = 0.39439966\n",
            "Iteration 509, loss = 0.39436070\n",
            "Iteration 510, loss = 0.39429557\n",
            "Iteration 511, loss = 0.39418343\n",
            "Iteration 512, loss = 0.39421475\n",
            "Iteration 513, loss = 0.39409572\n",
            "Iteration 514, loss = 0.39405841\n",
            "Iteration 515, loss = 0.39419651\n",
            "Iteration 516, loss = 0.39407278\n",
            "Iteration 517, loss = 0.39384991\n",
            "Iteration 518, loss = 0.39382319\n",
            "Iteration 519, loss = 0.39378964\n",
            "Iteration 520, loss = 0.39384254\n",
            "Iteration 521, loss = 0.39370728\n",
            "Iteration 522, loss = 0.39359695\n",
            "Iteration 523, loss = 0.39358834\n",
            "Iteration 524, loss = 0.39350374\n",
            "Iteration 525, loss = 0.39347168\n",
            "Iteration 526, loss = 0.39343966\n",
            "Iteration 527, loss = 0.39339039\n",
            "Iteration 528, loss = 0.39333614\n",
            "Iteration 529, loss = 0.39331164\n",
            "Iteration 530, loss = 0.39326686\n",
            "Iteration 531, loss = 0.39314758\n",
            "Iteration 532, loss = 0.39305317\n",
            "Iteration 533, loss = 0.39328693\n",
            "Iteration 534, loss = 0.39319709\n",
            "Iteration 535, loss = 0.39311471\n",
            "Iteration 536, loss = 0.39295551\n",
            "Iteration 537, loss = 0.39294201\n",
            "Iteration 538, loss = 0.39286606\n",
            "Iteration 539, loss = 0.39288766\n",
            "Iteration 540, loss = 0.39290718\n",
            "Iteration 541, loss = 0.39279626\n",
            "Iteration 542, loss = 0.39270560\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.88736743\n",
            "Iteration 2, loss = 0.88235151\n",
            "Iteration 3, loss = 0.87737107\n",
            "Iteration 4, loss = 0.87262263\n",
            "Iteration 5, loss = 0.86780539\n",
            "Iteration 6, loss = 0.86316987\n",
            "Iteration 7, loss = 0.85863161\n",
            "Iteration 8, loss = 0.85433307\n",
            "Iteration 9, loss = 0.85010014\n",
            "Iteration 10, loss = 0.84585866\n",
            "Iteration 11, loss = 0.84166943\n",
            "Iteration 12, loss = 0.83750662\n",
            "Iteration 13, loss = 0.83359658\n",
            "Iteration 14, loss = 0.82977218\n",
            "Iteration 15, loss = 0.82585888\n",
            "Iteration 16, loss = 0.82181269\n",
            "Iteration 17, loss = 0.81809385\n",
            "Iteration 18, loss = 0.81420008\n",
            "Iteration 19, loss = 0.81036193\n",
            "Iteration 20, loss = 0.80657033\n",
            "Iteration 21, loss = 0.80267679\n",
            "Iteration 22, loss = 0.79903060\n",
            "Iteration 23, loss = 0.79515809\n",
            "Iteration 24, loss = 0.79114745\n",
            "Iteration 25, loss = 0.78734695\n",
            "Iteration 26, loss = 0.78370396\n",
            "Iteration 27, loss = 0.77967517\n",
            "Iteration 28, loss = 0.77586700\n",
            "Iteration 29, loss = 0.77194068\n",
            "Iteration 30, loss = 0.76805334\n",
            "Iteration 31, loss = 0.76421680\n",
            "Iteration 32, loss = 0.76033287\n",
            "Iteration 33, loss = 0.75655665\n",
            "Iteration 34, loss = 0.75270872\n",
            "Iteration 35, loss = 0.74893593\n",
            "Iteration 36, loss = 0.74517253\n",
            "Iteration 37, loss = 0.74138385\n",
            "Iteration 38, loss = 0.73770966\n",
            "Iteration 39, loss = 0.73430414\n",
            "Iteration 40, loss = 0.73067919\n",
            "Iteration 41, loss = 0.72733264\n",
            "Iteration 42, loss = 0.72390303\n",
            "Iteration 43, loss = 0.72054882\n",
            "Iteration 44, loss = 0.71743380\n",
            "Iteration 45, loss = 0.71451951\n",
            "Iteration 46, loss = 0.71136407\n",
            "Iteration 47, loss = 0.70857483\n",
            "Iteration 48, loss = 0.70564205\n",
            "Iteration 49, loss = 0.70312952\n",
            "Iteration 50, loss = 0.70039840\n",
            "Iteration 51, loss = 0.69772912\n",
            "Iteration 52, loss = 0.69534710\n",
            "Iteration 53, loss = 0.69287630\n",
            "Iteration 54, loss = 0.69064439\n",
            "Iteration 55, loss = 0.68822562\n",
            "Iteration 56, loss = 0.68604193\n",
            "Iteration 57, loss = 0.68381469\n",
            "Iteration 58, loss = 0.68163453\n",
            "Iteration 59, loss = 0.67952471\n",
            "Iteration 60, loss = 0.67728788\n",
            "Iteration 61, loss = 0.67523035\n",
            "Iteration 62, loss = 0.67315497\n",
            "Iteration 63, loss = 0.67099820\n",
            "Iteration 64, loss = 0.66902389\n",
            "Iteration 65, loss = 0.66685950\n",
            "Iteration 66, loss = 0.66472340\n",
            "Iteration 67, loss = 0.66265728\n",
            "Iteration 68, loss = 0.66051258\n",
            "Iteration 69, loss = 0.65829251\n",
            "Iteration 70, loss = 0.65620183\n",
            "Iteration 71, loss = 0.65406169\n",
            "Iteration 72, loss = 0.65193517\n",
            "Iteration 73, loss = 0.64970122\n",
            "Iteration 74, loss = 0.64748963\n",
            "Iteration 75, loss = 0.64541345\n",
            "Iteration 76, loss = 0.64315211\n",
            "Iteration 77, loss = 0.64106225\n",
            "Iteration 78, loss = 0.63879388\n",
            "Iteration 79, loss = 0.63660583\n",
            "Iteration 80, loss = 0.63448996\n",
            "Iteration 81, loss = 0.63226256\n",
            "Iteration 82, loss = 0.63015951\n",
            "Iteration 83, loss = 0.62791230\n",
            "Iteration 84, loss = 0.62573140\n",
            "Iteration 85, loss = 0.62348018\n",
            "Iteration 86, loss = 0.62122615\n",
            "Iteration 87, loss = 0.61907918\n",
            "Iteration 88, loss = 0.61691279\n",
            "Iteration 89, loss = 0.61463324\n",
            "Iteration 90, loss = 0.61250685\n",
            "Iteration 91, loss = 0.61023572\n",
            "Iteration 92, loss = 0.60793514\n",
            "Iteration 93, loss = 0.60574367\n",
            "Iteration 94, loss = 0.60339491\n",
            "Iteration 95, loss = 0.60109649\n",
            "Iteration 96, loss = 0.59882432\n",
            "Iteration 97, loss = 0.59653688\n",
            "Iteration 98, loss = 0.59415471\n",
            "Iteration 99, loss = 0.59189558\n",
            "Iteration 100, loss = 0.58953310\n",
            "Iteration 101, loss = 0.58731663\n",
            "Iteration 102, loss = 0.58485993\n",
            "Iteration 103, loss = 0.58248609\n",
            "Iteration 104, loss = 0.58014631\n",
            "Iteration 105, loss = 0.57777666\n",
            "Iteration 106, loss = 0.57541156\n",
            "Iteration 107, loss = 0.57318585\n",
            "Iteration 108, loss = 0.57080097\n",
            "Iteration 109, loss = 0.56844951\n",
            "Iteration 110, loss = 0.56611438\n",
            "Iteration 111, loss = 0.56371231\n",
            "Iteration 112, loss = 0.56139332\n",
            "Iteration 113, loss = 0.55904167\n",
            "Iteration 114, loss = 0.55679914\n",
            "Iteration 115, loss = 0.55438792\n",
            "Iteration 116, loss = 0.55211020\n",
            "Iteration 117, loss = 0.54974357\n",
            "Iteration 118, loss = 0.54749563\n",
            "Iteration 119, loss = 0.54516930\n",
            "Iteration 120, loss = 0.54284215\n",
            "Iteration 121, loss = 0.54066200\n",
            "Iteration 122, loss = 0.53835628\n",
            "Iteration 123, loss = 0.53597859\n",
            "Iteration 124, loss = 0.53374317\n",
            "Iteration 125, loss = 0.53144677\n",
            "Iteration 126, loss = 0.52913799\n",
            "Iteration 127, loss = 0.52691342\n",
            "Iteration 128, loss = 0.52469992\n",
            "Iteration 129, loss = 0.52249909\n",
            "Iteration 130, loss = 0.52034188\n",
            "Iteration 131, loss = 0.51806544\n",
            "Iteration 132, loss = 0.51586126\n",
            "Iteration 133, loss = 0.51380687\n",
            "Iteration 134, loss = 0.51167285\n",
            "Iteration 135, loss = 0.50962701\n",
            "Iteration 136, loss = 0.50762594\n",
            "Iteration 137, loss = 0.50555416\n",
            "Iteration 138, loss = 0.50354872\n",
            "Iteration 139, loss = 0.50167321\n",
            "Iteration 140, loss = 0.49966996\n",
            "Iteration 141, loss = 0.49779591\n",
            "Iteration 142, loss = 0.49591783\n",
            "Iteration 143, loss = 0.49412814\n",
            "Iteration 144, loss = 0.49232142\n",
            "Iteration 145, loss = 0.49055445\n",
            "Iteration 146, loss = 0.48888060\n",
            "Iteration 147, loss = 0.48711258\n",
            "Iteration 148, loss = 0.48555211\n",
            "Iteration 149, loss = 0.48392715\n",
            "Iteration 150, loss = 0.48238371\n",
            "Iteration 151, loss = 0.48082929\n",
            "Iteration 152, loss = 0.47929527\n",
            "Iteration 153, loss = 0.47795850\n",
            "Iteration 154, loss = 0.47637744\n",
            "Iteration 155, loss = 0.47495540\n",
            "Iteration 156, loss = 0.47366131\n",
            "Iteration 157, loss = 0.47234153\n",
            "Iteration 158, loss = 0.47102631\n",
            "Iteration 159, loss = 0.46986568\n",
            "Iteration 160, loss = 0.46850652\n",
            "Iteration 161, loss = 0.46739906\n",
            "Iteration 162, loss = 0.46617411\n",
            "Iteration 163, loss = 0.46504773\n",
            "Iteration 164, loss = 0.46400598\n",
            "Iteration 165, loss = 0.46286254\n",
            "Iteration 166, loss = 0.46190615\n",
            "Iteration 167, loss = 0.46086102\n",
            "Iteration 168, loss = 0.45984918\n",
            "Iteration 169, loss = 0.45892022\n",
            "Iteration 170, loss = 0.45795134\n",
            "Iteration 171, loss = 0.45701732\n",
            "Iteration 172, loss = 0.45609618\n",
            "Iteration 173, loss = 0.45523878\n",
            "Iteration 174, loss = 0.45443736\n",
            "Iteration 175, loss = 0.45367201\n",
            "Iteration 176, loss = 0.45282068\n",
            "Iteration 177, loss = 0.45215267\n",
            "Iteration 178, loss = 0.45129668\n",
            "Iteration 179, loss = 0.45056037\n",
            "Iteration 180, loss = 0.44989207\n",
            "Iteration 181, loss = 0.44919334\n",
            "Iteration 182, loss = 0.44848540\n",
            "Iteration 183, loss = 0.44787490\n",
            "Iteration 184, loss = 0.44727313\n",
            "Iteration 185, loss = 0.44662589\n",
            "Iteration 186, loss = 0.44600036\n",
            "Iteration 187, loss = 0.44546576\n",
            "Iteration 188, loss = 0.44486291\n",
            "Iteration 189, loss = 0.44430205\n",
            "Iteration 190, loss = 0.44380415\n",
            "Iteration 191, loss = 0.44325749\n",
            "Iteration 192, loss = 0.44274298\n",
            "Iteration 193, loss = 0.44222605\n",
            "Iteration 194, loss = 0.44177090\n",
            "Iteration 195, loss = 0.44138606\n",
            "Iteration 196, loss = 0.44080489\n",
            "Iteration 197, loss = 0.44036086\n",
            "Iteration 198, loss = 0.44002866\n",
            "Iteration 199, loss = 0.43957342\n",
            "Iteration 200, loss = 0.43911181\n",
            "Iteration 201, loss = 0.43866524\n",
            "Iteration 202, loss = 0.43830058\n",
            "Iteration 203, loss = 0.43788268\n",
            "Iteration 204, loss = 0.43746851\n",
            "Iteration 205, loss = 0.43715143\n",
            "Iteration 206, loss = 0.43672769\n",
            "Iteration 207, loss = 0.43634208\n",
            "Iteration 208, loss = 0.43602588\n",
            "Iteration 209, loss = 0.43563965\n",
            "Iteration 210, loss = 0.43530936\n",
            "Iteration 211, loss = 0.43490232\n",
            "Iteration 212, loss = 0.43459637\n",
            "Iteration 213, loss = 0.43424552\n",
            "Iteration 214, loss = 0.43394281\n",
            "Iteration 215, loss = 0.43353629\n",
            "Iteration 216, loss = 0.43327247\n",
            "Iteration 217, loss = 0.43292309\n",
            "Iteration 218, loss = 0.43258651\n",
            "Iteration 219, loss = 0.43235286\n",
            "Iteration 220, loss = 0.43193249\n",
            "Iteration 221, loss = 0.43164342\n",
            "Iteration 222, loss = 0.43130925\n",
            "Iteration 223, loss = 0.43106967\n",
            "Iteration 224, loss = 0.43074272\n",
            "Iteration 225, loss = 0.43040409\n",
            "Iteration 226, loss = 0.43011081\n",
            "Iteration 227, loss = 0.42987218\n",
            "Iteration 228, loss = 0.42953521\n",
            "Iteration 229, loss = 0.42931606\n",
            "Iteration 230, loss = 0.42892780\n",
            "Iteration 231, loss = 0.42866369\n",
            "Iteration 232, loss = 0.42834392\n",
            "Iteration 233, loss = 0.42803916\n",
            "Iteration 234, loss = 0.42777740\n",
            "Iteration 235, loss = 0.42746116\n",
            "Iteration 236, loss = 0.42718029\n",
            "Iteration 237, loss = 0.42691856\n",
            "Iteration 238, loss = 0.42661676\n",
            "Iteration 239, loss = 0.42644427\n",
            "Iteration 240, loss = 0.42608574\n",
            "Iteration 241, loss = 0.42570148\n",
            "Iteration 242, loss = 0.42539593\n",
            "Iteration 243, loss = 0.42510304\n",
            "Iteration 244, loss = 0.42485064\n",
            "Iteration 245, loss = 0.42459929\n",
            "Iteration 246, loss = 0.42425051\n",
            "Iteration 247, loss = 0.42390450\n",
            "Iteration 248, loss = 0.42364818\n",
            "Iteration 249, loss = 0.42337942\n",
            "Iteration 250, loss = 0.42301973\n",
            "Iteration 251, loss = 0.42278555\n",
            "Iteration 252, loss = 0.42245754\n",
            "Iteration 253, loss = 0.42214886\n",
            "Iteration 254, loss = 0.42182942\n",
            "Iteration 255, loss = 0.42152511\n",
            "Iteration 256, loss = 0.42124492\n",
            "Iteration 257, loss = 0.42092589\n",
            "Iteration 258, loss = 0.42066462\n",
            "Iteration 259, loss = 0.42034120\n",
            "Iteration 260, loss = 0.42009489\n",
            "Iteration 261, loss = 0.41976059\n",
            "Iteration 262, loss = 0.41950314\n",
            "Iteration 263, loss = 0.41915774\n",
            "Iteration 264, loss = 0.41883818\n",
            "Iteration 265, loss = 0.41861300\n",
            "Iteration 266, loss = 0.41825887\n",
            "Iteration 267, loss = 0.41801747\n",
            "Iteration 268, loss = 0.41773162\n",
            "Iteration 269, loss = 0.41738582\n",
            "Iteration 270, loss = 0.41714401\n",
            "Iteration 271, loss = 0.41676500\n",
            "Iteration 272, loss = 0.41670662\n",
            "Iteration 273, loss = 0.41621524\n",
            "Iteration 274, loss = 0.41590888\n",
            "Iteration 275, loss = 0.41565753\n",
            "Iteration 276, loss = 0.41528597\n",
            "Iteration 277, loss = 0.41505093\n",
            "Iteration 278, loss = 0.41478643\n",
            "Iteration 279, loss = 0.41447702\n",
            "Iteration 280, loss = 0.41420943\n",
            "Iteration 281, loss = 0.41392067\n",
            "Iteration 282, loss = 0.41361741\n",
            "Iteration 283, loss = 0.41337568\n",
            "Iteration 284, loss = 0.41312274\n",
            "Iteration 285, loss = 0.41286421\n",
            "Iteration 286, loss = 0.41255153\n",
            "Iteration 287, loss = 0.41229218\n",
            "Iteration 288, loss = 0.41195582\n",
            "Iteration 289, loss = 0.41181468\n",
            "Iteration 290, loss = 0.41143512\n",
            "Iteration 291, loss = 0.41125030\n",
            "Iteration 292, loss = 0.41093436\n",
            "Iteration 293, loss = 0.41063515\n",
            "Iteration 294, loss = 0.41037535\n",
            "Iteration 295, loss = 0.41017100\n",
            "Iteration 296, loss = 0.40983612\n",
            "Iteration 297, loss = 0.40966268\n",
            "Iteration 298, loss = 0.40934857\n",
            "Iteration 299, loss = 0.40917310\n",
            "Iteration 300, loss = 0.40880222\n",
            "Iteration 301, loss = 0.40852110\n",
            "Iteration 302, loss = 0.40824827\n",
            "Iteration 303, loss = 0.40804868\n",
            "Iteration 304, loss = 0.40775217\n",
            "Iteration 305, loss = 0.40749645\n",
            "Iteration 306, loss = 0.40727134\n",
            "Iteration 307, loss = 0.40701992\n",
            "Iteration 308, loss = 0.40676258\n",
            "Iteration 309, loss = 0.40651520\n",
            "Iteration 310, loss = 0.40631161\n",
            "Iteration 311, loss = 0.40603625\n",
            "Iteration 312, loss = 0.40581487\n",
            "Iteration 313, loss = 0.40554160\n",
            "Iteration 314, loss = 0.40530041\n",
            "Iteration 315, loss = 0.40505698\n",
            "Iteration 316, loss = 0.40483083\n",
            "Iteration 317, loss = 0.40466115\n",
            "Iteration 318, loss = 0.40439413\n",
            "Iteration 319, loss = 0.40422276\n",
            "Iteration 320, loss = 0.40388969\n",
            "Iteration 321, loss = 0.40366511\n",
            "Iteration 322, loss = 0.40344139\n",
            "Iteration 323, loss = 0.40319692\n",
            "Iteration 324, loss = 0.40297627\n",
            "Iteration 325, loss = 0.40283221\n",
            "Iteration 326, loss = 0.40255282\n",
            "Iteration 327, loss = 0.40227889\n",
            "Iteration 328, loss = 0.40212229\n",
            "Iteration 329, loss = 0.40191726\n",
            "Iteration 330, loss = 0.40169621\n",
            "Iteration 331, loss = 0.40150463\n",
            "Iteration 332, loss = 0.40134001\n",
            "Iteration 333, loss = 0.40101174\n",
            "Iteration 334, loss = 0.40091229\n",
            "Iteration 335, loss = 0.40070174\n",
            "Iteration 336, loss = 0.40046962\n",
            "Iteration 337, loss = 0.40016768\n",
            "Iteration 338, loss = 0.39998494\n",
            "Iteration 339, loss = 0.39987204\n",
            "Iteration 340, loss = 0.39952935\n",
            "Iteration 341, loss = 0.39936852\n",
            "Iteration 342, loss = 0.39913556\n",
            "Iteration 343, loss = 0.39893025\n",
            "Iteration 344, loss = 0.39879437\n",
            "Iteration 345, loss = 0.39858822\n",
            "Iteration 346, loss = 0.39843192\n",
            "Iteration 347, loss = 0.39821120\n",
            "Iteration 348, loss = 0.39795986\n",
            "Iteration 349, loss = 0.39784549\n",
            "Iteration 350, loss = 0.39758949\n",
            "Iteration 351, loss = 0.39742489\n",
            "Iteration 352, loss = 0.39727142\n",
            "Iteration 353, loss = 0.39707299\n",
            "Iteration 354, loss = 0.39689868\n",
            "Iteration 355, loss = 0.39670185\n",
            "Iteration 356, loss = 0.39650523\n",
            "Iteration 357, loss = 0.39631080\n",
            "Iteration 358, loss = 0.39613924\n",
            "Iteration 359, loss = 0.39604892\n",
            "Iteration 360, loss = 0.39590894\n",
            "Iteration 361, loss = 0.39559833\n",
            "Iteration 362, loss = 0.39542003\n",
            "Iteration 363, loss = 0.39538867\n",
            "Iteration 364, loss = 0.39510309\n",
            "Iteration 365, loss = 0.39491368\n",
            "Iteration 366, loss = 0.39493357\n",
            "Iteration 367, loss = 0.39458689\n",
            "Iteration 368, loss = 0.39449519\n",
            "Iteration 369, loss = 0.39423026\n",
            "Iteration 370, loss = 0.39403974\n",
            "Iteration 371, loss = 0.39392273\n",
            "Iteration 372, loss = 0.39381355\n",
            "Iteration 373, loss = 0.39360404\n",
            "Iteration 374, loss = 0.39340572\n",
            "Iteration 375, loss = 0.39341495\n",
            "Iteration 376, loss = 0.39309876\n",
            "Iteration 377, loss = 0.39293746\n",
            "Iteration 378, loss = 0.39276582\n",
            "Iteration 379, loss = 0.39263285\n",
            "Iteration 380, loss = 0.39244316\n",
            "Iteration 381, loss = 0.39231406\n",
            "Iteration 382, loss = 0.39217639\n",
            "Iteration 383, loss = 0.39212700\n",
            "Iteration 384, loss = 0.39188799\n",
            "Iteration 385, loss = 0.39172005\n",
            "Iteration 386, loss = 0.39163114\n",
            "Iteration 387, loss = 0.39141959\n",
            "Iteration 388, loss = 0.39126264\n",
            "Iteration 389, loss = 0.39115301\n",
            "Iteration 390, loss = 0.39094784\n",
            "Iteration 391, loss = 0.39083312\n",
            "Iteration 392, loss = 0.39070180\n",
            "Iteration 393, loss = 0.39053051\n",
            "Iteration 394, loss = 0.39038353\n",
            "Iteration 395, loss = 0.39029885\n",
            "Iteration 396, loss = 0.39011692\n",
            "Iteration 397, loss = 0.38997689\n",
            "Iteration 398, loss = 0.38986068\n",
            "Iteration 399, loss = 0.38974750\n",
            "Iteration 400, loss = 0.38958420\n",
            "Iteration 401, loss = 0.38943918\n",
            "Iteration 402, loss = 0.38932416\n",
            "Iteration 403, loss = 0.38918195\n",
            "Iteration 404, loss = 0.38911093\n",
            "Iteration 405, loss = 0.38888136\n",
            "Iteration 406, loss = 0.38886538\n",
            "Iteration 407, loss = 0.38865520\n",
            "Iteration 408, loss = 0.38855090\n",
            "Iteration 409, loss = 0.38838917\n",
            "Iteration 410, loss = 0.38828601\n",
            "Iteration 411, loss = 0.38823328\n",
            "Iteration 412, loss = 0.38797700\n",
            "Iteration 413, loss = 0.38789441\n",
            "Iteration 414, loss = 0.38784739\n",
            "Iteration 415, loss = 0.38763237\n",
            "Iteration 416, loss = 0.38750307\n",
            "Iteration 417, loss = 0.38747317\n",
            "Iteration 418, loss = 0.38724484\n",
            "Iteration 419, loss = 0.38711764\n",
            "Iteration 420, loss = 0.38701887\n",
            "Iteration 421, loss = 0.38697815\n",
            "Iteration 422, loss = 0.38686172\n",
            "Iteration 423, loss = 0.38665189\n",
            "Iteration 424, loss = 0.38658423\n",
            "Iteration 425, loss = 0.38653714\n",
            "Iteration 426, loss = 0.38633374\n",
            "Iteration 427, loss = 0.38621244\n",
            "Iteration 428, loss = 0.38610033\n",
            "Iteration 429, loss = 0.38596285\n",
            "Iteration 430, loss = 0.38586642\n",
            "Iteration 431, loss = 0.38584093\n",
            "Iteration 432, loss = 0.38570555\n",
            "Iteration 433, loss = 0.38554840\n",
            "Iteration 434, loss = 0.38545481\n",
            "Iteration 435, loss = 0.38532805\n",
            "Iteration 436, loss = 0.38531191\n",
            "Iteration 437, loss = 0.38517250\n",
            "Iteration 438, loss = 0.38500082\n",
            "Iteration 439, loss = 0.38495654\n",
            "Iteration 440, loss = 0.38475438\n",
            "Iteration 441, loss = 0.38465848\n",
            "Iteration 442, loss = 0.38487715\n",
            "Iteration 443, loss = 0.38449576\n",
            "Iteration 444, loss = 0.38440502\n",
            "Iteration 445, loss = 0.38449874\n",
            "Iteration 446, loss = 0.38433669\n",
            "Iteration 447, loss = 0.38404902\n",
            "Iteration 448, loss = 0.38395301\n",
            "Iteration 449, loss = 0.38385327\n",
            "Iteration 450, loss = 0.38377244\n",
            "Iteration 451, loss = 0.38363978\n",
            "Iteration 452, loss = 0.38357051\n",
            "Iteration 453, loss = 0.38364168\n",
            "Iteration 454, loss = 0.38347133\n",
            "Iteration 455, loss = 0.38332235\n",
            "Iteration 456, loss = 0.38319286\n",
            "Iteration 457, loss = 0.38305200\n",
            "Iteration 458, loss = 0.38300343\n",
            "Iteration 459, loss = 0.38291340\n",
            "Iteration 460, loss = 0.38284480\n",
            "Iteration 461, loss = 0.38272239\n",
            "Iteration 462, loss = 0.38264041\n",
            "Iteration 463, loss = 0.38254592\n",
            "Iteration 464, loss = 0.38244392\n",
            "Iteration 465, loss = 0.38236231\n",
            "Iteration 466, loss = 0.38231692\n",
            "Iteration 467, loss = 0.38226799\n",
            "Iteration 468, loss = 0.38211719\n",
            "Iteration 469, loss = 0.38202186\n",
            "Iteration 470, loss = 0.38192473\n",
            "Iteration 471, loss = 0.38181281\n",
            "Iteration 472, loss = 0.38177200\n",
            "Iteration 473, loss = 0.38168768\n",
            "Iteration 474, loss = 0.38156232\n",
            "Iteration 475, loss = 0.38146653\n",
            "Iteration 476, loss = 0.38138172\n",
            "Iteration 477, loss = 0.38137378\n",
            "Iteration 478, loss = 0.38120951\n",
            "Iteration 479, loss = 0.38114096\n",
            "Iteration 480, loss = 0.38104089\n",
            "Iteration 481, loss = 0.38096526\n",
            "Iteration 482, loss = 0.38089281\n",
            "Iteration 483, loss = 0.38087969\n",
            "Iteration 484, loss = 0.38082513\n",
            "Iteration 485, loss = 0.38068814\n",
            "Iteration 486, loss = 0.38057861\n",
            "Iteration 487, loss = 0.38060908\n",
            "Iteration 488, loss = 0.38046404\n",
            "Iteration 489, loss = 0.38034637\n",
            "Iteration 490, loss = 0.38038762\n",
            "Iteration 491, loss = 0.38017755\n",
            "Iteration 492, loss = 0.38014890\n",
            "Iteration 493, loss = 0.38005751\n",
            "Iteration 494, loss = 0.38006308\n",
            "Iteration 495, loss = 0.37985431\n",
            "Iteration 496, loss = 0.37977169\n",
            "Iteration 497, loss = 0.37966724\n",
            "Iteration 498, loss = 0.37972884\n",
            "Iteration 499, loss = 0.37967734\n",
            "Iteration 500, loss = 0.37956632\n",
            "Iteration 501, loss = 0.37949213\n",
            "Iteration 502, loss = 0.37947405\n",
            "Iteration 503, loss = 0.37934565\n",
            "Iteration 504, loss = 0.37922340\n",
            "Iteration 505, loss = 0.37915470\n",
            "Iteration 506, loss = 0.37909663\n",
            "Iteration 507, loss = 0.37914490\n",
            "Iteration 508, loss = 0.37892838\n",
            "Iteration 509, loss = 0.37911588\n",
            "Iteration 510, loss = 0.37878721\n",
            "Iteration 511, loss = 0.37875883\n",
            "Iteration 512, loss = 0.37865619\n",
            "Iteration 513, loss = 0.37870705\n",
            "Iteration 514, loss = 0.37855426\n",
            "Iteration 515, loss = 0.37850564\n",
            "Iteration 516, loss = 0.37844389\n",
            "Iteration 517, loss = 0.37836515\n",
            "Iteration 518, loss = 0.37824326\n",
            "Iteration 519, loss = 0.37823938\n",
            "Iteration 520, loss = 0.37813740\n",
            "Iteration 521, loss = 0.37806509\n",
            "Iteration 522, loss = 0.37819322\n",
            "Iteration 523, loss = 0.37799362\n",
            "Iteration 524, loss = 0.37788132\n",
            "Iteration 525, loss = 0.37783534\n",
            "Iteration 526, loss = 0.37776956\n",
            "Iteration 527, loss = 0.37769271\n",
            "Iteration 528, loss = 0.37764481\n",
            "Iteration 529, loss = 0.37753747\n",
            "Iteration 530, loss = 0.37750371\n",
            "Iteration 531, loss = 0.37746232\n",
            "Iteration 532, loss = 0.37761008\n",
            "Iteration 533, loss = 0.37735483\n",
            "Iteration 534, loss = 0.37730741\n",
            "Iteration 535, loss = 0.37725909\n",
            "Iteration 536, loss = 0.37713215\n",
            "Iteration 537, loss = 0.37714777\n",
            "Iteration 538, loss = 0.37705187\n",
            "Iteration 539, loss = 0.37697086\n",
            "Iteration 540, loss = 0.37697227\n",
            "Iteration 541, loss = 0.37685535\n",
            "Iteration 542, loss = 0.37689754\n",
            "Iteration 543, loss = 0.37674571\n",
            "Iteration 544, loss = 0.37673440\n",
            "Iteration 545, loss = 0.37666843\n",
            "Iteration 546, loss = 0.37653848\n",
            "Iteration 547, loss = 0.37654294\n",
            "Iteration 548, loss = 0.37653781\n",
            "Iteration 549, loss = 0.37641361\n",
            "Iteration 550, loss = 0.37639090\n",
            "Iteration 551, loss = 0.37628629\n",
            "Iteration 552, loss = 0.37625317\n",
            "Iteration 553, loss = 0.37620301\n",
            "Iteration 554, loss = 0.37622564\n",
            "Iteration 555, loss = 0.37609168\n",
            "Iteration 556, loss = 0.37600929\n",
            "Iteration 557, loss = 0.37603498\n",
            "Iteration 558, loss = 0.37601897\n",
            "Iteration 559, loss = 0.37593892\n",
            "Iteration 560, loss = 0.37584076\n",
            "Iteration 561, loss = 0.37580247\n",
            "Iteration 562, loss = 0.37573084\n",
            "Iteration 563, loss = 0.37577979\n",
            "Iteration 564, loss = 0.37570812\n",
            "Iteration 565, loss = 0.37558252\n",
            "Iteration 566, loss = 0.37560579\n",
            "Iteration 567, loss = 0.37546290\n",
            "Iteration 568, loss = 0.37552440\n",
            "Iteration 569, loss = 0.37536871\n",
            "Iteration 570, loss = 0.37543170\n",
            "Iteration 571, loss = 0.37527549\n",
            "Iteration 572, loss = 0.37532844\n",
            "Iteration 573, loss = 0.37519027\n",
            "Iteration 574, loss = 0.37531602\n",
            "Iteration 575, loss = 0.37514892\n",
            "Iteration 576, loss = 0.37507890\n",
            "Iteration 577, loss = 0.37499235\n",
            "Iteration 578, loss = 0.37499715\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Melhores hiperparâmetros encontrados através do Random Search:\n",
            "{'tol': 0.0001, 'solver': 'adam', 'max_iter': 2000, 'activation': 'tanh'}\n",
            "Melhor pontuação (acurácia) encontrada através do Random Search: 0.8210526315789475\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ajuste do modelo MLP aos dados de treinamento\n",
        "rna2_best_random = MLPClassifier(**best_params_random)\n",
        "rna2_best_random.fit(X_train_oversampled, y_train_oversampled)\n",
        "\n",
        "# Predições nos dados de teste usando o modelo com melhores hiperparâmetros encontrados pelo Random Search\n",
        "pred_random2 = rna2_best_random.predict(X_test)"
      ],
      "metadata": {
        "id": "ua3moDapU5-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cálculo e impressão da acurácia nos dados de teste\n",
        "accuracy_random = accuracy_score(y_test, pred_random2)\n",
        "print(\"Acurácia do modelo MLP com melhores hiperparâmetros pelo Random Search:\", accuracy_random)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "576d6af5-c58b-408d-e0ba-ef537d7b4331",
        "id": "dC2Hi35kU5-5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acurácia do modelo MLP com melhores hiperparâmetros pelo Random Search: 0.7611940298507462\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Matriz de confusão para o modelo com melhores hiperparâmetros pelo Random Search\n",
        "print(\"Matriz de Confusão - Random Search\")\n",
        "cm_rna2_random = confusion_matrix(y_test, pred_random2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f824777-6e70-4c05-c0a4-2a2f039298fe",
        "id": "o4J2e0TSU5-5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matriz de Confusão - Random Search\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtém a média das acurácias (10 folds) referente ao conjunto treino\n",
        "rna2_random = r_results.loc[r_search.best_index_,'mean_test_score']"
      ],
      "metadata": {
        "id": "5RjqesZiU5-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrizxZdeOxbK"
      },
      "source": [
        "## Rede Neural (Scikit) - 3 camadas\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rna3 = MLPClassifier(hidden_layer_sizes=(3,3, 3), activation='relu', solver='sgd', max_iter =100,\n",
        "                              tol=0.0001, random_state = 3, verbose = True)"
      ],
      "metadata": {
        "id": "MrGw2jUtCP69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rna3.fit(X_train_oversampled, y_train_oversampled)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CzzvtWo2FeId",
        "outputId": "d0b3e9a1-f7f6-4c48-fb4f-9f4ec2c84e99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 0.71485857\n",
            "Iteration 2, loss = 0.71467110\n",
            "Iteration 3, loss = 0.71435316\n",
            "Iteration 4, loss = 0.71401184\n",
            "Iteration 5, loss = 0.71360826\n",
            "Iteration 6, loss = 0.71318709\n",
            "Iteration 7, loss = 0.71275256\n",
            "Iteration 8, loss = 0.71238837\n",
            "Iteration 9, loss = 0.71188757\n",
            "Iteration 10, loss = 0.71148940\n",
            "Iteration 11, loss = 0.71111371\n",
            "Iteration 12, loss = 0.71073823\n",
            "Iteration 13, loss = 0.71029859\n",
            "Iteration 14, loss = 0.70992359\n",
            "Iteration 15, loss = 0.70953323\n",
            "Iteration 16, loss = 0.70921348\n",
            "Iteration 17, loss = 0.70887219\n",
            "Iteration 18, loss = 0.70848628\n",
            "Iteration 19, loss = 0.70815813\n",
            "Iteration 20, loss = 0.70785125\n",
            "Iteration 21, loss = 0.70752390\n",
            "Iteration 22, loss = 0.70726086\n",
            "Iteration 23, loss = 0.70692093\n",
            "Iteration 24, loss = 0.70661867\n",
            "Iteration 25, loss = 0.70636358\n",
            "Iteration 26, loss = 0.70605827\n",
            "Iteration 27, loss = 0.70581300\n",
            "Iteration 28, loss = 0.70552894\n",
            "Iteration 29, loss = 0.70527824\n",
            "Iteration 30, loss = 0.70503388\n",
            "Iteration 31, loss = 0.70479254\n",
            "Iteration 32, loss = 0.70455673\n",
            "Iteration 33, loss = 0.70432875\n",
            "Iteration 34, loss = 0.70409486\n",
            "Iteration 35, loss = 0.70387662\n",
            "Iteration 36, loss = 0.70366133\n",
            "Iteration 37, loss = 0.70345813\n",
            "Iteration 38, loss = 0.70325360\n",
            "Iteration 39, loss = 0.70304679\n",
            "Iteration 40, loss = 0.70284564\n",
            "Iteration 41, loss = 0.70265962\n",
            "Iteration 42, loss = 0.70246667\n",
            "Iteration 43, loss = 0.70231529\n",
            "Iteration 44, loss = 0.70210507\n",
            "Iteration 45, loss = 0.70195192\n",
            "Iteration 46, loss = 0.70174710\n",
            "Iteration 47, loss = 0.70158831\n",
            "Iteration 48, loss = 0.70143118\n",
            "Iteration 49, loss = 0.70126557\n",
            "Iteration 50, loss = 0.70112777\n",
            "Iteration 51, loss = 0.70095635\n",
            "Iteration 52, loss = 0.70081253\n",
            "Iteration 53, loss = 0.70064583\n",
            "Iteration 54, loss = 0.70050386\n",
            "Iteration 55, loss = 0.70036691\n",
            "Iteration 56, loss = 0.70023165\n",
            "Iteration 57, loss = 0.70008073\n",
            "Iteration 58, loss = 0.69995065\n",
            "Iteration 59, loss = 0.69981408\n",
            "Iteration 60, loss = 0.69968533\n",
            "Iteration 61, loss = 0.69954806\n",
            "Iteration 62, loss = 0.69942467\n",
            "Iteration 63, loss = 0.69929584\n",
            "Iteration 64, loss = 0.69916652\n",
            "Iteration 65, loss = 0.69904059\n",
            "Iteration 66, loss = 0.69891530\n",
            "Iteration 67, loss = 0.69880649\n",
            "Iteration 68, loss = 0.69868401\n",
            "Iteration 69, loss = 0.69855886\n",
            "Iteration 70, loss = 0.69845241\n",
            "Iteration 71, loss = 0.69833845\n",
            "Iteration 72, loss = 0.69822950\n",
            "Iteration 73, loss = 0.69812807\n",
            "Iteration 74, loss = 0.69801799\n",
            "Iteration 75, loss = 0.69790016\n",
            "Iteration 76, loss = 0.69780563\n",
            "Iteration 77, loss = 0.69769879\n",
            "Iteration 78, loss = 0.69759684\n",
            "Iteration 79, loss = 0.69749614\n",
            "Iteration 80, loss = 0.69739363\n",
            "Iteration 81, loss = 0.69730178\n",
            "Iteration 82, loss = 0.69721202\n",
            "Iteration 83, loss = 0.69710395\n",
            "Iteration 84, loss = 0.69700140\n",
            "Iteration 85, loss = 0.69690069\n",
            "Iteration 86, loss = 0.69680678\n",
            "Iteration 87, loss = 0.69673346\n",
            "Iteration 88, loss = 0.69662970\n",
            "Iteration 89, loss = 0.69652439\n",
            "Iteration 90, loss = 0.69643497\n",
            "Iteration 91, loss = 0.69634598\n",
            "Iteration 92, loss = 0.69626046\n",
            "Iteration 93, loss = 0.69616541\n",
            "Iteration 94, loss = 0.69608158\n",
            "Iteration 95, loss = 0.69599753\n",
            "Iteration 96, loss = 0.69590226\n",
            "Iteration 97, loss = 0.69582360\n",
            "Iteration 98, loss = 0.69573675\n",
            "Iteration 99, loss = 0.69565747\n",
            "Iteration 100, loss = 0.69556285\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(hidden_layer_sizes=(3, 3, 3), max_iter=100, random_state=3,\n",
              "              solver='sgd', verbose=True)"
            ],
            "text/html": [
              "<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(hidden_layer_sizes=(3, 3, 3), max_iter=100, random_state=3,\n",
              "              solver=&#x27;sgd&#x27;, verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(hidden_layer_sizes=(3, 3, 3), max_iter=100, random_state=3,\n",
              "              solver=&#x27;sgd&#x27;, verbose=True)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "previsoes3 = rna3.predict(X_test)"
      ],
      "metadata": {
        "id": "b-apl7JRFeIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rna3_teste = accuracy_score(y_test, previsoes3)"
      ],
      "metadata": {
        "id": "nUXSGp2EFeIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(rna3_teste)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf20a274-8d91-4578-e1da-2d2498f6ed3b",
        "id": "TY619q6xFeIe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6082089552238806\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cm_rna3_teste = confusion_matrix(y_test, previsoes3)"
      ],
      "metadata": {
        "id": "_TZHPHPXFeIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_test, previsoes3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OTEeGJ0XFeIe",
        "outputId": "b8a3eae1-0e03-4d0c-f37e-2c48ff89119e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.92      0.75       169\n",
            "           1       0.36      0.08      0.13        99\n",
            "\n",
            "    accuracy                           0.61       268\n",
            "   macro avg       0.50      0.50      0.44       268\n",
            "weighted avg       0.53      0.61      0.52       268\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "previsoes_treino3 = rna3.predict(X_train_oversampled)"
      ],
      "metadata": {
        "id": "t20L0SuGFeIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rna3_treino = accuracy_score(y_train_oversampled, previsoes_treino3)"
      ],
      "metadata": {
        "id": "tbB-FTMLFeIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm_rna3_treino = confusion_matrix(y_train_oversampled, previsoes_treino3)"
      ],
      "metadata": {
        "id": "SpBZ_cpEFeIf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GridSearch"
      ],
      "metadata": {
        "id": "rOrCHBeUXZq-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Grid Search 3\n",
        "param_grid = {\n",
        "    'activation': ['relu', 'tanh', 'logistic'],\n",
        "    'solver': ['sgd', 'adam'],\n",
        "    'max_iter': [1000, 2000],\n",
        "    'tol': [0.0001, 0.001],\n",
        "}\n",
        "\n",
        "g_search = GridSearchCV(estimator=rna3, param_grid=param_grid, cv=10)\n",
        "g_search.fit(X_train_oversampled, y_train_oversampled)\n",
        "\n",
        "best_params_grid3 = g_search.best_params_\n",
        "best_score_grid3 = g_search.best_score_\n",
        "\n",
        "print(\"Melhores hiperparâmetros encontrados através do Grid Search:\")\n",
        "print(best_params_grid3)\n",
        "print(\"Melhor pontuação (acurácia) encontrada através do Grid Search:\", best_score_grid3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4wE2vqIXZq-",
        "outputId": "9996ec76-6dfb-47f9-f898-7eaede4fb520"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mA saída de streaming foi truncada nas últimas 5000 linhas.\u001b[0m\n",
            "Iteration 345, loss = 0.38244376\n",
            "Iteration 346, loss = 0.38221742\n",
            "Iteration 347, loss = 0.38200876\n",
            "Iteration 348, loss = 0.38176592\n",
            "Iteration 349, loss = 0.38179097\n",
            "Iteration 350, loss = 0.38146889\n",
            "Iteration 351, loss = 0.38132825\n",
            "Iteration 352, loss = 0.38108295\n",
            "Iteration 353, loss = 0.38100353\n",
            "Iteration 354, loss = 0.38085568\n",
            "Iteration 355, loss = 0.38106170\n",
            "Iteration 356, loss = 0.38065721\n",
            "Iteration 357, loss = 0.38032376\n",
            "Iteration 358, loss = 0.38020946\n",
            "Iteration 359, loss = 0.38025530\n",
            "Iteration 360, loss = 0.37999445\n",
            "Iteration 361, loss = 0.37960933\n",
            "Iteration 362, loss = 0.37957039\n",
            "Iteration 363, loss = 0.37947003\n",
            "Iteration 364, loss = 0.37964045\n",
            "Iteration 365, loss = 0.37946092\n",
            "Iteration 366, loss = 0.37915244\n",
            "Iteration 367, loss = 0.37909945\n",
            "Iteration 368, loss = 0.37869767\n",
            "Iteration 369, loss = 0.37858744\n",
            "Iteration 370, loss = 0.37843425\n",
            "Iteration 371, loss = 0.37828115\n",
            "Iteration 372, loss = 0.37810981\n",
            "Iteration 373, loss = 0.37804080\n",
            "Iteration 374, loss = 0.37793431\n",
            "Iteration 375, loss = 0.37774100\n",
            "Iteration 376, loss = 0.37757434\n",
            "Iteration 377, loss = 0.37746556\n",
            "Iteration 378, loss = 0.37738788\n",
            "Iteration 379, loss = 0.37744631\n",
            "Iteration 380, loss = 0.37724881\n",
            "Iteration 381, loss = 0.37707311\n",
            "Iteration 382, loss = 0.37690636\n",
            "Iteration 383, loss = 0.37668835\n",
            "Iteration 384, loss = 0.37652676\n",
            "Iteration 385, loss = 0.37626268\n",
            "Iteration 386, loss = 0.37647455\n",
            "Iteration 387, loss = 0.37612479\n",
            "Iteration 388, loss = 0.37614712\n",
            "Iteration 389, loss = 0.37583943\n",
            "Iteration 390, loss = 0.37570055\n",
            "Iteration 391, loss = 0.37584478\n",
            "Iteration 392, loss = 0.37569269\n",
            "Iteration 393, loss = 0.37533661\n",
            "Iteration 394, loss = 0.37515752\n",
            "Iteration 395, loss = 0.37511949\n",
            "Iteration 396, loss = 0.37502702\n",
            "Iteration 397, loss = 0.37498633\n",
            "Iteration 398, loss = 0.37510207\n",
            "Iteration 399, loss = 0.37496208\n",
            "Iteration 400, loss = 0.37463379\n",
            "Iteration 401, loss = 0.37462231\n",
            "Iteration 402, loss = 0.37450883\n",
            "Iteration 403, loss = 0.37432529\n",
            "Iteration 404, loss = 0.37425060\n",
            "Iteration 405, loss = 0.37416435\n",
            "Iteration 406, loss = 0.37399358\n",
            "Iteration 407, loss = 0.37385789\n",
            "Iteration 408, loss = 0.37367645\n",
            "Iteration 409, loss = 0.37358885\n",
            "Iteration 410, loss = 0.37349484\n",
            "Iteration 411, loss = 0.37348458\n",
            "Iteration 412, loss = 0.37331770\n",
            "Iteration 413, loss = 0.37321465\n",
            "Iteration 414, loss = 0.37300837\n",
            "Iteration 415, loss = 0.37309963\n",
            "Iteration 416, loss = 0.37288858\n",
            "Iteration 417, loss = 0.37284959\n",
            "Iteration 418, loss = 0.37269421\n",
            "Iteration 419, loss = 0.37263501\n",
            "Iteration 420, loss = 0.37239180\n",
            "Iteration 421, loss = 0.37223337\n",
            "Iteration 422, loss = 0.37214152\n",
            "Iteration 423, loss = 0.37202423\n",
            "Iteration 424, loss = 0.37191848\n",
            "Iteration 425, loss = 0.37184519\n",
            "Iteration 426, loss = 0.37173935\n",
            "Iteration 427, loss = 0.37160200\n",
            "Iteration 428, loss = 0.37182522\n",
            "Iteration 429, loss = 0.37154917\n",
            "Iteration 430, loss = 0.37131625\n",
            "Iteration 431, loss = 0.37128930\n",
            "Iteration 432, loss = 0.37132355\n",
            "Iteration 433, loss = 0.37116394\n",
            "Iteration 434, loss = 0.37098437\n",
            "Iteration 435, loss = 0.37076405\n",
            "Iteration 436, loss = 0.37095060\n",
            "Iteration 437, loss = 0.37104700\n",
            "Iteration 438, loss = 0.37095325\n",
            "Iteration 439, loss = 0.37076853\n",
            "Iteration 440, loss = 0.37039444\n",
            "Iteration 441, loss = 0.37034607\n",
            "Iteration 442, loss = 0.37026767\n",
            "Iteration 443, loss = 0.37017036\n",
            "Iteration 444, loss = 0.37003955\n",
            "Iteration 445, loss = 0.36994875\n",
            "Iteration 446, loss = 0.36982110\n",
            "Iteration 447, loss = 0.36986889\n",
            "Iteration 448, loss = 0.36974121\n",
            "Iteration 449, loss = 0.36971837\n",
            "Iteration 450, loss = 0.36983006\n",
            "Iteration 451, loss = 0.36972069\n",
            "Iteration 452, loss = 0.36937505\n",
            "Iteration 453, loss = 0.36919273\n",
            "Iteration 454, loss = 0.36920392\n",
            "Iteration 455, loss = 0.36920769\n",
            "Iteration 456, loss = 0.36922082\n",
            "Iteration 457, loss = 0.36911199\n",
            "Iteration 458, loss = 0.36889749\n",
            "Iteration 459, loss = 0.36879856\n",
            "Iteration 460, loss = 0.36894219\n",
            "Iteration 461, loss = 0.36862439\n",
            "Iteration 462, loss = 0.36853275\n",
            "Iteration 463, loss = 0.36844312\n",
            "Iteration 464, loss = 0.36838265\n",
            "Iteration 465, loss = 0.36838903\n",
            "Iteration 466, loss = 0.36817345\n",
            "Iteration 467, loss = 0.36841416\n",
            "Iteration 468, loss = 0.36810627\n",
            "Iteration 469, loss = 0.36799419\n",
            "Iteration 470, loss = 0.36789484\n",
            "Iteration 471, loss = 0.36787295\n",
            "Iteration 472, loss = 0.36774810\n",
            "Iteration 473, loss = 0.36778423\n",
            "Iteration 474, loss = 0.36766794\n",
            "Iteration 475, loss = 0.36755850\n",
            "Iteration 476, loss = 0.36745598\n",
            "Iteration 477, loss = 0.36750154\n",
            "Iteration 478, loss = 0.36745262\n",
            "Iteration 479, loss = 0.36732538\n",
            "Iteration 480, loss = 0.36716159\n",
            "Iteration 481, loss = 0.36707259\n",
            "Iteration 482, loss = 0.36709430\n",
            "Iteration 483, loss = 0.36703610\n",
            "Iteration 484, loss = 0.36695218\n",
            "Iteration 485, loss = 0.36689074\n",
            "Iteration 486, loss = 0.36694518\n",
            "Iteration 487, loss = 0.36675178\n",
            "Iteration 488, loss = 0.36683617\n",
            "Iteration 489, loss = 0.36673876\n",
            "Iteration 490, loss = 0.36659264\n",
            "Iteration 491, loss = 0.36653090\n",
            "Iteration 492, loss = 0.36644490\n",
            "Iteration 493, loss = 0.36628975\n",
            "Iteration 494, loss = 0.36631167\n",
            "Iteration 495, loss = 0.36640718\n",
            "Iteration 496, loss = 0.36649241\n",
            "Iteration 497, loss = 0.36619036\n",
            "Iteration 498, loss = 0.36601496\n",
            "Iteration 499, loss = 0.36596798\n",
            "Iteration 500, loss = 0.36591929\n",
            "Iteration 501, loss = 0.36603792\n",
            "Iteration 502, loss = 0.36591533\n",
            "Iteration 503, loss = 0.36573875\n",
            "Iteration 504, loss = 0.36573381\n",
            "Iteration 505, loss = 0.36563403\n",
            "Iteration 506, loss = 0.36569247\n",
            "Iteration 507, loss = 0.36546927\n",
            "Iteration 508, loss = 0.36563148\n",
            "Iteration 509, loss = 0.36556484\n",
            "Iteration 510, loss = 0.36524532\n",
            "Iteration 511, loss = 0.36535080\n",
            "Iteration 512, loss = 0.36550057\n",
            "Iteration 513, loss = 0.36534408\n",
            "Iteration 514, loss = 0.36571781\n",
            "Iteration 515, loss = 0.36518220\n",
            "Iteration 516, loss = 0.36509005\n",
            "Iteration 517, loss = 0.36500167\n",
            "Iteration 518, loss = 0.36497338\n",
            "Iteration 519, loss = 0.36493240\n",
            "Iteration 520, loss = 0.36482638\n",
            "Iteration 521, loss = 0.36479515\n",
            "Iteration 522, loss = 0.36464808\n",
            "Iteration 523, loss = 0.36468346\n",
            "Iteration 524, loss = 0.36461635\n",
            "Iteration 525, loss = 0.36453250\n",
            "Iteration 526, loss = 0.36452642\n",
            "Iteration 527, loss = 0.36466685\n",
            "Iteration 528, loss = 0.36436623\n",
            "Iteration 529, loss = 0.36426497\n",
            "Iteration 530, loss = 0.36420208\n",
            "Iteration 531, loss = 0.36423601\n",
            "Iteration 532, loss = 0.36421952\n",
            "Iteration 533, loss = 0.36403963\n",
            "Iteration 534, loss = 0.36401999\n",
            "Iteration 535, loss = 0.36394887\n",
            "Iteration 536, loss = 0.36395485\n",
            "Iteration 537, loss = 0.36388832\n",
            "Iteration 538, loss = 0.36398785\n",
            "Iteration 539, loss = 0.36378197\n",
            "Iteration 540, loss = 0.36366088\n",
            "Iteration 541, loss = 0.36374376\n",
            "Iteration 542, loss = 0.36366529\n",
            "Iteration 543, loss = 0.36368497\n",
            "Iteration 544, loss = 0.36367193\n",
            "Iteration 545, loss = 0.36362493\n",
            "Iteration 546, loss = 0.36335668\n",
            "Iteration 547, loss = 0.36330108\n",
            "Iteration 548, loss = 0.36353620\n",
            "Iteration 549, loss = 0.36336700\n",
            "Iteration 550, loss = 0.36351317\n",
            "Iteration 551, loss = 0.36336045\n",
            "Iteration 552, loss = 0.36316830\n",
            "Iteration 553, loss = 0.36303104\n",
            "Iteration 554, loss = 0.36342305\n",
            "Iteration 555, loss = 0.36327116\n",
            "Iteration 556, loss = 0.36311986\n",
            "Iteration 557, loss = 0.36306219\n",
            "Iteration 558, loss = 0.36291240\n",
            "Iteration 559, loss = 0.36288871\n",
            "Iteration 560, loss = 0.36274710\n",
            "Iteration 561, loss = 0.36271716\n",
            "Iteration 562, loss = 0.36281027\n",
            "Iteration 563, loss = 0.36281579\n",
            "Iteration 564, loss = 0.36268511\n",
            "Iteration 565, loss = 0.36244658\n",
            "Iteration 566, loss = 0.36260140\n",
            "Iteration 567, loss = 0.36252705\n",
            "Iteration 568, loss = 0.36251683\n",
            "Iteration 569, loss = 0.36230929\n",
            "Iteration 570, loss = 0.36240583\n",
            "Iteration 571, loss = 0.36231286\n",
            "Iteration 572, loss = 0.36249428\n",
            "Iteration 573, loss = 0.36223353\n",
            "Iteration 574, loss = 0.36223038\n",
            "Iteration 575, loss = 0.36219588\n",
            "Iteration 576, loss = 0.36205622\n",
            "Iteration 577, loss = 0.36202936\n",
            "Iteration 578, loss = 0.36206298\n",
            "Iteration 579, loss = 0.36186705\n",
            "Iteration 580, loss = 0.36214080\n",
            "Iteration 581, loss = 0.36183334\n",
            "Iteration 582, loss = 0.36187849\n",
            "Iteration 583, loss = 0.36187181\n",
            "Iteration 584, loss = 0.36185437\n",
            "Iteration 585, loss = 0.36172902\n",
            "Iteration 586, loss = 0.36193470\n",
            "Iteration 587, loss = 0.36160350\n",
            "Iteration 588, loss = 0.36159573\n",
            "Iteration 589, loss = 0.36184176\n",
            "Iteration 590, loss = 0.36137987\n",
            "Iteration 591, loss = 0.36140927\n",
            "Iteration 592, loss = 0.36141491\n",
            "Iteration 593, loss = 0.36169598\n",
            "Iteration 594, loss = 0.36136967\n",
            "Iteration 595, loss = 0.36131222\n",
            "Iteration 596, loss = 0.36126181\n",
            "Iteration 597, loss = 0.36117276\n",
            "Iteration 598, loss = 0.36110409\n",
            "Iteration 599, loss = 0.36105394\n",
            "Iteration 600, loss = 0.36132306\n",
            "Iteration 601, loss = 0.36113830\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.75775441\n",
            "Iteration 2, loss = 0.75243319\n",
            "Iteration 3, loss = 0.74755596\n",
            "Iteration 4, loss = 0.74297823\n",
            "Iteration 5, loss = 0.73815897\n",
            "Iteration 6, loss = 0.73416311\n",
            "Iteration 7, loss = 0.73015614\n",
            "Iteration 8, loss = 0.72625260\n",
            "Iteration 9, loss = 0.72275917\n",
            "Iteration 10, loss = 0.71901404\n",
            "Iteration 11, loss = 0.71595904\n",
            "Iteration 12, loss = 0.71299836\n",
            "Iteration 13, loss = 0.70993720\n",
            "Iteration 14, loss = 0.70713516\n",
            "Iteration 15, loss = 0.70464491\n",
            "Iteration 16, loss = 0.70191857\n",
            "Iteration 17, loss = 0.69963407\n",
            "Iteration 18, loss = 0.69741324\n",
            "Iteration 19, loss = 0.69514889\n",
            "Iteration 20, loss = 0.69315202\n",
            "Iteration 21, loss = 0.69123284\n",
            "Iteration 22, loss = 0.68915827\n",
            "Iteration 23, loss = 0.68738344\n",
            "Iteration 24, loss = 0.68572038\n",
            "Iteration 25, loss = 0.68388599\n",
            "Iteration 26, loss = 0.68221573\n",
            "Iteration 27, loss = 0.68048597\n",
            "Iteration 28, loss = 0.67877032\n",
            "Iteration 29, loss = 0.67713401\n",
            "Iteration 30, loss = 0.67546863\n",
            "Iteration 31, loss = 0.67382019\n",
            "Iteration 32, loss = 0.67217660\n",
            "Iteration 33, loss = 0.67042438\n",
            "Iteration 34, loss = 0.66885723\n",
            "Iteration 35, loss = 0.66730639\n",
            "Iteration 36, loss = 0.66549610\n",
            "Iteration 37, loss = 0.66388493\n",
            "Iteration 38, loss = 0.66222263\n",
            "Iteration 39, loss = 0.66069245\n",
            "Iteration 40, loss = 0.65889109\n",
            "Iteration 41, loss = 0.65727268\n",
            "Iteration 42, loss = 0.65560200\n",
            "Iteration 43, loss = 0.65386598\n",
            "Iteration 44, loss = 0.65217357\n",
            "Iteration 45, loss = 0.65044596\n",
            "Iteration 46, loss = 0.64874690\n",
            "Iteration 47, loss = 0.64700074\n",
            "Iteration 48, loss = 0.64522705\n",
            "Iteration 49, loss = 0.64344682\n",
            "Iteration 50, loss = 0.64168966\n",
            "Iteration 51, loss = 0.63996914\n",
            "Iteration 52, loss = 0.63817945\n",
            "Iteration 53, loss = 0.63642003\n",
            "Iteration 54, loss = 0.63466680\n",
            "Iteration 55, loss = 0.63271936\n",
            "Iteration 56, loss = 0.63097261\n",
            "Iteration 57, loss = 0.62905551\n",
            "Iteration 58, loss = 0.62723813\n",
            "Iteration 59, loss = 0.62531559\n",
            "Iteration 60, loss = 0.62340245\n",
            "Iteration 61, loss = 0.62152098\n",
            "Iteration 62, loss = 0.61967004\n",
            "Iteration 63, loss = 0.61762063\n",
            "Iteration 64, loss = 0.61575842\n",
            "Iteration 65, loss = 0.61383621\n",
            "Iteration 66, loss = 0.61173805\n",
            "Iteration 67, loss = 0.60997245\n",
            "Iteration 68, loss = 0.60779381\n",
            "Iteration 69, loss = 0.60583214\n",
            "Iteration 70, loss = 0.60374580\n",
            "Iteration 71, loss = 0.60163866\n",
            "Iteration 72, loss = 0.59966571\n",
            "Iteration 73, loss = 0.59750662\n",
            "Iteration 74, loss = 0.59528949\n",
            "Iteration 75, loss = 0.59313778\n",
            "Iteration 76, loss = 0.59097556\n",
            "Iteration 77, loss = 0.58873114\n",
            "Iteration 78, loss = 0.58634454\n",
            "Iteration 79, loss = 0.58419775\n",
            "Iteration 80, loss = 0.58183084\n",
            "Iteration 81, loss = 0.57934689\n",
            "Iteration 82, loss = 0.57696989\n",
            "Iteration 83, loss = 0.57449515\n",
            "Iteration 84, loss = 0.57212327\n",
            "Iteration 85, loss = 0.56959007\n",
            "Iteration 86, loss = 0.56689814\n",
            "Iteration 87, loss = 0.56420756\n",
            "Iteration 88, loss = 0.56167700\n",
            "Iteration 89, loss = 0.55892367\n",
            "Iteration 90, loss = 0.55630738\n",
            "Iteration 91, loss = 0.55359331\n",
            "Iteration 92, loss = 0.55082783\n",
            "Iteration 93, loss = 0.54798011\n",
            "Iteration 94, loss = 0.54535255\n",
            "Iteration 95, loss = 0.54233338\n",
            "Iteration 96, loss = 0.53952895\n",
            "Iteration 97, loss = 0.53675512\n",
            "Iteration 98, loss = 0.53382467\n",
            "Iteration 99, loss = 0.53098228\n",
            "Iteration 100, loss = 0.52823432\n",
            "Iteration 101, loss = 0.52553160\n",
            "Iteration 102, loss = 0.52273950\n",
            "Iteration 103, loss = 0.51980486\n",
            "Iteration 104, loss = 0.51716745\n",
            "Iteration 105, loss = 0.51440409\n",
            "Iteration 106, loss = 0.51165515\n",
            "Iteration 107, loss = 0.50889971\n",
            "Iteration 108, loss = 0.50630421\n",
            "Iteration 109, loss = 0.50366709\n",
            "Iteration 110, loss = 0.50099868\n",
            "Iteration 111, loss = 0.49842258\n",
            "Iteration 112, loss = 0.49594701\n",
            "Iteration 113, loss = 0.49339031\n",
            "Iteration 114, loss = 0.49102036\n",
            "Iteration 115, loss = 0.48842099\n",
            "Iteration 116, loss = 0.48617745\n",
            "Iteration 117, loss = 0.48382533\n",
            "Iteration 118, loss = 0.48131578\n",
            "Iteration 119, loss = 0.47929971\n",
            "Iteration 120, loss = 0.47716317\n",
            "Iteration 121, loss = 0.47506929\n",
            "Iteration 122, loss = 0.47283125\n",
            "Iteration 123, loss = 0.47071701\n",
            "Iteration 124, loss = 0.46891169\n",
            "Iteration 125, loss = 0.46706329\n",
            "Iteration 126, loss = 0.46529456\n",
            "Iteration 127, loss = 0.46335648\n",
            "Iteration 128, loss = 0.46165091\n",
            "Iteration 129, loss = 0.45998577\n",
            "Iteration 130, loss = 0.45847807\n",
            "Iteration 131, loss = 0.45707797\n",
            "Iteration 132, loss = 0.45566218\n",
            "Iteration 133, loss = 0.45427286\n",
            "Iteration 134, loss = 0.45309202\n",
            "Iteration 135, loss = 0.45168669\n",
            "Iteration 136, loss = 0.45062935\n",
            "Iteration 137, loss = 0.44936977\n",
            "Iteration 138, loss = 0.44842834\n",
            "Iteration 139, loss = 0.44733582\n",
            "Iteration 140, loss = 0.44629093\n",
            "Iteration 141, loss = 0.44546860\n",
            "Iteration 142, loss = 0.44467088\n",
            "Iteration 143, loss = 0.44393659\n",
            "Iteration 144, loss = 0.44303001\n",
            "Iteration 145, loss = 0.44228344\n",
            "Iteration 146, loss = 0.44177255\n",
            "Iteration 147, loss = 0.44100423\n",
            "Iteration 148, loss = 0.44051086\n",
            "Iteration 149, loss = 0.43978707\n",
            "Iteration 150, loss = 0.43924049\n",
            "Iteration 151, loss = 0.43865340\n",
            "Iteration 152, loss = 0.43811186\n",
            "Iteration 153, loss = 0.43767764\n",
            "Iteration 154, loss = 0.43713466\n",
            "Iteration 155, loss = 0.43666986\n",
            "Iteration 156, loss = 0.43613272\n",
            "Iteration 157, loss = 0.43562746\n",
            "Iteration 158, loss = 0.43516609\n",
            "Iteration 159, loss = 0.43478809\n",
            "Iteration 160, loss = 0.43425693\n",
            "Iteration 161, loss = 0.43370725\n",
            "Iteration 162, loss = 0.43327199\n",
            "Iteration 163, loss = 0.43289668\n",
            "Iteration 164, loss = 0.43237702\n",
            "Iteration 165, loss = 0.43215449\n",
            "Iteration 166, loss = 0.43172685\n",
            "Iteration 167, loss = 0.43122827\n",
            "Iteration 168, loss = 0.43075805\n",
            "Iteration 169, loss = 0.43040496\n",
            "Iteration 170, loss = 0.42997758\n",
            "Iteration 171, loss = 0.42956404\n",
            "Iteration 172, loss = 0.42917291\n",
            "Iteration 173, loss = 0.42876734\n",
            "Iteration 174, loss = 0.42836567\n",
            "Iteration 175, loss = 0.42802979\n",
            "Iteration 176, loss = 0.42753134\n",
            "Iteration 177, loss = 0.42713083\n",
            "Iteration 178, loss = 0.42669955\n",
            "Iteration 179, loss = 0.42635622\n",
            "Iteration 180, loss = 0.42600568\n",
            "Iteration 181, loss = 0.42547500\n",
            "Iteration 182, loss = 0.42504622\n",
            "Iteration 183, loss = 0.42458578\n",
            "Iteration 184, loss = 0.42412306\n",
            "Iteration 185, loss = 0.42389544\n",
            "Iteration 186, loss = 0.42311642\n",
            "Iteration 187, loss = 0.42286213\n",
            "Iteration 188, loss = 0.42267818\n",
            "Iteration 189, loss = 0.42202157\n",
            "Iteration 190, loss = 0.42153513\n",
            "Iteration 191, loss = 0.42119108\n",
            "Iteration 192, loss = 0.42074959\n",
            "Iteration 193, loss = 0.42006962\n",
            "Iteration 194, loss = 0.41954627\n",
            "Iteration 195, loss = 0.41901224\n",
            "Iteration 196, loss = 0.41898050\n",
            "Iteration 197, loss = 0.41829542\n",
            "Iteration 198, loss = 0.41782643\n",
            "Iteration 199, loss = 0.41727848\n",
            "Iteration 200, loss = 0.41696524\n",
            "Iteration 201, loss = 0.41645002\n",
            "Iteration 202, loss = 0.41587922\n",
            "Iteration 203, loss = 0.41570762\n",
            "Iteration 204, loss = 0.41532069\n",
            "Iteration 205, loss = 0.41475990\n",
            "Iteration 206, loss = 0.41412969\n",
            "Iteration 207, loss = 0.41363051\n",
            "Iteration 208, loss = 0.41337796\n",
            "Iteration 209, loss = 0.41282195\n",
            "Iteration 210, loss = 0.41239031\n",
            "Iteration 211, loss = 0.41189547\n",
            "Iteration 212, loss = 0.41178243\n",
            "Iteration 213, loss = 0.41122208\n",
            "Iteration 214, loss = 0.41079374\n",
            "Iteration 215, loss = 0.41038847\n",
            "Iteration 216, loss = 0.40998945\n",
            "Iteration 217, loss = 0.40977026\n",
            "Iteration 218, loss = 0.40925401\n",
            "Iteration 219, loss = 0.40899540\n",
            "Iteration 220, loss = 0.40864896\n",
            "Iteration 221, loss = 0.40815837\n",
            "Iteration 222, loss = 0.40779421\n",
            "Iteration 223, loss = 0.40747358\n",
            "Iteration 224, loss = 0.40709912\n",
            "Iteration 225, loss = 0.40672093\n",
            "Iteration 226, loss = 0.40634438\n",
            "Iteration 227, loss = 0.40609502\n",
            "Iteration 228, loss = 0.40559849\n",
            "Iteration 229, loss = 0.40532670\n",
            "Iteration 230, loss = 0.40507164\n",
            "Iteration 231, loss = 0.40454763\n",
            "Iteration 232, loss = 0.40429171\n",
            "Iteration 233, loss = 0.40424026\n",
            "Iteration 234, loss = 0.40386101\n",
            "Iteration 235, loss = 0.40332283\n",
            "Iteration 236, loss = 0.40323986\n",
            "Iteration 237, loss = 0.40300281\n",
            "Iteration 238, loss = 0.40251322\n",
            "Iteration 239, loss = 0.40192163\n",
            "Iteration 240, loss = 0.40231351\n",
            "Iteration 241, loss = 0.40184847\n",
            "Iteration 242, loss = 0.40147315\n",
            "Iteration 243, loss = 0.40099117\n",
            "Iteration 244, loss = 0.40074298\n",
            "Iteration 245, loss = 0.40048792\n",
            "Iteration 246, loss = 0.40032779\n",
            "Iteration 247, loss = 0.40001090\n",
            "Iteration 248, loss = 0.39967072\n",
            "Iteration 249, loss = 0.39930393\n",
            "Iteration 250, loss = 0.39912403\n",
            "Iteration 251, loss = 0.39898676\n",
            "Iteration 252, loss = 0.39859586\n",
            "Iteration 253, loss = 0.39837422\n",
            "Iteration 254, loss = 0.39799497\n",
            "Iteration 255, loss = 0.39769622\n",
            "Iteration 256, loss = 0.39762218\n",
            "Iteration 257, loss = 0.39725461\n",
            "Iteration 258, loss = 0.39703363\n",
            "Iteration 259, loss = 0.39686199\n",
            "Iteration 260, loss = 0.39675721\n",
            "Iteration 261, loss = 0.39631386\n",
            "Iteration 262, loss = 0.39612610\n",
            "Iteration 263, loss = 0.39582736\n",
            "Iteration 264, loss = 0.39555020\n",
            "Iteration 265, loss = 0.39530623\n",
            "Iteration 266, loss = 0.39513102\n",
            "Iteration 267, loss = 0.39484775\n",
            "Iteration 268, loss = 0.39446703\n",
            "Iteration 269, loss = 0.39438880\n",
            "Iteration 270, loss = 0.39424151\n",
            "Iteration 271, loss = 0.39389927\n",
            "Iteration 272, loss = 0.39374042\n",
            "Iteration 273, loss = 0.39343917\n",
            "Iteration 274, loss = 0.39324501\n",
            "Iteration 275, loss = 0.39294663\n",
            "Iteration 276, loss = 0.39281896\n",
            "Iteration 277, loss = 0.39257565\n",
            "Iteration 278, loss = 0.39230216\n",
            "Iteration 279, loss = 0.39213691\n",
            "Iteration 280, loss = 0.39194063\n",
            "Iteration 281, loss = 0.39169938\n",
            "Iteration 282, loss = 0.39181965\n",
            "Iteration 283, loss = 0.39143052\n",
            "Iteration 284, loss = 0.39105377\n",
            "Iteration 285, loss = 0.39094776\n",
            "Iteration 286, loss = 0.39073855\n",
            "Iteration 287, loss = 0.39051886\n",
            "Iteration 288, loss = 0.39035613\n",
            "Iteration 289, loss = 0.39018365\n",
            "Iteration 290, loss = 0.38998509\n",
            "Iteration 291, loss = 0.38969925\n",
            "Iteration 292, loss = 0.38955513\n",
            "Iteration 293, loss = 0.38956787\n",
            "Iteration 294, loss = 0.38917415\n",
            "Iteration 295, loss = 0.38889759\n",
            "Iteration 296, loss = 0.38887967\n",
            "Iteration 297, loss = 0.38875644\n",
            "Iteration 298, loss = 0.38854260\n",
            "Iteration 299, loss = 0.38839606\n",
            "Iteration 300, loss = 0.38814826\n",
            "Iteration 301, loss = 0.38820448\n",
            "Iteration 302, loss = 0.38796889\n",
            "Iteration 303, loss = 0.38773423\n",
            "Iteration 304, loss = 0.38736030\n",
            "Iteration 305, loss = 0.38714409\n",
            "Iteration 306, loss = 0.38689655\n",
            "Iteration 307, loss = 0.38688715\n",
            "Iteration 308, loss = 0.38715927\n",
            "Iteration 309, loss = 0.38672759\n",
            "Iteration 310, loss = 0.38640534\n",
            "Iteration 311, loss = 0.38608358\n",
            "Iteration 312, loss = 0.38602004\n",
            "Iteration 313, loss = 0.38587992\n",
            "Iteration 314, loss = 0.38570889\n",
            "Iteration 315, loss = 0.38552486\n",
            "Iteration 316, loss = 0.38537704\n",
            "Iteration 317, loss = 0.38517362\n",
            "Iteration 318, loss = 0.38501637\n",
            "Iteration 319, loss = 0.38476051\n",
            "Iteration 320, loss = 0.38468379\n",
            "Iteration 321, loss = 0.38448975\n",
            "Iteration 322, loss = 0.38433819\n",
            "Iteration 323, loss = 0.38406740\n",
            "Iteration 324, loss = 0.38410427\n",
            "Iteration 325, loss = 0.38399967\n",
            "Iteration 326, loss = 0.38370686\n",
            "Iteration 327, loss = 0.38364859\n",
            "Iteration 328, loss = 0.38341009\n",
            "Iteration 329, loss = 0.38338004\n",
            "Iteration 330, loss = 0.38318731\n",
            "Iteration 331, loss = 0.38303318\n",
            "Iteration 332, loss = 0.38283432\n",
            "Iteration 333, loss = 0.38285000\n",
            "Iteration 334, loss = 0.38265255\n",
            "Iteration 335, loss = 0.38241604\n",
            "Iteration 336, loss = 0.38234899\n",
            "Iteration 337, loss = 0.38214153\n",
            "Iteration 338, loss = 0.38206390\n",
            "Iteration 339, loss = 0.38185461\n",
            "Iteration 340, loss = 0.38192880\n",
            "Iteration 341, loss = 0.38158867\n",
            "Iteration 342, loss = 0.38139982\n",
            "Iteration 343, loss = 0.38128750\n",
            "Iteration 344, loss = 0.38102805\n",
            "Iteration 345, loss = 0.38122186\n",
            "Iteration 346, loss = 0.38104960\n",
            "Iteration 347, loss = 0.38078406\n",
            "Iteration 348, loss = 0.38064125\n",
            "Iteration 349, loss = 0.38054335\n",
            "Iteration 350, loss = 0.38042236\n",
            "Iteration 351, loss = 0.38030473\n",
            "Iteration 352, loss = 0.38027903\n",
            "Iteration 353, loss = 0.38000215\n",
            "Iteration 354, loss = 0.37982132\n",
            "Iteration 355, loss = 0.37993582\n",
            "Iteration 356, loss = 0.37973652\n",
            "Iteration 357, loss = 0.37951751\n",
            "Iteration 358, loss = 0.37945299\n",
            "Iteration 359, loss = 0.37942404\n",
            "Iteration 360, loss = 0.37932529\n",
            "Iteration 361, loss = 0.37904880\n",
            "Iteration 362, loss = 0.37907114\n",
            "Iteration 363, loss = 0.37885030\n",
            "Iteration 364, loss = 0.37886664\n",
            "Iteration 365, loss = 0.37876618\n",
            "Iteration 366, loss = 0.37853055\n",
            "Iteration 367, loss = 0.37866681\n",
            "Iteration 368, loss = 0.37829862\n",
            "Iteration 369, loss = 0.37822546\n",
            "Iteration 370, loss = 0.37798493\n",
            "Iteration 371, loss = 0.37806950\n",
            "Iteration 372, loss = 0.37773337\n",
            "Iteration 373, loss = 0.37769943\n",
            "Iteration 374, loss = 0.37759882\n",
            "Iteration 375, loss = 0.37755632\n",
            "Iteration 376, loss = 0.37757971\n",
            "Iteration 377, loss = 0.37752316\n",
            "Iteration 378, loss = 0.37740093\n",
            "Iteration 379, loss = 0.37712646\n",
            "Iteration 380, loss = 0.37700758\n",
            "Iteration 381, loss = 0.37694299\n",
            "Iteration 382, loss = 0.37681516\n",
            "Iteration 383, loss = 0.37668831\n",
            "Iteration 384, loss = 0.37694825\n",
            "Iteration 385, loss = 0.37644819\n",
            "Iteration 386, loss = 0.37643820\n",
            "Iteration 387, loss = 0.37643425\n",
            "Iteration 388, loss = 0.37658914\n",
            "Iteration 389, loss = 0.37617940\n",
            "Iteration 390, loss = 0.37603819\n",
            "Iteration 391, loss = 0.37615588\n",
            "Iteration 392, loss = 0.37602511\n",
            "Iteration 393, loss = 0.37577572\n",
            "Iteration 394, loss = 0.37552126\n",
            "Iteration 395, loss = 0.37561016\n",
            "Iteration 396, loss = 0.37551785\n",
            "Iteration 397, loss = 0.37561492\n",
            "Iteration 398, loss = 0.37558379\n",
            "Iteration 399, loss = 0.37557297\n",
            "Iteration 400, loss = 0.37536811\n",
            "Iteration 401, loss = 0.37520904\n",
            "Iteration 402, loss = 0.37513946\n",
            "Iteration 403, loss = 0.37497830\n",
            "Iteration 404, loss = 0.37490658\n",
            "Iteration 405, loss = 0.37483761\n",
            "Iteration 406, loss = 0.37471615\n",
            "Iteration 407, loss = 0.37467207\n",
            "Iteration 408, loss = 0.37458062\n",
            "Iteration 409, loss = 0.37457774\n",
            "Iteration 410, loss = 0.37432362\n",
            "Iteration 411, loss = 0.37464855\n",
            "Iteration 412, loss = 0.37440566\n",
            "Iteration 413, loss = 0.37445371\n",
            "Iteration 414, loss = 0.37408415\n",
            "Iteration 415, loss = 0.37403779\n",
            "Iteration 416, loss = 0.37385151\n",
            "Iteration 417, loss = 0.37389413\n",
            "Iteration 418, loss = 0.37395406\n",
            "Iteration 419, loss = 0.37396731\n",
            "Iteration 420, loss = 0.37372685\n",
            "Iteration 421, loss = 0.37362856\n",
            "Iteration 422, loss = 0.37346445\n",
            "Iteration 423, loss = 0.37343436\n",
            "Iteration 424, loss = 0.37325637\n",
            "Iteration 425, loss = 0.37326933\n",
            "Iteration 426, loss = 0.37325173\n",
            "Iteration 427, loss = 0.37314795\n",
            "Iteration 428, loss = 0.37319366\n",
            "Iteration 429, loss = 0.37292708\n",
            "Iteration 430, loss = 0.37281924\n",
            "Iteration 431, loss = 0.37324860\n",
            "Iteration 432, loss = 0.37307251\n",
            "Iteration 433, loss = 0.37283299\n",
            "Iteration 434, loss = 0.37265649\n",
            "Iteration 435, loss = 0.37234174\n",
            "Iteration 436, loss = 0.37249712\n",
            "Iteration 437, loss = 0.37258644\n",
            "Iteration 438, loss = 0.37256408\n",
            "Iteration 439, loss = 0.37240264\n",
            "Iteration 440, loss = 0.37208523\n",
            "Iteration 441, loss = 0.37202773\n",
            "Iteration 442, loss = 0.37200754\n",
            "Iteration 443, loss = 0.37199732\n",
            "Iteration 444, loss = 0.37183676\n",
            "Iteration 445, loss = 0.37174773\n",
            "Iteration 446, loss = 0.37172378\n",
            "Iteration 447, loss = 0.37161781\n",
            "Iteration 448, loss = 0.37164010\n",
            "Iteration 449, loss = 0.37163287\n",
            "Iteration 450, loss = 0.37169560\n",
            "Iteration 451, loss = 0.37153692\n",
            "Iteration 452, loss = 0.37148732\n",
            "Iteration 453, loss = 0.37127983\n",
            "Iteration 454, loss = 0.37118394\n",
            "Iteration 455, loss = 0.37120059\n",
            "Iteration 456, loss = 0.37118906\n",
            "Iteration 457, loss = 0.37117926\n",
            "Iteration 458, loss = 0.37112378\n",
            "Iteration 459, loss = 0.37101292\n",
            "Iteration 460, loss = 0.37138769\n",
            "Iteration 461, loss = 0.37075924\n",
            "Iteration 462, loss = 0.37075149\n",
            "Iteration 463, loss = 0.37062399\n",
            "Iteration 464, loss = 0.37066864\n",
            "Iteration 465, loss = 0.37056873\n",
            "Iteration 466, loss = 0.37059289\n",
            "Iteration 467, loss = 0.37050713\n",
            "Iteration 468, loss = 0.37041962\n",
            "Iteration 469, loss = 0.37037592\n",
            "Iteration 470, loss = 0.37032125\n",
            "Iteration 471, loss = 0.37036465\n",
            "Iteration 472, loss = 0.37026273\n",
            "Iteration 473, loss = 0.37034263\n",
            "Iteration 474, loss = 0.37014440\n",
            "Iteration 475, loss = 0.36999281\n",
            "Iteration 476, loss = 0.36986443\n",
            "Iteration 477, loss = 0.37008731\n",
            "Iteration 478, loss = 0.37020902\n",
            "Iteration 479, loss = 0.37007104\n",
            "Iteration 480, loss = 0.36972098\n",
            "Iteration 481, loss = 0.36967144\n",
            "Iteration 482, loss = 0.36962035\n",
            "Iteration 483, loss = 0.36963914\n",
            "Iteration 484, loss = 0.36952405\n",
            "Iteration 485, loss = 0.36947696\n",
            "Iteration 486, loss = 0.36955265\n",
            "Iteration 487, loss = 0.36937959\n",
            "Iteration 488, loss = 0.36942060\n",
            "Iteration 489, loss = 0.36927600\n",
            "Iteration 490, loss = 0.36927324\n",
            "Iteration 491, loss = 0.36919786\n",
            "Iteration 492, loss = 0.36909163\n",
            "Iteration 493, loss = 0.36906974\n",
            "Iteration 494, loss = 0.36905380\n",
            "Iteration 495, loss = 0.36917400\n",
            "Iteration 496, loss = 0.36924389\n",
            "Iteration 497, loss = 0.36906317\n",
            "Iteration 498, loss = 0.36883365\n",
            "Iteration 499, loss = 0.36880508\n",
            "Iteration 500, loss = 0.36892193\n",
            "Iteration 501, loss = 0.36873072\n",
            "Iteration 502, loss = 0.36876820\n",
            "Iteration 503, loss = 0.36871531\n",
            "Iteration 504, loss = 0.36864154\n",
            "Iteration 505, loss = 0.36849499\n",
            "Iteration 506, loss = 0.36851674\n",
            "Iteration 507, loss = 0.36826259\n",
            "Iteration 508, loss = 0.36840352\n",
            "Iteration 509, loss = 0.36851573\n",
            "Iteration 510, loss = 0.36830495\n",
            "Iteration 511, loss = 0.36818705\n",
            "Iteration 512, loss = 0.36829507\n",
            "Iteration 513, loss = 0.36821352\n",
            "Iteration 514, loss = 0.36838079\n",
            "Iteration 515, loss = 0.36819273\n",
            "Iteration 516, loss = 0.36810404\n",
            "Iteration 517, loss = 0.36800431\n",
            "Iteration 518, loss = 0.36798437\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.75881546\n",
            "Iteration 2, loss = 0.75346722\n",
            "Iteration 3, loss = 0.74839520\n",
            "Iteration 4, loss = 0.74370273\n",
            "Iteration 5, loss = 0.73880438\n",
            "Iteration 6, loss = 0.73463356\n",
            "Iteration 7, loss = 0.73044612\n",
            "Iteration 8, loss = 0.72639426\n",
            "Iteration 9, loss = 0.72273030\n",
            "Iteration 10, loss = 0.71881835\n",
            "Iteration 11, loss = 0.71561209\n",
            "Iteration 12, loss = 0.71253459\n",
            "Iteration 13, loss = 0.70933421\n",
            "Iteration 14, loss = 0.70643104\n",
            "Iteration 15, loss = 0.70383568\n",
            "Iteration 16, loss = 0.70094925\n",
            "Iteration 17, loss = 0.69860265\n",
            "Iteration 18, loss = 0.69627842\n",
            "Iteration 19, loss = 0.69386896\n",
            "Iteration 20, loss = 0.69177832\n",
            "Iteration 21, loss = 0.68975885\n",
            "Iteration 22, loss = 0.68765178\n",
            "Iteration 23, loss = 0.68576690\n",
            "Iteration 24, loss = 0.68393863\n",
            "Iteration 25, loss = 0.68206593\n",
            "Iteration 26, loss = 0.68031263\n",
            "Iteration 27, loss = 0.67849115\n",
            "Iteration 28, loss = 0.67665292\n",
            "Iteration 29, loss = 0.67491960\n",
            "Iteration 30, loss = 0.67317453\n",
            "Iteration 31, loss = 0.67143114\n",
            "Iteration 32, loss = 0.66973743\n",
            "Iteration 33, loss = 0.66793570\n",
            "Iteration 34, loss = 0.66629663\n",
            "Iteration 35, loss = 0.66470663\n",
            "Iteration 36, loss = 0.66291448\n",
            "Iteration 37, loss = 0.66124130\n",
            "Iteration 38, loss = 0.65954948\n",
            "Iteration 39, loss = 0.65797177\n",
            "Iteration 40, loss = 0.65614870\n",
            "Iteration 41, loss = 0.65452935\n",
            "Iteration 42, loss = 0.65282210\n",
            "Iteration 43, loss = 0.65112667\n",
            "Iteration 44, loss = 0.64947194\n",
            "Iteration 45, loss = 0.64773789\n",
            "Iteration 46, loss = 0.64612235\n",
            "Iteration 47, loss = 0.64440125\n",
            "Iteration 48, loss = 0.64270067\n",
            "Iteration 49, loss = 0.64091470\n",
            "Iteration 50, loss = 0.63926965\n",
            "Iteration 51, loss = 0.63755701\n",
            "Iteration 52, loss = 0.63591244\n",
            "Iteration 53, loss = 0.63416663\n",
            "Iteration 54, loss = 0.63254602\n",
            "Iteration 55, loss = 0.63068683\n",
            "Iteration 56, loss = 0.62903625\n",
            "Iteration 57, loss = 0.62723641\n",
            "Iteration 58, loss = 0.62552816\n",
            "Iteration 59, loss = 0.62373396\n",
            "Iteration 60, loss = 0.62194768\n",
            "Iteration 61, loss = 0.62031078\n",
            "Iteration 62, loss = 0.61858647\n",
            "Iteration 63, loss = 0.61668939\n",
            "Iteration 64, loss = 0.61502946\n",
            "Iteration 65, loss = 0.61338180\n",
            "Iteration 66, loss = 0.61148210\n",
            "Iteration 67, loss = 0.60998413\n",
            "Iteration 68, loss = 0.60806165\n",
            "Iteration 69, loss = 0.60635721\n",
            "Iteration 70, loss = 0.60459760\n",
            "Iteration 71, loss = 0.60280133\n",
            "Iteration 72, loss = 0.60112619\n",
            "Iteration 73, loss = 0.59933919\n",
            "Iteration 74, loss = 0.59746808\n",
            "Iteration 75, loss = 0.59569626\n",
            "Iteration 76, loss = 0.59384612\n",
            "Iteration 77, loss = 0.59198095\n",
            "Iteration 78, loss = 0.58999706\n",
            "Iteration 79, loss = 0.58818952\n",
            "Iteration 80, loss = 0.58616389\n",
            "Iteration 81, loss = 0.58409001\n",
            "Iteration 82, loss = 0.58211640\n",
            "Iteration 83, loss = 0.58002250\n",
            "Iteration 84, loss = 0.57795645\n",
            "Iteration 85, loss = 0.57582792\n",
            "Iteration 86, loss = 0.57356636\n",
            "Iteration 87, loss = 0.57125110\n",
            "Iteration 88, loss = 0.56900466\n",
            "Iteration 89, loss = 0.56656855\n",
            "Iteration 90, loss = 0.56420606\n",
            "Iteration 91, loss = 0.56185288\n",
            "Iteration 92, loss = 0.55923808\n",
            "Iteration 93, loss = 0.55661283\n",
            "Iteration 94, loss = 0.55411199\n",
            "Iteration 95, loss = 0.55133412\n",
            "Iteration 96, loss = 0.54877348\n",
            "Iteration 97, loss = 0.54608377\n",
            "Iteration 98, loss = 0.54332552\n",
            "Iteration 99, loss = 0.54058762\n",
            "Iteration 100, loss = 0.53797760\n",
            "Iteration 101, loss = 0.53534111\n",
            "Iteration 102, loss = 0.53262467\n",
            "Iteration 103, loss = 0.52972567\n",
            "Iteration 104, loss = 0.52711472\n",
            "Iteration 105, loss = 0.52432578\n",
            "Iteration 106, loss = 0.52160870\n",
            "Iteration 107, loss = 0.51894308\n",
            "Iteration 108, loss = 0.51626896\n",
            "Iteration 109, loss = 0.51356002\n",
            "Iteration 110, loss = 0.51090625\n",
            "Iteration 111, loss = 0.50827815\n",
            "Iteration 112, loss = 0.50570397\n",
            "Iteration 113, loss = 0.50312069\n",
            "Iteration 114, loss = 0.50064213\n",
            "Iteration 115, loss = 0.49806757\n",
            "Iteration 116, loss = 0.49568199\n",
            "Iteration 117, loss = 0.49332204\n",
            "Iteration 118, loss = 0.49070561\n",
            "Iteration 119, loss = 0.48850597\n",
            "Iteration 120, loss = 0.48626717\n",
            "Iteration 121, loss = 0.48395552\n",
            "Iteration 122, loss = 0.48157425\n",
            "Iteration 123, loss = 0.47935214\n",
            "Iteration 124, loss = 0.47730095\n",
            "Iteration 125, loss = 0.47512980\n",
            "Iteration 126, loss = 0.47325677\n",
            "Iteration 127, loss = 0.47097955\n",
            "Iteration 128, loss = 0.46900698\n",
            "Iteration 129, loss = 0.46708058\n",
            "Iteration 130, loss = 0.46525817\n",
            "Iteration 131, loss = 0.46346294\n",
            "Iteration 132, loss = 0.46184875\n",
            "Iteration 133, loss = 0.46012884\n",
            "Iteration 134, loss = 0.45862115\n",
            "Iteration 135, loss = 0.45708762\n",
            "Iteration 136, loss = 0.45573716\n",
            "Iteration 137, loss = 0.45430643\n",
            "Iteration 138, loss = 0.45316990\n",
            "Iteration 139, loss = 0.45183603\n",
            "Iteration 140, loss = 0.45065616\n",
            "Iteration 141, loss = 0.44972114\n",
            "Iteration 142, loss = 0.44874211\n",
            "Iteration 143, loss = 0.44783894\n",
            "Iteration 144, loss = 0.44693500\n",
            "Iteration 145, loss = 0.44599648\n",
            "Iteration 146, loss = 0.44538765\n",
            "Iteration 147, loss = 0.44456735\n",
            "Iteration 148, loss = 0.44401268\n",
            "Iteration 149, loss = 0.44333060\n",
            "Iteration 150, loss = 0.44264835\n",
            "Iteration 151, loss = 0.44206976\n",
            "Iteration 152, loss = 0.44149101\n",
            "Iteration 153, loss = 0.44108274\n",
            "Iteration 154, loss = 0.44059317\n",
            "Iteration 155, loss = 0.44002392\n",
            "Iteration 156, loss = 0.43950808\n",
            "Iteration 157, loss = 0.43904998\n",
            "Iteration 158, loss = 0.43855160\n",
            "Iteration 159, loss = 0.43817401\n",
            "Iteration 160, loss = 0.43769081\n",
            "Iteration 161, loss = 0.43739959\n",
            "Iteration 162, loss = 0.43689781\n",
            "Iteration 163, loss = 0.43642292\n",
            "Iteration 164, loss = 0.43600780\n",
            "Iteration 165, loss = 0.43581373\n",
            "Iteration 166, loss = 0.43544728\n",
            "Iteration 167, loss = 0.43509750\n",
            "Iteration 168, loss = 0.43463653\n",
            "Iteration 169, loss = 0.43441117\n",
            "Iteration 170, loss = 0.43403418\n",
            "Iteration 171, loss = 0.43369451\n",
            "Iteration 172, loss = 0.43336359\n",
            "Iteration 173, loss = 0.43309793\n",
            "Iteration 174, loss = 0.43278168\n",
            "Iteration 175, loss = 0.43246897\n",
            "Iteration 176, loss = 0.43213476\n",
            "Iteration 177, loss = 0.43184768\n",
            "Iteration 178, loss = 0.43150758\n",
            "Iteration 179, loss = 0.43131653\n",
            "Iteration 180, loss = 0.43108506\n",
            "Iteration 181, loss = 0.43073355\n",
            "Iteration 182, loss = 0.43039641\n",
            "Iteration 183, loss = 0.43011256\n",
            "Iteration 184, loss = 0.42984029\n",
            "Iteration 185, loss = 0.42974289\n",
            "Iteration 186, loss = 0.42912354\n",
            "Iteration 187, loss = 0.42891259\n",
            "Iteration 188, loss = 0.42872864\n",
            "Iteration 189, loss = 0.42835200\n",
            "Iteration 190, loss = 0.42806992\n",
            "Iteration 191, loss = 0.42785907\n",
            "Iteration 192, loss = 0.42758480\n",
            "Iteration 193, loss = 0.42719870\n",
            "Iteration 194, loss = 0.42677574\n",
            "Iteration 195, loss = 0.42637745\n",
            "Iteration 196, loss = 0.42626194\n",
            "Iteration 197, loss = 0.42587926\n",
            "Iteration 198, loss = 0.42561166\n",
            "Iteration 199, loss = 0.42525631\n",
            "Iteration 200, loss = 0.42511957\n",
            "Iteration 201, loss = 0.42467339\n",
            "Iteration 202, loss = 0.42409326\n",
            "Iteration 203, loss = 0.42397234\n",
            "Iteration 204, loss = 0.42385035\n",
            "Iteration 205, loss = 0.42347282\n",
            "Iteration 206, loss = 0.42304493\n",
            "Iteration 207, loss = 0.42251823\n",
            "Iteration 208, loss = 0.42225042\n",
            "Iteration 209, loss = 0.42160719\n",
            "Iteration 210, loss = 0.42130207\n",
            "Iteration 211, loss = 0.42097265\n",
            "Iteration 212, loss = 0.42085755\n",
            "Iteration 213, loss = 0.42035607\n",
            "Iteration 214, loss = 0.41996521\n",
            "Iteration 215, loss = 0.41951494\n",
            "Iteration 216, loss = 0.41919946\n",
            "Iteration 217, loss = 0.41900308\n",
            "Iteration 218, loss = 0.41855920\n",
            "Iteration 219, loss = 0.41822386\n",
            "Iteration 220, loss = 0.41796683\n",
            "Iteration 221, loss = 0.41756722\n",
            "Iteration 222, loss = 0.41728740\n",
            "Iteration 223, loss = 0.41703025\n",
            "Iteration 224, loss = 0.41670435\n",
            "Iteration 225, loss = 0.41638740\n",
            "Iteration 226, loss = 0.41608388\n",
            "Iteration 227, loss = 0.41584354\n",
            "Iteration 228, loss = 0.41547699\n",
            "Iteration 229, loss = 0.41521500\n",
            "Iteration 230, loss = 0.41500812\n",
            "Iteration 231, loss = 0.41464922\n",
            "Iteration 232, loss = 0.41437591\n",
            "Iteration 233, loss = 0.41426399\n",
            "Iteration 234, loss = 0.41392161\n",
            "Iteration 235, loss = 0.41358265\n",
            "Iteration 236, loss = 0.41345913\n",
            "Iteration 237, loss = 0.41334140\n",
            "Iteration 238, loss = 0.41274667\n",
            "Iteration 239, loss = 0.41237674\n",
            "Iteration 240, loss = 0.41268323\n",
            "Iteration 241, loss = 0.41251700\n",
            "Iteration 242, loss = 0.41220721\n",
            "Iteration 243, loss = 0.41172840\n",
            "Iteration 244, loss = 0.41148258\n",
            "Iteration 245, loss = 0.41114487\n",
            "Iteration 246, loss = 0.41103407\n",
            "Iteration 247, loss = 0.41077195\n",
            "Iteration 248, loss = 0.41057311\n",
            "Iteration 249, loss = 0.41024411\n",
            "Iteration 250, loss = 0.41006235\n",
            "Iteration 251, loss = 0.40991478\n",
            "Iteration 252, loss = 0.40966687\n",
            "Iteration 253, loss = 0.40951804\n",
            "Iteration 254, loss = 0.40920016\n",
            "Iteration 255, loss = 0.40903168\n",
            "Iteration 256, loss = 0.40893780\n",
            "Iteration 257, loss = 0.40868431\n",
            "Iteration 258, loss = 0.40838289\n",
            "Iteration 259, loss = 0.40816410\n",
            "Iteration 260, loss = 0.40834273\n",
            "Iteration 261, loss = 0.40784961\n",
            "Iteration 262, loss = 0.40758198\n",
            "Iteration 263, loss = 0.40730243\n",
            "Iteration 264, loss = 0.40704665\n",
            "Iteration 265, loss = 0.40684231\n",
            "Iteration 266, loss = 0.40678757\n",
            "Iteration 267, loss = 0.40646355\n",
            "Iteration 268, loss = 0.40612789\n",
            "Iteration 269, loss = 0.40596817\n",
            "Iteration 270, loss = 0.40598174\n",
            "Iteration 271, loss = 0.40563370\n",
            "Iteration 272, loss = 0.40554930\n",
            "Iteration 273, loss = 0.40525556\n",
            "Iteration 274, loss = 0.40504273\n",
            "Iteration 275, loss = 0.40474254\n",
            "Iteration 276, loss = 0.40457390\n",
            "Iteration 277, loss = 0.40443388\n",
            "Iteration 278, loss = 0.40423636\n",
            "Iteration 279, loss = 0.40404818\n",
            "Iteration 280, loss = 0.40383694\n",
            "Iteration 281, loss = 0.40365074\n",
            "Iteration 282, loss = 0.40368490\n",
            "Iteration 283, loss = 0.40341216\n",
            "Iteration 284, loss = 0.40309163\n",
            "Iteration 285, loss = 0.40296713\n",
            "Iteration 286, loss = 0.40275490\n",
            "Iteration 287, loss = 0.40257933\n",
            "Iteration 288, loss = 0.40246747\n",
            "Iteration 289, loss = 0.40225584\n",
            "Iteration 290, loss = 0.40208289\n",
            "Iteration 291, loss = 0.40189350\n",
            "Iteration 292, loss = 0.40175437\n",
            "Iteration 293, loss = 0.40155058\n",
            "Iteration 294, loss = 0.40124003\n",
            "Iteration 295, loss = 0.40110156\n",
            "Iteration 296, loss = 0.40098876\n",
            "Iteration 297, loss = 0.40075378\n",
            "Iteration 298, loss = 0.40060685\n",
            "Iteration 299, loss = 0.40045674\n",
            "Iteration 300, loss = 0.40023559\n",
            "Iteration 301, loss = 0.40015045\n",
            "Iteration 302, loss = 0.39995632\n",
            "Iteration 303, loss = 0.39981255\n",
            "Iteration 304, loss = 0.39969835\n",
            "Iteration 305, loss = 0.39935756\n",
            "Iteration 306, loss = 0.39916540\n",
            "Iteration 307, loss = 0.39915023\n",
            "Iteration 308, loss = 0.39955199\n",
            "Iteration 309, loss = 0.39910755\n",
            "Iteration 310, loss = 0.39888647\n",
            "Iteration 311, loss = 0.39848519\n",
            "Iteration 312, loss = 0.39840837\n",
            "Iteration 313, loss = 0.39839583\n",
            "Iteration 314, loss = 0.39823759\n",
            "Iteration 315, loss = 0.39796511\n",
            "Iteration 316, loss = 0.39779104\n",
            "Iteration 317, loss = 0.39754414\n",
            "Iteration 318, loss = 0.39744635\n",
            "Iteration 319, loss = 0.39715064\n",
            "Iteration 320, loss = 0.39705692\n",
            "Iteration 321, loss = 0.39692000\n",
            "Iteration 322, loss = 0.39683632\n",
            "Iteration 323, loss = 0.39654663\n",
            "Iteration 324, loss = 0.39659001\n",
            "Iteration 325, loss = 0.39636620\n",
            "Iteration 326, loss = 0.39614698\n",
            "Iteration 327, loss = 0.39607878\n",
            "Iteration 328, loss = 0.39589051\n",
            "Iteration 329, loss = 0.39582997\n",
            "Iteration 330, loss = 0.39566793\n",
            "Iteration 331, loss = 0.39552015\n",
            "Iteration 332, loss = 0.39531619\n",
            "Iteration 333, loss = 0.39537217\n",
            "Iteration 334, loss = 0.39512223\n",
            "Iteration 335, loss = 0.39489772\n",
            "Iteration 336, loss = 0.39476334\n",
            "Iteration 337, loss = 0.39459803\n",
            "Iteration 338, loss = 0.39451009\n",
            "Iteration 339, loss = 0.39427898\n",
            "Iteration 340, loss = 0.39426945\n",
            "Iteration 341, loss = 0.39400666\n",
            "Iteration 342, loss = 0.39385419\n",
            "Iteration 343, loss = 0.39374604\n",
            "Iteration 344, loss = 0.39351793\n",
            "Iteration 345, loss = 0.39361119\n",
            "Iteration 346, loss = 0.39360829\n",
            "Iteration 347, loss = 0.39322471\n",
            "Iteration 348, loss = 0.39311489\n",
            "Iteration 349, loss = 0.39301627\n",
            "Iteration 350, loss = 0.39300256\n",
            "Iteration 351, loss = 0.39282305\n",
            "Iteration 352, loss = 0.39261799\n",
            "Iteration 353, loss = 0.39247043\n",
            "Iteration 354, loss = 0.39222303\n",
            "Iteration 355, loss = 0.39227224\n",
            "Iteration 356, loss = 0.39207704\n",
            "Iteration 357, loss = 0.39199045\n",
            "Iteration 358, loss = 0.39177953\n",
            "Iteration 359, loss = 0.39177893\n",
            "Iteration 360, loss = 0.39177138\n",
            "Iteration 361, loss = 0.39150021\n",
            "Iteration 362, loss = 0.39144238\n",
            "Iteration 363, loss = 0.39127243\n",
            "Iteration 364, loss = 0.39121650\n",
            "Iteration 365, loss = 0.39107009\n",
            "Iteration 366, loss = 0.39087917\n",
            "Iteration 367, loss = 0.39091188\n",
            "Iteration 368, loss = 0.39078459\n",
            "Iteration 369, loss = 0.39076519\n",
            "Iteration 370, loss = 0.39056323\n",
            "Iteration 371, loss = 0.39052317\n",
            "Iteration 372, loss = 0.39012668\n",
            "Iteration 373, loss = 0.39010826\n",
            "Iteration 374, loss = 0.39007324\n",
            "Iteration 375, loss = 0.38997671\n",
            "Iteration 376, loss = 0.38983485\n",
            "Iteration 377, loss = 0.38982957\n",
            "Iteration 378, loss = 0.38956044\n",
            "Iteration 379, loss = 0.38940026\n",
            "Iteration 380, loss = 0.38937671\n",
            "Iteration 381, loss = 0.38926040\n",
            "Iteration 382, loss = 0.38915183\n",
            "Iteration 383, loss = 0.38904930\n",
            "Iteration 384, loss = 0.38920056\n",
            "Iteration 385, loss = 0.38884657\n",
            "Iteration 386, loss = 0.38883437\n",
            "Iteration 387, loss = 0.38877303\n",
            "Iteration 388, loss = 0.38881915\n",
            "Iteration 389, loss = 0.38848636\n",
            "Iteration 390, loss = 0.38837402\n",
            "Iteration 391, loss = 0.38825848\n",
            "Iteration 392, loss = 0.38831483\n",
            "Iteration 393, loss = 0.38814406\n",
            "Iteration 394, loss = 0.38792330\n",
            "Iteration 395, loss = 0.38785551\n",
            "Iteration 396, loss = 0.38770862\n",
            "Iteration 397, loss = 0.38785428\n",
            "Iteration 398, loss = 0.38789920\n",
            "Iteration 399, loss = 0.38803086\n",
            "Iteration 400, loss = 0.38788236\n",
            "Iteration 401, loss = 0.38761503\n",
            "Iteration 402, loss = 0.38757720\n",
            "Iteration 403, loss = 0.38726832\n",
            "Iteration 404, loss = 0.38718484\n",
            "Iteration 405, loss = 0.38709113\n",
            "Iteration 406, loss = 0.38697708\n",
            "Iteration 407, loss = 0.38691084\n",
            "Iteration 408, loss = 0.38678775\n",
            "Iteration 409, loss = 0.38679306\n",
            "Iteration 410, loss = 0.38655677\n",
            "Iteration 411, loss = 0.38690668\n",
            "Iteration 412, loss = 0.38653502\n",
            "Iteration 413, loss = 0.38659635\n",
            "Iteration 414, loss = 0.38622133\n",
            "Iteration 415, loss = 0.38618545\n",
            "Iteration 416, loss = 0.38611743\n",
            "Iteration 417, loss = 0.38617907\n",
            "Iteration 418, loss = 0.38606209\n",
            "Iteration 419, loss = 0.38592044\n",
            "Iteration 420, loss = 0.38577807\n",
            "Iteration 421, loss = 0.38562515\n",
            "Iteration 422, loss = 0.38555994\n",
            "Iteration 423, loss = 0.38548264\n",
            "Iteration 424, loss = 0.38541572\n",
            "Iteration 425, loss = 0.38539925\n",
            "Iteration 426, loss = 0.38531421\n",
            "Iteration 427, loss = 0.38514423\n",
            "Iteration 428, loss = 0.38514550\n",
            "Iteration 429, loss = 0.38500099\n",
            "Iteration 430, loss = 0.38495629\n",
            "Iteration 431, loss = 0.38520192\n",
            "Iteration 432, loss = 0.38498805\n",
            "Iteration 433, loss = 0.38481987\n",
            "Iteration 434, loss = 0.38469226\n",
            "Iteration 435, loss = 0.38450187\n",
            "Iteration 436, loss = 0.38462054\n",
            "Iteration 437, loss = 0.38459317\n",
            "Iteration 438, loss = 0.38462100\n",
            "Iteration 439, loss = 0.38439336\n",
            "Iteration 440, loss = 0.38422635\n",
            "Iteration 441, loss = 0.38429337\n",
            "Iteration 442, loss = 0.38410829\n",
            "Iteration 443, loss = 0.38412764\n",
            "Iteration 444, loss = 0.38404698\n",
            "Iteration 445, loss = 0.38392714\n",
            "Iteration 446, loss = 0.38392171\n",
            "Iteration 447, loss = 0.38382876\n",
            "Iteration 448, loss = 0.38373472\n",
            "Iteration 449, loss = 0.38370066\n",
            "Iteration 450, loss = 0.38366131\n",
            "Iteration 451, loss = 0.38355311\n",
            "Iteration 452, loss = 0.38356126\n",
            "Iteration 453, loss = 0.38348558\n",
            "Iteration 454, loss = 0.38337428\n",
            "Iteration 455, loss = 0.38341389\n",
            "Iteration 456, loss = 0.38329504\n",
            "Iteration 457, loss = 0.38318683\n",
            "Iteration 458, loss = 0.38317622\n",
            "Iteration 459, loss = 0.38305945\n",
            "Iteration 460, loss = 0.38331386\n",
            "Iteration 461, loss = 0.38294645\n",
            "Iteration 462, loss = 0.38292096\n",
            "Iteration 463, loss = 0.38280668\n",
            "Iteration 464, loss = 0.38285864\n",
            "Iteration 465, loss = 0.38275244\n",
            "Iteration 466, loss = 0.38272896\n",
            "Iteration 467, loss = 0.38263072\n",
            "Iteration 468, loss = 0.38260133\n",
            "Iteration 469, loss = 0.38253103\n",
            "Iteration 470, loss = 0.38255123\n",
            "Iteration 471, loss = 0.38244408\n",
            "Iteration 472, loss = 0.38248170\n",
            "Iteration 473, loss = 0.38261436\n",
            "Iteration 474, loss = 0.38224897\n",
            "Iteration 475, loss = 0.38217483\n",
            "Iteration 476, loss = 0.38207505\n",
            "Iteration 477, loss = 0.38226676\n",
            "Iteration 478, loss = 0.38221110\n",
            "Iteration 479, loss = 0.38211225\n",
            "Iteration 480, loss = 0.38185749\n",
            "Iteration 481, loss = 0.38182298\n",
            "Iteration 482, loss = 0.38168020\n",
            "Iteration 483, loss = 0.38162220\n",
            "Iteration 484, loss = 0.38156514\n",
            "Iteration 485, loss = 0.38153073\n",
            "Iteration 486, loss = 0.38164617\n",
            "Iteration 487, loss = 0.38146356\n",
            "Iteration 488, loss = 0.38157674\n",
            "Iteration 489, loss = 0.38135991\n",
            "Iteration 490, loss = 0.38131489\n",
            "Iteration 491, loss = 0.38131130\n",
            "Iteration 492, loss = 0.38122239\n",
            "Iteration 493, loss = 0.38118257\n",
            "Iteration 494, loss = 0.38112773\n",
            "Iteration 495, loss = 0.38131913\n",
            "Iteration 496, loss = 0.38131464\n",
            "Iteration 497, loss = 0.38103095\n",
            "Iteration 498, loss = 0.38094762\n",
            "Iteration 499, loss = 0.38083520\n",
            "Iteration 500, loss = 0.38083296\n",
            "Iteration 501, loss = 0.38088889\n",
            "Iteration 502, loss = 0.38086292\n",
            "Iteration 503, loss = 0.38093126\n",
            "Iteration 504, loss = 0.38076417\n",
            "Iteration 505, loss = 0.38067627\n",
            "Iteration 506, loss = 0.38061221\n",
            "Iteration 507, loss = 0.38043552\n",
            "Iteration 508, loss = 0.38048483\n",
            "Iteration 509, loss = 0.38067247\n",
            "Iteration 510, loss = 0.38039629\n",
            "Iteration 511, loss = 0.38035462\n",
            "Iteration 512, loss = 0.38045155\n",
            "Iteration 513, loss = 0.38028511\n",
            "Iteration 514, loss = 0.38047378\n",
            "Iteration 515, loss = 0.38038259\n",
            "Iteration 516, loss = 0.38026637\n",
            "Iteration 517, loss = 0.38014990\n",
            "Iteration 518, loss = 0.38020874\n",
            "Iteration 519, loss = 0.38009115\n",
            "Iteration 520, loss = 0.37995786\n",
            "Iteration 521, loss = 0.38004218\n",
            "Iteration 522, loss = 0.37997374\n",
            "Iteration 523, loss = 0.37996273\n",
            "Iteration 524, loss = 0.37987380\n",
            "Iteration 525, loss = 0.37984791\n",
            "Iteration 526, loss = 0.37977001\n",
            "Iteration 527, loss = 0.38010814\n",
            "Iteration 528, loss = 0.37973828\n",
            "Iteration 529, loss = 0.37967613\n",
            "Iteration 530, loss = 0.37965742\n",
            "Iteration 531, loss = 0.37959053\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.75891501\n",
            "Iteration 2, loss = 0.75357549\n",
            "Iteration 3, loss = 0.74848304\n",
            "Iteration 4, loss = 0.74386753\n",
            "Iteration 5, loss = 0.73910571\n",
            "Iteration 6, loss = 0.73485746\n",
            "Iteration 7, loss = 0.73093887\n",
            "Iteration 8, loss = 0.72698997\n",
            "Iteration 9, loss = 0.72335252\n",
            "Iteration 10, loss = 0.71964206\n",
            "Iteration 11, loss = 0.71660568\n",
            "Iteration 12, loss = 0.71367950\n",
            "Iteration 13, loss = 0.71057258\n",
            "Iteration 14, loss = 0.70784096\n",
            "Iteration 15, loss = 0.70539138\n",
            "Iteration 16, loss = 0.70264305\n",
            "Iteration 17, loss = 0.70047959\n",
            "Iteration 18, loss = 0.69825224\n",
            "Iteration 19, loss = 0.69607673\n",
            "Iteration 20, loss = 0.69403587\n",
            "Iteration 21, loss = 0.69213976\n",
            "Iteration 22, loss = 0.69014389\n",
            "Iteration 23, loss = 0.68831953\n",
            "Iteration 24, loss = 0.68660000\n",
            "Iteration 25, loss = 0.68480604\n",
            "Iteration 26, loss = 0.68313158\n",
            "Iteration 27, loss = 0.68133000\n",
            "Iteration 28, loss = 0.67964709\n",
            "Iteration 29, loss = 0.67799500\n",
            "Iteration 30, loss = 0.67633717\n",
            "Iteration 31, loss = 0.67465897\n",
            "Iteration 32, loss = 0.67310877\n",
            "Iteration 33, loss = 0.67138547\n",
            "Iteration 34, loss = 0.66981162\n",
            "Iteration 35, loss = 0.66824772\n",
            "Iteration 36, loss = 0.66656635\n",
            "Iteration 37, loss = 0.66499178\n",
            "Iteration 38, loss = 0.66339096\n",
            "Iteration 39, loss = 0.66184958\n",
            "Iteration 40, loss = 0.66008675\n",
            "Iteration 41, loss = 0.65854082\n",
            "Iteration 42, loss = 0.65690076\n",
            "Iteration 43, loss = 0.65521692\n",
            "Iteration 44, loss = 0.65365277\n",
            "Iteration 45, loss = 0.65188264\n",
            "Iteration 46, loss = 0.65030073\n",
            "Iteration 47, loss = 0.64861109\n",
            "Iteration 48, loss = 0.64686204\n",
            "Iteration 49, loss = 0.64516030\n",
            "Iteration 50, loss = 0.64343757\n",
            "Iteration 51, loss = 0.64169007\n",
            "Iteration 52, loss = 0.64006290\n",
            "Iteration 53, loss = 0.63826896\n",
            "Iteration 54, loss = 0.63660636\n",
            "Iteration 55, loss = 0.63477768\n",
            "Iteration 56, loss = 0.63309106\n",
            "Iteration 57, loss = 0.63116532\n",
            "Iteration 58, loss = 0.62946298\n",
            "Iteration 59, loss = 0.62756410\n",
            "Iteration 60, loss = 0.62572838\n",
            "Iteration 61, loss = 0.62392302\n",
            "Iteration 62, loss = 0.62214239\n",
            "Iteration 63, loss = 0.62016912\n",
            "Iteration 64, loss = 0.61840499\n",
            "Iteration 65, loss = 0.61655710\n",
            "Iteration 66, loss = 0.61462462\n",
            "Iteration 67, loss = 0.61283788\n",
            "Iteration 68, loss = 0.61083037\n",
            "Iteration 69, loss = 0.60888330\n",
            "Iteration 70, loss = 0.60692498\n",
            "Iteration 71, loss = 0.60491944\n",
            "Iteration 72, loss = 0.60297183\n",
            "Iteration 73, loss = 0.60097165\n",
            "Iteration 74, loss = 0.59881139\n",
            "Iteration 75, loss = 0.59665839\n",
            "Iteration 76, loss = 0.59459505\n",
            "Iteration 77, loss = 0.59240985\n",
            "Iteration 78, loss = 0.59011447\n",
            "Iteration 79, loss = 0.58792103\n",
            "Iteration 80, loss = 0.58563842\n",
            "Iteration 81, loss = 0.58308211\n",
            "Iteration 82, loss = 0.58077269\n",
            "Iteration 83, loss = 0.57829259\n",
            "Iteration 84, loss = 0.57581176\n",
            "Iteration 85, loss = 0.57322825\n",
            "Iteration 86, loss = 0.57060236\n",
            "Iteration 87, loss = 0.56789429\n",
            "Iteration 88, loss = 0.56517055\n",
            "Iteration 89, loss = 0.56236263\n",
            "Iteration 90, loss = 0.55963609\n",
            "Iteration 91, loss = 0.55689754\n",
            "Iteration 92, loss = 0.55404813\n",
            "Iteration 93, loss = 0.55109009\n",
            "Iteration 94, loss = 0.54832429\n",
            "Iteration 95, loss = 0.54527681\n",
            "Iteration 96, loss = 0.54237435\n",
            "Iteration 97, loss = 0.53958529\n",
            "Iteration 98, loss = 0.53650121\n",
            "Iteration 99, loss = 0.53365602\n",
            "Iteration 100, loss = 0.53088189\n",
            "Iteration 101, loss = 0.52811993\n",
            "Iteration 102, loss = 0.52531555\n",
            "Iteration 103, loss = 0.52234889\n",
            "Iteration 104, loss = 0.51965208\n",
            "Iteration 105, loss = 0.51688184\n",
            "Iteration 106, loss = 0.51412214\n",
            "Iteration 107, loss = 0.51144409\n",
            "Iteration 108, loss = 0.50879039\n",
            "Iteration 109, loss = 0.50609311\n",
            "Iteration 110, loss = 0.50352818\n",
            "Iteration 111, loss = 0.50096214\n",
            "Iteration 112, loss = 0.49842328\n",
            "Iteration 113, loss = 0.49602347\n",
            "Iteration 114, loss = 0.49363792\n",
            "Iteration 115, loss = 0.49116419\n",
            "Iteration 116, loss = 0.48888992\n",
            "Iteration 117, loss = 0.48671773\n",
            "Iteration 118, loss = 0.48430754\n",
            "Iteration 119, loss = 0.48226264\n",
            "Iteration 120, loss = 0.48009054\n",
            "Iteration 121, loss = 0.47802306\n",
            "Iteration 122, loss = 0.47588566\n",
            "Iteration 123, loss = 0.47382427\n",
            "Iteration 124, loss = 0.47194404\n",
            "Iteration 125, loss = 0.46987601\n",
            "Iteration 126, loss = 0.46814481\n",
            "Iteration 127, loss = 0.46628554\n",
            "Iteration 128, loss = 0.46442588\n",
            "Iteration 129, loss = 0.46267142\n",
            "Iteration 130, loss = 0.46110279\n",
            "Iteration 131, loss = 0.45960581\n",
            "Iteration 132, loss = 0.45811835\n",
            "Iteration 133, loss = 0.45651780\n",
            "Iteration 134, loss = 0.45513667\n",
            "Iteration 135, loss = 0.45381530\n",
            "Iteration 136, loss = 0.45251396\n",
            "Iteration 137, loss = 0.45122778\n",
            "Iteration 138, loss = 0.45013480\n",
            "Iteration 139, loss = 0.44907003\n",
            "Iteration 140, loss = 0.44788750\n",
            "Iteration 141, loss = 0.44700922\n",
            "Iteration 142, loss = 0.44601768\n",
            "Iteration 143, loss = 0.44521837\n",
            "Iteration 144, loss = 0.44429559\n",
            "Iteration 145, loss = 0.44333132\n",
            "Iteration 146, loss = 0.44260441\n",
            "Iteration 147, loss = 0.44182101\n",
            "Iteration 148, loss = 0.44111550\n",
            "Iteration 149, loss = 0.44055289\n",
            "Iteration 150, loss = 0.43966835\n",
            "Iteration 151, loss = 0.43903853\n",
            "Iteration 152, loss = 0.43823640\n",
            "Iteration 153, loss = 0.43770500\n",
            "Iteration 154, loss = 0.43701863\n",
            "Iteration 155, loss = 0.43635833\n",
            "Iteration 156, loss = 0.43574533\n",
            "Iteration 157, loss = 0.43517155\n",
            "Iteration 158, loss = 0.43457084\n",
            "Iteration 159, loss = 0.43413200\n",
            "Iteration 160, loss = 0.43348814\n",
            "Iteration 161, loss = 0.43302813\n",
            "Iteration 162, loss = 0.43253019\n",
            "Iteration 163, loss = 0.43187292\n",
            "Iteration 164, loss = 0.43132804\n",
            "Iteration 165, loss = 0.43089026\n",
            "Iteration 166, loss = 0.43040767\n",
            "Iteration 167, loss = 0.42995401\n",
            "Iteration 168, loss = 0.42950530\n",
            "Iteration 169, loss = 0.42913511\n",
            "Iteration 170, loss = 0.42858716\n",
            "Iteration 171, loss = 0.42810167\n",
            "Iteration 172, loss = 0.42770054\n",
            "Iteration 173, loss = 0.42720806\n",
            "Iteration 174, loss = 0.42683993\n",
            "Iteration 175, loss = 0.42638144\n",
            "Iteration 176, loss = 0.42590922\n",
            "Iteration 177, loss = 0.42547660\n",
            "Iteration 178, loss = 0.42508928\n",
            "Iteration 179, loss = 0.42473013\n",
            "Iteration 180, loss = 0.42433813\n",
            "Iteration 181, loss = 0.42394645\n",
            "Iteration 182, loss = 0.42357796\n",
            "Iteration 183, loss = 0.42314575\n",
            "Iteration 184, loss = 0.42279758\n",
            "Iteration 185, loss = 0.42248452\n",
            "Iteration 186, loss = 0.42192139\n",
            "Iteration 187, loss = 0.42167409\n",
            "Iteration 188, loss = 0.42140023\n",
            "Iteration 189, loss = 0.42113146\n",
            "Iteration 190, loss = 0.42078810\n",
            "Iteration 191, loss = 0.42037563\n",
            "Iteration 192, loss = 0.42000933\n",
            "Iteration 193, loss = 0.41959756\n",
            "Iteration 194, loss = 0.41919127\n",
            "Iteration 195, loss = 0.41885078\n",
            "Iteration 196, loss = 0.41860748\n",
            "Iteration 197, loss = 0.41829591\n",
            "Iteration 198, loss = 0.41799181\n",
            "Iteration 199, loss = 0.41767722\n",
            "Iteration 200, loss = 0.41743821\n",
            "Iteration 201, loss = 0.41704574\n",
            "Iteration 202, loss = 0.41671920\n",
            "Iteration 203, loss = 0.41653777\n",
            "Iteration 204, loss = 0.41640798\n",
            "Iteration 205, loss = 0.41613606\n",
            "Iteration 206, loss = 0.41574928\n",
            "Iteration 207, loss = 0.41522326\n",
            "Iteration 208, loss = 0.41480814\n",
            "Iteration 209, loss = 0.41453834\n",
            "Iteration 210, loss = 0.41428673\n",
            "Iteration 211, loss = 0.41400864\n",
            "Iteration 212, loss = 0.41392615\n",
            "Iteration 213, loss = 0.41358034\n",
            "Iteration 214, loss = 0.41318028\n",
            "Iteration 215, loss = 0.41279250\n",
            "Iteration 216, loss = 0.41249491\n",
            "Iteration 217, loss = 0.41241021\n",
            "Iteration 218, loss = 0.41196700\n",
            "Iteration 219, loss = 0.41171822\n",
            "Iteration 220, loss = 0.41147163\n",
            "Iteration 221, loss = 0.41139686\n",
            "Iteration 222, loss = 0.41094497\n",
            "Iteration 223, loss = 0.41066859\n",
            "Iteration 224, loss = 0.41037977\n",
            "Iteration 225, loss = 0.41005562\n",
            "Iteration 226, loss = 0.40978740\n",
            "Iteration 227, loss = 0.40956576\n",
            "Iteration 228, loss = 0.40931538\n",
            "Iteration 229, loss = 0.40903407\n",
            "Iteration 230, loss = 0.40879861\n",
            "Iteration 231, loss = 0.40854696\n",
            "Iteration 232, loss = 0.40834347\n",
            "Iteration 233, loss = 0.40813051\n",
            "Iteration 234, loss = 0.40781033\n",
            "Iteration 235, loss = 0.40751626\n",
            "Iteration 236, loss = 0.40739724\n",
            "Iteration 237, loss = 0.40717406\n",
            "Iteration 238, loss = 0.40673296\n",
            "Iteration 239, loss = 0.40635516\n",
            "Iteration 240, loss = 0.40648584\n",
            "Iteration 241, loss = 0.40656320\n",
            "Iteration 242, loss = 0.40635368\n",
            "Iteration 243, loss = 0.40591085\n",
            "Iteration 244, loss = 0.40541204\n",
            "Iteration 245, loss = 0.40513934\n",
            "Iteration 246, loss = 0.40505175\n",
            "Iteration 247, loss = 0.40482978\n",
            "Iteration 248, loss = 0.40453675\n",
            "Iteration 249, loss = 0.40423425\n",
            "Iteration 250, loss = 0.40405055\n",
            "Iteration 251, loss = 0.40397774\n",
            "Iteration 252, loss = 0.40378744\n",
            "Iteration 253, loss = 0.40361323\n",
            "Iteration 254, loss = 0.40328940\n",
            "Iteration 255, loss = 0.40308372\n",
            "Iteration 256, loss = 0.40286379\n",
            "Iteration 257, loss = 0.40267230\n",
            "Iteration 258, loss = 0.40248148\n",
            "Iteration 259, loss = 0.40231786\n",
            "Iteration 260, loss = 0.40232455\n",
            "Iteration 261, loss = 0.40191992\n",
            "Iteration 262, loss = 0.40172183\n",
            "Iteration 263, loss = 0.40150667\n",
            "Iteration 264, loss = 0.40130265\n",
            "Iteration 265, loss = 0.40099819\n",
            "Iteration 266, loss = 0.40099595\n",
            "Iteration 267, loss = 0.40070726\n",
            "Iteration 268, loss = 0.40054277\n",
            "Iteration 269, loss = 0.40033671\n",
            "Iteration 270, loss = 0.40017382\n",
            "Iteration 271, loss = 0.39991786\n",
            "Iteration 272, loss = 0.39983913\n",
            "Iteration 273, loss = 0.39963590\n",
            "Iteration 274, loss = 0.39935312\n",
            "Iteration 275, loss = 0.39912647\n",
            "Iteration 276, loss = 0.39904703\n",
            "Iteration 277, loss = 0.39879639\n",
            "Iteration 278, loss = 0.39861150\n",
            "Iteration 279, loss = 0.39843111\n",
            "Iteration 280, loss = 0.39833373\n",
            "Iteration 281, loss = 0.39813198\n",
            "Iteration 282, loss = 0.39807013\n",
            "Iteration 283, loss = 0.39781774\n",
            "Iteration 284, loss = 0.39758641\n",
            "Iteration 285, loss = 0.39737408\n",
            "Iteration 286, loss = 0.39717288\n",
            "Iteration 287, loss = 0.39705001\n",
            "Iteration 288, loss = 0.39701008\n",
            "Iteration 289, loss = 0.39670965\n",
            "Iteration 290, loss = 0.39662282\n",
            "Iteration 291, loss = 0.39641863\n",
            "Iteration 292, loss = 0.39623202\n",
            "Iteration 293, loss = 0.39598834\n",
            "Iteration 294, loss = 0.39576308\n",
            "Iteration 295, loss = 0.39562212\n",
            "Iteration 296, loss = 0.39546587\n",
            "Iteration 297, loss = 0.39529826\n",
            "Iteration 298, loss = 0.39521820\n",
            "Iteration 299, loss = 0.39501319\n",
            "Iteration 300, loss = 0.39481535\n",
            "Iteration 301, loss = 0.39481952\n",
            "Iteration 302, loss = 0.39451841\n",
            "Iteration 303, loss = 0.39444321\n",
            "Iteration 304, loss = 0.39425135\n",
            "Iteration 305, loss = 0.39388333\n",
            "Iteration 306, loss = 0.39372089\n",
            "Iteration 307, loss = 0.39364600\n",
            "Iteration 308, loss = 0.39391852\n",
            "Iteration 309, loss = 0.39337975\n",
            "Iteration 310, loss = 0.39314521\n",
            "Iteration 311, loss = 0.39300556\n",
            "Iteration 312, loss = 0.39290668\n",
            "Iteration 313, loss = 0.39289405\n",
            "Iteration 314, loss = 0.39271221\n",
            "Iteration 315, loss = 0.39256000\n",
            "Iteration 316, loss = 0.39221282\n",
            "Iteration 317, loss = 0.39196190\n",
            "Iteration 318, loss = 0.39184897\n",
            "Iteration 319, loss = 0.39169451\n",
            "Iteration 320, loss = 0.39151084\n",
            "Iteration 321, loss = 0.39143963\n",
            "Iteration 322, loss = 0.39133034\n",
            "Iteration 323, loss = 0.39102797\n",
            "Iteration 324, loss = 0.39105765\n",
            "Iteration 325, loss = 0.39078211\n",
            "Iteration 326, loss = 0.39053996\n",
            "Iteration 327, loss = 0.39037871\n",
            "Iteration 328, loss = 0.39042585\n",
            "Iteration 329, loss = 0.39030834\n",
            "Iteration 330, loss = 0.39007518\n",
            "Iteration 331, loss = 0.38997019\n",
            "Iteration 332, loss = 0.38970265\n",
            "Iteration 333, loss = 0.38970703\n",
            "Iteration 334, loss = 0.38943835\n",
            "Iteration 335, loss = 0.38926140\n",
            "Iteration 336, loss = 0.38908774\n",
            "Iteration 337, loss = 0.38912494\n",
            "Iteration 338, loss = 0.38901906\n",
            "Iteration 339, loss = 0.38875745\n",
            "Iteration 340, loss = 0.38857496\n",
            "Iteration 341, loss = 0.38831772\n",
            "Iteration 342, loss = 0.38817508\n",
            "Iteration 343, loss = 0.38818144\n",
            "Iteration 344, loss = 0.38784656\n",
            "Iteration 345, loss = 0.38806391\n",
            "Iteration 346, loss = 0.38773829\n",
            "Iteration 347, loss = 0.38757054\n",
            "Iteration 348, loss = 0.38737761\n",
            "Iteration 349, loss = 0.38723694\n",
            "Iteration 350, loss = 0.38728072\n",
            "Iteration 351, loss = 0.38698771\n",
            "Iteration 352, loss = 0.38686072\n",
            "Iteration 353, loss = 0.38662917\n",
            "Iteration 354, loss = 0.38661470\n",
            "Iteration 355, loss = 0.38658426\n",
            "Iteration 356, loss = 0.38627630\n",
            "Iteration 357, loss = 0.38618452\n",
            "Iteration 358, loss = 0.38603546\n",
            "Iteration 359, loss = 0.38595838\n",
            "Iteration 360, loss = 0.38585804\n",
            "Iteration 361, loss = 0.38568214\n",
            "Iteration 362, loss = 0.38558670\n",
            "Iteration 363, loss = 0.38543423\n",
            "Iteration 364, loss = 0.38536469\n",
            "Iteration 365, loss = 0.38522567\n",
            "Iteration 366, loss = 0.38506207\n",
            "Iteration 367, loss = 0.38534136\n",
            "Iteration 368, loss = 0.38480777\n",
            "Iteration 369, loss = 0.38479395\n",
            "Iteration 370, loss = 0.38452364\n",
            "Iteration 371, loss = 0.38439048\n",
            "Iteration 372, loss = 0.38427313\n",
            "Iteration 373, loss = 0.38419314\n",
            "Iteration 374, loss = 0.38417830\n",
            "Iteration 375, loss = 0.38401749\n",
            "Iteration 376, loss = 0.38381224\n",
            "Iteration 377, loss = 0.38379907\n",
            "Iteration 378, loss = 0.38364251\n",
            "Iteration 379, loss = 0.38337748\n",
            "Iteration 380, loss = 0.38327055\n",
            "Iteration 381, loss = 0.38315077\n",
            "Iteration 382, loss = 0.38299568\n",
            "Iteration 383, loss = 0.38285075\n",
            "Iteration 384, loss = 0.38280294\n",
            "Iteration 385, loss = 0.38271964\n",
            "Iteration 386, loss = 0.38262545\n",
            "Iteration 387, loss = 0.38246151\n",
            "Iteration 388, loss = 0.38244068\n",
            "Iteration 389, loss = 0.38236176\n",
            "Iteration 390, loss = 0.38218291\n",
            "Iteration 391, loss = 0.38198254\n",
            "Iteration 392, loss = 0.38186463\n",
            "Iteration 393, loss = 0.38180429\n",
            "Iteration 394, loss = 0.38168300\n",
            "Iteration 395, loss = 0.38155162\n",
            "Iteration 396, loss = 0.38139067\n",
            "Iteration 397, loss = 0.38120477\n",
            "Iteration 398, loss = 0.38118729\n",
            "Iteration 399, loss = 0.38135566\n",
            "Iteration 400, loss = 0.38116670\n",
            "Iteration 401, loss = 0.38099418\n",
            "Iteration 402, loss = 0.38082326\n",
            "Iteration 403, loss = 0.38066401\n",
            "Iteration 404, loss = 0.38071928\n",
            "Iteration 405, loss = 0.38056860\n",
            "Iteration 406, loss = 0.38043890\n",
            "Iteration 407, loss = 0.38035866\n",
            "Iteration 408, loss = 0.38024725\n",
            "Iteration 409, loss = 0.38010143\n",
            "Iteration 410, loss = 0.37986265\n",
            "Iteration 411, loss = 0.38009928\n",
            "Iteration 412, loss = 0.38005333\n",
            "Iteration 413, loss = 0.37989193\n",
            "Iteration 414, loss = 0.37951625\n",
            "Iteration 415, loss = 0.37936991\n",
            "Iteration 416, loss = 0.37945809\n",
            "Iteration 417, loss = 0.37945220\n",
            "Iteration 418, loss = 0.37934250\n",
            "Iteration 419, loss = 0.37922303\n",
            "Iteration 420, loss = 0.37890751\n",
            "Iteration 421, loss = 0.37882570\n",
            "Iteration 422, loss = 0.37874942\n",
            "Iteration 423, loss = 0.37875031\n",
            "Iteration 424, loss = 0.37848189\n",
            "Iteration 425, loss = 0.37847618\n",
            "Iteration 426, loss = 0.37841198\n",
            "Iteration 427, loss = 0.37829777\n",
            "Iteration 428, loss = 0.37833986\n",
            "Iteration 429, loss = 0.37807879\n",
            "Iteration 430, loss = 0.37795503\n",
            "Iteration 431, loss = 0.37807031\n",
            "Iteration 432, loss = 0.37791736\n",
            "Iteration 433, loss = 0.37775940\n",
            "Iteration 434, loss = 0.37755950\n",
            "Iteration 435, loss = 0.37743115\n",
            "Iteration 436, loss = 0.37790752\n",
            "Iteration 437, loss = 0.37776453\n",
            "Iteration 438, loss = 0.37744844\n",
            "Iteration 439, loss = 0.37719671\n",
            "Iteration 440, loss = 0.37706167\n",
            "Iteration 441, loss = 0.37717318\n",
            "Iteration 442, loss = 0.37698737\n",
            "Iteration 443, loss = 0.37686234\n",
            "Iteration 444, loss = 0.37676375\n",
            "Iteration 445, loss = 0.37663140\n",
            "Iteration 446, loss = 0.37653465\n",
            "Iteration 447, loss = 0.37649393\n",
            "Iteration 448, loss = 0.37642081\n",
            "Iteration 449, loss = 0.37627600\n",
            "Iteration 450, loss = 0.37623919\n",
            "Iteration 451, loss = 0.37614368\n",
            "Iteration 452, loss = 0.37615625\n",
            "Iteration 453, loss = 0.37598128\n",
            "Iteration 454, loss = 0.37596200\n",
            "Iteration 455, loss = 0.37589792\n",
            "Iteration 456, loss = 0.37580826\n",
            "Iteration 457, loss = 0.37574972\n",
            "Iteration 458, loss = 0.37561235\n",
            "Iteration 459, loss = 0.37550635\n",
            "Iteration 460, loss = 0.37554358\n",
            "Iteration 461, loss = 0.37539407\n",
            "Iteration 462, loss = 0.37530249\n",
            "Iteration 463, loss = 0.37514822\n",
            "Iteration 464, loss = 0.37512553\n",
            "Iteration 465, loss = 0.37515553\n",
            "Iteration 466, loss = 0.37509476\n",
            "Iteration 467, loss = 0.37519446\n",
            "Iteration 468, loss = 0.37484573\n",
            "Iteration 469, loss = 0.37475386\n",
            "Iteration 470, loss = 0.37472461\n",
            "Iteration 471, loss = 0.37456378\n",
            "Iteration 472, loss = 0.37454416\n",
            "Iteration 473, loss = 0.37462115\n",
            "Iteration 474, loss = 0.37451157\n",
            "Iteration 475, loss = 0.37428963\n",
            "Iteration 476, loss = 0.37414993\n",
            "Iteration 477, loss = 0.37414363\n",
            "Iteration 478, loss = 0.37402836\n",
            "Iteration 479, loss = 0.37400430\n",
            "Iteration 480, loss = 0.37384864\n",
            "Iteration 481, loss = 0.37393627\n",
            "Iteration 482, loss = 0.37378672\n",
            "Iteration 483, loss = 0.37372371\n",
            "Iteration 484, loss = 0.37356888\n",
            "Iteration 485, loss = 0.37364834\n",
            "Iteration 486, loss = 0.37352271\n",
            "Iteration 487, loss = 0.37342825\n",
            "Iteration 488, loss = 0.37330227\n",
            "Iteration 489, loss = 0.37320940\n",
            "Iteration 490, loss = 0.37323183\n",
            "Iteration 491, loss = 0.37326717\n",
            "Iteration 492, loss = 0.37317021\n",
            "Iteration 493, loss = 0.37299172\n",
            "Iteration 494, loss = 0.37297301\n",
            "Iteration 495, loss = 0.37303251\n",
            "Iteration 496, loss = 0.37312295\n",
            "Iteration 497, loss = 0.37285650\n",
            "Iteration 498, loss = 0.37281857\n",
            "Iteration 499, loss = 0.37259643\n",
            "Iteration 500, loss = 0.37255520\n",
            "Iteration 501, loss = 0.37257961\n",
            "Iteration 502, loss = 0.37246377\n",
            "Iteration 503, loss = 0.37243633\n",
            "Iteration 504, loss = 0.37234788\n",
            "Iteration 505, loss = 0.37237636\n",
            "Iteration 506, loss = 0.37227215\n",
            "Iteration 507, loss = 0.37212943\n",
            "Iteration 508, loss = 0.37201375\n",
            "Iteration 509, loss = 0.37238692\n",
            "Iteration 510, loss = 0.37198246\n",
            "Iteration 511, loss = 0.37206500\n",
            "Iteration 512, loss = 0.37187530\n",
            "Iteration 513, loss = 0.37174772\n",
            "Iteration 514, loss = 0.37187984\n",
            "Iteration 515, loss = 0.37184566\n",
            "Iteration 516, loss = 0.37175288\n",
            "Iteration 517, loss = 0.37173071\n",
            "Iteration 518, loss = 0.37160442\n",
            "Iteration 519, loss = 0.37160660\n",
            "Iteration 520, loss = 0.37134139\n",
            "Iteration 521, loss = 0.37131138\n",
            "Iteration 522, loss = 0.37124192\n",
            "Iteration 523, loss = 0.37114687\n",
            "Iteration 524, loss = 0.37107851\n",
            "Iteration 525, loss = 0.37117618\n",
            "Iteration 526, loss = 0.37099140\n",
            "Iteration 527, loss = 0.37108347\n",
            "Iteration 528, loss = 0.37087588\n",
            "Iteration 529, loss = 0.37081459\n",
            "Iteration 530, loss = 0.37083729\n",
            "Iteration 531, loss = 0.37065057\n",
            "Iteration 532, loss = 0.37076808\n",
            "Iteration 533, loss = 0.37059155\n",
            "Iteration 534, loss = 0.37051141\n",
            "Iteration 535, loss = 0.37041471\n",
            "Iteration 536, loss = 0.37039135\n",
            "Iteration 537, loss = 0.37034971\n",
            "Iteration 538, loss = 0.37033742\n",
            "Iteration 539, loss = 0.37020422\n",
            "Iteration 540, loss = 0.37016736\n",
            "Iteration 541, loss = 0.37018146\n",
            "Iteration 542, loss = 0.37007711\n",
            "Iteration 543, loss = 0.37014500\n",
            "Iteration 544, loss = 0.37006365\n",
            "Iteration 545, loss = 0.36999640\n",
            "Iteration 546, loss = 0.36989798\n",
            "Iteration 547, loss = 0.36990305\n",
            "Iteration 548, loss = 0.36996159\n",
            "Iteration 549, loss = 0.36972745\n",
            "Iteration 550, loss = 0.37031777\n",
            "Iteration 551, loss = 0.37009285\n",
            "Iteration 552, loss = 0.36985261\n",
            "Iteration 553, loss = 0.36964593\n",
            "Iteration 554, loss = 0.36973986\n",
            "Iteration 555, loss = 0.36964112\n",
            "Iteration 556, loss = 0.36944649\n",
            "Iteration 557, loss = 0.36948361\n",
            "Iteration 558, loss = 0.36940999\n",
            "Iteration 559, loss = 0.36938816\n",
            "Iteration 560, loss = 0.36922899\n",
            "Iteration 561, loss = 0.36921335\n",
            "Iteration 562, loss = 0.36921194\n",
            "Iteration 563, loss = 0.36910988\n",
            "Iteration 564, loss = 0.36906817\n",
            "Iteration 565, loss = 0.36901868\n",
            "Iteration 566, loss = 0.36893562\n",
            "Iteration 567, loss = 0.36870316\n",
            "Iteration 568, loss = 0.36876984\n",
            "Iteration 569, loss = 0.36870776\n",
            "Iteration 570, loss = 0.36875407\n",
            "Iteration 571, loss = 0.36864077\n",
            "Iteration 572, loss = 0.36858637\n",
            "Iteration 573, loss = 0.36854823\n",
            "Iteration 574, loss = 0.36854329\n",
            "Iteration 575, loss = 0.36841208\n",
            "Iteration 576, loss = 0.36836818\n",
            "Iteration 577, loss = 0.36825788\n",
            "Iteration 578, loss = 0.36834204\n",
            "Iteration 579, loss = 0.36823302\n",
            "Iteration 580, loss = 0.36822846\n",
            "Iteration 581, loss = 0.36814002\n",
            "Iteration 582, loss = 0.36809733\n",
            "Iteration 583, loss = 0.36803015\n",
            "Iteration 584, loss = 0.36803742\n",
            "Iteration 585, loss = 0.36790552\n",
            "Iteration 586, loss = 0.36812345\n",
            "Iteration 587, loss = 0.36790784\n",
            "Iteration 588, loss = 0.36780879\n",
            "Iteration 589, loss = 0.36815529\n",
            "Iteration 590, loss = 0.36768497\n",
            "Iteration 591, loss = 0.36778252\n",
            "Iteration 592, loss = 0.36766629\n",
            "Iteration 593, loss = 0.36754168\n",
            "Iteration 594, loss = 0.36752965\n",
            "Iteration 595, loss = 0.36752354\n",
            "Iteration 596, loss = 0.36750377\n",
            "Iteration 597, loss = 0.36740324\n",
            "Iteration 598, loss = 0.36734653\n",
            "Iteration 599, loss = 0.36729875\n",
            "Iteration 600, loss = 0.36761466\n",
            "Iteration 601, loss = 0.36730529\n",
            "Iteration 602, loss = 0.36736658\n",
            "Iteration 603, loss = 0.36747148\n",
            "Iteration 604, loss = 0.36734220\n",
            "Iteration 605, loss = 0.36718596\n",
            "Iteration 606, loss = 0.36700838\n",
            "Iteration 607, loss = 0.36692714\n",
            "Iteration 608, loss = 0.36709297\n",
            "Iteration 609, loss = 0.36701126\n",
            "Iteration 610, loss = 0.36689802\n",
            "Iteration 611, loss = 0.36677255\n",
            "Iteration 612, loss = 0.36674491\n",
            "Iteration 613, loss = 0.36675348\n",
            "Iteration 614, loss = 0.36662728\n",
            "Iteration 615, loss = 0.36660954\n",
            "Iteration 616, loss = 0.36656594\n",
            "Iteration 617, loss = 0.36656181\n",
            "Iteration 618, loss = 0.36653228\n",
            "Iteration 619, loss = 0.36646671\n",
            "Iteration 620, loss = 0.36643882\n",
            "Iteration 621, loss = 0.36642423\n",
            "Iteration 622, loss = 0.36664122\n",
            "Iteration 623, loss = 0.36662544\n",
            "Iteration 624, loss = 0.36644020\n",
            "Iteration 625, loss = 0.36634812\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.76040131\n",
            "Iteration 2, loss = 0.75506528\n",
            "Iteration 3, loss = 0.75043658\n",
            "Iteration 4, loss = 0.74546357\n",
            "Iteration 5, loss = 0.74112513\n",
            "Iteration 6, loss = 0.73700480\n",
            "Iteration 7, loss = 0.73286189\n",
            "Iteration 8, loss = 0.72935381\n",
            "Iteration 9, loss = 0.72579050\n",
            "Iteration 10, loss = 0.72197391\n",
            "Iteration 11, loss = 0.71895061\n",
            "Iteration 12, loss = 0.71597137\n",
            "Iteration 13, loss = 0.71295906\n",
            "Iteration 14, loss = 0.71021330\n",
            "Iteration 15, loss = 0.70765523\n",
            "Iteration 16, loss = 0.70493147\n",
            "Iteration 17, loss = 0.70255063\n",
            "Iteration 18, loss = 0.70035305\n",
            "Iteration 19, loss = 0.69814197\n",
            "Iteration 20, loss = 0.69599721\n",
            "Iteration 21, loss = 0.69404420\n",
            "Iteration 22, loss = 0.69199535\n",
            "Iteration 23, loss = 0.69030816\n",
            "Iteration 24, loss = 0.68848848\n",
            "Iteration 25, loss = 0.68674453\n",
            "Iteration 26, loss = 0.68507417\n",
            "Iteration 27, loss = 0.68338176\n",
            "Iteration 28, loss = 0.68181789\n",
            "Iteration 29, loss = 0.68017919\n",
            "Iteration 30, loss = 0.67860782\n",
            "Iteration 31, loss = 0.67706879\n",
            "Iteration 32, loss = 0.67547826\n",
            "Iteration 33, loss = 0.67383585\n",
            "Iteration 34, loss = 0.67239194\n",
            "Iteration 35, loss = 0.67077695\n",
            "Iteration 36, loss = 0.66917766\n",
            "Iteration 37, loss = 0.66764694\n",
            "Iteration 38, loss = 0.66601332\n",
            "Iteration 39, loss = 0.66445621\n",
            "Iteration 40, loss = 0.66281274\n",
            "Iteration 41, loss = 0.66110258\n",
            "Iteration 42, loss = 0.65944806\n",
            "Iteration 43, loss = 0.65779467\n",
            "Iteration 44, loss = 0.65595941\n",
            "Iteration 45, loss = 0.65425972\n",
            "Iteration 46, loss = 0.65249337\n",
            "Iteration 47, loss = 0.65084609\n",
            "Iteration 48, loss = 0.64913765\n",
            "Iteration 49, loss = 0.64734987\n",
            "Iteration 50, loss = 0.64557487\n",
            "Iteration 51, loss = 0.64379614\n",
            "Iteration 52, loss = 0.64204100\n",
            "Iteration 53, loss = 0.64017216\n",
            "Iteration 54, loss = 0.63845688\n",
            "Iteration 55, loss = 0.63654842\n",
            "Iteration 56, loss = 0.63470891\n",
            "Iteration 57, loss = 0.63286054\n",
            "Iteration 58, loss = 0.63095786\n",
            "Iteration 59, loss = 0.62909932\n",
            "Iteration 60, loss = 0.62722792\n",
            "Iteration 61, loss = 0.62522883\n",
            "Iteration 62, loss = 0.62333598\n",
            "Iteration 63, loss = 0.62140753\n",
            "Iteration 64, loss = 0.61945760\n",
            "Iteration 65, loss = 0.61749442\n",
            "Iteration 66, loss = 0.61549314\n",
            "Iteration 67, loss = 0.61342974\n",
            "Iteration 68, loss = 0.61146428\n",
            "Iteration 69, loss = 0.60940100\n",
            "Iteration 70, loss = 0.60718656\n",
            "Iteration 71, loss = 0.60508222\n",
            "Iteration 72, loss = 0.60288987\n",
            "Iteration 73, loss = 0.60065748\n",
            "Iteration 74, loss = 0.59831628\n",
            "Iteration 75, loss = 0.59603501\n",
            "Iteration 76, loss = 0.59377067\n",
            "Iteration 77, loss = 0.59122866\n",
            "Iteration 78, loss = 0.58873781\n",
            "Iteration 79, loss = 0.58629691\n",
            "Iteration 80, loss = 0.58376745\n",
            "Iteration 81, loss = 0.58126699\n",
            "Iteration 82, loss = 0.57873418\n",
            "Iteration 83, loss = 0.57599964\n",
            "Iteration 84, loss = 0.57339616\n",
            "Iteration 85, loss = 0.57089222\n",
            "Iteration 86, loss = 0.56792971\n",
            "Iteration 87, loss = 0.56529539\n",
            "Iteration 88, loss = 0.56252026\n",
            "Iteration 89, loss = 0.55963016\n",
            "Iteration 90, loss = 0.55676363\n",
            "Iteration 91, loss = 0.55380874\n",
            "Iteration 92, loss = 0.55097136\n",
            "Iteration 93, loss = 0.54798672\n",
            "Iteration 94, loss = 0.54516910\n",
            "Iteration 95, loss = 0.54207563\n",
            "Iteration 96, loss = 0.53912250\n",
            "Iteration 97, loss = 0.53602670\n",
            "Iteration 98, loss = 0.53306792\n",
            "Iteration 99, loss = 0.53004635\n",
            "Iteration 100, loss = 0.52697238\n",
            "Iteration 101, loss = 0.52403242\n",
            "Iteration 102, loss = 0.52123793\n",
            "Iteration 103, loss = 0.51839324\n",
            "Iteration 104, loss = 0.51568271\n",
            "Iteration 105, loss = 0.51274752\n",
            "Iteration 106, loss = 0.51014794\n",
            "Iteration 107, loss = 0.50747625\n",
            "Iteration 108, loss = 0.50474666\n",
            "Iteration 109, loss = 0.50212683\n",
            "Iteration 110, loss = 0.49952776\n",
            "Iteration 111, loss = 0.49685277\n",
            "Iteration 112, loss = 0.49423731\n",
            "Iteration 113, loss = 0.49175257\n",
            "Iteration 114, loss = 0.48930107\n",
            "Iteration 115, loss = 0.48662606\n",
            "Iteration 116, loss = 0.48429007\n",
            "Iteration 117, loss = 0.48180434\n",
            "Iteration 118, loss = 0.47949512\n",
            "Iteration 119, loss = 0.47719505\n",
            "Iteration 120, loss = 0.47492657\n",
            "Iteration 121, loss = 0.47287217\n",
            "Iteration 122, loss = 0.47059373\n",
            "Iteration 123, loss = 0.46856590\n",
            "Iteration 124, loss = 0.46639970\n",
            "Iteration 125, loss = 0.46457952\n",
            "Iteration 126, loss = 0.46245112\n",
            "Iteration 127, loss = 0.46052849\n",
            "Iteration 128, loss = 0.45871200\n",
            "Iteration 129, loss = 0.45688531\n",
            "Iteration 130, loss = 0.45519781\n",
            "Iteration 131, loss = 0.45371079\n",
            "Iteration 132, loss = 0.45201095\n",
            "Iteration 133, loss = 0.45051013\n",
            "Iteration 134, loss = 0.44925052\n",
            "Iteration 135, loss = 0.44779156\n",
            "Iteration 136, loss = 0.44657885\n",
            "Iteration 137, loss = 0.44536587\n",
            "Iteration 138, loss = 0.44422912\n",
            "Iteration 139, loss = 0.44328483\n",
            "Iteration 140, loss = 0.44203274\n",
            "Iteration 141, loss = 0.44077906\n",
            "Iteration 142, loss = 0.43973458\n",
            "Iteration 143, loss = 0.43890840\n",
            "Iteration 144, loss = 0.43835790\n",
            "Iteration 145, loss = 0.43711375\n",
            "Iteration 146, loss = 0.43633337\n",
            "Iteration 147, loss = 0.43536044\n",
            "Iteration 148, loss = 0.43477016\n",
            "Iteration 149, loss = 0.43397707\n",
            "Iteration 150, loss = 0.43333774\n",
            "Iteration 151, loss = 0.43267477\n",
            "Iteration 152, loss = 0.43218786\n",
            "Iteration 153, loss = 0.43154364\n",
            "Iteration 154, loss = 0.43126471\n",
            "Iteration 155, loss = 0.43072666\n",
            "Iteration 156, loss = 0.42992533\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.75891674\n",
            "Iteration 2, loss = 0.75353090\n",
            "Iteration 3, loss = 0.74892270\n",
            "Iteration 4, loss = 0.74404230\n",
            "Iteration 5, loss = 0.73988027\n",
            "Iteration 6, loss = 0.73580515\n",
            "Iteration 7, loss = 0.73180540\n",
            "Iteration 8, loss = 0.72836472\n",
            "Iteration 9, loss = 0.72492825\n",
            "Iteration 10, loss = 0.72113500\n",
            "Iteration 11, loss = 0.71823147\n",
            "Iteration 12, loss = 0.71537391\n",
            "Iteration 13, loss = 0.71238309\n",
            "Iteration 14, loss = 0.70977774\n",
            "Iteration 15, loss = 0.70721483\n",
            "Iteration 16, loss = 0.70455740\n",
            "Iteration 17, loss = 0.70230308\n",
            "Iteration 18, loss = 0.70015516\n",
            "Iteration 19, loss = 0.69803582\n",
            "Iteration 20, loss = 0.69595133\n",
            "Iteration 21, loss = 0.69394823\n",
            "Iteration 22, loss = 0.69208656\n",
            "Iteration 23, loss = 0.69040533\n",
            "Iteration 24, loss = 0.68854596\n",
            "Iteration 25, loss = 0.68686892\n",
            "Iteration 26, loss = 0.68517244\n",
            "Iteration 27, loss = 0.68354253\n",
            "Iteration 28, loss = 0.68199052\n",
            "Iteration 29, loss = 0.68035317\n",
            "Iteration 30, loss = 0.67880912\n",
            "Iteration 31, loss = 0.67723863\n",
            "Iteration 32, loss = 0.67569388\n",
            "Iteration 33, loss = 0.67402839\n",
            "Iteration 34, loss = 0.67257672\n",
            "Iteration 35, loss = 0.67095772\n",
            "Iteration 36, loss = 0.66934690\n",
            "Iteration 37, loss = 0.66778515\n",
            "Iteration 38, loss = 0.66616521\n",
            "Iteration 39, loss = 0.66462279\n",
            "Iteration 40, loss = 0.66293157\n",
            "Iteration 41, loss = 0.66131229\n",
            "Iteration 42, loss = 0.65961350\n",
            "Iteration 43, loss = 0.65796946\n",
            "Iteration 44, loss = 0.65614844\n",
            "Iteration 45, loss = 0.65452721\n",
            "Iteration 46, loss = 0.65279723\n",
            "Iteration 47, loss = 0.65114531\n",
            "Iteration 48, loss = 0.64941475\n",
            "Iteration 49, loss = 0.64765348\n",
            "Iteration 50, loss = 0.64594823\n",
            "Iteration 51, loss = 0.64415897\n",
            "Iteration 52, loss = 0.64243617\n",
            "Iteration 53, loss = 0.64056424\n",
            "Iteration 54, loss = 0.63894911\n",
            "Iteration 55, loss = 0.63704344\n",
            "Iteration 56, loss = 0.63521920\n",
            "Iteration 57, loss = 0.63336524\n",
            "Iteration 58, loss = 0.63156314\n",
            "Iteration 59, loss = 0.62964999\n",
            "Iteration 60, loss = 0.62778604\n",
            "Iteration 61, loss = 0.62591768\n",
            "Iteration 62, loss = 0.62404599\n",
            "Iteration 63, loss = 0.62212335\n",
            "Iteration 64, loss = 0.62026894\n",
            "Iteration 65, loss = 0.61831077\n",
            "Iteration 66, loss = 0.61639109\n",
            "Iteration 67, loss = 0.61446775\n",
            "Iteration 68, loss = 0.61254220\n",
            "Iteration 69, loss = 0.61050331\n",
            "Iteration 70, loss = 0.60846230\n",
            "Iteration 71, loss = 0.60637476\n",
            "Iteration 72, loss = 0.60431197\n",
            "Iteration 73, loss = 0.60210095\n",
            "Iteration 74, loss = 0.59992594\n",
            "Iteration 75, loss = 0.59767829\n",
            "Iteration 76, loss = 0.59545782\n",
            "Iteration 77, loss = 0.59307303\n",
            "Iteration 78, loss = 0.59063793\n",
            "Iteration 79, loss = 0.58828255\n",
            "Iteration 80, loss = 0.58578105\n",
            "Iteration 81, loss = 0.58331113\n",
            "Iteration 82, loss = 0.58078688\n",
            "Iteration 83, loss = 0.57812354\n",
            "Iteration 84, loss = 0.57548167\n",
            "Iteration 85, loss = 0.57293454\n",
            "Iteration 86, loss = 0.57000160\n",
            "Iteration 87, loss = 0.56720154\n",
            "Iteration 88, loss = 0.56449959\n",
            "Iteration 89, loss = 0.56154526\n",
            "Iteration 90, loss = 0.55855329\n",
            "Iteration 91, loss = 0.55552439\n",
            "Iteration 92, loss = 0.55260857\n",
            "Iteration 93, loss = 0.54959432\n",
            "Iteration 94, loss = 0.54673812\n",
            "Iteration 95, loss = 0.54360840\n",
            "Iteration 96, loss = 0.54050135\n",
            "Iteration 97, loss = 0.53740101\n",
            "Iteration 98, loss = 0.53438955\n",
            "Iteration 99, loss = 0.53130675\n",
            "Iteration 100, loss = 0.52831561\n",
            "Iteration 101, loss = 0.52525928\n",
            "Iteration 102, loss = 0.52230078\n",
            "Iteration 103, loss = 0.51933196\n",
            "Iteration 104, loss = 0.51657655\n",
            "Iteration 105, loss = 0.51348513\n",
            "Iteration 106, loss = 0.51071142\n",
            "Iteration 107, loss = 0.50786990\n",
            "Iteration 108, loss = 0.50495762\n",
            "Iteration 109, loss = 0.50216100\n",
            "Iteration 110, loss = 0.49940581\n",
            "Iteration 111, loss = 0.49656425\n",
            "Iteration 112, loss = 0.49385131\n",
            "Iteration 113, loss = 0.49119690\n",
            "Iteration 114, loss = 0.48836001\n",
            "Iteration 115, loss = 0.48566554\n",
            "Iteration 116, loss = 0.48311243\n",
            "Iteration 117, loss = 0.48060305\n",
            "Iteration 118, loss = 0.47815452\n",
            "Iteration 119, loss = 0.47571189\n",
            "Iteration 120, loss = 0.47323553\n",
            "Iteration 121, loss = 0.47106474\n",
            "Iteration 122, loss = 0.46856314\n",
            "Iteration 123, loss = 0.46640913\n",
            "Iteration 124, loss = 0.46419676\n",
            "Iteration 125, loss = 0.46227014\n",
            "Iteration 126, loss = 0.46016226\n",
            "Iteration 127, loss = 0.45819181\n",
            "Iteration 128, loss = 0.45625398\n",
            "Iteration 129, loss = 0.45443106\n",
            "Iteration 130, loss = 0.45268284\n",
            "Iteration 131, loss = 0.45116586\n",
            "Iteration 132, loss = 0.44947705\n",
            "Iteration 133, loss = 0.44795635\n",
            "Iteration 134, loss = 0.44663383\n",
            "Iteration 135, loss = 0.44516018\n",
            "Iteration 136, loss = 0.44392590\n",
            "Iteration 137, loss = 0.44268615\n",
            "Iteration 138, loss = 0.44157380\n",
            "Iteration 139, loss = 0.44061255\n",
            "Iteration 140, loss = 0.43922929\n",
            "Iteration 141, loss = 0.43841196\n",
            "Iteration 142, loss = 0.43721709\n",
            "Iteration 143, loss = 0.43643919\n",
            "Iteration 144, loss = 0.43552910\n",
            "Iteration 145, loss = 0.43447710\n",
            "Iteration 146, loss = 0.43400120\n",
            "Iteration 147, loss = 0.43302228\n",
            "Iteration 148, loss = 0.43247103\n",
            "Iteration 149, loss = 0.43160614\n",
            "Iteration 150, loss = 0.43105748\n",
            "Iteration 151, loss = 0.43037736\n",
            "Iteration 152, loss = 0.42979292\n",
            "Iteration 153, loss = 0.42919309\n",
            "Iteration 154, loss = 0.42861150\n",
            "Iteration 155, loss = 0.42811899\n",
            "Iteration 156, loss = 0.42746163\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.75914368\n",
            "Iteration 2, loss = 0.75365252\n",
            "Iteration 3, loss = 0.74903953\n",
            "Iteration 4, loss = 0.74401337\n",
            "Iteration 5, loss = 0.73984866\n",
            "Iteration 6, loss = 0.73561154\n",
            "Iteration 7, loss = 0.73166380\n",
            "Iteration 8, loss = 0.72799520\n",
            "Iteration 9, loss = 0.72449652\n",
            "Iteration 10, loss = 0.72079263\n",
            "Iteration 11, loss = 0.71793476\n",
            "Iteration 12, loss = 0.71487950\n",
            "Iteration 13, loss = 0.71182382\n",
            "Iteration 14, loss = 0.70918377\n",
            "Iteration 15, loss = 0.70669434\n",
            "Iteration 16, loss = 0.70396215\n",
            "Iteration 17, loss = 0.70157479\n",
            "Iteration 18, loss = 0.69936266\n",
            "Iteration 19, loss = 0.69717195\n",
            "Iteration 20, loss = 0.69512249\n",
            "Iteration 21, loss = 0.69311782\n",
            "Iteration 22, loss = 0.69115331\n",
            "Iteration 23, loss = 0.68941769\n",
            "Iteration 24, loss = 0.68763222\n",
            "Iteration 25, loss = 0.68589408\n",
            "Iteration 26, loss = 0.68422079\n",
            "Iteration 27, loss = 0.68258991\n",
            "Iteration 28, loss = 0.68095570\n",
            "Iteration 29, loss = 0.67937963\n",
            "Iteration 30, loss = 0.67786334\n",
            "Iteration 31, loss = 0.67627551\n",
            "Iteration 32, loss = 0.67467192\n",
            "Iteration 33, loss = 0.67297441\n",
            "Iteration 34, loss = 0.67160289\n",
            "Iteration 35, loss = 0.66989271\n",
            "Iteration 36, loss = 0.66823569\n",
            "Iteration 37, loss = 0.66665847\n",
            "Iteration 38, loss = 0.66498715\n",
            "Iteration 39, loss = 0.66336993\n",
            "Iteration 40, loss = 0.66164717\n",
            "Iteration 41, loss = 0.65993163\n",
            "Iteration 42, loss = 0.65825685\n",
            "Iteration 43, loss = 0.65652794\n",
            "Iteration 44, loss = 0.65475301\n",
            "Iteration 45, loss = 0.65306233\n",
            "Iteration 46, loss = 0.65132867\n",
            "Iteration 47, loss = 0.64971525\n",
            "Iteration 48, loss = 0.64784329\n",
            "Iteration 49, loss = 0.64604618\n",
            "Iteration 50, loss = 0.64431958\n",
            "Iteration 51, loss = 0.64249726\n",
            "Iteration 52, loss = 0.64068181\n",
            "Iteration 53, loss = 0.63881843\n",
            "Iteration 54, loss = 0.63702251\n",
            "Iteration 55, loss = 0.63504125\n",
            "Iteration 56, loss = 0.63323766\n",
            "Iteration 57, loss = 0.63127836\n",
            "Iteration 58, loss = 0.62944866\n",
            "Iteration 59, loss = 0.62747126\n",
            "Iteration 60, loss = 0.62551701\n",
            "Iteration 61, loss = 0.62357347\n",
            "Iteration 62, loss = 0.62166666\n",
            "Iteration 63, loss = 0.61961538\n",
            "Iteration 64, loss = 0.61771511\n",
            "Iteration 65, loss = 0.61568907\n",
            "Iteration 66, loss = 0.61357757\n",
            "Iteration 67, loss = 0.61158518\n",
            "Iteration 68, loss = 0.60955263\n",
            "Iteration 69, loss = 0.60738554\n",
            "Iteration 70, loss = 0.60524979\n",
            "Iteration 71, loss = 0.60307657\n",
            "Iteration 72, loss = 0.60093628\n",
            "Iteration 73, loss = 0.59865610\n",
            "Iteration 74, loss = 0.59632908\n",
            "Iteration 75, loss = 0.59404343\n",
            "Iteration 76, loss = 0.59171347\n",
            "Iteration 77, loss = 0.58932323\n",
            "Iteration 78, loss = 0.58677856\n",
            "Iteration 79, loss = 0.58444748\n",
            "Iteration 80, loss = 0.58184610\n",
            "Iteration 81, loss = 0.57934441\n",
            "Iteration 82, loss = 0.57673096\n",
            "Iteration 83, loss = 0.57402066\n",
            "Iteration 84, loss = 0.57142345\n",
            "Iteration 85, loss = 0.56877073\n",
            "Iteration 86, loss = 0.56583627\n",
            "Iteration 87, loss = 0.56308047\n",
            "Iteration 88, loss = 0.56034915\n",
            "Iteration 89, loss = 0.55750068\n",
            "Iteration 90, loss = 0.55459360\n",
            "Iteration 91, loss = 0.55169789\n",
            "Iteration 92, loss = 0.54887774\n",
            "Iteration 93, loss = 0.54592127\n",
            "Iteration 94, loss = 0.54322880\n",
            "Iteration 95, loss = 0.54021494\n",
            "Iteration 96, loss = 0.53718091\n",
            "Iteration 97, loss = 0.53424742\n",
            "Iteration 98, loss = 0.53124954\n",
            "Iteration 99, loss = 0.52843288\n",
            "Iteration 100, loss = 0.52555212\n",
            "Iteration 101, loss = 0.52272130\n",
            "Iteration 102, loss = 0.51995480\n",
            "Iteration 103, loss = 0.51710135\n",
            "Iteration 104, loss = 0.51438949\n",
            "Iteration 105, loss = 0.51161868\n",
            "Iteration 106, loss = 0.50894413\n",
            "Iteration 107, loss = 0.50632661\n",
            "Iteration 108, loss = 0.50353683\n",
            "Iteration 109, loss = 0.50098781\n",
            "Iteration 110, loss = 0.49836837\n",
            "Iteration 111, loss = 0.49580435\n",
            "Iteration 112, loss = 0.49323686\n",
            "Iteration 113, loss = 0.49083032\n",
            "Iteration 114, loss = 0.48844998\n",
            "Iteration 115, loss = 0.48575512\n",
            "Iteration 116, loss = 0.48349653\n",
            "Iteration 117, loss = 0.48112852\n",
            "Iteration 118, loss = 0.47905945\n",
            "Iteration 119, loss = 0.47668849\n",
            "Iteration 120, loss = 0.47452056\n",
            "Iteration 121, loss = 0.47248018\n",
            "Iteration 122, loss = 0.47031308\n",
            "Iteration 123, loss = 0.46829551\n",
            "Iteration 124, loss = 0.46628052\n",
            "Iteration 125, loss = 0.46456509\n",
            "Iteration 126, loss = 0.46260875\n",
            "Iteration 127, loss = 0.46067992\n",
            "Iteration 128, loss = 0.45904107\n",
            "Iteration 129, loss = 0.45727411\n",
            "Iteration 130, loss = 0.45565911\n",
            "Iteration 131, loss = 0.45417202\n",
            "Iteration 132, loss = 0.45261421\n",
            "Iteration 133, loss = 0.45117784\n",
            "Iteration 134, loss = 0.44980588\n",
            "Iteration 135, loss = 0.44834248\n",
            "Iteration 136, loss = 0.44718400\n",
            "Iteration 137, loss = 0.44604824\n",
            "Iteration 138, loss = 0.44492536\n",
            "Iteration 139, loss = 0.44374942\n",
            "Iteration 140, loss = 0.44252061\n",
            "Iteration 141, loss = 0.44160412\n",
            "Iteration 142, loss = 0.44073869\n",
            "Iteration 143, loss = 0.43985038\n",
            "Iteration 144, loss = 0.43892601\n",
            "Iteration 145, loss = 0.43799511\n",
            "Iteration 146, loss = 0.43739191\n",
            "Iteration 147, loss = 0.43650284\n",
            "Iteration 148, loss = 0.43590222\n",
            "Iteration 149, loss = 0.43515267\n",
            "Iteration 150, loss = 0.43450117\n",
            "Iteration 151, loss = 0.43376146\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.75953291\n",
            "Iteration 2, loss = 0.75417243\n",
            "Iteration 3, loss = 0.74943076\n",
            "Iteration 4, loss = 0.74438786\n",
            "Iteration 5, loss = 0.74016053\n",
            "Iteration 6, loss = 0.73586091\n",
            "Iteration 7, loss = 0.73200649\n",
            "Iteration 8, loss = 0.72814469\n",
            "Iteration 9, loss = 0.72464777\n",
            "Iteration 10, loss = 0.72101594\n",
            "Iteration 11, loss = 0.71809953\n",
            "Iteration 12, loss = 0.71486209\n",
            "Iteration 13, loss = 0.71209564\n",
            "Iteration 14, loss = 0.70942513\n",
            "Iteration 15, loss = 0.70682926\n",
            "Iteration 16, loss = 0.70411184\n",
            "Iteration 17, loss = 0.70189777\n",
            "Iteration 18, loss = 0.69976570\n",
            "Iteration 19, loss = 0.69746951\n",
            "Iteration 20, loss = 0.69555623\n",
            "Iteration 21, loss = 0.69356050\n",
            "Iteration 22, loss = 0.69165340\n",
            "Iteration 23, loss = 0.68988708\n",
            "Iteration 24, loss = 0.68814454\n",
            "Iteration 25, loss = 0.68647856\n",
            "Iteration 26, loss = 0.68474028\n",
            "Iteration 27, loss = 0.68302684\n",
            "Iteration 28, loss = 0.68138147\n",
            "Iteration 29, loss = 0.67980522\n",
            "Iteration 30, loss = 0.67820022\n",
            "Iteration 31, loss = 0.67657792\n",
            "Iteration 32, loss = 0.67501853\n",
            "Iteration 33, loss = 0.67330621\n",
            "Iteration 34, loss = 0.67179857\n",
            "Iteration 35, loss = 0.67012619\n",
            "Iteration 36, loss = 0.66841623\n",
            "Iteration 37, loss = 0.66685178\n",
            "Iteration 38, loss = 0.66524259\n",
            "Iteration 39, loss = 0.66359989\n",
            "Iteration 40, loss = 0.66178579\n",
            "Iteration 41, loss = 0.66016219\n",
            "Iteration 42, loss = 0.65848494\n",
            "Iteration 43, loss = 0.65679940\n",
            "Iteration 44, loss = 0.65496874\n",
            "Iteration 45, loss = 0.65331496\n",
            "Iteration 46, loss = 0.65150942\n",
            "Iteration 47, loss = 0.64990839\n",
            "Iteration 48, loss = 0.64804652\n",
            "Iteration 49, loss = 0.64626367\n",
            "Iteration 50, loss = 0.64454359\n",
            "Iteration 51, loss = 0.64269139\n",
            "Iteration 52, loss = 0.64092792\n",
            "Iteration 53, loss = 0.63907712\n",
            "Iteration 54, loss = 0.63735698\n",
            "Iteration 55, loss = 0.63534407\n",
            "Iteration 56, loss = 0.63350633\n",
            "Iteration 57, loss = 0.63155043\n",
            "Iteration 58, loss = 0.62965365\n",
            "Iteration 59, loss = 0.62770569\n",
            "Iteration 60, loss = 0.62563249\n",
            "Iteration 61, loss = 0.62375692\n",
            "Iteration 62, loss = 0.62179546\n",
            "Iteration 63, loss = 0.61974841\n",
            "Iteration 64, loss = 0.61779221\n",
            "Iteration 65, loss = 0.61573300\n",
            "Iteration 66, loss = 0.61366582\n",
            "Iteration 67, loss = 0.61166447\n",
            "Iteration 68, loss = 0.60959893\n",
            "Iteration 69, loss = 0.60734534\n",
            "Iteration 70, loss = 0.60519150\n",
            "Iteration 71, loss = 0.60297470\n",
            "Iteration 72, loss = 0.60081175\n",
            "Iteration 73, loss = 0.59846819\n",
            "Iteration 74, loss = 0.59606229\n",
            "Iteration 75, loss = 0.59376623\n",
            "Iteration 76, loss = 0.59132884\n",
            "Iteration 77, loss = 0.58889967\n",
            "Iteration 78, loss = 0.58629634\n",
            "Iteration 79, loss = 0.58391074\n",
            "Iteration 80, loss = 0.58126922\n",
            "Iteration 81, loss = 0.57868529\n",
            "Iteration 82, loss = 0.57604350\n",
            "Iteration 83, loss = 0.57331558\n",
            "Iteration 84, loss = 0.57060817\n",
            "Iteration 85, loss = 0.56792992\n",
            "Iteration 86, loss = 0.56494215\n",
            "Iteration 87, loss = 0.56207035\n",
            "Iteration 88, loss = 0.55930479\n",
            "Iteration 89, loss = 0.55638948\n",
            "Iteration 90, loss = 0.55332213\n",
            "Iteration 91, loss = 0.55042962\n",
            "Iteration 92, loss = 0.54758966\n",
            "Iteration 93, loss = 0.54449339\n",
            "Iteration 94, loss = 0.54168314\n",
            "Iteration 95, loss = 0.53847032\n",
            "Iteration 96, loss = 0.53535034\n",
            "Iteration 97, loss = 0.53216104\n",
            "Iteration 98, loss = 0.52901696\n",
            "Iteration 99, loss = 0.52596746\n",
            "Iteration 100, loss = 0.52292258\n",
            "Iteration 101, loss = 0.51997077\n",
            "Iteration 102, loss = 0.51675818\n",
            "Iteration 103, loss = 0.51367653\n",
            "Iteration 104, loss = 0.51079430\n",
            "Iteration 105, loss = 0.50772990\n",
            "Iteration 106, loss = 0.50482858\n",
            "Iteration 107, loss = 0.50199880\n",
            "Iteration 108, loss = 0.49891078\n",
            "Iteration 109, loss = 0.49602651\n",
            "Iteration 110, loss = 0.49315469\n",
            "Iteration 111, loss = 0.49029353\n",
            "Iteration 112, loss = 0.48740123\n",
            "Iteration 113, loss = 0.48467733\n",
            "Iteration 114, loss = 0.48183321\n",
            "Iteration 115, loss = 0.47910683\n",
            "Iteration 116, loss = 0.47638676\n",
            "Iteration 117, loss = 0.47381542\n",
            "Iteration 118, loss = 0.47134203\n",
            "Iteration 119, loss = 0.46882755\n",
            "Iteration 120, loss = 0.46625181\n",
            "Iteration 121, loss = 0.46392824\n",
            "Iteration 122, loss = 0.46157005\n",
            "Iteration 123, loss = 0.45937010\n",
            "Iteration 124, loss = 0.45714303\n",
            "Iteration 125, loss = 0.45514339\n",
            "Iteration 126, loss = 0.45292922\n",
            "Iteration 127, loss = 0.45103981\n",
            "Iteration 128, loss = 0.44904779\n",
            "Iteration 129, loss = 0.44727846\n",
            "Iteration 130, loss = 0.44549954\n",
            "Iteration 131, loss = 0.44384559\n",
            "Iteration 132, loss = 0.44223444\n",
            "Iteration 133, loss = 0.44081938\n",
            "Iteration 134, loss = 0.43940922\n",
            "Iteration 135, loss = 0.43799812\n",
            "Iteration 136, loss = 0.43676226\n",
            "Iteration 137, loss = 0.43556079\n",
            "Iteration 138, loss = 0.43438284\n",
            "Iteration 139, loss = 0.43332021\n",
            "Iteration 140, loss = 0.43216640\n",
            "Iteration 141, loss = 0.43108853\n",
            "Iteration 142, loss = 0.43019369\n",
            "Iteration 143, loss = 0.42923099\n",
            "Iteration 144, loss = 0.42842318\n",
            "Iteration 145, loss = 0.42741465\n",
            "Iteration 146, loss = 0.42672318\n",
            "Iteration 147, loss = 0.42595310\n",
            "Iteration 148, loss = 0.42527555\n",
            "Iteration 149, loss = 0.42450932\n",
            "Iteration 150, loss = 0.42385039\n",
            "Iteration 151, loss = 0.42324106\n",
            "Iteration 152, loss = 0.42269043\n",
            "Iteration 153, loss = 0.42217051\n",
            "Iteration 154, loss = 0.42156908\n",
            "Iteration 155, loss = 0.42101535\n",
            "Iteration 156, loss = 0.42068912\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.76058778\n",
            "Iteration 2, loss = 0.75507986\n",
            "Iteration 3, loss = 0.75025602\n",
            "Iteration 4, loss = 0.74522093\n",
            "Iteration 5, loss = 0.74068733\n",
            "Iteration 6, loss = 0.73663146\n",
            "Iteration 7, loss = 0.73248556\n",
            "Iteration 8, loss = 0.72871671\n",
            "Iteration 9, loss = 0.72514066\n",
            "Iteration 10, loss = 0.72144522\n",
            "Iteration 11, loss = 0.71845660\n",
            "Iteration 12, loss = 0.71538132\n",
            "Iteration 13, loss = 0.71235231\n",
            "Iteration 14, loss = 0.70962974\n",
            "Iteration 15, loss = 0.70700934\n",
            "Iteration 16, loss = 0.70434934\n",
            "Iteration 17, loss = 0.70199086\n",
            "Iteration 18, loss = 0.69984498\n",
            "Iteration 19, loss = 0.69764991\n",
            "Iteration 20, loss = 0.69565748\n",
            "Iteration 21, loss = 0.69369294\n",
            "Iteration 22, loss = 0.69171519\n",
            "Iteration 23, loss = 0.69005662\n",
            "Iteration 24, loss = 0.68823147\n",
            "Iteration 25, loss = 0.68658151\n",
            "Iteration 26, loss = 0.68493504\n",
            "Iteration 27, loss = 0.68317996\n",
            "Iteration 28, loss = 0.68161154\n",
            "Iteration 29, loss = 0.68004968\n",
            "Iteration 30, loss = 0.67851250\n",
            "Iteration 31, loss = 0.67689211\n",
            "Iteration 32, loss = 0.67531274\n",
            "Iteration 33, loss = 0.67373546\n",
            "Iteration 34, loss = 0.67226887\n",
            "Iteration 35, loss = 0.67066120\n",
            "Iteration 36, loss = 0.66903910\n",
            "Iteration 37, loss = 0.66754631\n",
            "Iteration 38, loss = 0.66595037\n",
            "Iteration 39, loss = 0.66446290\n",
            "Iteration 40, loss = 0.66269075\n",
            "Iteration 41, loss = 0.66117340\n",
            "Iteration 42, loss = 0.65949663\n",
            "Iteration 43, loss = 0.65787337\n",
            "Iteration 44, loss = 0.65616877\n",
            "Iteration 45, loss = 0.65456583\n",
            "Iteration 46, loss = 0.65286949\n",
            "Iteration 47, loss = 0.65129850\n",
            "Iteration 48, loss = 0.64952027\n",
            "Iteration 49, loss = 0.64779723\n",
            "Iteration 50, loss = 0.64613641\n",
            "Iteration 51, loss = 0.64442511\n",
            "Iteration 52, loss = 0.64266095\n",
            "Iteration 53, loss = 0.64086752\n",
            "Iteration 54, loss = 0.63922277\n",
            "Iteration 55, loss = 0.63730772\n",
            "Iteration 56, loss = 0.63552507\n",
            "Iteration 57, loss = 0.63358714\n",
            "Iteration 58, loss = 0.63178580\n",
            "Iteration 59, loss = 0.62988393\n",
            "Iteration 60, loss = 0.62785405\n",
            "Iteration 61, loss = 0.62595778\n",
            "Iteration 62, loss = 0.62402838\n",
            "Iteration 63, loss = 0.62208391\n",
            "Iteration 64, loss = 0.62016001\n",
            "Iteration 65, loss = 0.61818697\n",
            "Iteration 66, loss = 0.61624511\n",
            "Iteration 67, loss = 0.61421879\n",
            "Iteration 68, loss = 0.61223356\n",
            "Iteration 69, loss = 0.61019615\n",
            "Iteration 70, loss = 0.60812785\n",
            "Iteration 71, loss = 0.60605098\n",
            "Iteration 72, loss = 0.60397939\n",
            "Iteration 73, loss = 0.60185931\n",
            "Iteration 74, loss = 0.59957460\n",
            "Iteration 75, loss = 0.59740907\n",
            "Iteration 76, loss = 0.59523747\n",
            "Iteration 77, loss = 0.59296187\n",
            "Iteration 78, loss = 0.59062545\n",
            "Iteration 79, loss = 0.58846589\n",
            "Iteration 80, loss = 0.58605842\n",
            "Iteration 81, loss = 0.58371066\n",
            "Iteration 82, loss = 0.58136939\n",
            "Iteration 83, loss = 0.57882217\n",
            "Iteration 84, loss = 0.57645429\n",
            "Iteration 85, loss = 0.57394681\n",
            "Iteration 86, loss = 0.57118675\n",
            "Iteration 87, loss = 0.56859749\n",
            "Iteration 88, loss = 0.56607496\n",
            "Iteration 89, loss = 0.56329273\n",
            "Iteration 90, loss = 0.56061401\n",
            "Iteration 91, loss = 0.55791634\n",
            "Iteration 92, loss = 0.55524934\n",
            "Iteration 93, loss = 0.55244713\n",
            "Iteration 94, loss = 0.54980285\n",
            "Iteration 95, loss = 0.54681206\n",
            "Iteration 96, loss = 0.54397313\n",
            "Iteration 97, loss = 0.54105365\n",
            "Iteration 98, loss = 0.53800898\n",
            "Iteration 99, loss = 0.53525073\n",
            "Iteration 100, loss = 0.53236641\n",
            "Iteration 101, loss = 0.52959359\n",
            "Iteration 102, loss = 0.52667312\n",
            "Iteration 103, loss = 0.52388526\n",
            "Iteration 104, loss = 0.52112741\n",
            "Iteration 105, loss = 0.51829733\n",
            "Iteration 106, loss = 0.51555991\n",
            "Iteration 107, loss = 0.51293480\n",
            "Iteration 108, loss = 0.51016301\n",
            "Iteration 109, loss = 0.50754844\n",
            "Iteration 110, loss = 0.50489325\n",
            "Iteration 111, loss = 0.50234018\n",
            "Iteration 112, loss = 0.49971806\n",
            "Iteration 113, loss = 0.49741166\n",
            "Iteration 114, loss = 0.49484671\n",
            "Iteration 115, loss = 0.49254044\n",
            "Iteration 116, loss = 0.49012478\n",
            "Iteration 117, loss = 0.48788846\n",
            "Iteration 118, loss = 0.48577406\n",
            "Iteration 119, loss = 0.48376312\n",
            "Iteration 120, loss = 0.48156042\n",
            "Iteration 121, loss = 0.47943533\n",
            "Iteration 122, loss = 0.47740170\n",
            "Iteration 123, loss = 0.47545197\n",
            "Iteration 124, loss = 0.47355031\n",
            "Iteration 125, loss = 0.47195715\n",
            "Iteration 126, loss = 0.46998922\n",
            "Iteration 127, loss = 0.46824923\n",
            "Iteration 128, loss = 0.46650778\n",
            "Iteration 129, loss = 0.46483751\n",
            "Iteration 130, loss = 0.46330234\n",
            "Iteration 131, loss = 0.46183116\n",
            "Iteration 132, loss = 0.46031824\n",
            "Iteration 133, loss = 0.45898271\n",
            "Iteration 134, loss = 0.45768737\n",
            "Iteration 135, loss = 0.45624355\n",
            "Iteration 136, loss = 0.45503983\n",
            "Iteration 137, loss = 0.45393086\n",
            "Iteration 138, loss = 0.45274034\n",
            "Iteration 139, loss = 0.45161871\n",
            "Iteration 140, loss = 0.45052968\n",
            "Iteration 141, loss = 0.44947956\n",
            "Iteration 142, loss = 0.44850484\n",
            "Iteration 143, loss = 0.44765240\n",
            "Iteration 144, loss = 0.44686350\n",
            "Iteration 145, loss = 0.44585630\n",
            "Iteration 146, loss = 0.44503291\n",
            "Iteration 147, loss = 0.44441387\n",
            "Iteration 148, loss = 0.44371878\n",
            "Iteration 149, loss = 0.44301413\n",
            "Iteration 150, loss = 0.44233744\n",
            "Iteration 151, loss = 0.44150625\n",
            "Iteration 152, loss = 0.44090188\n",
            "Iteration 153, loss = 0.44036877\n",
            "Iteration 154, loss = 0.43977258\n",
            "Iteration 155, loss = 0.43924302\n",
            "Iteration 156, loss = 0.43871938\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.75779631\n",
            "Iteration 2, loss = 0.75240383\n",
            "Iteration 3, loss = 0.74749813\n",
            "Iteration 4, loss = 0.74267498\n",
            "Iteration 5, loss = 0.73820095\n",
            "Iteration 6, loss = 0.73410299\n",
            "Iteration 7, loss = 0.73010241\n",
            "Iteration 8, loss = 0.72631787\n",
            "Iteration 9, loss = 0.72291995\n",
            "Iteration 10, loss = 0.71918914\n",
            "Iteration 11, loss = 0.71611632\n",
            "Iteration 12, loss = 0.71322858\n",
            "Iteration 13, loss = 0.71026386\n",
            "Iteration 14, loss = 0.70737242\n",
            "Iteration 15, loss = 0.70496315\n",
            "Iteration 16, loss = 0.70223407\n",
            "Iteration 17, loss = 0.70002558\n",
            "Iteration 18, loss = 0.69769211\n",
            "Iteration 19, loss = 0.69561203\n",
            "Iteration 20, loss = 0.69358145\n",
            "Iteration 21, loss = 0.69166693\n",
            "Iteration 22, loss = 0.68980242\n",
            "Iteration 23, loss = 0.68808049\n",
            "Iteration 24, loss = 0.68645314\n",
            "Iteration 25, loss = 0.68470545\n",
            "Iteration 26, loss = 0.68312144\n",
            "Iteration 27, loss = 0.68145126\n",
            "Iteration 28, loss = 0.67989803\n",
            "Iteration 29, loss = 0.67837809\n",
            "Iteration 30, loss = 0.67684987\n",
            "Iteration 31, loss = 0.67527589\n",
            "Iteration 32, loss = 0.67377180\n",
            "Iteration 33, loss = 0.67218679\n",
            "Iteration 34, loss = 0.67072992\n",
            "Iteration 35, loss = 0.66924414\n",
            "Iteration 36, loss = 0.66760006\n",
            "Iteration 37, loss = 0.66605267\n",
            "Iteration 38, loss = 0.66451173\n",
            "Iteration 39, loss = 0.66302294\n",
            "Iteration 40, loss = 0.66125214\n",
            "Iteration 41, loss = 0.65978461\n",
            "Iteration 42, loss = 0.65814698\n",
            "Iteration 43, loss = 0.65655785\n",
            "Iteration 44, loss = 0.65491839\n",
            "Iteration 45, loss = 0.65333278\n",
            "Iteration 46, loss = 0.65165971\n",
            "Iteration 47, loss = 0.65008737\n",
            "Iteration 48, loss = 0.64837287\n",
            "Iteration 49, loss = 0.64670175\n",
            "Iteration 50, loss = 0.64502097\n",
            "Iteration 51, loss = 0.64336950\n",
            "Iteration 52, loss = 0.64164304\n",
            "Iteration 53, loss = 0.63995356\n",
            "Iteration 54, loss = 0.63827438\n",
            "Iteration 55, loss = 0.63652251\n",
            "Iteration 56, loss = 0.63475391\n",
            "Iteration 57, loss = 0.63295407\n",
            "Iteration 58, loss = 0.63115929\n",
            "Iteration 59, loss = 0.62935431\n",
            "Iteration 60, loss = 0.62748335\n",
            "Iteration 61, loss = 0.62562071\n",
            "Iteration 62, loss = 0.62389688\n",
            "Iteration 63, loss = 0.62198718\n",
            "Iteration 64, loss = 0.62013754\n",
            "Iteration 65, loss = 0.61830645\n",
            "Iteration 66, loss = 0.61636304\n",
            "Iteration 67, loss = 0.61449548\n",
            "Iteration 68, loss = 0.61257982\n",
            "Iteration 69, loss = 0.61059985\n",
            "Iteration 70, loss = 0.60859097\n",
            "Iteration 71, loss = 0.60662091\n",
            "Iteration 72, loss = 0.60466471\n",
            "Iteration 73, loss = 0.60252255\n",
            "Iteration 74, loss = 0.60042944\n",
            "Iteration 75, loss = 0.59828358\n",
            "Iteration 76, loss = 0.59618178\n",
            "Iteration 77, loss = 0.59392548\n",
            "Iteration 78, loss = 0.59163232\n",
            "Iteration 79, loss = 0.58955239\n",
            "Iteration 80, loss = 0.58711462\n",
            "Iteration 81, loss = 0.58470137\n",
            "Iteration 82, loss = 0.58238225\n",
            "Iteration 83, loss = 0.57977099\n",
            "Iteration 84, loss = 0.57733040\n",
            "Iteration 85, loss = 0.57480996\n",
            "Iteration 86, loss = 0.57214922\n",
            "Iteration 87, loss = 0.56948650\n",
            "Iteration 88, loss = 0.56690234\n",
            "Iteration 89, loss = 0.56401376\n",
            "Iteration 90, loss = 0.56130818\n",
            "Iteration 91, loss = 0.55862007\n",
            "Iteration 92, loss = 0.55586163\n",
            "Iteration 93, loss = 0.55294021\n",
            "Iteration 94, loss = 0.55029755\n",
            "Iteration 95, loss = 0.54729310\n",
            "Iteration 96, loss = 0.54434687\n",
            "Iteration 97, loss = 0.54148311\n",
            "Iteration 98, loss = 0.53834990\n",
            "Iteration 99, loss = 0.53564029\n",
            "Iteration 100, loss = 0.53276903\n",
            "Iteration 101, loss = 0.52995052\n",
            "Iteration 102, loss = 0.52718483\n",
            "Iteration 103, loss = 0.52446250\n",
            "Iteration 104, loss = 0.52161353\n",
            "Iteration 105, loss = 0.51890074\n",
            "Iteration 106, loss = 0.51625466\n",
            "Iteration 107, loss = 0.51355848\n",
            "Iteration 108, loss = 0.51086315\n",
            "Iteration 109, loss = 0.50814060\n",
            "Iteration 110, loss = 0.50547422\n",
            "Iteration 111, loss = 0.50287025\n",
            "Iteration 112, loss = 0.50031573\n",
            "Iteration 113, loss = 0.49773656\n",
            "Iteration 114, loss = 0.49522131\n",
            "Iteration 115, loss = 0.49264665\n",
            "Iteration 116, loss = 0.49020256\n",
            "Iteration 117, loss = 0.48801099\n",
            "Iteration 118, loss = 0.48557274\n",
            "Iteration 119, loss = 0.48333303\n",
            "Iteration 120, loss = 0.48089994\n",
            "Iteration 121, loss = 0.47868457\n",
            "Iteration 122, loss = 0.47657849\n",
            "Iteration 123, loss = 0.47445921\n",
            "Iteration 124, loss = 0.47251091\n",
            "Iteration 125, loss = 0.47042285\n",
            "Iteration 126, loss = 0.46825379\n",
            "Iteration 127, loss = 0.46631401\n",
            "Iteration 128, loss = 0.46432822\n",
            "Iteration 129, loss = 0.46250481\n",
            "Iteration 130, loss = 0.46072313\n",
            "Iteration 131, loss = 0.45903452\n",
            "Iteration 132, loss = 0.45745114\n",
            "Iteration 133, loss = 0.45601218\n",
            "Iteration 134, loss = 0.45421004\n",
            "Iteration 135, loss = 0.45264666\n",
            "Iteration 136, loss = 0.45125924\n",
            "Iteration 137, loss = 0.44991341\n",
            "Iteration 138, loss = 0.44854708\n",
            "Iteration 139, loss = 0.44727700\n",
            "Iteration 140, loss = 0.44612024\n",
            "Iteration 141, loss = 0.44490149\n",
            "Iteration 142, loss = 0.44386733\n",
            "Iteration 143, loss = 0.44276411\n",
            "Iteration 144, loss = 0.44189042\n",
            "Iteration 145, loss = 0.44086481\n",
            "Iteration 146, loss = 0.44000075\n",
            "Iteration 147, loss = 0.43908069\n",
            "Iteration 148, loss = 0.43822479\n",
            "Iteration 149, loss = 0.43743920\n",
            "Iteration 150, loss = 0.43664044\n",
            "Iteration 151, loss = 0.43586215\n",
            "Iteration 152, loss = 0.43509092\n",
            "Iteration 153, loss = 0.43453114\n",
            "Iteration 154, loss = 0.43373983\n",
            "Iteration 155, loss = 0.43297950\n",
            "Iteration 156, loss = 0.43237230\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.75913425\n",
            "Iteration 2, loss = 0.75388585\n",
            "Iteration 3, loss = 0.74881340\n",
            "Iteration 4, loss = 0.74413510\n",
            "Iteration 5, loss = 0.73940839\n",
            "Iteration 6, loss = 0.73523402\n",
            "Iteration 7, loss = 0.73104754\n",
            "Iteration 8, loss = 0.72704380\n",
            "Iteration 9, loss = 0.72357753\n",
            "Iteration 10, loss = 0.71966952\n",
            "Iteration 11, loss = 0.71650654\n",
            "Iteration 12, loss = 0.71345175\n",
            "Iteration 13, loss = 0.71041614\n",
            "Iteration 14, loss = 0.70749506\n",
            "Iteration 15, loss = 0.70490016\n",
            "Iteration 16, loss = 0.70212025\n",
            "Iteration 17, loss = 0.69975824\n",
            "Iteration 18, loss = 0.69745541\n",
            "Iteration 19, loss = 0.69513718\n",
            "Iteration 20, loss = 0.69310528\n",
            "Iteration 21, loss = 0.69099671\n",
            "Iteration 22, loss = 0.68898554\n",
            "Iteration 23, loss = 0.68708495\n",
            "Iteration 24, loss = 0.68543631\n",
            "Iteration 25, loss = 0.68350654\n",
            "Iteration 26, loss = 0.68173033\n",
            "Iteration 27, loss = 0.67996747\n",
            "Iteration 28, loss = 0.67824814\n",
            "Iteration 29, loss = 0.67656494\n",
            "Iteration 30, loss = 0.67486414\n",
            "Iteration 31, loss = 0.67313249\n",
            "Iteration 32, loss = 0.67139967\n",
            "Iteration 33, loss = 0.66964318\n",
            "Iteration 34, loss = 0.66800071\n",
            "Iteration 35, loss = 0.66639216\n",
            "Iteration 36, loss = 0.66455460\n",
            "Iteration 37, loss = 0.66288916\n",
            "Iteration 38, loss = 0.66121399\n",
            "Iteration 39, loss = 0.65961219\n",
            "Iteration 40, loss = 0.65774576\n",
            "Iteration 41, loss = 0.65613232\n",
            "Iteration 42, loss = 0.65447698\n",
            "Iteration 43, loss = 0.65276631\n",
            "Iteration 44, loss = 0.65103287\n",
            "Iteration 45, loss = 0.64935963\n",
            "Iteration 46, loss = 0.64762847\n",
            "Iteration 47, loss = 0.64598099\n",
            "Iteration 48, loss = 0.64418326\n",
            "Iteration 49, loss = 0.64248950\n",
            "Iteration 50, loss = 0.64071371\n",
            "Iteration 51, loss = 0.63901786\n",
            "Iteration 52, loss = 0.63731572\n",
            "Iteration 53, loss = 0.63560108\n",
            "Iteration 54, loss = 0.63385417\n",
            "Iteration 55, loss = 0.63205942\n",
            "Iteration 56, loss = 0.63033657\n",
            "Iteration 57, loss = 0.62850993\n",
            "Iteration 58, loss = 0.62672534\n",
            "Iteration 59, loss = 0.62496716\n",
            "Iteration 60, loss = 0.62315569\n",
            "Iteration 61, loss = 0.62139426\n",
            "Iteration 62, loss = 0.61970842\n",
            "Iteration 63, loss = 0.61778775\n",
            "Iteration 64, loss = 0.61609866\n",
            "Iteration 65, loss = 0.61429004\n",
            "Iteration 66, loss = 0.61243045\n",
            "Iteration 67, loss = 0.61069917\n",
            "Iteration 68, loss = 0.60885757\n",
            "Iteration 69, loss = 0.60700774\n",
            "Iteration 70, loss = 0.60512929\n",
            "Iteration 71, loss = 0.60324028\n",
            "Iteration 72, loss = 0.60148282\n",
            "Iteration 73, loss = 0.59946162\n",
            "Iteration 74, loss = 0.59755990\n",
            "Iteration 75, loss = 0.59551262\n",
            "Iteration 76, loss = 0.59355823\n",
            "Iteration 77, loss = 0.59146277\n",
            "Iteration 78, loss = 0.58934573\n",
            "Iteration 79, loss = 0.58737504\n",
            "Iteration 80, loss = 0.58509332\n",
            "Iteration 81, loss = 0.58284369\n",
            "Iteration 82, loss = 0.58058069\n",
            "Iteration 83, loss = 0.57826768\n",
            "Iteration 84, loss = 0.57596965\n",
            "Iteration 85, loss = 0.57357555\n",
            "Iteration 86, loss = 0.57098751\n",
            "Iteration 87, loss = 0.56854294\n",
            "Iteration 88, loss = 0.56604295\n",
            "Iteration 89, loss = 0.56336907\n",
            "Iteration 90, loss = 0.56073663\n",
            "Iteration 91, loss = 0.55813754\n",
            "Iteration 92, loss = 0.55540124\n",
            "Iteration 93, loss = 0.55253558\n",
            "Iteration 94, loss = 0.54993259\n",
            "Iteration 95, loss = 0.54686754\n",
            "Iteration 96, loss = 0.54399795\n",
            "Iteration 97, loss = 0.54111467\n",
            "Iteration 98, loss = 0.53809132\n",
            "Iteration 99, loss = 0.53515544\n",
            "Iteration 100, loss = 0.53227137\n",
            "Iteration 101, loss = 0.52942367\n",
            "Iteration 102, loss = 0.52661173\n",
            "Iteration 103, loss = 0.52366608\n",
            "Iteration 104, loss = 0.52079070\n",
            "Iteration 105, loss = 0.51795096\n",
            "Iteration 106, loss = 0.51514237\n",
            "Iteration 107, loss = 0.51234273\n",
            "Iteration 108, loss = 0.50953491\n",
            "Iteration 109, loss = 0.50667134\n",
            "Iteration 110, loss = 0.50388159\n",
            "Iteration 111, loss = 0.50120815\n",
            "Iteration 112, loss = 0.49854336\n",
            "Iteration 113, loss = 0.49586308\n",
            "Iteration 114, loss = 0.49325471\n",
            "Iteration 115, loss = 0.49059716\n",
            "Iteration 116, loss = 0.48840432\n",
            "Iteration 117, loss = 0.48581760\n",
            "Iteration 118, loss = 0.48323896\n",
            "Iteration 119, loss = 0.48091794\n",
            "Iteration 120, loss = 0.47853823\n",
            "Iteration 121, loss = 0.47623041\n",
            "Iteration 122, loss = 0.47396840\n",
            "Iteration 123, loss = 0.47179691\n",
            "Iteration 124, loss = 0.46966454\n",
            "Iteration 125, loss = 0.46776013\n",
            "Iteration 126, loss = 0.46572926\n",
            "Iteration 127, loss = 0.46374543\n",
            "Iteration 128, loss = 0.46165701\n",
            "Iteration 129, loss = 0.45984864\n",
            "Iteration 130, loss = 0.45816679\n",
            "Iteration 131, loss = 0.45645128\n",
            "Iteration 132, loss = 0.45485856\n",
            "Iteration 133, loss = 0.45331653\n",
            "Iteration 134, loss = 0.45177847\n",
            "Iteration 135, loss = 0.45035390\n",
            "Iteration 136, loss = 0.44883037\n",
            "Iteration 137, loss = 0.44755713\n",
            "Iteration 138, loss = 0.44633782\n",
            "Iteration 139, loss = 0.44516235\n",
            "Iteration 140, loss = 0.44406517\n",
            "Iteration 141, loss = 0.44296358\n",
            "Iteration 142, loss = 0.44202267\n",
            "Iteration 143, loss = 0.44107271\n",
            "Iteration 144, loss = 0.44016126\n",
            "Iteration 145, loss = 0.43928773\n",
            "Iteration 146, loss = 0.43866228\n",
            "Iteration 147, loss = 0.43773348\n",
            "Iteration 148, loss = 0.43723502\n",
            "Iteration 149, loss = 0.43656757\n",
            "Iteration 150, loss = 0.43596681\n",
            "Iteration 151, loss = 0.43525559\n",
            "Iteration 152, loss = 0.43463538\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.75775441\n",
            "Iteration 2, loss = 0.75243319\n",
            "Iteration 3, loss = 0.74755596\n",
            "Iteration 4, loss = 0.74297823\n",
            "Iteration 5, loss = 0.73815897\n",
            "Iteration 6, loss = 0.73416311\n",
            "Iteration 7, loss = 0.73015614\n",
            "Iteration 8, loss = 0.72625260\n",
            "Iteration 9, loss = 0.72275917\n",
            "Iteration 10, loss = 0.71901404\n",
            "Iteration 11, loss = 0.71595904\n",
            "Iteration 12, loss = 0.71299836\n",
            "Iteration 13, loss = 0.70993720\n",
            "Iteration 14, loss = 0.70713516\n",
            "Iteration 15, loss = 0.70464491\n",
            "Iteration 16, loss = 0.70191857\n",
            "Iteration 17, loss = 0.69963407\n",
            "Iteration 18, loss = 0.69741324\n",
            "Iteration 19, loss = 0.69514889\n",
            "Iteration 20, loss = 0.69315202\n",
            "Iteration 21, loss = 0.69123284\n",
            "Iteration 22, loss = 0.68915827\n",
            "Iteration 23, loss = 0.68738344\n",
            "Iteration 24, loss = 0.68572038\n",
            "Iteration 25, loss = 0.68388599\n",
            "Iteration 26, loss = 0.68221573\n",
            "Iteration 27, loss = 0.68048597\n",
            "Iteration 28, loss = 0.67877032\n",
            "Iteration 29, loss = 0.67713401\n",
            "Iteration 30, loss = 0.67546863\n",
            "Iteration 31, loss = 0.67382019\n",
            "Iteration 32, loss = 0.67217660\n",
            "Iteration 33, loss = 0.67042438\n",
            "Iteration 34, loss = 0.66885723\n",
            "Iteration 35, loss = 0.66730639\n",
            "Iteration 36, loss = 0.66549610\n",
            "Iteration 37, loss = 0.66388493\n",
            "Iteration 38, loss = 0.66222263\n",
            "Iteration 39, loss = 0.66069245\n",
            "Iteration 40, loss = 0.65889109\n",
            "Iteration 41, loss = 0.65727268\n",
            "Iteration 42, loss = 0.65560200\n",
            "Iteration 43, loss = 0.65386598\n",
            "Iteration 44, loss = 0.65217357\n",
            "Iteration 45, loss = 0.65044596\n",
            "Iteration 46, loss = 0.64874690\n",
            "Iteration 47, loss = 0.64700074\n",
            "Iteration 48, loss = 0.64522705\n",
            "Iteration 49, loss = 0.64344682\n",
            "Iteration 50, loss = 0.64168966\n",
            "Iteration 51, loss = 0.63996914\n",
            "Iteration 52, loss = 0.63817945\n",
            "Iteration 53, loss = 0.63642003\n",
            "Iteration 54, loss = 0.63466680\n",
            "Iteration 55, loss = 0.63271936\n",
            "Iteration 56, loss = 0.63097261\n",
            "Iteration 57, loss = 0.62905551\n",
            "Iteration 58, loss = 0.62723813\n",
            "Iteration 59, loss = 0.62531559\n",
            "Iteration 60, loss = 0.62340245\n",
            "Iteration 61, loss = 0.62152098\n",
            "Iteration 62, loss = 0.61967004\n",
            "Iteration 63, loss = 0.61762063\n",
            "Iteration 64, loss = 0.61575842\n",
            "Iteration 65, loss = 0.61383621\n",
            "Iteration 66, loss = 0.61173805\n",
            "Iteration 67, loss = 0.60997245\n",
            "Iteration 68, loss = 0.60779381\n",
            "Iteration 69, loss = 0.60583214\n",
            "Iteration 70, loss = 0.60374580\n",
            "Iteration 71, loss = 0.60163866\n",
            "Iteration 72, loss = 0.59966571\n",
            "Iteration 73, loss = 0.59750662\n",
            "Iteration 74, loss = 0.59528949\n",
            "Iteration 75, loss = 0.59313778\n",
            "Iteration 76, loss = 0.59097556\n",
            "Iteration 77, loss = 0.58873114\n",
            "Iteration 78, loss = 0.58634454\n",
            "Iteration 79, loss = 0.58419775\n",
            "Iteration 80, loss = 0.58183084\n",
            "Iteration 81, loss = 0.57934689\n",
            "Iteration 82, loss = 0.57696989\n",
            "Iteration 83, loss = 0.57449515\n",
            "Iteration 84, loss = 0.57212327\n",
            "Iteration 85, loss = 0.56959007\n",
            "Iteration 86, loss = 0.56689814\n",
            "Iteration 87, loss = 0.56420756\n",
            "Iteration 88, loss = 0.56167700\n",
            "Iteration 89, loss = 0.55892367\n",
            "Iteration 90, loss = 0.55630738\n",
            "Iteration 91, loss = 0.55359331\n",
            "Iteration 92, loss = 0.55082783\n",
            "Iteration 93, loss = 0.54798011\n",
            "Iteration 94, loss = 0.54535255\n",
            "Iteration 95, loss = 0.54233338\n",
            "Iteration 96, loss = 0.53952895\n",
            "Iteration 97, loss = 0.53675512\n",
            "Iteration 98, loss = 0.53382467\n",
            "Iteration 99, loss = 0.53098228\n",
            "Iteration 100, loss = 0.52823432\n",
            "Iteration 101, loss = 0.52553160\n",
            "Iteration 102, loss = 0.52273950\n",
            "Iteration 103, loss = 0.51980486\n",
            "Iteration 104, loss = 0.51716745\n",
            "Iteration 105, loss = 0.51440409\n",
            "Iteration 106, loss = 0.51165515\n",
            "Iteration 107, loss = 0.50889971\n",
            "Iteration 108, loss = 0.50630421\n",
            "Iteration 109, loss = 0.50366709\n",
            "Iteration 110, loss = 0.50099868\n",
            "Iteration 111, loss = 0.49842258\n",
            "Iteration 112, loss = 0.49594701\n",
            "Iteration 113, loss = 0.49339031\n",
            "Iteration 114, loss = 0.49102036\n",
            "Iteration 115, loss = 0.48842099\n",
            "Iteration 116, loss = 0.48617745\n",
            "Iteration 117, loss = 0.48382533\n",
            "Iteration 118, loss = 0.48131578\n",
            "Iteration 119, loss = 0.47929971\n",
            "Iteration 120, loss = 0.47716317\n",
            "Iteration 121, loss = 0.47506929\n",
            "Iteration 122, loss = 0.47283125\n",
            "Iteration 123, loss = 0.47071701\n",
            "Iteration 124, loss = 0.46891169\n",
            "Iteration 125, loss = 0.46706329\n",
            "Iteration 126, loss = 0.46529456\n",
            "Iteration 127, loss = 0.46335648\n",
            "Iteration 128, loss = 0.46165091\n",
            "Iteration 129, loss = 0.45998577\n",
            "Iteration 130, loss = 0.45847807\n",
            "Iteration 131, loss = 0.45707797\n",
            "Iteration 132, loss = 0.45566218\n",
            "Iteration 133, loss = 0.45427286\n",
            "Iteration 134, loss = 0.45309202\n",
            "Iteration 135, loss = 0.45168669\n",
            "Iteration 136, loss = 0.45062935\n",
            "Iteration 137, loss = 0.44936977\n",
            "Iteration 138, loss = 0.44842834\n",
            "Iteration 139, loss = 0.44733582\n",
            "Iteration 140, loss = 0.44629093\n",
            "Iteration 141, loss = 0.44546860\n",
            "Iteration 142, loss = 0.44467088\n",
            "Iteration 143, loss = 0.44393659\n",
            "Iteration 144, loss = 0.44303001\n",
            "Iteration 145, loss = 0.44228344\n",
            "Iteration 146, loss = 0.44177255\n",
            "Iteration 147, loss = 0.44100423\n",
            "Iteration 148, loss = 0.44051086\n",
            "Iteration 149, loss = 0.43978707\n",
            "Iteration 150, loss = 0.43924049\n",
            "Iteration 151, loss = 0.43865340\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.75881546\n",
            "Iteration 2, loss = 0.75346722\n",
            "Iteration 3, loss = 0.74839520\n",
            "Iteration 4, loss = 0.74370273\n",
            "Iteration 5, loss = 0.73880438\n",
            "Iteration 6, loss = 0.73463356\n",
            "Iteration 7, loss = 0.73044612\n",
            "Iteration 8, loss = 0.72639426\n",
            "Iteration 9, loss = 0.72273030\n",
            "Iteration 10, loss = 0.71881835\n",
            "Iteration 11, loss = 0.71561209\n",
            "Iteration 12, loss = 0.71253459\n",
            "Iteration 13, loss = 0.70933421\n",
            "Iteration 14, loss = 0.70643104\n",
            "Iteration 15, loss = 0.70383568\n",
            "Iteration 16, loss = 0.70094925\n",
            "Iteration 17, loss = 0.69860265\n",
            "Iteration 18, loss = 0.69627842\n",
            "Iteration 19, loss = 0.69386896\n",
            "Iteration 20, loss = 0.69177832\n",
            "Iteration 21, loss = 0.68975885\n",
            "Iteration 22, loss = 0.68765178\n",
            "Iteration 23, loss = 0.68576690\n",
            "Iteration 24, loss = 0.68393863\n",
            "Iteration 25, loss = 0.68206593\n",
            "Iteration 26, loss = 0.68031263\n",
            "Iteration 27, loss = 0.67849115\n",
            "Iteration 28, loss = 0.67665292\n",
            "Iteration 29, loss = 0.67491960\n",
            "Iteration 30, loss = 0.67317453\n",
            "Iteration 31, loss = 0.67143114\n",
            "Iteration 32, loss = 0.66973743\n",
            "Iteration 33, loss = 0.66793570\n",
            "Iteration 34, loss = 0.66629663\n",
            "Iteration 35, loss = 0.66470663\n",
            "Iteration 36, loss = 0.66291448\n",
            "Iteration 37, loss = 0.66124130\n",
            "Iteration 38, loss = 0.65954948\n",
            "Iteration 39, loss = 0.65797177\n",
            "Iteration 40, loss = 0.65614870\n",
            "Iteration 41, loss = 0.65452935\n",
            "Iteration 42, loss = 0.65282210\n",
            "Iteration 43, loss = 0.65112667\n",
            "Iteration 44, loss = 0.64947194\n",
            "Iteration 45, loss = 0.64773789\n",
            "Iteration 46, loss = 0.64612235\n",
            "Iteration 47, loss = 0.64440125\n",
            "Iteration 48, loss = 0.64270067\n",
            "Iteration 49, loss = 0.64091470\n",
            "Iteration 50, loss = 0.63926965\n",
            "Iteration 51, loss = 0.63755701\n",
            "Iteration 52, loss = 0.63591244\n",
            "Iteration 53, loss = 0.63416663\n",
            "Iteration 54, loss = 0.63254602\n",
            "Iteration 55, loss = 0.63068683\n",
            "Iteration 56, loss = 0.62903625\n",
            "Iteration 57, loss = 0.62723641\n",
            "Iteration 58, loss = 0.62552816\n",
            "Iteration 59, loss = 0.62373396\n",
            "Iteration 60, loss = 0.62194768\n",
            "Iteration 61, loss = 0.62031078\n",
            "Iteration 62, loss = 0.61858647\n",
            "Iteration 63, loss = 0.61668939\n",
            "Iteration 64, loss = 0.61502946\n",
            "Iteration 65, loss = 0.61338180\n",
            "Iteration 66, loss = 0.61148210\n",
            "Iteration 67, loss = 0.60998413\n",
            "Iteration 68, loss = 0.60806165\n",
            "Iteration 69, loss = 0.60635721\n",
            "Iteration 70, loss = 0.60459760\n",
            "Iteration 71, loss = 0.60280133\n",
            "Iteration 72, loss = 0.60112619\n",
            "Iteration 73, loss = 0.59933919\n",
            "Iteration 74, loss = 0.59746808\n",
            "Iteration 75, loss = 0.59569626\n",
            "Iteration 76, loss = 0.59384612\n",
            "Iteration 77, loss = 0.59198095\n",
            "Iteration 78, loss = 0.58999706\n",
            "Iteration 79, loss = 0.58818952\n",
            "Iteration 80, loss = 0.58616389\n",
            "Iteration 81, loss = 0.58409001\n",
            "Iteration 82, loss = 0.58211640\n",
            "Iteration 83, loss = 0.58002250\n",
            "Iteration 84, loss = 0.57795645\n",
            "Iteration 85, loss = 0.57582792\n",
            "Iteration 86, loss = 0.57356636\n",
            "Iteration 87, loss = 0.57125110\n",
            "Iteration 88, loss = 0.56900466\n",
            "Iteration 89, loss = 0.56656855\n",
            "Iteration 90, loss = 0.56420606\n",
            "Iteration 91, loss = 0.56185288\n",
            "Iteration 92, loss = 0.55923808\n",
            "Iteration 93, loss = 0.55661283\n",
            "Iteration 94, loss = 0.55411199\n",
            "Iteration 95, loss = 0.55133412\n",
            "Iteration 96, loss = 0.54877348\n",
            "Iteration 97, loss = 0.54608377\n",
            "Iteration 98, loss = 0.54332552\n",
            "Iteration 99, loss = 0.54058762\n",
            "Iteration 100, loss = 0.53797760\n",
            "Iteration 101, loss = 0.53534111\n",
            "Iteration 102, loss = 0.53262467\n",
            "Iteration 103, loss = 0.52972567\n",
            "Iteration 104, loss = 0.52711472\n",
            "Iteration 105, loss = 0.52432578\n",
            "Iteration 106, loss = 0.52160870\n",
            "Iteration 107, loss = 0.51894308\n",
            "Iteration 108, loss = 0.51626896\n",
            "Iteration 109, loss = 0.51356002\n",
            "Iteration 110, loss = 0.51090625\n",
            "Iteration 111, loss = 0.50827815\n",
            "Iteration 112, loss = 0.50570397\n",
            "Iteration 113, loss = 0.50312069\n",
            "Iteration 114, loss = 0.50064213\n",
            "Iteration 115, loss = 0.49806757\n",
            "Iteration 116, loss = 0.49568199\n",
            "Iteration 117, loss = 0.49332204\n",
            "Iteration 118, loss = 0.49070561\n",
            "Iteration 119, loss = 0.48850597\n",
            "Iteration 120, loss = 0.48626717\n",
            "Iteration 121, loss = 0.48395552\n",
            "Iteration 122, loss = 0.48157425\n",
            "Iteration 123, loss = 0.47935214\n",
            "Iteration 124, loss = 0.47730095\n",
            "Iteration 125, loss = 0.47512980\n",
            "Iteration 126, loss = 0.47325677\n",
            "Iteration 127, loss = 0.47097955\n",
            "Iteration 128, loss = 0.46900698\n",
            "Iteration 129, loss = 0.46708058\n",
            "Iteration 130, loss = 0.46525817\n",
            "Iteration 131, loss = 0.46346294\n",
            "Iteration 132, loss = 0.46184875\n",
            "Iteration 133, loss = 0.46012884\n",
            "Iteration 134, loss = 0.45862115\n",
            "Iteration 135, loss = 0.45708762\n",
            "Iteration 136, loss = 0.45573716\n",
            "Iteration 137, loss = 0.45430643\n",
            "Iteration 138, loss = 0.45316990\n",
            "Iteration 139, loss = 0.45183603\n",
            "Iteration 140, loss = 0.45065616\n",
            "Iteration 141, loss = 0.44972114\n",
            "Iteration 142, loss = 0.44874211\n",
            "Iteration 143, loss = 0.44783894\n",
            "Iteration 144, loss = 0.44693500\n",
            "Iteration 145, loss = 0.44599648\n",
            "Iteration 146, loss = 0.44538765\n",
            "Iteration 147, loss = 0.44456735\n",
            "Iteration 148, loss = 0.44401268\n",
            "Iteration 149, loss = 0.44333060\n",
            "Iteration 150, loss = 0.44264835\n",
            "Iteration 151, loss = 0.44206976\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.75891501\n",
            "Iteration 2, loss = 0.75357549\n",
            "Iteration 3, loss = 0.74848304\n",
            "Iteration 4, loss = 0.74386753\n",
            "Iteration 5, loss = 0.73910571\n",
            "Iteration 6, loss = 0.73485746\n",
            "Iteration 7, loss = 0.73093887\n",
            "Iteration 8, loss = 0.72698997\n",
            "Iteration 9, loss = 0.72335252\n",
            "Iteration 10, loss = 0.71964206\n",
            "Iteration 11, loss = 0.71660568\n",
            "Iteration 12, loss = 0.71367950\n",
            "Iteration 13, loss = 0.71057258\n",
            "Iteration 14, loss = 0.70784096\n",
            "Iteration 15, loss = 0.70539138\n",
            "Iteration 16, loss = 0.70264305\n",
            "Iteration 17, loss = 0.70047959\n",
            "Iteration 18, loss = 0.69825224\n",
            "Iteration 19, loss = 0.69607673\n",
            "Iteration 20, loss = 0.69403587\n",
            "Iteration 21, loss = 0.69213976\n",
            "Iteration 22, loss = 0.69014389\n",
            "Iteration 23, loss = 0.68831953\n",
            "Iteration 24, loss = 0.68660000\n",
            "Iteration 25, loss = 0.68480604\n",
            "Iteration 26, loss = 0.68313158\n",
            "Iteration 27, loss = 0.68133000\n",
            "Iteration 28, loss = 0.67964709\n",
            "Iteration 29, loss = 0.67799500\n",
            "Iteration 30, loss = 0.67633717\n",
            "Iteration 31, loss = 0.67465897\n",
            "Iteration 32, loss = 0.67310877\n",
            "Iteration 33, loss = 0.67138547\n",
            "Iteration 34, loss = 0.66981162\n",
            "Iteration 35, loss = 0.66824772\n",
            "Iteration 36, loss = 0.66656635\n",
            "Iteration 37, loss = 0.66499178\n",
            "Iteration 38, loss = 0.66339096\n",
            "Iteration 39, loss = 0.66184958\n",
            "Iteration 40, loss = 0.66008675\n",
            "Iteration 41, loss = 0.65854082\n",
            "Iteration 42, loss = 0.65690076\n",
            "Iteration 43, loss = 0.65521692\n",
            "Iteration 44, loss = 0.65365277\n",
            "Iteration 45, loss = 0.65188264\n",
            "Iteration 46, loss = 0.65030073\n",
            "Iteration 47, loss = 0.64861109\n",
            "Iteration 48, loss = 0.64686204\n",
            "Iteration 49, loss = 0.64516030\n",
            "Iteration 50, loss = 0.64343757\n",
            "Iteration 51, loss = 0.64169007\n",
            "Iteration 52, loss = 0.64006290\n",
            "Iteration 53, loss = 0.63826896\n",
            "Iteration 54, loss = 0.63660636\n",
            "Iteration 55, loss = 0.63477768\n",
            "Iteration 56, loss = 0.63309106\n",
            "Iteration 57, loss = 0.63116532\n",
            "Iteration 58, loss = 0.62946298\n",
            "Iteration 59, loss = 0.62756410\n",
            "Iteration 60, loss = 0.62572838\n",
            "Iteration 61, loss = 0.62392302\n",
            "Iteration 62, loss = 0.62214239\n",
            "Iteration 63, loss = 0.62016912\n",
            "Iteration 64, loss = 0.61840499\n",
            "Iteration 65, loss = 0.61655710\n",
            "Iteration 66, loss = 0.61462462\n",
            "Iteration 67, loss = 0.61283788\n",
            "Iteration 68, loss = 0.61083037\n",
            "Iteration 69, loss = 0.60888330\n",
            "Iteration 70, loss = 0.60692498\n",
            "Iteration 71, loss = 0.60491944\n",
            "Iteration 72, loss = 0.60297183\n",
            "Iteration 73, loss = 0.60097165\n",
            "Iteration 74, loss = 0.59881139\n",
            "Iteration 75, loss = 0.59665839\n",
            "Iteration 76, loss = 0.59459505\n",
            "Iteration 77, loss = 0.59240985\n",
            "Iteration 78, loss = 0.59011447\n",
            "Iteration 79, loss = 0.58792103\n",
            "Iteration 80, loss = 0.58563842\n",
            "Iteration 81, loss = 0.58308211\n",
            "Iteration 82, loss = 0.58077269\n",
            "Iteration 83, loss = 0.57829259\n",
            "Iteration 84, loss = 0.57581176\n",
            "Iteration 85, loss = 0.57322825\n",
            "Iteration 86, loss = 0.57060236\n",
            "Iteration 87, loss = 0.56789429\n",
            "Iteration 88, loss = 0.56517055\n",
            "Iteration 89, loss = 0.56236263\n",
            "Iteration 90, loss = 0.55963609\n",
            "Iteration 91, loss = 0.55689754\n",
            "Iteration 92, loss = 0.55404813\n",
            "Iteration 93, loss = 0.55109009\n",
            "Iteration 94, loss = 0.54832429\n",
            "Iteration 95, loss = 0.54527681\n",
            "Iteration 96, loss = 0.54237435\n",
            "Iteration 97, loss = 0.53958529\n",
            "Iteration 98, loss = 0.53650121\n",
            "Iteration 99, loss = 0.53365602\n",
            "Iteration 100, loss = 0.53088189\n",
            "Iteration 101, loss = 0.52811993\n",
            "Iteration 102, loss = 0.52531555\n",
            "Iteration 103, loss = 0.52234889\n",
            "Iteration 104, loss = 0.51965208\n",
            "Iteration 105, loss = 0.51688184\n",
            "Iteration 106, loss = 0.51412214\n",
            "Iteration 107, loss = 0.51144409\n",
            "Iteration 108, loss = 0.50879039\n",
            "Iteration 109, loss = 0.50609311\n",
            "Iteration 110, loss = 0.50352818\n",
            "Iteration 111, loss = 0.50096214\n",
            "Iteration 112, loss = 0.49842328\n",
            "Iteration 113, loss = 0.49602347\n",
            "Iteration 114, loss = 0.49363792\n",
            "Iteration 115, loss = 0.49116419\n",
            "Iteration 116, loss = 0.48888992\n",
            "Iteration 117, loss = 0.48671773\n",
            "Iteration 118, loss = 0.48430754\n",
            "Iteration 119, loss = 0.48226264\n",
            "Iteration 120, loss = 0.48009054\n",
            "Iteration 121, loss = 0.47802306\n",
            "Iteration 122, loss = 0.47588566\n",
            "Iteration 123, loss = 0.47382427\n",
            "Iteration 124, loss = 0.47194404\n",
            "Iteration 125, loss = 0.46987601\n",
            "Iteration 126, loss = 0.46814481\n",
            "Iteration 127, loss = 0.46628554\n",
            "Iteration 128, loss = 0.46442588\n",
            "Iteration 129, loss = 0.46267142\n",
            "Iteration 130, loss = 0.46110279\n",
            "Iteration 131, loss = 0.45960581\n",
            "Iteration 132, loss = 0.45811835\n",
            "Iteration 133, loss = 0.45651780\n",
            "Iteration 134, loss = 0.45513667\n",
            "Iteration 135, loss = 0.45381530\n",
            "Iteration 136, loss = 0.45251396\n",
            "Iteration 137, loss = 0.45122778\n",
            "Iteration 138, loss = 0.45013480\n",
            "Iteration 139, loss = 0.44907003\n",
            "Iteration 140, loss = 0.44788750\n",
            "Iteration 141, loss = 0.44700922\n",
            "Iteration 142, loss = 0.44601768\n",
            "Iteration 143, loss = 0.44521837\n",
            "Iteration 144, loss = 0.44429559\n",
            "Iteration 145, loss = 0.44333132\n",
            "Iteration 146, loss = 0.44260441\n",
            "Iteration 147, loss = 0.44182101\n",
            "Iteration 148, loss = 0.44111550\n",
            "Iteration 149, loss = 0.44055289\n",
            "Iteration 150, loss = 0.43966835\n",
            "Iteration 151, loss = 0.43903853\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69355305\n",
            "Iteration 2, loss = 0.69354727\n",
            "Iteration 3, loss = 0.69354625\n",
            "Iteration 4, loss = 0.69353080\n",
            "Iteration 5, loss = 0.69352620\n",
            "Iteration 6, loss = 0.69352123\n",
            "Iteration 7, loss = 0.69351513\n",
            "Iteration 8, loss = 0.69351335\n",
            "Iteration 9, loss = 0.69350879\n",
            "Iteration 10, loss = 0.69350257\n",
            "Iteration 11, loss = 0.69349137\n",
            "Iteration 12, loss = 0.69349129\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69355000\n",
            "Iteration 2, loss = 0.69354143\n",
            "Iteration 3, loss = 0.69354061\n",
            "Iteration 4, loss = 0.69352403\n",
            "Iteration 5, loss = 0.69351860\n",
            "Iteration 6, loss = 0.69351125\n",
            "Iteration 7, loss = 0.69350409\n",
            "Iteration 8, loss = 0.69350057\n",
            "Iteration 9, loss = 0.69349882\n",
            "Iteration 10, loss = 0.69348637\n",
            "Iteration 11, loss = 0.69347505\n",
            "Iteration 12, loss = 0.69347488\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69354881\n",
            "Iteration 2, loss = 0.69354329\n",
            "Iteration 3, loss = 0.69354229\n",
            "Iteration 4, loss = 0.69352397\n",
            "Iteration 5, loss = 0.69351906\n",
            "Iteration 6, loss = 0.69351026\n",
            "Iteration 7, loss = 0.69350331\n",
            "Iteration 8, loss = 0.69349768\n",
            "Iteration 9, loss = 0.69349366\n",
            "Iteration 10, loss = 0.69348515\n",
            "Iteration 11, loss = 0.69347934\n",
            "Iteration 12, loss = 0.69347160\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69355075\n",
            "Iteration 2, loss = 0.69354299\n",
            "Iteration 3, loss = 0.69354046\n",
            "Iteration 4, loss = 0.69352637\n",
            "Iteration 5, loss = 0.69352031\n",
            "Iteration 6, loss = 0.69351077\n",
            "Iteration 7, loss = 0.69350522\n",
            "Iteration 8, loss = 0.69349390\n",
            "Iteration 9, loss = 0.69348818\n",
            "Iteration 10, loss = 0.69347588\n",
            "Iteration 11, loss = 0.69347206\n",
            "Iteration 12, loss = 0.69345709\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69355238\n",
            "Iteration 2, loss = 0.69354660\n",
            "Iteration 3, loss = 0.69354169\n",
            "Iteration 4, loss = 0.69352882\n",
            "Iteration 5, loss = 0.69352061\n",
            "Iteration 6, loss = 0.69351722\n",
            "Iteration 7, loss = 0.69350531\n",
            "Iteration 8, loss = 0.69349841\n",
            "Iteration 9, loss = 0.69349199\n",
            "Iteration 10, loss = 0.69348315\n",
            "Iteration 11, loss = 0.69347394\n",
            "Iteration 12, loss = 0.69346619\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69354613\n",
            "Iteration 2, loss = 0.69354055\n",
            "Iteration 3, loss = 0.69353484\n",
            "Iteration 4, loss = 0.69352423\n",
            "Iteration 5, loss = 0.69351649\n",
            "Iteration 6, loss = 0.69351295\n",
            "Iteration 7, loss = 0.69350472\n",
            "Iteration 8, loss = 0.69349822\n",
            "Iteration 9, loss = 0.69349490\n",
            "Iteration 10, loss = 0.69348348\n",
            "Iteration 11, loss = 0.69347252\n",
            "Iteration 12, loss = 0.69346994\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69355281\n",
            "Iteration 2, loss = 0.69354418\n",
            "Iteration 3, loss = 0.69353551\n",
            "Iteration 4, loss = 0.69352791\n",
            "Iteration 5, loss = 0.69351254\n",
            "Iteration 6, loss = 0.69350788\n",
            "Iteration 7, loss = 0.69349542\n",
            "Iteration 8, loss = 0.69348292\n",
            "Iteration 9, loss = 0.69348534\n",
            "Iteration 10, loss = 0.69346249\n",
            "Iteration 11, loss = 0.69345414\n",
            "Iteration 12, loss = 0.69344806\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69354885\n",
            "Iteration 2, loss = 0.69354064\n",
            "Iteration 3, loss = 0.69353394\n",
            "Iteration 4, loss = 0.69352949\n",
            "Iteration 5, loss = 0.69350943\n",
            "Iteration 6, loss = 0.69350561\n",
            "Iteration 7, loss = 0.69349763\n",
            "Iteration 8, loss = 0.69348435\n",
            "Iteration 9, loss = 0.69348302\n",
            "Iteration 10, loss = 0.69346708\n",
            "Iteration 11, loss = 0.69345932\n",
            "Iteration 12, loss = 0.69345530\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69355039\n",
            "Iteration 2, loss = 0.69354228\n",
            "Iteration 3, loss = 0.69353373\n",
            "Iteration 4, loss = 0.69352902\n",
            "Iteration 5, loss = 0.69351072\n",
            "Iteration 6, loss = 0.69350783\n",
            "Iteration 7, loss = 0.69349582\n",
            "Iteration 8, loss = 0.69348276\n",
            "Iteration 9, loss = 0.69347917\n",
            "Iteration 10, loss = 0.69346440\n",
            "Iteration 11, loss = 0.69345514\n",
            "Iteration 12, loss = 0.69345045\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69355006\n",
            "Iteration 2, loss = 0.69354362\n",
            "Iteration 3, loss = 0.69353558\n",
            "Iteration 4, loss = 0.69353057\n",
            "Iteration 5, loss = 0.69351441\n",
            "Iteration 6, loss = 0.69350586\n",
            "Iteration 7, loss = 0.69350112\n",
            "Iteration 8, loss = 0.69348621\n",
            "Iteration 9, loss = 0.69348001\n",
            "Iteration 10, loss = 0.69346621\n",
            "Iteration 11, loss = 0.69345528\n",
            "Iteration 12, loss = 0.69345271\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69355305\n",
            "Iteration 2, loss = 0.69354727\n",
            "Iteration 3, loss = 0.69354625\n",
            "Iteration 4, loss = 0.69353080\n",
            "Iteration 5, loss = 0.69352620\n",
            "Iteration 6, loss = 0.69352123\n",
            "Iteration 7, loss = 0.69351513\n",
            "Iteration 8, loss = 0.69351335\n",
            "Iteration 9, loss = 0.69350879\n",
            "Iteration 10, loss = 0.69350257\n",
            "Iteration 11, loss = 0.69349137\n",
            "Iteration 12, loss = 0.69349129\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69355000\n",
            "Iteration 2, loss = 0.69354143\n",
            "Iteration 3, loss = 0.69354061\n",
            "Iteration 4, loss = 0.69352403\n",
            "Iteration 5, loss = 0.69351860\n",
            "Iteration 6, loss = 0.69351125\n",
            "Iteration 7, loss = 0.69350409\n",
            "Iteration 8, loss = 0.69350057\n",
            "Iteration 9, loss = 0.69349882\n",
            "Iteration 10, loss = 0.69348637\n",
            "Iteration 11, loss = 0.69347505\n",
            "Iteration 12, loss = 0.69347488\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69354881\n",
            "Iteration 2, loss = 0.69354329\n",
            "Iteration 3, loss = 0.69354229\n",
            "Iteration 4, loss = 0.69352397\n",
            "Iteration 5, loss = 0.69351906\n",
            "Iteration 6, loss = 0.69351026\n",
            "Iteration 7, loss = 0.69350331\n",
            "Iteration 8, loss = 0.69349768\n",
            "Iteration 9, loss = 0.69349366\n",
            "Iteration 10, loss = 0.69348515\n",
            "Iteration 11, loss = 0.69347934\n",
            "Iteration 12, loss = 0.69347160\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69355075\n",
            "Iteration 2, loss = 0.69354299\n",
            "Iteration 3, loss = 0.69354046\n",
            "Iteration 4, loss = 0.69352637\n",
            "Iteration 5, loss = 0.69352031\n",
            "Iteration 6, loss = 0.69351077\n",
            "Iteration 7, loss = 0.69350522\n",
            "Iteration 8, loss = 0.69349390\n",
            "Iteration 9, loss = 0.69348818\n",
            "Iteration 10, loss = 0.69347588\n",
            "Iteration 11, loss = 0.69347206\n",
            "Iteration 12, loss = 0.69345709\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69355238\n",
            "Iteration 2, loss = 0.69354660\n",
            "Iteration 3, loss = 0.69354169\n",
            "Iteration 4, loss = 0.69352882\n",
            "Iteration 5, loss = 0.69352061\n",
            "Iteration 6, loss = 0.69351722\n",
            "Iteration 7, loss = 0.69350531\n",
            "Iteration 8, loss = 0.69349841\n",
            "Iteration 9, loss = 0.69349199\n",
            "Iteration 10, loss = 0.69348315\n",
            "Iteration 11, loss = 0.69347394\n",
            "Iteration 12, loss = 0.69346619\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69354613\n",
            "Iteration 2, loss = 0.69354055\n",
            "Iteration 3, loss = 0.69353484\n",
            "Iteration 4, loss = 0.69352423\n",
            "Iteration 5, loss = 0.69351649\n",
            "Iteration 6, loss = 0.69351295\n",
            "Iteration 7, loss = 0.69350472\n",
            "Iteration 8, loss = 0.69349822\n",
            "Iteration 9, loss = 0.69349490\n",
            "Iteration 10, loss = 0.69348348\n",
            "Iteration 11, loss = 0.69347252\n",
            "Iteration 12, loss = 0.69346994\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69355281\n",
            "Iteration 2, loss = 0.69354418\n",
            "Iteration 3, loss = 0.69353551\n",
            "Iteration 4, loss = 0.69352791\n",
            "Iteration 5, loss = 0.69351254\n",
            "Iteration 6, loss = 0.69350788\n",
            "Iteration 7, loss = 0.69349542\n",
            "Iteration 8, loss = 0.69348292\n",
            "Iteration 9, loss = 0.69348534\n",
            "Iteration 10, loss = 0.69346249\n",
            "Iteration 11, loss = 0.69345414\n",
            "Iteration 12, loss = 0.69344806\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69354885\n",
            "Iteration 2, loss = 0.69354064\n",
            "Iteration 3, loss = 0.69353394\n",
            "Iteration 4, loss = 0.69352949\n",
            "Iteration 5, loss = 0.69350943\n",
            "Iteration 6, loss = 0.69350561\n",
            "Iteration 7, loss = 0.69349763\n",
            "Iteration 8, loss = 0.69348435\n",
            "Iteration 9, loss = 0.69348302\n",
            "Iteration 10, loss = 0.69346708\n",
            "Iteration 11, loss = 0.69345932\n",
            "Iteration 12, loss = 0.69345530\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69355039\n",
            "Iteration 2, loss = 0.69354228\n",
            "Iteration 3, loss = 0.69353373\n",
            "Iteration 4, loss = 0.69352902\n",
            "Iteration 5, loss = 0.69351072\n",
            "Iteration 6, loss = 0.69350783\n",
            "Iteration 7, loss = 0.69349582\n",
            "Iteration 8, loss = 0.69348276\n",
            "Iteration 9, loss = 0.69347917\n",
            "Iteration 10, loss = 0.69346440\n",
            "Iteration 11, loss = 0.69345514\n",
            "Iteration 12, loss = 0.69345045\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69355006\n",
            "Iteration 2, loss = 0.69354362\n",
            "Iteration 3, loss = 0.69353558\n",
            "Iteration 4, loss = 0.69353057\n",
            "Iteration 5, loss = 0.69351441\n",
            "Iteration 6, loss = 0.69350586\n",
            "Iteration 7, loss = 0.69350112\n",
            "Iteration 8, loss = 0.69348621\n",
            "Iteration 9, loss = 0.69348001\n",
            "Iteration 10, loss = 0.69346621\n",
            "Iteration 11, loss = 0.69345528\n",
            "Iteration 12, loss = 0.69345271\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69359076\n",
            "Iteration 2, loss = 0.69350714\n",
            "Iteration 3, loss = 0.69350728\n",
            "Iteration 4, loss = 0.69340443\n",
            "Iteration 5, loss = 0.69338353\n",
            "Iteration 6, loss = 0.69336107\n",
            "Iteration 7, loss = 0.69333924\n",
            "Iteration 8, loss = 0.69333332\n",
            "Iteration 9, loss = 0.69331422\n",
            "Iteration 10, loss = 0.69330662\n",
            "Iteration 11, loss = 0.69326948\n",
            "Iteration 12, loss = 0.69327902\n",
            "Iteration 13, loss = 0.69323342\n",
            "Iteration 14, loss = 0.69321130\n",
            "Iteration 15, loss = 0.69320687\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69358208\n",
            "Iteration 2, loss = 0.69349029\n",
            "Iteration 3, loss = 0.69348860\n",
            "Iteration 4, loss = 0.69339786\n",
            "Iteration 5, loss = 0.69337809\n",
            "Iteration 6, loss = 0.69335347\n",
            "Iteration 7, loss = 0.69332948\n",
            "Iteration 8, loss = 0.69332101\n",
            "Iteration 9, loss = 0.69331468\n",
            "Iteration 10, loss = 0.69328031\n",
            "Iteration 11, loss = 0.69324631\n",
            "Iteration 12, loss = 0.69325585\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69357730\n",
            "Iteration 2, loss = 0.69346348\n",
            "Iteration 3, loss = 0.69347117\n",
            "Iteration 4, loss = 0.69336040\n",
            "Iteration 5, loss = 0.69334686\n",
            "Iteration 6, loss = 0.69331067\n",
            "Iteration 7, loss = 0.69328749\n",
            "Iteration 8, loss = 0.69327295\n",
            "Iteration 9, loss = 0.69326248\n",
            "Iteration 10, loss = 0.69324918\n",
            "Iteration 11, loss = 0.69323644\n",
            "Iteration 12, loss = 0.69321891\n",
            "Iteration 13, loss = 0.69318069\n",
            "Iteration 14, loss = 0.69317133\n",
            "Iteration 15, loss = 0.69318140\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69352593\n",
            "Iteration 2, loss = 0.69343509\n",
            "Iteration 3, loss = 0.69341913\n",
            "Iteration 4, loss = 0.69333959\n",
            "Iteration 5, loss = 0.69331927\n",
            "Iteration 6, loss = 0.69328665\n",
            "Iteration 7, loss = 0.69327354\n",
            "Iteration 8, loss = 0.69323887\n",
            "Iteration 9, loss = 0.69322761\n",
            "Iteration 10, loss = 0.69319634\n",
            "Iteration 11, loss = 0.69319422\n",
            "Iteration 12, loss = 0.69315328\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69350414\n",
            "Iteration 2, loss = 0.69341530\n",
            "Iteration 3, loss = 0.69339031\n",
            "Iteration 4, loss = 0.69330991\n",
            "Iteration 5, loss = 0.69327626\n",
            "Iteration 6, loss = 0.69327702\n",
            "Iteration 7, loss = 0.69323314\n",
            "Iteration 8, loss = 0.69321846\n",
            "Iteration 9, loss = 0.69320509\n",
            "Iteration 10, loss = 0.69319994\n",
            "Iteration 11, loss = 0.69317043\n",
            "Iteration 12, loss = 0.69316067\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69356384\n",
            "Iteration 2, loss = 0.69346868\n",
            "Iteration 3, loss = 0.69341421\n",
            "Iteration 4, loss = 0.69334792\n",
            "Iteration 5, loss = 0.69331006\n",
            "Iteration 6, loss = 0.69329775\n",
            "Iteration 7, loss = 0.69326716\n",
            "Iteration 8, loss = 0.69325144\n",
            "Iteration 9, loss = 0.69324573\n",
            "Iteration 10, loss = 0.69321849\n",
            "Iteration 11, loss = 0.69318582\n",
            "Iteration 12, loss = 0.69319428\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69362374\n",
            "Iteration 2, loss = 0.69352627\n",
            "Iteration 3, loss = 0.69345621\n",
            "Iteration 4, loss = 0.69342370\n",
            "Iteration 5, loss = 0.69334405\n",
            "Iteration 6, loss = 0.69332846\n",
            "Iteration 7, loss = 0.69328005\n",
            "Iteration 8, loss = 0.69323802\n",
            "Iteration 9, loss = 0.69326516\n",
            "Iteration 10, loss = 0.69318873\n",
            "Iteration 11, loss = 0.69316780\n",
            "Iteration 12, loss = 0.69316001\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69361476\n",
            "Iteration 2, loss = 0.69351091\n",
            "Iteration 3, loss = 0.69346582\n",
            "Iteration 4, loss = 0.69344619\n",
            "Iteration 5, loss = 0.69334398\n",
            "Iteration 6, loss = 0.69333656\n",
            "Iteration 7, loss = 0.69330584\n",
            "Iteration 8, loss = 0.69325978\n",
            "Iteration 9, loss = 0.69325852\n",
            "Iteration 10, loss = 0.69321689\n",
            "Iteration 11, loss = 0.69319523\n",
            "Iteration 12, loss = 0.69319414\n",
            "Iteration 13, loss = 0.69315325\n",
            "Iteration 14, loss = 0.69313260\n",
            "Iteration 15, loss = 0.69313424\n",
            "Iteration 16, loss = 0.69309897\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69361942\n",
            "Iteration 2, loss = 0.69351104\n",
            "Iteration 3, loss = 0.69344773\n",
            "Iteration 4, loss = 0.69342519\n",
            "Iteration 5, loss = 0.69332241\n",
            "Iteration 6, loss = 0.69332265\n",
            "Iteration 7, loss = 0.69327685\n",
            "Iteration 8, loss = 0.69323152\n",
            "Iteration 9, loss = 0.69322665\n",
            "Iteration 10, loss = 0.69319078\n",
            "Iteration 11, loss = 0.69316485\n",
            "Iteration 12, loss = 0.69316611\n",
            "Iteration 13, loss = 0.69313233\n",
            "Iteration 14, loss = 0.69312063\n",
            "Iteration 15, loss = 0.69312912\n",
            "Iteration 16, loss = 0.69308867\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69360028\n",
            "Iteration 2, loss = 0.69349796\n",
            "Iteration 3, loss = 0.69341863\n",
            "Iteration 4, loss = 0.69339689\n",
            "Iteration 5, loss = 0.69329576\n",
            "Iteration 6, loss = 0.69326491\n",
            "Iteration 7, loss = 0.69325880\n",
            "Iteration 8, loss = 0.69320269\n",
            "Iteration 9, loss = 0.69319184\n",
            "Iteration 10, loss = 0.69316382\n",
            "Iteration 11, loss = 0.69313740\n",
            "Iteration 12, loss = 0.69315735\n",
            "Iteration 13, loss = 0.69311342\n",
            "Iteration 14, loss = 0.69310405\n",
            "Iteration 15, loss = 0.69311537\n",
            "Iteration 16, loss = 0.69308473\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69359076\n",
            "Iteration 2, loss = 0.69350714\n",
            "Iteration 3, loss = 0.69350728\n",
            "Iteration 4, loss = 0.69340443\n",
            "Iteration 5, loss = 0.69338353\n",
            "Iteration 6, loss = 0.69336107\n",
            "Iteration 7, loss = 0.69333924\n",
            "Iteration 8, loss = 0.69333332\n",
            "Iteration 9, loss = 0.69331422\n",
            "Iteration 10, loss = 0.69330662\n",
            "Iteration 11, loss = 0.69326948\n",
            "Iteration 12, loss = 0.69327902\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69358208\n",
            "Iteration 2, loss = 0.69349029\n",
            "Iteration 3, loss = 0.69348860\n",
            "Iteration 4, loss = 0.69339786\n",
            "Iteration 5, loss = 0.69337809\n",
            "Iteration 6, loss = 0.69335347\n",
            "Iteration 7, loss = 0.69332948\n",
            "Iteration 8, loss = 0.69332101\n",
            "Iteration 9, loss = 0.69331468\n",
            "Iteration 10, loss = 0.69328031\n",
            "Iteration 11, loss = 0.69324631\n",
            "Iteration 12, loss = 0.69325585\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69357730\n",
            "Iteration 2, loss = 0.69346348\n",
            "Iteration 3, loss = 0.69347117\n",
            "Iteration 4, loss = 0.69336040\n",
            "Iteration 5, loss = 0.69334686\n",
            "Iteration 6, loss = 0.69331067\n",
            "Iteration 7, loss = 0.69328749\n",
            "Iteration 8, loss = 0.69327295\n",
            "Iteration 9, loss = 0.69326248\n",
            "Iteration 10, loss = 0.69324918\n",
            "Iteration 11, loss = 0.69323644\n",
            "Iteration 12, loss = 0.69321891\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69352593\n",
            "Iteration 2, loss = 0.69343509\n",
            "Iteration 3, loss = 0.69341913\n",
            "Iteration 4, loss = 0.69333959\n",
            "Iteration 5, loss = 0.69331927\n",
            "Iteration 6, loss = 0.69328665\n",
            "Iteration 7, loss = 0.69327354\n",
            "Iteration 8, loss = 0.69323887\n",
            "Iteration 9, loss = 0.69322761\n",
            "Iteration 10, loss = 0.69319634\n",
            "Iteration 11, loss = 0.69319422\n",
            "Iteration 12, loss = 0.69315328\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69350414\n",
            "Iteration 2, loss = 0.69341530\n",
            "Iteration 3, loss = 0.69339031\n",
            "Iteration 4, loss = 0.69330991\n",
            "Iteration 5, loss = 0.69327626\n",
            "Iteration 6, loss = 0.69327702\n",
            "Iteration 7, loss = 0.69323314\n",
            "Iteration 8, loss = 0.69321846\n",
            "Iteration 9, loss = 0.69320509\n",
            "Iteration 10, loss = 0.69319994\n",
            "Iteration 11, loss = 0.69317043\n",
            "Iteration 12, loss = 0.69316067\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69356384\n",
            "Iteration 2, loss = 0.69346868\n",
            "Iteration 3, loss = 0.69341421\n",
            "Iteration 4, loss = 0.69334792\n",
            "Iteration 5, loss = 0.69331006\n",
            "Iteration 6, loss = 0.69329775\n",
            "Iteration 7, loss = 0.69326716\n",
            "Iteration 8, loss = 0.69325144\n",
            "Iteration 9, loss = 0.69324573\n",
            "Iteration 10, loss = 0.69321849\n",
            "Iteration 11, loss = 0.69318582\n",
            "Iteration 12, loss = 0.69319428\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69362374\n",
            "Iteration 2, loss = 0.69352627\n",
            "Iteration 3, loss = 0.69345621\n",
            "Iteration 4, loss = 0.69342370\n",
            "Iteration 5, loss = 0.69334405\n",
            "Iteration 6, loss = 0.69332846\n",
            "Iteration 7, loss = 0.69328005\n",
            "Iteration 8, loss = 0.69323802\n",
            "Iteration 9, loss = 0.69326516\n",
            "Iteration 10, loss = 0.69318873\n",
            "Iteration 11, loss = 0.69316780\n",
            "Iteration 12, loss = 0.69316001\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69361476\n",
            "Iteration 2, loss = 0.69351091\n",
            "Iteration 3, loss = 0.69346582\n",
            "Iteration 4, loss = 0.69344619\n",
            "Iteration 5, loss = 0.69334398\n",
            "Iteration 6, loss = 0.69333656\n",
            "Iteration 7, loss = 0.69330584\n",
            "Iteration 8, loss = 0.69325978\n",
            "Iteration 9, loss = 0.69325852\n",
            "Iteration 10, loss = 0.69321689\n",
            "Iteration 11, loss = 0.69319523\n",
            "Iteration 12, loss = 0.69319414\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69361942\n",
            "Iteration 2, loss = 0.69351104\n",
            "Iteration 3, loss = 0.69344773\n",
            "Iteration 4, loss = 0.69342519\n",
            "Iteration 5, loss = 0.69332241\n",
            "Iteration 6, loss = 0.69332265\n",
            "Iteration 7, loss = 0.69327685\n",
            "Iteration 8, loss = 0.69323152\n",
            "Iteration 9, loss = 0.69322665\n",
            "Iteration 10, loss = 0.69319078\n",
            "Iteration 11, loss = 0.69316485\n",
            "Iteration 12, loss = 0.69316611\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69360028\n",
            "Iteration 2, loss = 0.69349796\n",
            "Iteration 3, loss = 0.69341863\n",
            "Iteration 4, loss = 0.69339689\n",
            "Iteration 5, loss = 0.69329576\n",
            "Iteration 6, loss = 0.69326491\n",
            "Iteration 7, loss = 0.69325880\n",
            "Iteration 8, loss = 0.69320269\n",
            "Iteration 9, loss = 0.69319184\n",
            "Iteration 10, loss = 0.69316382\n",
            "Iteration 11, loss = 0.69313740\n",
            "Iteration 12, loss = 0.69315735\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69355305\n",
            "Iteration 2, loss = 0.69354727\n",
            "Iteration 3, loss = 0.69354625\n",
            "Iteration 4, loss = 0.69353080\n",
            "Iteration 5, loss = 0.69352620\n",
            "Iteration 6, loss = 0.69352123\n",
            "Iteration 7, loss = 0.69351513\n",
            "Iteration 8, loss = 0.69351335\n",
            "Iteration 9, loss = 0.69350879\n",
            "Iteration 10, loss = 0.69350257\n",
            "Iteration 11, loss = 0.69349137\n",
            "Iteration 12, loss = 0.69349129\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69355000\n",
            "Iteration 2, loss = 0.69354143\n",
            "Iteration 3, loss = 0.69354061\n",
            "Iteration 4, loss = 0.69352403\n",
            "Iteration 5, loss = 0.69351860\n",
            "Iteration 6, loss = 0.69351125\n",
            "Iteration 7, loss = 0.69350409\n",
            "Iteration 8, loss = 0.69350057\n",
            "Iteration 9, loss = 0.69349882\n",
            "Iteration 10, loss = 0.69348637\n",
            "Iteration 11, loss = 0.69347505\n",
            "Iteration 12, loss = 0.69347488\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69354881\n",
            "Iteration 2, loss = 0.69354329\n",
            "Iteration 3, loss = 0.69354229\n",
            "Iteration 4, loss = 0.69352397\n",
            "Iteration 5, loss = 0.69351906\n",
            "Iteration 6, loss = 0.69351026\n",
            "Iteration 7, loss = 0.69350331\n",
            "Iteration 8, loss = 0.69349768\n",
            "Iteration 9, loss = 0.69349366\n",
            "Iteration 10, loss = 0.69348515\n",
            "Iteration 11, loss = 0.69347934\n",
            "Iteration 12, loss = 0.69347160\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69355075\n",
            "Iteration 2, loss = 0.69354299\n",
            "Iteration 3, loss = 0.69354046\n",
            "Iteration 4, loss = 0.69352637\n",
            "Iteration 5, loss = 0.69352031\n",
            "Iteration 6, loss = 0.69351077\n",
            "Iteration 7, loss = 0.69350522\n",
            "Iteration 8, loss = 0.69349390\n",
            "Iteration 9, loss = 0.69348818\n",
            "Iteration 10, loss = 0.69347588\n",
            "Iteration 11, loss = 0.69347206\n",
            "Iteration 12, loss = 0.69345709\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69355238\n",
            "Iteration 2, loss = 0.69354660\n",
            "Iteration 3, loss = 0.69354169\n",
            "Iteration 4, loss = 0.69352882\n",
            "Iteration 5, loss = 0.69352061\n",
            "Iteration 6, loss = 0.69351722\n",
            "Iteration 7, loss = 0.69350531\n",
            "Iteration 8, loss = 0.69349841\n",
            "Iteration 9, loss = 0.69349199\n",
            "Iteration 10, loss = 0.69348315\n",
            "Iteration 11, loss = 0.69347394\n",
            "Iteration 12, loss = 0.69346619\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69354613\n",
            "Iteration 2, loss = 0.69354055\n",
            "Iteration 3, loss = 0.69353484\n",
            "Iteration 4, loss = 0.69352423\n",
            "Iteration 5, loss = 0.69351649\n",
            "Iteration 6, loss = 0.69351295\n",
            "Iteration 7, loss = 0.69350472\n",
            "Iteration 8, loss = 0.69349822\n",
            "Iteration 9, loss = 0.69349490\n",
            "Iteration 10, loss = 0.69348348\n",
            "Iteration 11, loss = 0.69347252\n",
            "Iteration 12, loss = 0.69346994\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69355281\n",
            "Iteration 2, loss = 0.69354418\n",
            "Iteration 3, loss = 0.69353551\n",
            "Iteration 4, loss = 0.69352791\n",
            "Iteration 5, loss = 0.69351254\n",
            "Iteration 6, loss = 0.69350788\n",
            "Iteration 7, loss = 0.69349542\n",
            "Iteration 8, loss = 0.69348292\n",
            "Iteration 9, loss = 0.69348534\n",
            "Iteration 10, loss = 0.69346249\n",
            "Iteration 11, loss = 0.69345414\n",
            "Iteration 12, loss = 0.69344806\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69354885\n",
            "Iteration 2, loss = 0.69354064\n",
            "Iteration 3, loss = 0.69353394\n",
            "Iteration 4, loss = 0.69352949\n",
            "Iteration 5, loss = 0.69350943\n",
            "Iteration 6, loss = 0.69350561\n",
            "Iteration 7, loss = 0.69349763\n",
            "Iteration 8, loss = 0.69348435\n",
            "Iteration 9, loss = 0.69348302\n",
            "Iteration 10, loss = 0.69346708\n",
            "Iteration 11, loss = 0.69345932\n",
            "Iteration 12, loss = 0.69345530\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69355039\n",
            "Iteration 2, loss = 0.69354228\n",
            "Iteration 3, loss = 0.69353373\n",
            "Iteration 4, loss = 0.69352902\n",
            "Iteration 5, loss = 0.69351072\n",
            "Iteration 6, loss = 0.69350783\n",
            "Iteration 7, loss = 0.69349582\n",
            "Iteration 8, loss = 0.69348276\n",
            "Iteration 9, loss = 0.69347917\n",
            "Iteration 10, loss = 0.69346440\n",
            "Iteration 11, loss = 0.69345514\n",
            "Iteration 12, loss = 0.69345045\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69355006\n",
            "Iteration 2, loss = 0.69354362\n",
            "Iteration 3, loss = 0.69353558\n",
            "Iteration 4, loss = 0.69353057\n",
            "Iteration 5, loss = 0.69351441\n",
            "Iteration 6, loss = 0.69350586\n",
            "Iteration 7, loss = 0.69350112\n",
            "Iteration 8, loss = 0.69348621\n",
            "Iteration 9, loss = 0.69348001\n",
            "Iteration 10, loss = 0.69346621\n",
            "Iteration 11, loss = 0.69345528\n",
            "Iteration 12, loss = 0.69345271\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69355305\n",
            "Iteration 2, loss = 0.69354727\n",
            "Iteration 3, loss = 0.69354625\n",
            "Iteration 4, loss = 0.69353080\n",
            "Iteration 5, loss = 0.69352620\n",
            "Iteration 6, loss = 0.69352123\n",
            "Iteration 7, loss = 0.69351513\n",
            "Iteration 8, loss = 0.69351335\n",
            "Iteration 9, loss = 0.69350879\n",
            "Iteration 10, loss = 0.69350257\n",
            "Iteration 11, loss = 0.69349137\n",
            "Iteration 12, loss = 0.69349129\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69355000\n",
            "Iteration 2, loss = 0.69354143\n",
            "Iteration 3, loss = 0.69354061\n",
            "Iteration 4, loss = 0.69352403\n",
            "Iteration 5, loss = 0.69351860\n",
            "Iteration 6, loss = 0.69351125\n",
            "Iteration 7, loss = 0.69350409\n",
            "Iteration 8, loss = 0.69350057\n",
            "Iteration 9, loss = 0.69349882\n",
            "Iteration 10, loss = 0.69348637\n",
            "Iteration 11, loss = 0.69347505\n",
            "Iteration 12, loss = 0.69347488\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69354881\n",
            "Iteration 2, loss = 0.69354329\n",
            "Iteration 3, loss = 0.69354229\n",
            "Iteration 4, loss = 0.69352397\n",
            "Iteration 5, loss = 0.69351906\n",
            "Iteration 6, loss = 0.69351026\n",
            "Iteration 7, loss = 0.69350331\n",
            "Iteration 8, loss = 0.69349768\n",
            "Iteration 9, loss = 0.69349366\n",
            "Iteration 10, loss = 0.69348515\n",
            "Iteration 11, loss = 0.69347934\n",
            "Iteration 12, loss = 0.69347160\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69355075\n",
            "Iteration 2, loss = 0.69354299\n",
            "Iteration 3, loss = 0.69354046\n",
            "Iteration 4, loss = 0.69352637\n",
            "Iteration 5, loss = 0.69352031\n",
            "Iteration 6, loss = 0.69351077\n",
            "Iteration 7, loss = 0.69350522\n",
            "Iteration 8, loss = 0.69349390\n",
            "Iteration 9, loss = 0.69348818\n",
            "Iteration 10, loss = 0.69347588\n",
            "Iteration 11, loss = 0.69347206\n",
            "Iteration 12, loss = 0.69345709\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69355238\n",
            "Iteration 2, loss = 0.69354660\n",
            "Iteration 3, loss = 0.69354169\n",
            "Iteration 4, loss = 0.69352882\n",
            "Iteration 5, loss = 0.69352061\n",
            "Iteration 6, loss = 0.69351722\n",
            "Iteration 7, loss = 0.69350531\n",
            "Iteration 8, loss = 0.69349841\n",
            "Iteration 9, loss = 0.69349199\n",
            "Iteration 10, loss = 0.69348315\n",
            "Iteration 11, loss = 0.69347394\n",
            "Iteration 12, loss = 0.69346619\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69354613\n",
            "Iteration 2, loss = 0.69354055\n",
            "Iteration 3, loss = 0.69353484\n",
            "Iteration 4, loss = 0.69352423\n",
            "Iteration 5, loss = 0.69351649\n",
            "Iteration 6, loss = 0.69351295\n",
            "Iteration 7, loss = 0.69350472\n",
            "Iteration 8, loss = 0.69349822\n",
            "Iteration 9, loss = 0.69349490\n",
            "Iteration 10, loss = 0.69348348\n",
            "Iteration 11, loss = 0.69347252\n",
            "Iteration 12, loss = 0.69346994\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69355281\n",
            "Iteration 2, loss = 0.69354418\n",
            "Iteration 3, loss = 0.69353551\n",
            "Iteration 4, loss = 0.69352791\n",
            "Iteration 5, loss = 0.69351254\n",
            "Iteration 6, loss = 0.69350788\n",
            "Iteration 7, loss = 0.69349542\n",
            "Iteration 8, loss = 0.69348292\n",
            "Iteration 9, loss = 0.69348534\n",
            "Iteration 10, loss = 0.69346249\n",
            "Iteration 11, loss = 0.69345414\n",
            "Iteration 12, loss = 0.69344806\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69354885\n",
            "Iteration 2, loss = 0.69354064\n",
            "Iteration 3, loss = 0.69353394\n",
            "Iteration 4, loss = 0.69352949\n",
            "Iteration 5, loss = 0.69350943\n",
            "Iteration 6, loss = 0.69350561\n",
            "Iteration 7, loss = 0.69349763\n",
            "Iteration 8, loss = 0.69348435\n",
            "Iteration 9, loss = 0.69348302\n",
            "Iteration 10, loss = 0.69346708\n",
            "Iteration 11, loss = 0.69345932\n",
            "Iteration 12, loss = 0.69345530\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69355039\n",
            "Iteration 2, loss = 0.69354228\n",
            "Iteration 3, loss = 0.69353373\n",
            "Iteration 4, loss = 0.69352902\n",
            "Iteration 5, loss = 0.69351072\n",
            "Iteration 6, loss = 0.69350783\n",
            "Iteration 7, loss = 0.69349582\n",
            "Iteration 8, loss = 0.69348276\n",
            "Iteration 9, loss = 0.69347917\n",
            "Iteration 10, loss = 0.69346440\n",
            "Iteration 11, loss = 0.69345514\n",
            "Iteration 12, loss = 0.69345045\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69355006\n",
            "Iteration 2, loss = 0.69354362\n",
            "Iteration 3, loss = 0.69353558\n",
            "Iteration 4, loss = 0.69353057\n",
            "Iteration 5, loss = 0.69351441\n",
            "Iteration 6, loss = 0.69350586\n",
            "Iteration 7, loss = 0.69350112\n",
            "Iteration 8, loss = 0.69348621\n",
            "Iteration 9, loss = 0.69348001\n",
            "Iteration 10, loss = 0.69346621\n",
            "Iteration 11, loss = 0.69345528\n",
            "Iteration 12, loss = 0.69345271\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69359076\n",
            "Iteration 2, loss = 0.69350714\n",
            "Iteration 3, loss = 0.69350728\n",
            "Iteration 4, loss = 0.69340443\n",
            "Iteration 5, loss = 0.69338353\n",
            "Iteration 6, loss = 0.69336107\n",
            "Iteration 7, loss = 0.69333924\n",
            "Iteration 8, loss = 0.69333332\n",
            "Iteration 9, loss = 0.69331422\n",
            "Iteration 10, loss = 0.69330662\n",
            "Iteration 11, loss = 0.69326948\n",
            "Iteration 12, loss = 0.69327902\n",
            "Iteration 13, loss = 0.69323342\n",
            "Iteration 14, loss = 0.69321130\n",
            "Iteration 15, loss = 0.69320687\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69358208\n",
            "Iteration 2, loss = 0.69349029\n",
            "Iteration 3, loss = 0.69348860\n",
            "Iteration 4, loss = 0.69339786\n",
            "Iteration 5, loss = 0.69337809\n",
            "Iteration 6, loss = 0.69335347\n",
            "Iteration 7, loss = 0.69332948\n",
            "Iteration 8, loss = 0.69332101\n",
            "Iteration 9, loss = 0.69331468\n",
            "Iteration 10, loss = 0.69328031\n",
            "Iteration 11, loss = 0.69324631\n",
            "Iteration 12, loss = 0.69325585\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69357730\n",
            "Iteration 2, loss = 0.69346348\n",
            "Iteration 3, loss = 0.69347117\n",
            "Iteration 4, loss = 0.69336040\n",
            "Iteration 5, loss = 0.69334686\n",
            "Iteration 6, loss = 0.69331067\n",
            "Iteration 7, loss = 0.69328749\n",
            "Iteration 8, loss = 0.69327295\n",
            "Iteration 9, loss = 0.69326248\n",
            "Iteration 10, loss = 0.69324918\n",
            "Iteration 11, loss = 0.69323644\n",
            "Iteration 12, loss = 0.69321891\n",
            "Iteration 13, loss = 0.69318069\n",
            "Iteration 14, loss = 0.69317133\n",
            "Iteration 15, loss = 0.69318140\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69352593\n",
            "Iteration 2, loss = 0.69343509\n",
            "Iteration 3, loss = 0.69341913\n",
            "Iteration 4, loss = 0.69333959\n",
            "Iteration 5, loss = 0.69331927\n",
            "Iteration 6, loss = 0.69328665\n",
            "Iteration 7, loss = 0.69327354\n",
            "Iteration 8, loss = 0.69323887\n",
            "Iteration 9, loss = 0.69322761\n",
            "Iteration 10, loss = 0.69319634\n",
            "Iteration 11, loss = 0.69319422\n",
            "Iteration 12, loss = 0.69315328\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69350414\n",
            "Iteration 2, loss = 0.69341530\n",
            "Iteration 3, loss = 0.69339031\n",
            "Iteration 4, loss = 0.69330991\n",
            "Iteration 5, loss = 0.69327626\n",
            "Iteration 6, loss = 0.69327702\n",
            "Iteration 7, loss = 0.69323314\n",
            "Iteration 8, loss = 0.69321846\n",
            "Iteration 9, loss = 0.69320509\n",
            "Iteration 10, loss = 0.69319994\n",
            "Iteration 11, loss = 0.69317043\n",
            "Iteration 12, loss = 0.69316067\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69356384\n",
            "Iteration 2, loss = 0.69346868\n",
            "Iteration 3, loss = 0.69341421\n",
            "Iteration 4, loss = 0.69334792\n",
            "Iteration 5, loss = 0.69331006\n",
            "Iteration 6, loss = 0.69329775\n",
            "Iteration 7, loss = 0.69326716\n",
            "Iteration 8, loss = 0.69325144\n",
            "Iteration 9, loss = 0.69324573\n",
            "Iteration 10, loss = 0.69321849\n",
            "Iteration 11, loss = 0.69318582\n",
            "Iteration 12, loss = 0.69319428\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69362374\n",
            "Iteration 2, loss = 0.69352627\n",
            "Iteration 3, loss = 0.69345621\n",
            "Iteration 4, loss = 0.69342370\n",
            "Iteration 5, loss = 0.69334405\n",
            "Iteration 6, loss = 0.69332846\n",
            "Iteration 7, loss = 0.69328005\n",
            "Iteration 8, loss = 0.69323802\n",
            "Iteration 9, loss = 0.69326516\n",
            "Iteration 10, loss = 0.69318873\n",
            "Iteration 11, loss = 0.69316780\n",
            "Iteration 12, loss = 0.69316001\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69361476\n",
            "Iteration 2, loss = 0.69351091\n",
            "Iteration 3, loss = 0.69346582\n",
            "Iteration 4, loss = 0.69344619\n",
            "Iteration 5, loss = 0.69334398\n",
            "Iteration 6, loss = 0.69333656\n",
            "Iteration 7, loss = 0.69330584\n",
            "Iteration 8, loss = 0.69325978\n",
            "Iteration 9, loss = 0.69325852\n",
            "Iteration 10, loss = 0.69321689\n",
            "Iteration 11, loss = 0.69319523\n",
            "Iteration 12, loss = 0.69319414\n",
            "Iteration 13, loss = 0.69315325\n",
            "Iteration 14, loss = 0.69313260\n",
            "Iteration 15, loss = 0.69313424\n",
            "Iteration 16, loss = 0.69309897\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69361942\n",
            "Iteration 2, loss = 0.69351104\n",
            "Iteration 3, loss = 0.69344773\n",
            "Iteration 4, loss = 0.69342519\n",
            "Iteration 5, loss = 0.69332241\n",
            "Iteration 6, loss = 0.69332265\n",
            "Iteration 7, loss = 0.69327685\n",
            "Iteration 8, loss = 0.69323152\n",
            "Iteration 9, loss = 0.69322665\n",
            "Iteration 10, loss = 0.69319078\n",
            "Iteration 11, loss = 0.69316485\n",
            "Iteration 12, loss = 0.69316611\n",
            "Iteration 13, loss = 0.69313233\n",
            "Iteration 14, loss = 0.69312063\n",
            "Iteration 15, loss = 0.69312912\n",
            "Iteration 16, loss = 0.69308867\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69360028\n",
            "Iteration 2, loss = 0.69349796\n",
            "Iteration 3, loss = 0.69341863\n",
            "Iteration 4, loss = 0.69339689\n",
            "Iteration 5, loss = 0.69329576\n",
            "Iteration 6, loss = 0.69326491\n",
            "Iteration 7, loss = 0.69325880\n",
            "Iteration 8, loss = 0.69320269\n",
            "Iteration 9, loss = 0.69319184\n",
            "Iteration 10, loss = 0.69316382\n",
            "Iteration 11, loss = 0.69313740\n",
            "Iteration 12, loss = 0.69315735\n",
            "Iteration 13, loss = 0.69311342\n",
            "Iteration 14, loss = 0.69310405\n",
            "Iteration 15, loss = 0.69311537\n",
            "Iteration 16, loss = 0.69308473\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69359076\n",
            "Iteration 2, loss = 0.69350714\n",
            "Iteration 3, loss = 0.69350728\n",
            "Iteration 4, loss = 0.69340443\n",
            "Iteration 5, loss = 0.69338353\n",
            "Iteration 6, loss = 0.69336107\n",
            "Iteration 7, loss = 0.69333924\n",
            "Iteration 8, loss = 0.69333332\n",
            "Iteration 9, loss = 0.69331422\n",
            "Iteration 10, loss = 0.69330662\n",
            "Iteration 11, loss = 0.69326948\n",
            "Iteration 12, loss = 0.69327902\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69358208\n",
            "Iteration 2, loss = 0.69349029\n",
            "Iteration 3, loss = 0.69348860\n",
            "Iteration 4, loss = 0.69339786\n",
            "Iteration 5, loss = 0.69337809\n",
            "Iteration 6, loss = 0.69335347\n",
            "Iteration 7, loss = 0.69332948\n",
            "Iteration 8, loss = 0.69332101\n",
            "Iteration 9, loss = 0.69331468\n",
            "Iteration 10, loss = 0.69328031\n",
            "Iteration 11, loss = 0.69324631\n",
            "Iteration 12, loss = 0.69325585\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69357730\n",
            "Iteration 2, loss = 0.69346348\n",
            "Iteration 3, loss = 0.69347117\n",
            "Iteration 4, loss = 0.69336040\n",
            "Iteration 5, loss = 0.69334686\n",
            "Iteration 6, loss = 0.69331067\n",
            "Iteration 7, loss = 0.69328749\n",
            "Iteration 8, loss = 0.69327295\n",
            "Iteration 9, loss = 0.69326248\n",
            "Iteration 10, loss = 0.69324918\n",
            "Iteration 11, loss = 0.69323644\n",
            "Iteration 12, loss = 0.69321891\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69352593\n",
            "Iteration 2, loss = 0.69343509\n",
            "Iteration 3, loss = 0.69341913\n",
            "Iteration 4, loss = 0.69333959\n",
            "Iteration 5, loss = 0.69331927\n",
            "Iteration 6, loss = 0.69328665\n",
            "Iteration 7, loss = 0.69327354\n",
            "Iteration 8, loss = 0.69323887\n",
            "Iteration 9, loss = 0.69322761\n",
            "Iteration 10, loss = 0.69319634\n",
            "Iteration 11, loss = 0.69319422\n",
            "Iteration 12, loss = 0.69315328\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69350414\n",
            "Iteration 2, loss = 0.69341530\n",
            "Iteration 3, loss = 0.69339031\n",
            "Iteration 4, loss = 0.69330991\n",
            "Iteration 5, loss = 0.69327626\n",
            "Iteration 6, loss = 0.69327702\n",
            "Iteration 7, loss = 0.69323314\n",
            "Iteration 8, loss = 0.69321846\n",
            "Iteration 9, loss = 0.69320509\n",
            "Iteration 10, loss = 0.69319994\n",
            "Iteration 11, loss = 0.69317043\n",
            "Iteration 12, loss = 0.69316067\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69356384\n",
            "Iteration 2, loss = 0.69346868\n",
            "Iteration 3, loss = 0.69341421\n",
            "Iteration 4, loss = 0.69334792\n",
            "Iteration 5, loss = 0.69331006\n",
            "Iteration 6, loss = 0.69329775\n",
            "Iteration 7, loss = 0.69326716\n",
            "Iteration 8, loss = 0.69325144\n",
            "Iteration 9, loss = 0.69324573\n",
            "Iteration 10, loss = 0.69321849\n",
            "Iteration 11, loss = 0.69318582\n",
            "Iteration 12, loss = 0.69319428\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69362374\n",
            "Iteration 2, loss = 0.69352627\n",
            "Iteration 3, loss = 0.69345621\n",
            "Iteration 4, loss = 0.69342370\n",
            "Iteration 5, loss = 0.69334405\n",
            "Iteration 6, loss = 0.69332846\n",
            "Iteration 7, loss = 0.69328005\n",
            "Iteration 8, loss = 0.69323802\n",
            "Iteration 9, loss = 0.69326516\n",
            "Iteration 10, loss = 0.69318873\n",
            "Iteration 11, loss = 0.69316780\n",
            "Iteration 12, loss = 0.69316001\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69361476\n",
            "Iteration 2, loss = 0.69351091\n",
            "Iteration 3, loss = 0.69346582\n",
            "Iteration 4, loss = 0.69344619\n",
            "Iteration 5, loss = 0.69334398\n",
            "Iteration 6, loss = 0.69333656\n",
            "Iteration 7, loss = 0.69330584\n",
            "Iteration 8, loss = 0.69325978\n",
            "Iteration 9, loss = 0.69325852\n",
            "Iteration 10, loss = 0.69321689\n",
            "Iteration 11, loss = 0.69319523\n",
            "Iteration 12, loss = 0.69319414\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69361942\n",
            "Iteration 2, loss = 0.69351104\n",
            "Iteration 3, loss = 0.69344773\n",
            "Iteration 4, loss = 0.69342519\n",
            "Iteration 5, loss = 0.69332241\n",
            "Iteration 6, loss = 0.69332265\n",
            "Iteration 7, loss = 0.69327685\n",
            "Iteration 8, loss = 0.69323152\n",
            "Iteration 9, loss = 0.69322665\n",
            "Iteration 10, loss = 0.69319078\n",
            "Iteration 11, loss = 0.69316485\n",
            "Iteration 12, loss = 0.69316611\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.69360028\n",
            "Iteration 2, loss = 0.69349796\n",
            "Iteration 3, loss = 0.69341863\n",
            "Iteration 4, loss = 0.69339689\n",
            "Iteration 5, loss = 0.69329576\n",
            "Iteration 6, loss = 0.69326491\n",
            "Iteration 7, loss = 0.69325880\n",
            "Iteration 8, loss = 0.69320269\n",
            "Iteration 9, loss = 0.69319184\n",
            "Iteration 10, loss = 0.69316382\n",
            "Iteration 11, loss = 0.69313740\n",
            "Iteration 12, loss = 0.69315735\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.71436866\n",
            "Iteration 2, loss = 0.71298470\n",
            "Iteration 3, loss = 0.71155144\n",
            "Iteration 4, loss = 0.71028647\n",
            "Iteration 5, loss = 0.70896064\n",
            "Iteration 6, loss = 0.70770178\n",
            "Iteration 7, loss = 0.70647573\n",
            "Iteration 8, loss = 0.70547062\n",
            "Iteration 9, loss = 0.70419805\n",
            "Iteration 10, loss = 0.70317954\n",
            "Iteration 11, loss = 0.70219892\n",
            "Iteration 12, loss = 0.70127937\n",
            "Iteration 13, loss = 0.70023070\n",
            "Iteration 14, loss = 0.69931209\n",
            "Iteration 15, loss = 0.69836777\n",
            "Iteration 16, loss = 0.69755130\n",
            "Iteration 17, loss = 0.69672000\n",
            "Iteration 18, loss = 0.69581642\n",
            "Iteration 19, loss = 0.69502530\n",
            "Iteration 20, loss = 0.69427321\n",
            "Iteration 21, loss = 0.69345774\n",
            "Iteration 22, loss = 0.69281167\n",
            "Iteration 23, loss = 0.69196743\n",
            "Iteration 24, loss = 0.69121860\n",
            "Iteration 25, loss = 0.69052842\n",
            "Iteration 26, loss = 0.68972307\n",
            "Iteration 27, loss = 0.68896869\n",
            "Iteration 28, loss = 0.68812995\n",
            "Iteration 29, loss = 0.68729404\n",
            "Iteration 30, loss = 0.68637688\n",
            "Iteration 31, loss = 0.68542227\n",
            "Iteration 32, loss = 0.68434155\n",
            "Iteration 33, loss = 0.68325400\n",
            "Iteration 34, loss = 0.68191939\n",
            "Iteration 35, loss = 0.68054939\n",
            "Iteration 36, loss = 0.67885404\n",
            "Iteration 37, loss = 0.67713631\n",
            "Iteration 38, loss = 0.67521563\n",
            "Iteration 39, loss = 0.67334233\n",
            "Iteration 40, loss = 0.67145680\n",
            "Iteration 41, loss = 0.66941992\n",
            "Iteration 42, loss = 0.66731086\n",
            "Iteration 43, loss = 0.66514131\n",
            "Iteration 44, loss = 0.66281762\n",
            "Iteration 45, loss = 0.66068185\n",
            "Iteration 46, loss = 0.65833686\n",
            "Iteration 47, loss = 0.65609875\n",
            "Iteration 48, loss = 0.65392692\n",
            "Iteration 49, loss = 0.65194546\n",
            "Iteration 50, loss = 0.64994150\n",
            "Iteration 51, loss = 0.64751793\n",
            "Iteration 52, loss = 0.64532684\n",
            "Iteration 53, loss = 0.64301287\n",
            "Iteration 54, loss = 0.64097946\n",
            "Iteration 55, loss = 0.63905572\n",
            "Iteration 56, loss = 0.63735068\n",
            "Iteration 57, loss = 0.63561865\n",
            "Iteration 58, loss = 0.63404152\n",
            "Iteration 59, loss = 0.63244765\n",
            "Iteration 60, loss = 0.63104276\n",
            "Iteration 61, loss = 0.62953840\n",
            "Iteration 62, loss = 0.62806525\n",
            "Iteration 63, loss = 0.62652560\n",
            "Iteration 64, loss = 0.62491959\n",
            "Iteration 65, loss = 0.62319919\n",
            "Iteration 66, loss = 0.62159656\n",
            "Iteration 67, loss = 0.61998672\n",
            "Iteration 68, loss = 0.61822360\n",
            "Iteration 69, loss = 0.61651340\n",
            "Iteration 70, loss = 0.61471815\n",
            "Iteration 71, loss = 0.61302066\n",
            "Iteration 72, loss = 0.61140251\n",
            "Iteration 73, loss = 0.60980674\n",
            "Iteration 74, loss = 0.60817142\n",
            "Iteration 75, loss = 0.60647817\n",
            "Iteration 76, loss = 0.60475580\n",
            "Iteration 77, loss = 0.60298172\n",
            "Iteration 78, loss = 0.60122843\n",
            "Iteration 79, loss = 0.59939725\n",
            "Iteration 80, loss = 0.59740746\n",
            "Iteration 81, loss = 0.59545157\n",
            "Iteration 82, loss = 0.59348525\n",
            "Iteration 83, loss = 0.59138442\n",
            "Iteration 84, loss = 0.58909578\n",
            "Iteration 85, loss = 0.58642418\n",
            "Iteration 86, loss = 0.58367510\n",
            "Iteration 87, loss = 0.58086702\n",
            "Iteration 88, loss = 0.57778830\n",
            "Iteration 89, loss = 0.57483844\n",
            "Iteration 90, loss = 0.57177864\n",
            "Iteration 91, loss = 0.56874960\n",
            "Iteration 92, loss = 0.56552271\n",
            "Iteration 93, loss = 0.56244126\n",
            "Iteration 94, loss = 0.55902497\n",
            "Iteration 95, loss = 0.55612706\n",
            "Iteration 96, loss = 0.55274194\n",
            "Iteration 97, loss = 0.54944077\n",
            "Iteration 98, loss = 0.54638804\n",
            "Iteration 99, loss = 0.54312252\n",
            "Iteration 100, loss = 0.53979725\n",
            "Iteration 101, loss = 0.53662470\n",
            "Iteration 102, loss = 0.53347158\n",
            "Iteration 103, loss = 0.53066218\n",
            "Iteration 104, loss = 0.52784879\n",
            "Iteration 105, loss = 0.52513184\n",
            "Iteration 106, loss = 0.52293509\n",
            "Iteration 107, loss = 0.52025176\n",
            "Iteration 108, loss = 0.51802391\n",
            "Iteration 109, loss = 0.51583627\n",
            "Iteration 110, loss = 0.51390106\n",
            "Iteration 111, loss = 0.51186364\n",
            "Iteration 112, loss = 0.50976400\n",
            "Iteration 113, loss = 0.50788703\n",
            "Iteration 114, loss = 0.50602841\n",
            "Iteration 115, loss = 0.50426711\n",
            "Iteration 116, loss = 0.50255519\n",
            "Iteration 117, loss = 0.50089726\n",
            "Iteration 118, loss = 0.49925036\n",
            "Iteration 119, loss = 0.49760243\n",
            "Iteration 120, loss = 0.49604437\n",
            "Iteration 121, loss = 0.49455642\n",
            "Iteration 122, loss = 0.49310537\n",
            "Iteration 123, loss = 0.49169696\n",
            "Iteration 124, loss = 0.49020270\n",
            "Iteration 125, loss = 0.48880190\n",
            "Iteration 126, loss = 0.48757120\n",
            "Iteration 127, loss = 0.48622578\n",
            "Iteration 128, loss = 0.48488589\n",
            "Iteration 129, loss = 0.48357933\n",
            "Iteration 130, loss = 0.48227846\n",
            "Iteration 131, loss = 0.48106975\n",
            "Iteration 132, loss = 0.47985496\n",
            "Iteration 133, loss = 0.47874135\n",
            "Iteration 134, loss = 0.47742241\n",
            "Iteration 135, loss = 0.47630752\n",
            "Iteration 136, loss = 0.47513997\n",
            "Iteration 137, loss = 0.47392190\n",
            "Iteration 138, loss = 0.47287117\n",
            "Iteration 139, loss = 0.47173976\n",
            "Iteration 140, loss = 0.47061560\n",
            "Iteration 141, loss = 0.46963345\n",
            "Iteration 142, loss = 0.46854473\n",
            "Iteration 143, loss = 0.46755233\n",
            "Iteration 144, loss = 0.46650889\n",
            "Iteration 145, loss = 0.46547016\n",
            "Iteration 146, loss = 0.46455294\n",
            "Iteration 147, loss = 0.46358548\n",
            "Iteration 148, loss = 0.46263003\n",
            "Iteration 149, loss = 0.46183301\n",
            "Iteration 150, loss = 0.46069190\n",
            "Iteration 151, loss = 0.45978193\n",
            "Iteration 152, loss = 0.45889381\n",
            "Iteration 153, loss = 0.45801085\n",
            "Iteration 154, loss = 0.45704234\n",
            "Iteration 155, loss = 0.45616448\n",
            "Iteration 156, loss = 0.45535535\n",
            "Iteration 157, loss = 0.45451620\n",
            "Iteration 158, loss = 0.45380139\n",
            "Iteration 159, loss = 0.45299652\n",
            "Iteration 160, loss = 0.45217335\n",
            "Iteration 161, loss = 0.45140140\n",
            "Iteration 162, loss = 0.45065064\n",
            "Iteration 163, loss = 0.44998447\n",
            "Iteration 164, loss = 0.44920413\n",
            "Iteration 165, loss = 0.44859533\n",
            "Iteration 166, loss = 0.44782006\n",
            "Iteration 167, loss = 0.44710385\n",
            "Iteration 168, loss = 0.44639360\n",
            "Iteration 169, loss = 0.44571490\n",
            "Iteration 170, loss = 0.44514482\n",
            "Iteration 171, loss = 0.44445105\n",
            "Iteration 172, loss = 0.44381936\n",
            "Iteration 173, loss = 0.44316721\n",
            "Iteration 174, loss = 0.44256493\n",
            "Iteration 175, loss = 0.44196929\n",
            "Iteration 176, loss = 0.44124637\n",
            "Iteration 177, loss = 0.44081203\n",
            "Iteration 178, loss = 0.44037001\n",
            "Iteration 179, loss = 0.43950856\n",
            "Iteration 180, loss = 0.43889165\n",
            "Iteration 181, loss = 0.43848815\n",
            "Iteration 182, loss = 0.43781310\n",
            "Iteration 183, loss = 0.43718786\n",
            "Iteration 184, loss = 0.43664502\n",
            "Iteration 185, loss = 0.43624056\n",
            "Iteration 186, loss = 0.43563571\n",
            "Iteration 187, loss = 0.43509901\n",
            "Iteration 188, loss = 0.43458097\n",
            "Iteration 189, loss = 0.43407435\n",
            "Iteration 190, loss = 0.43352046\n",
            "Iteration 191, loss = 0.43308542\n",
            "Iteration 192, loss = 0.43255121\n",
            "Iteration 193, loss = 0.43206666\n",
            "Iteration 194, loss = 0.43164378\n",
            "Iteration 195, loss = 0.43115643\n",
            "Iteration 196, loss = 0.43067406\n",
            "Iteration 197, loss = 0.43018084\n",
            "Iteration 198, loss = 0.42975603\n",
            "Iteration 199, loss = 0.42938489\n",
            "Iteration 200, loss = 0.42883304\n",
            "Iteration 201, loss = 0.42829885\n",
            "Iteration 202, loss = 0.42787333\n",
            "Iteration 203, loss = 0.42746061\n",
            "Iteration 204, loss = 0.42706963\n",
            "Iteration 205, loss = 0.42657284\n",
            "Iteration 206, loss = 0.42617682\n",
            "Iteration 207, loss = 0.42577324\n",
            "Iteration 208, loss = 0.42519410\n",
            "Iteration 209, loss = 0.42484251\n",
            "Iteration 210, loss = 0.42435014\n",
            "Iteration 211, loss = 0.42402041\n",
            "Iteration 212, loss = 0.42348916\n",
            "Iteration 213, loss = 0.42311840\n",
            "Iteration 214, loss = 0.42265312\n",
            "Iteration 215, loss = 0.42221378\n",
            "Iteration 216, loss = 0.42190195\n",
            "Iteration 217, loss = 0.42145215\n",
            "Iteration 218, loss = 0.42121295\n",
            "Iteration 219, loss = 0.42080155\n",
            "Iteration 220, loss = 0.42034705\n",
            "Iteration 221, loss = 0.41999610\n",
            "Iteration 222, loss = 0.41964580\n",
            "Iteration 223, loss = 0.41934934\n",
            "Iteration 224, loss = 0.41916394\n",
            "Iteration 225, loss = 0.41862046\n",
            "Iteration 226, loss = 0.41850487\n",
            "Iteration 227, loss = 0.41808654\n",
            "Iteration 228, loss = 0.41769004\n",
            "Iteration 229, loss = 0.41739748\n",
            "Iteration 230, loss = 0.41702452\n",
            "Iteration 231, loss = 0.41676242\n",
            "Iteration 232, loss = 0.41649931\n",
            "Iteration 233, loss = 0.41603762\n",
            "Iteration 234, loss = 0.41572983\n",
            "Iteration 235, loss = 0.41543584\n",
            "Iteration 236, loss = 0.41511029\n",
            "Iteration 237, loss = 0.41480372\n",
            "Iteration 238, loss = 0.41458596\n",
            "Iteration 239, loss = 0.41443819\n",
            "Iteration 240, loss = 0.41397656\n",
            "Iteration 241, loss = 0.41368089\n",
            "Iteration 242, loss = 0.41335850\n",
            "Iteration 243, loss = 0.41305139\n",
            "Iteration 244, loss = 0.41263849\n",
            "Iteration 245, loss = 0.41255489\n",
            "Iteration 246, loss = 0.41217171\n",
            "Iteration 247, loss = 0.41173162\n",
            "Iteration 248, loss = 0.41145007\n",
            "Iteration 249, loss = 0.41115669\n",
            "Iteration 250, loss = 0.41092875\n",
            "Iteration 251, loss = 0.41059179\n",
            "Iteration 252, loss = 0.41040227\n",
            "Iteration 253, loss = 0.41008763\n",
            "Iteration 254, loss = 0.40978392\n",
            "Iteration 255, loss = 0.40951109\n",
            "Iteration 256, loss = 0.40929860\n",
            "Iteration 257, loss = 0.40925924\n",
            "Iteration 258, loss = 0.40878167\n",
            "Iteration 259, loss = 0.40850066\n",
            "Iteration 260, loss = 0.40828421\n",
            "Iteration 261, loss = 0.40820107\n",
            "Iteration 262, loss = 0.40776081\n",
            "Iteration 263, loss = 0.40745116\n",
            "Iteration 264, loss = 0.40734584\n",
            "Iteration 265, loss = 0.40705954\n",
            "Iteration 266, loss = 0.40679379\n",
            "Iteration 267, loss = 0.40652085\n",
            "Iteration 268, loss = 0.40634489\n",
            "Iteration 269, loss = 0.40628220\n",
            "Iteration 270, loss = 0.40584693\n",
            "Iteration 271, loss = 0.40562984\n",
            "Iteration 272, loss = 0.40533394\n",
            "Iteration 273, loss = 0.40524570\n",
            "Iteration 274, loss = 0.40492586\n",
            "Iteration 275, loss = 0.40465904\n",
            "Iteration 276, loss = 0.40453719\n",
            "Iteration 277, loss = 0.40429081\n",
            "Iteration 278, loss = 0.40410599\n",
            "Iteration 279, loss = 0.40385187\n",
            "Iteration 280, loss = 0.40365186\n",
            "Iteration 281, loss = 0.40339188\n",
            "Iteration 282, loss = 0.40324111\n",
            "Iteration 283, loss = 0.40312098\n",
            "Iteration 284, loss = 0.40280962\n",
            "Iteration 285, loss = 0.40259551\n",
            "Iteration 286, loss = 0.40241629\n",
            "Iteration 287, loss = 0.40226383\n",
            "Iteration 288, loss = 0.40218468\n",
            "Iteration 289, loss = 0.40216453\n",
            "Iteration 290, loss = 0.40174757\n",
            "Iteration 291, loss = 0.40143134\n",
            "Iteration 292, loss = 0.40123658\n",
            "Iteration 293, loss = 0.40116061\n",
            "Iteration 294, loss = 0.40096650\n",
            "Iteration 295, loss = 0.40072234\n",
            "Iteration 296, loss = 0.40058093\n",
            "Iteration 297, loss = 0.40033974\n",
            "Iteration 298, loss = 0.40043088\n",
            "Iteration 299, loss = 0.40007507\n",
            "Iteration 300, loss = 0.39987545\n",
            "Iteration 301, loss = 0.39967260\n",
            "Iteration 302, loss = 0.39952394\n",
            "Iteration 303, loss = 0.39936403\n",
            "Iteration 304, loss = 0.39925167\n",
            "Iteration 305, loss = 0.39907246\n",
            "Iteration 306, loss = 0.39888978\n",
            "Iteration 307, loss = 0.39875946\n",
            "Iteration 308, loss = 0.39874608\n",
            "Iteration 309, loss = 0.39844630\n",
            "Iteration 310, loss = 0.39833908\n",
            "Iteration 311, loss = 0.39819530\n",
            "Iteration 312, loss = 0.39797390\n",
            "Iteration 313, loss = 0.39785572\n",
            "Iteration 314, loss = 0.39771523\n",
            "Iteration 315, loss = 0.39785598\n",
            "Iteration 316, loss = 0.39750317\n",
            "Iteration 317, loss = 0.39738370\n",
            "Iteration 318, loss = 0.39724919\n",
            "Iteration 319, loss = 0.39709356\n",
            "Iteration 320, loss = 0.39698929\n",
            "Iteration 321, loss = 0.39682182\n",
            "Iteration 322, loss = 0.39666256\n",
            "Iteration 323, loss = 0.39656251\n",
            "Iteration 324, loss = 0.39641515\n",
            "Iteration 325, loss = 0.39628335\n",
            "Iteration 326, loss = 0.39618216\n",
            "Iteration 327, loss = 0.39604716\n",
            "Iteration 328, loss = 0.39592786\n",
            "Iteration 329, loss = 0.39585543\n",
            "Iteration 330, loss = 0.39568447\n",
            "Iteration 331, loss = 0.39556756\n",
            "Iteration 332, loss = 0.39560163\n",
            "Iteration 333, loss = 0.39532767\n",
            "Iteration 334, loss = 0.39521951\n",
            "Iteration 335, loss = 0.39520398\n",
            "Iteration 336, loss = 0.39508849\n",
            "Iteration 337, loss = 0.39490839\n",
            "Iteration 338, loss = 0.39485525\n",
            "Iteration 339, loss = 0.39480839\n",
            "Iteration 340, loss = 0.39457939\n",
            "Iteration 341, loss = 0.39464878\n",
            "Iteration 342, loss = 0.39458420\n",
            "Iteration 343, loss = 0.39434598\n",
            "Iteration 344, loss = 0.39417486\n",
            "Iteration 345, loss = 0.39409409\n",
            "Iteration 346, loss = 0.39413964\n",
            "Iteration 347, loss = 0.39390737\n",
            "Iteration 348, loss = 0.39377362\n",
            "Iteration 349, loss = 0.39379667\n",
            "Iteration 350, loss = 0.39376375\n",
            "Iteration 351, loss = 0.39359107\n",
            "Iteration 352, loss = 0.39343511\n",
            "Iteration 353, loss = 0.39362844\n",
            "Iteration 354, loss = 0.39332694\n",
            "Iteration 355, loss = 0.39325414\n",
            "Iteration 356, loss = 0.39314084\n",
            "Iteration 357, loss = 0.39302489\n",
            "Iteration 358, loss = 0.39288193\n",
            "Iteration 359, loss = 0.39280935\n",
            "Iteration 360, loss = 0.39272524\n",
            "Iteration 361, loss = 0.39265029\n",
            "Iteration 362, loss = 0.39254886\n",
            "Iteration 363, loss = 0.39256692\n",
            "Iteration 364, loss = 0.39237287\n",
            "Iteration 365, loss = 0.39233960\n",
            "Iteration 366, loss = 0.39222598\n",
            "Iteration 367, loss = 0.39225019\n",
            "Iteration 368, loss = 0.39214527\n",
            "Iteration 369, loss = 0.39206407\n",
            "Iteration 370, loss = 0.39191243\n",
            "Iteration 371, loss = 0.39188876\n",
            "Iteration 372, loss = 0.39183156\n",
            "Iteration 373, loss = 0.39168040\n",
            "Iteration 374, loss = 0.39163261\n",
            "Iteration 375, loss = 0.39162150\n",
            "Iteration 376, loss = 0.39145090\n",
            "Iteration 377, loss = 0.39147066\n",
            "Iteration 378, loss = 0.39133228\n",
            "Iteration 379, loss = 0.39155573\n",
            "Iteration 380, loss = 0.39118617\n",
            "Iteration 381, loss = 0.39131336\n",
            "Iteration 382, loss = 0.39113404\n",
            "Iteration 383, loss = 0.39104528\n",
            "Iteration 384, loss = 0.39089439\n",
            "Iteration 385, loss = 0.39100460\n",
            "Iteration 386, loss = 0.39081413\n",
            "Iteration 387, loss = 0.39079279\n",
            "Iteration 388, loss = 0.39066933\n",
            "Iteration 389, loss = 0.39058647\n",
            "Iteration 390, loss = 0.39051401\n",
            "Iteration 391, loss = 0.39050502\n",
            "Iteration 392, loss = 0.39041573\n",
            "Iteration 393, loss = 0.39050190\n",
            "Iteration 394, loss = 0.39033883\n",
            "Iteration 395, loss = 0.39023212\n",
            "Iteration 396, loss = 0.39020753\n",
            "Iteration 397, loss = 0.39019807\n",
            "Iteration 398, loss = 0.39006689\n",
            "Iteration 399, loss = 0.39000814\n",
            "Iteration 400, loss = 0.38996926\n",
            "Iteration 401, loss = 0.38997485\n",
            "Iteration 402, loss = 0.38978884\n",
            "Iteration 403, loss = 0.38974648\n",
            "Iteration 404, loss = 0.38965462\n",
            "Iteration 405, loss = 0.38971995\n",
            "Iteration 406, loss = 0.38960030\n",
            "Iteration 407, loss = 0.38951848\n",
            "Iteration 408, loss = 0.38949947\n",
            "Iteration 409, loss = 0.38953193\n",
            "Iteration 410, loss = 0.38945962\n",
            "Iteration 411, loss = 0.38924015\n",
            "Iteration 412, loss = 0.38926658\n",
            "Iteration 413, loss = 0.38913009\n",
            "Iteration 414, loss = 0.38923163\n",
            "Iteration 415, loss = 0.38915300\n",
            "Iteration 416, loss = 0.38903943\n",
            "Iteration 417, loss = 0.38900036\n",
            "Iteration 418, loss = 0.38889484\n",
            "Iteration 419, loss = 0.38897103\n",
            "Iteration 420, loss = 0.38878258\n",
            "Iteration 421, loss = 0.38873612\n",
            "Iteration 422, loss = 0.38865695\n",
            "Iteration 423, loss = 0.38870450\n",
            "Iteration 424, loss = 0.38873018\n",
            "Iteration 425, loss = 0.38870221\n",
            "Iteration 426, loss = 0.38856109\n",
            "Iteration 427, loss = 0.38853617\n",
            "Iteration 428, loss = 0.38841887\n",
            "Iteration 429, loss = 0.38832437\n",
            "Iteration 430, loss = 0.38826946\n",
            "Iteration 431, loss = 0.38821906\n",
            "Iteration 432, loss = 0.38830331\n",
            "Iteration 433, loss = 0.38816586\n",
            "Iteration 434, loss = 0.38824081\n",
            "Iteration 435, loss = 0.38810695\n",
            "Iteration 436, loss = 0.38812757\n",
            "Iteration 437, loss = 0.38801699\n",
            "Iteration 438, loss = 0.38800108\n",
            "Iteration 439, loss = 0.38796410\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Melhores hiperparâmetros encontrados através do Grid Search:\n",
            "{'activation': 'relu', 'max_iter': 1000, 'solver': 'adam', 'tol': 0.0001}\n",
            "Melhor pontuação (acurácia) encontrada através do Grid Search: 0.8223684210526315\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ajuste do modelo MLP aos dados de treinamento\n",
        "rna3_best_grid = MLPClassifier(**best_params_grid3)\n",
        "rna3_best_grid.fit(X_train_oversampled, y_train_oversampled)\n",
        "\n",
        "# Predições nos dados de teste usando o modelo com melhores hiperparâmetros encontrados pelo Grid Search\n",
        "pred_grid3 = rna3_best_grid.predict(X_test)"
      ],
      "metadata": {
        "id": "I9f5D4vUXZq-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cálculo e impressão da acurácia nos dados de teste\n",
        "accuracy_grid = accuracy_score(y_test, pred_grid3)\n",
        "print(\"Acurácia do modelo MLP com melhores hiperparâmetros pelo Grid Search:\", accuracy_grid)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4b9dbe1-594c-4162-ea2e-df68f668da26",
        "id": "F_K2EGVHXZq_"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acurácia do modelo MLP com melhores hiperparâmetros pelo Grid Search: 0.7649253731343284\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Matriz de confusão para o modelo com melhores hiperparâmetros pelo Grid Search\n",
        "print(\"Matriz de Confusão - Grid Search\")\n",
        "cm_rna3_grid = confusion_matrix(y_test, pred_grid3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1ea089a-2519-4599-e641-78b0f63a5846",
        "id": "xGVpsBP1XZq_"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matriz de Confusão - Grid Search\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtém a média das acurácias (10 folds) referente ao conjunto treino\n",
        "rna3_grid = g_results.loc[g_search.best_index_,'mean_test_score']"
      ],
      "metadata": {
        "id": "LHY9yYGRXZq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RandomSearch"
      ],
      "metadata": {
        "id": "h-37MBn7XZq_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Random Search\n",
        "param_dist = {\n",
        "    'activation': ['relu', 'tanh', 'logistic'],\n",
        "    'solver': ['sgd', 'adam'],\n",
        "    'max_iter': [1000, 2000],\n",
        "    'tol': [0.0001, 0.001],\n",
        "}\n",
        "\n",
        "r_search = RandomizedSearchCV(estimator=rna3, param_distributions=param_dist, cv=10)\n",
        "r_search.fit(X_train_oversampled, y_train_oversampled)\n",
        "\n",
        "best_params_random = r_search.best_params_\n",
        "best_score_random = r_search.best_score_\n",
        "\n",
        "print(\"Melhores hiperparâmetros encontrados através do Random Search:\")\n",
        "print(best_params_random)\n",
        "print(\"Melhor pontuação (acurácia) encontrada através do Random Search:\", best_score_random)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QyuDCcA0XZq_",
        "outputId": "a670ca20-660a-401a-a936-ee10fb7f8d5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mA saída de streaming foi truncada nas últimas 5000 linhas.\u001b[0m\n",
            "Iteration 206, loss = 0.41144847\n",
            "Iteration 207, loss = 0.41112272\n",
            "Iteration 208, loss = 0.41082921\n",
            "Iteration 209, loss = 0.41058993\n",
            "Iteration 210, loss = 0.41023688\n",
            "Iteration 211, loss = 0.40997129\n",
            "Iteration 212, loss = 0.40998214\n",
            "Iteration 213, loss = 0.40969567\n",
            "Iteration 214, loss = 0.40932898\n",
            "Iteration 215, loss = 0.40888552\n",
            "Iteration 216, loss = 0.40856962\n",
            "Iteration 217, loss = 0.40839592\n",
            "Iteration 218, loss = 0.40794636\n",
            "Iteration 219, loss = 0.40785968\n",
            "Iteration 220, loss = 0.40796878\n",
            "Iteration 221, loss = 0.40755539\n",
            "Iteration 222, loss = 0.40711673\n",
            "Iteration 223, loss = 0.40678923\n",
            "Iteration 224, loss = 0.40650660\n",
            "Iteration 225, loss = 0.40613991\n",
            "Iteration 226, loss = 0.40589976\n",
            "Iteration 227, loss = 0.40556719\n",
            "Iteration 228, loss = 0.40536546\n",
            "Iteration 229, loss = 0.40512061\n",
            "Iteration 230, loss = 0.40481443\n",
            "Iteration 231, loss = 0.40455835\n",
            "Iteration 232, loss = 0.40444393\n",
            "Iteration 233, loss = 0.40414573\n",
            "Iteration 234, loss = 0.40393904\n",
            "Iteration 235, loss = 0.40372370\n",
            "Iteration 236, loss = 0.40369421\n",
            "Iteration 237, loss = 0.40317625\n",
            "Iteration 238, loss = 0.40297186\n",
            "Iteration 239, loss = 0.40283506\n",
            "Iteration 240, loss = 0.40257559\n",
            "Iteration 241, loss = 0.40239315\n",
            "Iteration 242, loss = 0.40212069\n",
            "Iteration 243, loss = 0.40213655\n",
            "Iteration 244, loss = 0.40167810\n",
            "Iteration 245, loss = 0.40152481\n",
            "Iteration 246, loss = 0.40128902\n",
            "Iteration 247, loss = 0.40101783\n",
            "Iteration 248, loss = 0.40084314\n",
            "Iteration 249, loss = 0.40043974\n",
            "Iteration 250, loss = 0.40021581\n",
            "Iteration 251, loss = 0.39998205\n",
            "Iteration 252, loss = 0.39990944\n",
            "Iteration 253, loss = 0.39969021\n",
            "Iteration 254, loss = 0.39949839\n",
            "Iteration 255, loss = 0.39926744\n",
            "Iteration 256, loss = 0.39907474\n",
            "Iteration 257, loss = 0.39889248\n",
            "Iteration 258, loss = 0.39876965\n",
            "Iteration 259, loss = 0.39853335\n",
            "Iteration 260, loss = 0.39829510\n",
            "Iteration 261, loss = 0.39814646\n",
            "Iteration 262, loss = 0.39782361\n",
            "Iteration 263, loss = 0.39767371\n",
            "Iteration 264, loss = 0.39748878\n",
            "Iteration 265, loss = 0.39722019\n",
            "Iteration 266, loss = 0.39698317\n",
            "Iteration 267, loss = 0.39671897\n",
            "Iteration 268, loss = 0.39662461\n",
            "Iteration 269, loss = 0.39666405\n",
            "Iteration 270, loss = 0.39626670\n",
            "Iteration 271, loss = 0.39602030\n",
            "Iteration 272, loss = 0.39597116\n",
            "Iteration 273, loss = 0.39577014\n",
            "Iteration 274, loss = 0.39551247\n",
            "Iteration 275, loss = 0.39530295\n",
            "Iteration 276, loss = 0.39540534\n",
            "Iteration 277, loss = 0.39509462\n",
            "Iteration 278, loss = 0.39490296\n",
            "Iteration 279, loss = 0.39461826\n",
            "Iteration 280, loss = 0.39431518\n",
            "Iteration 281, loss = 0.39425841\n",
            "Iteration 282, loss = 0.39406441\n",
            "Iteration 283, loss = 0.39406396\n",
            "Iteration 284, loss = 0.39383138\n",
            "Iteration 285, loss = 0.39351575\n",
            "Iteration 286, loss = 0.39331022\n",
            "Iteration 287, loss = 0.39323140\n",
            "Iteration 288, loss = 0.39322142\n",
            "Iteration 289, loss = 0.39295729\n",
            "Iteration 290, loss = 0.39265594\n",
            "Iteration 291, loss = 0.39253045\n",
            "Iteration 292, loss = 0.39241070\n",
            "Iteration 293, loss = 0.39226763\n",
            "Iteration 294, loss = 0.39205658\n",
            "Iteration 295, loss = 0.39187583\n",
            "Iteration 296, loss = 0.39181335\n",
            "Iteration 297, loss = 0.39172836\n",
            "Iteration 298, loss = 0.39142752\n",
            "Iteration 299, loss = 0.39128712\n",
            "Iteration 300, loss = 0.39103917\n",
            "Iteration 301, loss = 0.39094424\n",
            "Iteration 302, loss = 0.39084395\n",
            "Iteration 303, loss = 0.39081602\n",
            "Iteration 304, loss = 0.39053936\n",
            "Iteration 305, loss = 0.39037295\n",
            "Iteration 306, loss = 0.39013836\n",
            "Iteration 307, loss = 0.39001277\n",
            "Iteration 308, loss = 0.38985393\n",
            "Iteration 309, loss = 0.38961939\n",
            "Iteration 310, loss = 0.38947891\n",
            "Iteration 311, loss = 0.38941672\n",
            "Iteration 312, loss = 0.38932471\n",
            "Iteration 313, loss = 0.38915171\n",
            "Iteration 314, loss = 0.38899072\n",
            "Iteration 315, loss = 0.38882785\n",
            "Iteration 316, loss = 0.38867874\n",
            "Iteration 317, loss = 0.38855702\n",
            "Iteration 318, loss = 0.38840682\n",
            "Iteration 319, loss = 0.38826441\n",
            "Iteration 320, loss = 0.38818325\n",
            "Iteration 321, loss = 0.38797518\n",
            "Iteration 322, loss = 0.38786335\n",
            "Iteration 323, loss = 0.38768179\n",
            "Iteration 324, loss = 0.38758980\n",
            "Iteration 325, loss = 0.38744331\n",
            "Iteration 326, loss = 0.38724918\n",
            "Iteration 327, loss = 0.38728857\n",
            "Iteration 328, loss = 0.38704636\n",
            "Iteration 329, loss = 0.38689405\n",
            "Iteration 330, loss = 0.38696002\n",
            "Iteration 331, loss = 0.38669373\n",
            "Iteration 332, loss = 0.38658193\n",
            "Iteration 333, loss = 0.38643894\n",
            "Iteration 334, loss = 0.38625328\n",
            "Iteration 335, loss = 0.38612503\n",
            "Iteration 336, loss = 0.38600008\n",
            "Iteration 337, loss = 0.38580224\n",
            "Iteration 338, loss = 0.38578094\n",
            "Iteration 339, loss = 0.38547745\n",
            "Iteration 340, loss = 0.38538688\n",
            "Iteration 341, loss = 0.38538643\n",
            "Iteration 342, loss = 0.38527116\n",
            "Iteration 343, loss = 0.38494853\n",
            "Iteration 344, loss = 0.38491508\n",
            "Iteration 345, loss = 0.38491458\n",
            "Iteration 346, loss = 0.38468186\n",
            "Iteration 347, loss = 0.38451994\n",
            "Iteration 348, loss = 0.38448109\n",
            "Iteration 349, loss = 0.38450915\n",
            "Iteration 350, loss = 0.38432208\n",
            "Iteration 351, loss = 0.38407762\n",
            "Iteration 352, loss = 0.38417226\n",
            "Iteration 353, loss = 0.38379719\n",
            "Iteration 354, loss = 0.38379507\n",
            "Iteration 355, loss = 0.38358583\n",
            "Iteration 356, loss = 0.38335916\n",
            "Iteration 357, loss = 0.38331482\n",
            "Iteration 358, loss = 0.38340241\n",
            "Iteration 359, loss = 0.38333291\n",
            "Iteration 360, loss = 0.38316086\n",
            "Iteration 361, loss = 0.38293717\n",
            "Iteration 362, loss = 0.38281578\n",
            "Iteration 363, loss = 0.38263110\n",
            "Iteration 364, loss = 0.38251448\n",
            "Iteration 365, loss = 0.38240917\n",
            "Iteration 366, loss = 0.38220137\n",
            "Iteration 367, loss = 0.38213284\n",
            "Iteration 368, loss = 0.38211952\n",
            "Iteration 369, loss = 0.38198336\n",
            "Iteration 370, loss = 0.38192963\n",
            "Iteration 371, loss = 0.38163652\n",
            "Iteration 372, loss = 0.38156519\n",
            "Iteration 373, loss = 0.38154870\n",
            "Iteration 374, loss = 0.38131645\n",
            "Iteration 375, loss = 0.38123111\n",
            "Iteration 376, loss = 0.38128368\n",
            "Iteration 377, loss = 0.38107855\n",
            "Iteration 378, loss = 0.38116885\n",
            "Iteration 379, loss = 0.38090146\n",
            "Iteration 380, loss = 0.38075041\n",
            "Iteration 381, loss = 0.38050454\n",
            "Iteration 382, loss = 0.38038122\n",
            "Iteration 383, loss = 0.38045749\n",
            "Iteration 384, loss = 0.38040147\n",
            "Iteration 385, loss = 0.38013671\n",
            "Iteration 386, loss = 0.38007772\n",
            "Iteration 387, loss = 0.37987742\n",
            "Iteration 388, loss = 0.37979199\n",
            "Iteration 389, loss = 0.37985247\n",
            "Iteration 390, loss = 0.37969611\n",
            "Iteration 391, loss = 0.37937757\n",
            "Iteration 392, loss = 0.37969471\n",
            "Iteration 393, loss = 0.37945362\n",
            "Iteration 394, loss = 0.37934016\n",
            "Iteration 395, loss = 0.37919312\n",
            "Iteration 396, loss = 0.37903709\n",
            "Iteration 397, loss = 0.37893572\n",
            "Iteration 398, loss = 0.37885799\n",
            "Iteration 399, loss = 0.37874197\n",
            "Iteration 400, loss = 0.37864265\n",
            "Iteration 401, loss = 0.37876236\n",
            "Iteration 402, loss = 0.37865728\n",
            "Iteration 403, loss = 0.37835267\n",
            "Iteration 404, loss = 0.37819811\n",
            "Iteration 405, loss = 0.37813622\n",
            "Iteration 406, loss = 0.37806954\n",
            "Iteration 407, loss = 0.37829732\n",
            "Iteration 408, loss = 0.37789764\n",
            "Iteration 409, loss = 0.37775910\n",
            "Iteration 410, loss = 0.37790101\n",
            "Iteration 411, loss = 0.37767151\n",
            "Iteration 412, loss = 0.37763120\n",
            "Iteration 413, loss = 0.37736435\n",
            "Iteration 414, loss = 0.37740482\n",
            "Iteration 415, loss = 0.37725065\n",
            "Iteration 416, loss = 0.37717604\n",
            "Iteration 417, loss = 0.37702801\n",
            "Iteration 418, loss = 0.37693273\n",
            "Iteration 419, loss = 0.37691310\n",
            "Iteration 420, loss = 0.37681418\n",
            "Iteration 421, loss = 0.37671444\n",
            "Iteration 422, loss = 0.37678471\n",
            "Iteration 423, loss = 0.37656397\n",
            "Iteration 424, loss = 0.37653009\n",
            "Iteration 425, loss = 0.37639545\n",
            "Iteration 426, loss = 0.37617161\n",
            "Iteration 427, loss = 0.37623245\n",
            "Iteration 428, loss = 0.37613591\n",
            "Iteration 429, loss = 0.37607977\n",
            "Iteration 430, loss = 0.37592668\n",
            "Iteration 431, loss = 0.37584653\n",
            "Iteration 432, loss = 0.37581605\n",
            "Iteration 433, loss = 0.37562396\n",
            "Iteration 434, loss = 0.37566875\n",
            "Iteration 435, loss = 0.37549137\n",
            "Iteration 436, loss = 0.37539271\n",
            "Iteration 437, loss = 0.37526225\n",
            "Iteration 438, loss = 0.37512929\n",
            "Iteration 439, loss = 0.37513856\n",
            "Iteration 440, loss = 0.37499415\n",
            "Iteration 441, loss = 0.37489329\n",
            "Iteration 442, loss = 0.37498719\n",
            "Iteration 443, loss = 0.37470094\n",
            "Iteration 444, loss = 0.37463850\n",
            "Iteration 445, loss = 0.37461311\n",
            "Iteration 446, loss = 0.37448880\n",
            "Iteration 447, loss = 0.37439626\n",
            "Iteration 448, loss = 0.37427102\n",
            "Iteration 449, loss = 0.37441306\n",
            "Iteration 450, loss = 0.37418028\n",
            "Iteration 451, loss = 0.37410990\n",
            "Iteration 452, loss = 0.37405310\n",
            "Iteration 453, loss = 0.37403266\n",
            "Iteration 454, loss = 0.37372965\n",
            "Iteration 455, loss = 0.37394556\n",
            "Iteration 456, loss = 0.37371411\n",
            "Iteration 457, loss = 0.37356009\n",
            "Iteration 458, loss = 0.37340935\n",
            "Iteration 459, loss = 0.37337122\n",
            "Iteration 460, loss = 0.37338824\n",
            "Iteration 461, loss = 0.37374262\n",
            "Iteration 462, loss = 0.37338844\n",
            "Iteration 463, loss = 0.37322903\n",
            "Iteration 464, loss = 0.37287054\n",
            "Iteration 465, loss = 0.37290244\n",
            "Iteration 466, loss = 0.37291797\n",
            "Iteration 467, loss = 0.37286072\n",
            "Iteration 468, loss = 0.37277955\n",
            "Iteration 469, loss = 0.37272256\n",
            "Iteration 470, loss = 0.37252069\n",
            "Iteration 471, loss = 0.37258376\n",
            "Iteration 472, loss = 0.37247161\n",
            "Iteration 473, loss = 0.37229879\n",
            "Iteration 474, loss = 0.37221158\n",
            "Iteration 475, loss = 0.37238979\n",
            "Iteration 476, loss = 0.37208046\n",
            "Iteration 477, loss = 0.37194702\n",
            "Iteration 478, loss = 0.37190133\n",
            "Iteration 479, loss = 0.37190071\n",
            "Iteration 480, loss = 0.37190918\n",
            "Iteration 481, loss = 0.37187380\n",
            "Iteration 482, loss = 0.37168928\n",
            "Iteration 483, loss = 0.37156349\n",
            "Iteration 484, loss = 0.37147863\n",
            "Iteration 485, loss = 0.37138580\n",
            "Iteration 486, loss = 0.37137131\n",
            "Iteration 487, loss = 0.37123207\n",
            "Iteration 488, loss = 0.37119236\n",
            "Iteration 489, loss = 0.37110295\n",
            "Iteration 490, loss = 0.37103334\n",
            "Iteration 491, loss = 0.37093369\n",
            "Iteration 492, loss = 0.37089292\n",
            "Iteration 493, loss = 0.37093940\n",
            "Iteration 494, loss = 0.37083623\n",
            "Iteration 495, loss = 0.37066656\n",
            "Iteration 496, loss = 0.37055818\n",
            "Iteration 497, loss = 0.37052348\n",
            "Iteration 498, loss = 0.37051325\n",
            "Iteration 499, loss = 0.37040726\n",
            "Iteration 500, loss = 0.37041529\n",
            "Iteration 501, loss = 0.37031043\n",
            "Iteration 502, loss = 0.37019015\n",
            "Iteration 503, loss = 0.37014103\n",
            "Iteration 504, loss = 0.37001376\n",
            "Iteration 505, loss = 0.36999770\n",
            "Iteration 506, loss = 0.36994676\n",
            "Iteration 507, loss = 0.36983870\n",
            "Iteration 508, loss = 0.36970189\n",
            "Iteration 509, loss = 0.36972203\n",
            "Iteration 510, loss = 0.36958709\n",
            "Iteration 511, loss = 0.36951008\n",
            "Iteration 512, loss = 0.36951636\n",
            "Iteration 513, loss = 0.36937172\n",
            "Iteration 514, loss = 0.36939810\n",
            "Iteration 515, loss = 0.36939234\n",
            "Iteration 516, loss = 0.36931674\n",
            "Iteration 517, loss = 0.36931214\n",
            "Iteration 518, loss = 0.36939100\n",
            "Iteration 519, loss = 0.36897404\n",
            "Iteration 520, loss = 0.36906321\n",
            "Iteration 521, loss = 0.36902344\n",
            "Iteration 522, loss = 0.36884674\n",
            "Iteration 523, loss = 0.36894580\n",
            "Iteration 524, loss = 0.36867269\n",
            "Iteration 525, loss = 0.36858199\n",
            "Iteration 526, loss = 0.36870914\n",
            "Iteration 527, loss = 0.36876982\n",
            "Iteration 528, loss = 0.36863336\n",
            "Iteration 529, loss = 0.36848471\n",
            "Iteration 530, loss = 0.36840318\n",
            "Iteration 531, loss = 0.36828428\n",
            "Iteration 532, loss = 0.36822005\n",
            "Iteration 533, loss = 0.36817219\n",
            "Iteration 534, loss = 0.36819904\n",
            "Iteration 535, loss = 0.36796262\n",
            "Iteration 536, loss = 0.36799121\n",
            "Iteration 537, loss = 0.36784571\n",
            "Iteration 538, loss = 0.36781012\n",
            "Iteration 539, loss = 0.36774009\n",
            "Iteration 540, loss = 0.36771382\n",
            "Iteration 541, loss = 0.36772962\n",
            "Iteration 542, loss = 0.36767587\n",
            "Iteration 543, loss = 0.36756978\n",
            "Iteration 544, loss = 0.36750257\n",
            "Iteration 545, loss = 0.36768012\n",
            "Iteration 546, loss = 0.36757284\n",
            "Iteration 547, loss = 0.36766436\n",
            "Iteration 548, loss = 0.36739771\n",
            "Iteration 549, loss = 0.36753135\n",
            "Iteration 550, loss = 0.36749581\n",
            "Iteration 551, loss = 0.36726513\n",
            "Iteration 552, loss = 0.36712555\n",
            "Iteration 553, loss = 0.36710569\n",
            "Iteration 554, loss = 0.36706199\n",
            "Iteration 555, loss = 0.36699289\n",
            "Iteration 556, loss = 0.36682495\n",
            "Iteration 557, loss = 0.36671044\n",
            "Iteration 558, loss = 0.36668300\n",
            "Iteration 559, loss = 0.36712180\n",
            "Iteration 560, loss = 0.36676473\n",
            "Iteration 561, loss = 0.36658703\n",
            "Iteration 562, loss = 0.36653885\n",
            "Iteration 563, loss = 0.36654031\n",
            "Iteration 564, loss = 0.36640658\n",
            "Iteration 565, loss = 0.36634839\n",
            "Iteration 566, loss = 0.36621570\n",
            "Iteration 567, loss = 0.36617786\n",
            "Iteration 568, loss = 0.36633648\n",
            "Iteration 569, loss = 0.36628240\n",
            "Iteration 570, loss = 0.36640716\n",
            "Iteration 571, loss = 0.36595134\n",
            "Iteration 572, loss = 0.36610228\n",
            "Iteration 573, loss = 0.36595343\n",
            "Iteration 574, loss = 0.36594060\n",
            "Iteration 575, loss = 0.36597193\n",
            "Iteration 576, loss = 0.36592391\n",
            "Iteration 577, loss = 0.36570802\n",
            "Iteration 578, loss = 0.36554885\n",
            "Iteration 579, loss = 0.36552493\n",
            "Iteration 580, loss = 0.36551231\n",
            "Iteration 581, loss = 0.36561361\n",
            "Iteration 582, loss = 0.36540281\n",
            "Iteration 583, loss = 0.36533920\n",
            "Iteration 584, loss = 0.36530353\n",
            "Iteration 585, loss = 0.36537004\n",
            "Iteration 586, loss = 0.36529169\n",
            "Iteration 587, loss = 0.36528744\n",
            "Iteration 588, loss = 0.36513632\n",
            "Iteration 589, loss = 0.36502738\n",
            "Iteration 590, loss = 0.36496378\n",
            "Iteration 591, loss = 0.36502500\n",
            "Iteration 592, loss = 0.36488410\n",
            "Iteration 593, loss = 0.36502050\n",
            "Iteration 594, loss = 0.36495010\n",
            "Iteration 595, loss = 0.36475708\n",
            "Iteration 596, loss = 0.36478011\n",
            "Iteration 597, loss = 0.36463433\n",
            "Iteration 598, loss = 0.36454274\n",
            "Iteration 599, loss = 0.36445113\n",
            "Iteration 600, loss = 0.36478807\n",
            "Iteration 601, loss = 0.36484532\n",
            "Iteration 602, loss = 0.36466723\n",
            "Iteration 603, loss = 0.36438240\n",
            "Iteration 604, loss = 0.36446160\n",
            "Iteration 605, loss = 0.36436655\n",
            "Iteration 606, loss = 0.36452188\n",
            "Iteration 607, loss = 0.36430842\n",
            "Iteration 608, loss = 0.36426485\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.75953291\n",
            "Iteration 2, loss = 0.75417243\n",
            "Iteration 3, loss = 0.74943076\n",
            "Iteration 4, loss = 0.74438786\n",
            "Iteration 5, loss = 0.74016053\n",
            "Iteration 6, loss = 0.73586091\n",
            "Iteration 7, loss = 0.73200649\n",
            "Iteration 8, loss = 0.72814469\n",
            "Iteration 9, loss = 0.72464777\n",
            "Iteration 10, loss = 0.72101594\n",
            "Iteration 11, loss = 0.71809953\n",
            "Iteration 12, loss = 0.71486209\n",
            "Iteration 13, loss = 0.71209564\n",
            "Iteration 14, loss = 0.70942513\n",
            "Iteration 15, loss = 0.70682926\n",
            "Iteration 16, loss = 0.70411184\n",
            "Iteration 17, loss = 0.70189777\n",
            "Iteration 18, loss = 0.69976570\n",
            "Iteration 19, loss = 0.69746951\n",
            "Iteration 20, loss = 0.69555623\n",
            "Iteration 21, loss = 0.69356050\n",
            "Iteration 22, loss = 0.69165340\n",
            "Iteration 23, loss = 0.68988708\n",
            "Iteration 24, loss = 0.68814454\n",
            "Iteration 25, loss = 0.68647856\n",
            "Iteration 26, loss = 0.68474028\n",
            "Iteration 27, loss = 0.68302684\n",
            "Iteration 28, loss = 0.68138147\n",
            "Iteration 29, loss = 0.67980522\n",
            "Iteration 30, loss = 0.67820022\n",
            "Iteration 31, loss = 0.67657792\n",
            "Iteration 32, loss = 0.67501853\n",
            "Iteration 33, loss = 0.67330621\n",
            "Iteration 34, loss = 0.67179857\n",
            "Iteration 35, loss = 0.67012619\n",
            "Iteration 36, loss = 0.66841623\n",
            "Iteration 37, loss = 0.66685178\n",
            "Iteration 38, loss = 0.66524259\n",
            "Iteration 39, loss = 0.66359989\n",
            "Iteration 40, loss = 0.66178579\n",
            "Iteration 41, loss = 0.66016219\n",
            "Iteration 42, loss = 0.65848494\n",
            "Iteration 43, loss = 0.65679940\n",
            "Iteration 44, loss = 0.65496874\n",
            "Iteration 45, loss = 0.65331496\n",
            "Iteration 46, loss = 0.65150942\n",
            "Iteration 47, loss = 0.64990839\n",
            "Iteration 48, loss = 0.64804652\n",
            "Iteration 49, loss = 0.64626367\n",
            "Iteration 50, loss = 0.64454359\n",
            "Iteration 51, loss = 0.64269139\n",
            "Iteration 52, loss = 0.64092792\n",
            "Iteration 53, loss = 0.63907712\n",
            "Iteration 54, loss = 0.63735698\n",
            "Iteration 55, loss = 0.63534407\n",
            "Iteration 56, loss = 0.63350633\n",
            "Iteration 57, loss = 0.63155043\n",
            "Iteration 58, loss = 0.62965365\n",
            "Iteration 59, loss = 0.62770569\n",
            "Iteration 60, loss = 0.62563249\n",
            "Iteration 61, loss = 0.62375692\n",
            "Iteration 62, loss = 0.62179546\n",
            "Iteration 63, loss = 0.61974841\n",
            "Iteration 64, loss = 0.61779221\n",
            "Iteration 65, loss = 0.61573300\n",
            "Iteration 66, loss = 0.61366582\n",
            "Iteration 67, loss = 0.61166447\n",
            "Iteration 68, loss = 0.60959893\n",
            "Iteration 69, loss = 0.60734534\n",
            "Iteration 70, loss = 0.60519150\n",
            "Iteration 71, loss = 0.60297470\n",
            "Iteration 72, loss = 0.60081175\n",
            "Iteration 73, loss = 0.59846819\n",
            "Iteration 74, loss = 0.59606229\n",
            "Iteration 75, loss = 0.59376623\n",
            "Iteration 76, loss = 0.59132884\n",
            "Iteration 77, loss = 0.58889967\n",
            "Iteration 78, loss = 0.58629634\n",
            "Iteration 79, loss = 0.58391074\n",
            "Iteration 80, loss = 0.58126922\n",
            "Iteration 81, loss = 0.57868529\n",
            "Iteration 82, loss = 0.57604350\n",
            "Iteration 83, loss = 0.57331558\n",
            "Iteration 84, loss = 0.57060817\n",
            "Iteration 85, loss = 0.56792992\n",
            "Iteration 86, loss = 0.56494215\n",
            "Iteration 87, loss = 0.56207035\n",
            "Iteration 88, loss = 0.55930479\n",
            "Iteration 89, loss = 0.55638948\n",
            "Iteration 90, loss = 0.55332213\n",
            "Iteration 91, loss = 0.55042962\n",
            "Iteration 92, loss = 0.54758966\n",
            "Iteration 93, loss = 0.54449339\n",
            "Iteration 94, loss = 0.54168314\n",
            "Iteration 95, loss = 0.53847032\n",
            "Iteration 96, loss = 0.53535034\n",
            "Iteration 97, loss = 0.53216104\n",
            "Iteration 98, loss = 0.52901696\n",
            "Iteration 99, loss = 0.52596746\n",
            "Iteration 100, loss = 0.52292258\n",
            "Iteration 101, loss = 0.51997077\n",
            "Iteration 102, loss = 0.51675818\n",
            "Iteration 103, loss = 0.51367653\n",
            "Iteration 104, loss = 0.51079430\n",
            "Iteration 105, loss = 0.50772990\n",
            "Iteration 106, loss = 0.50482858\n",
            "Iteration 107, loss = 0.50199880\n",
            "Iteration 108, loss = 0.49891078\n",
            "Iteration 109, loss = 0.49602651\n",
            "Iteration 110, loss = 0.49315469\n",
            "Iteration 111, loss = 0.49029353\n",
            "Iteration 112, loss = 0.48740123\n",
            "Iteration 113, loss = 0.48467733\n",
            "Iteration 114, loss = 0.48183321\n",
            "Iteration 115, loss = 0.47910683\n",
            "Iteration 116, loss = 0.47638676\n",
            "Iteration 117, loss = 0.47381542\n",
            "Iteration 118, loss = 0.47134203\n",
            "Iteration 119, loss = 0.46882755\n",
            "Iteration 120, loss = 0.46625181\n",
            "Iteration 121, loss = 0.46392824\n",
            "Iteration 122, loss = 0.46157005\n",
            "Iteration 123, loss = 0.45937010\n",
            "Iteration 124, loss = 0.45714303\n",
            "Iteration 125, loss = 0.45514339\n",
            "Iteration 126, loss = 0.45292922\n",
            "Iteration 127, loss = 0.45103981\n",
            "Iteration 128, loss = 0.44904779\n",
            "Iteration 129, loss = 0.44727846\n",
            "Iteration 130, loss = 0.44549954\n",
            "Iteration 131, loss = 0.44384559\n",
            "Iteration 132, loss = 0.44223444\n",
            "Iteration 133, loss = 0.44081938\n",
            "Iteration 134, loss = 0.43940922\n",
            "Iteration 135, loss = 0.43799812\n",
            "Iteration 136, loss = 0.43676226\n",
            "Iteration 137, loss = 0.43556079\n",
            "Iteration 138, loss = 0.43438284\n",
            "Iteration 139, loss = 0.43332021\n",
            "Iteration 140, loss = 0.43216640\n",
            "Iteration 141, loss = 0.43108853\n",
            "Iteration 142, loss = 0.43019369\n",
            "Iteration 143, loss = 0.42923099\n",
            "Iteration 144, loss = 0.42842318\n",
            "Iteration 145, loss = 0.42741465\n",
            "Iteration 146, loss = 0.42672318\n",
            "Iteration 147, loss = 0.42595310\n",
            "Iteration 148, loss = 0.42527555\n",
            "Iteration 149, loss = 0.42450932\n",
            "Iteration 150, loss = 0.42385039\n",
            "Iteration 151, loss = 0.42324106\n",
            "Iteration 152, loss = 0.42269043\n",
            "Iteration 153, loss = 0.42217051\n",
            "Iteration 154, loss = 0.42156908\n",
            "Iteration 155, loss = 0.42101535\n",
            "Iteration 156, loss = 0.42068912\n",
            "Iteration 157, loss = 0.41999292\n",
            "Iteration 158, loss = 0.41950457\n",
            "Iteration 159, loss = 0.41904550\n",
            "Iteration 160, loss = 0.41848104\n",
            "Iteration 161, loss = 0.41795463\n",
            "Iteration 162, loss = 0.41758128\n",
            "Iteration 163, loss = 0.41712043\n",
            "Iteration 164, loss = 0.41662465\n",
            "Iteration 165, loss = 0.41636040\n",
            "Iteration 166, loss = 0.41582435\n",
            "Iteration 167, loss = 0.41537624\n",
            "Iteration 168, loss = 0.41494313\n",
            "Iteration 169, loss = 0.41452332\n",
            "Iteration 170, loss = 0.41408164\n",
            "Iteration 171, loss = 0.41384813\n",
            "Iteration 172, loss = 0.41350207\n",
            "Iteration 173, loss = 0.41324160\n",
            "Iteration 174, loss = 0.41273461\n",
            "Iteration 175, loss = 0.41245125\n",
            "Iteration 176, loss = 0.41201032\n",
            "Iteration 177, loss = 0.41152369\n",
            "Iteration 178, loss = 0.41111762\n",
            "Iteration 179, loss = 0.41094035\n",
            "Iteration 180, loss = 0.41029797\n",
            "Iteration 181, loss = 0.40985691\n",
            "Iteration 182, loss = 0.40971264\n",
            "Iteration 183, loss = 0.40961406\n",
            "Iteration 184, loss = 0.40917781\n",
            "Iteration 185, loss = 0.40874020\n",
            "Iteration 186, loss = 0.40822116\n",
            "Iteration 187, loss = 0.40778240\n",
            "Iteration 188, loss = 0.40739306\n",
            "Iteration 189, loss = 0.40716799\n",
            "Iteration 190, loss = 0.40697972\n",
            "Iteration 191, loss = 0.40670621\n",
            "Iteration 192, loss = 0.40626327\n",
            "Iteration 193, loss = 0.40583132\n",
            "Iteration 194, loss = 0.40558348\n",
            "Iteration 195, loss = 0.40521953\n",
            "Iteration 196, loss = 0.40492499\n",
            "Iteration 197, loss = 0.40452021\n",
            "Iteration 198, loss = 0.40420190\n",
            "Iteration 199, loss = 0.40400094\n",
            "Iteration 200, loss = 0.40356968\n",
            "Iteration 201, loss = 0.40329586\n",
            "Iteration 202, loss = 0.40285198\n",
            "Iteration 203, loss = 0.40258185\n",
            "Iteration 204, loss = 0.40247229\n",
            "Iteration 205, loss = 0.40194617\n",
            "Iteration 206, loss = 0.40167567\n",
            "Iteration 207, loss = 0.40124738\n",
            "Iteration 208, loss = 0.40100661\n",
            "Iteration 209, loss = 0.40064741\n",
            "Iteration 210, loss = 0.40021922\n",
            "Iteration 211, loss = 0.40010523\n",
            "Iteration 212, loss = 0.39994750\n",
            "Iteration 213, loss = 0.39950292\n",
            "Iteration 214, loss = 0.39918383\n",
            "Iteration 215, loss = 0.39878626\n",
            "Iteration 216, loss = 0.39849791\n",
            "Iteration 217, loss = 0.39860038\n",
            "Iteration 218, loss = 0.39783586\n",
            "Iteration 219, loss = 0.39768258\n",
            "Iteration 220, loss = 0.39745163\n",
            "Iteration 221, loss = 0.39707784\n",
            "Iteration 222, loss = 0.39675627\n",
            "Iteration 223, loss = 0.39646123\n",
            "Iteration 224, loss = 0.39612906\n",
            "Iteration 225, loss = 0.39581847\n",
            "Iteration 226, loss = 0.39553300\n",
            "Iteration 227, loss = 0.39521645\n",
            "Iteration 228, loss = 0.39499280\n",
            "Iteration 229, loss = 0.39480787\n",
            "Iteration 230, loss = 0.39448981\n",
            "Iteration 231, loss = 0.39413247\n",
            "Iteration 232, loss = 0.39390303\n",
            "Iteration 233, loss = 0.39369458\n",
            "Iteration 234, loss = 0.39349583\n",
            "Iteration 235, loss = 0.39330617\n",
            "Iteration 236, loss = 0.39294080\n",
            "Iteration 237, loss = 0.39257515\n",
            "Iteration 238, loss = 0.39230186\n",
            "Iteration 239, loss = 0.39213089\n",
            "Iteration 240, loss = 0.39186180\n",
            "Iteration 241, loss = 0.39164423\n",
            "Iteration 242, loss = 0.39122385\n",
            "Iteration 243, loss = 0.39092176\n",
            "Iteration 244, loss = 0.39067947\n",
            "Iteration 245, loss = 0.39077518\n",
            "Iteration 246, loss = 0.39058496\n",
            "Iteration 247, loss = 0.39028256\n",
            "Iteration 248, loss = 0.38997234\n",
            "Iteration 249, loss = 0.38941131\n",
            "Iteration 250, loss = 0.38917304\n",
            "Iteration 251, loss = 0.38891258\n",
            "Iteration 252, loss = 0.38908611\n",
            "Iteration 253, loss = 0.38895693\n",
            "Iteration 254, loss = 0.38846152\n",
            "Iteration 255, loss = 0.38814914\n",
            "Iteration 256, loss = 0.38780269\n",
            "Iteration 257, loss = 0.38752696\n",
            "Iteration 258, loss = 0.38726453\n",
            "Iteration 259, loss = 0.38703646\n",
            "Iteration 260, loss = 0.38700677\n",
            "Iteration 261, loss = 0.38677767\n",
            "Iteration 262, loss = 0.38655072\n",
            "Iteration 263, loss = 0.38657529\n",
            "Iteration 264, loss = 0.38599947\n",
            "Iteration 265, loss = 0.38568597\n",
            "Iteration 266, loss = 0.38570661\n",
            "Iteration 267, loss = 0.38556504\n",
            "Iteration 268, loss = 0.38547758\n",
            "Iteration 269, loss = 0.38513857\n",
            "Iteration 270, loss = 0.38487768\n",
            "Iteration 271, loss = 0.38463949\n",
            "Iteration 272, loss = 0.38443423\n",
            "Iteration 273, loss = 0.38415558\n",
            "Iteration 274, loss = 0.38395026\n",
            "Iteration 275, loss = 0.38377284\n",
            "Iteration 276, loss = 0.38361975\n",
            "Iteration 277, loss = 0.38340725\n",
            "Iteration 278, loss = 0.38331687\n",
            "Iteration 279, loss = 0.38305558\n",
            "Iteration 280, loss = 0.38282678\n",
            "Iteration 281, loss = 0.38261627\n",
            "Iteration 282, loss = 0.38241357\n",
            "Iteration 283, loss = 0.38218831\n",
            "Iteration 284, loss = 0.38196334\n",
            "Iteration 285, loss = 0.38191921\n",
            "Iteration 286, loss = 0.38159313\n",
            "Iteration 287, loss = 0.38139507\n",
            "Iteration 288, loss = 0.38177021\n",
            "Iteration 289, loss = 0.38098227\n",
            "Iteration 290, loss = 0.38082026\n",
            "Iteration 291, loss = 0.38070877\n",
            "Iteration 292, loss = 0.38059807\n",
            "Iteration 293, loss = 0.38045420\n",
            "Iteration 294, loss = 0.38019866\n",
            "Iteration 295, loss = 0.37991728\n",
            "Iteration 296, loss = 0.37982141\n",
            "Iteration 297, loss = 0.37977317\n",
            "Iteration 298, loss = 0.37968932\n",
            "Iteration 299, loss = 0.37971763\n",
            "Iteration 300, loss = 0.37929478\n",
            "Iteration 301, loss = 0.37912545\n",
            "Iteration 302, loss = 0.37876753\n",
            "Iteration 303, loss = 0.37871072\n",
            "Iteration 304, loss = 0.37851785\n",
            "Iteration 305, loss = 0.37832908\n",
            "Iteration 306, loss = 0.37811602\n",
            "Iteration 307, loss = 0.37801553\n",
            "Iteration 308, loss = 0.37788353\n",
            "Iteration 309, loss = 0.37774484\n",
            "Iteration 310, loss = 0.37769313\n",
            "Iteration 311, loss = 0.37735631\n",
            "Iteration 312, loss = 0.37718152\n",
            "Iteration 313, loss = 0.37706564\n",
            "Iteration 314, loss = 0.37692435\n",
            "Iteration 315, loss = 0.37675019\n",
            "Iteration 316, loss = 0.37667285\n",
            "Iteration 317, loss = 0.37643808\n",
            "Iteration 318, loss = 0.37617282\n",
            "Iteration 319, loss = 0.37602008\n",
            "Iteration 320, loss = 0.37590516\n",
            "Iteration 321, loss = 0.37600407\n",
            "Iteration 322, loss = 0.37572312\n",
            "Iteration 323, loss = 0.37551142\n",
            "Iteration 324, loss = 0.37556575\n",
            "Iteration 325, loss = 0.37522540\n",
            "Iteration 326, loss = 0.37506695\n",
            "Iteration 327, loss = 0.37499276\n",
            "Iteration 328, loss = 0.37484807\n",
            "Iteration 329, loss = 0.37482262\n",
            "Iteration 330, loss = 0.37483865\n",
            "Iteration 331, loss = 0.37447649\n",
            "Iteration 332, loss = 0.37417400\n",
            "Iteration 333, loss = 0.37409589\n",
            "Iteration 334, loss = 0.37401938\n",
            "Iteration 335, loss = 0.37381385\n",
            "Iteration 336, loss = 0.37367840\n",
            "Iteration 337, loss = 0.37352430\n",
            "Iteration 338, loss = 0.37345076\n",
            "Iteration 339, loss = 0.37317879\n",
            "Iteration 340, loss = 0.37317415\n",
            "Iteration 341, loss = 0.37296310\n",
            "Iteration 342, loss = 0.37296764\n",
            "Iteration 343, loss = 0.37274325\n",
            "Iteration 344, loss = 0.37255350\n",
            "Iteration 345, loss = 0.37253770\n",
            "Iteration 346, loss = 0.37241212\n",
            "Iteration 347, loss = 0.37224116\n",
            "Iteration 348, loss = 0.37202207\n",
            "Iteration 349, loss = 0.37188626\n",
            "Iteration 350, loss = 0.37184051\n",
            "Iteration 351, loss = 0.37167332\n",
            "Iteration 352, loss = 0.37170476\n",
            "Iteration 353, loss = 0.37137824\n",
            "Iteration 354, loss = 0.37125062\n",
            "Iteration 355, loss = 0.37108232\n",
            "Iteration 356, loss = 0.37100761\n",
            "Iteration 357, loss = 0.37096391\n",
            "Iteration 358, loss = 0.37092354\n",
            "Iteration 359, loss = 0.37090943\n",
            "Iteration 360, loss = 0.37074549\n",
            "Iteration 361, loss = 0.37043976\n",
            "Iteration 362, loss = 0.37032903\n",
            "Iteration 363, loss = 0.37022936\n",
            "Iteration 364, loss = 0.37002931\n",
            "Iteration 365, loss = 0.36994920\n",
            "Iteration 366, loss = 0.36978605\n",
            "Iteration 367, loss = 0.36984538\n",
            "Iteration 368, loss = 0.36960697\n",
            "Iteration 369, loss = 0.36953502\n",
            "Iteration 370, loss = 0.36943827\n",
            "Iteration 371, loss = 0.36917812\n",
            "Iteration 372, loss = 0.36906890\n",
            "Iteration 373, loss = 0.36897798\n",
            "Iteration 374, loss = 0.36880955\n",
            "Iteration 375, loss = 0.36883143\n",
            "Iteration 376, loss = 0.36873722\n",
            "Iteration 377, loss = 0.36865522\n",
            "Iteration 378, loss = 0.36865272\n",
            "Iteration 379, loss = 0.36842916\n",
            "Iteration 380, loss = 0.36818694\n",
            "Iteration 381, loss = 0.36816104\n",
            "Iteration 382, loss = 0.36797506\n",
            "Iteration 383, loss = 0.36791614\n",
            "Iteration 384, loss = 0.36784112\n",
            "Iteration 385, loss = 0.36767433\n",
            "Iteration 386, loss = 0.36759251\n",
            "Iteration 387, loss = 0.36745371\n",
            "Iteration 388, loss = 0.36727213\n",
            "Iteration 389, loss = 0.36726084\n",
            "Iteration 390, loss = 0.36716547\n",
            "Iteration 391, loss = 0.36705405\n",
            "Iteration 392, loss = 0.36698374\n",
            "Iteration 393, loss = 0.36689664\n",
            "Iteration 394, loss = 0.36693103\n",
            "Iteration 395, loss = 0.36687081\n",
            "Iteration 396, loss = 0.36674452\n",
            "Iteration 397, loss = 0.36656239\n",
            "Iteration 398, loss = 0.36653141\n",
            "Iteration 399, loss = 0.36640156\n",
            "Iteration 400, loss = 0.36629308\n",
            "Iteration 401, loss = 0.36625416\n",
            "Iteration 402, loss = 0.36618139\n",
            "Iteration 403, loss = 0.36600367\n",
            "Iteration 404, loss = 0.36582217\n",
            "Iteration 405, loss = 0.36576028\n",
            "Iteration 406, loss = 0.36565974\n",
            "Iteration 407, loss = 0.36615621\n",
            "Iteration 408, loss = 0.36568205\n",
            "Iteration 409, loss = 0.36548908\n",
            "Iteration 410, loss = 0.36534869\n",
            "Iteration 411, loss = 0.36542176\n",
            "Iteration 412, loss = 0.36554345\n",
            "Iteration 413, loss = 0.36517608\n",
            "Iteration 414, loss = 0.36498509\n",
            "Iteration 415, loss = 0.36484635\n",
            "Iteration 416, loss = 0.36490056\n",
            "Iteration 417, loss = 0.36479266\n",
            "Iteration 418, loss = 0.36463173\n",
            "Iteration 419, loss = 0.36460636\n",
            "Iteration 420, loss = 0.36447628\n",
            "Iteration 421, loss = 0.36441829\n",
            "Iteration 422, loss = 0.36431625\n",
            "Iteration 423, loss = 0.36417639\n",
            "Iteration 424, loss = 0.36418850\n",
            "Iteration 425, loss = 0.36422667\n",
            "Iteration 426, loss = 0.36387107\n",
            "Iteration 427, loss = 0.36391016\n",
            "Iteration 428, loss = 0.36385638\n",
            "Iteration 429, loss = 0.36379837\n",
            "Iteration 430, loss = 0.36370570\n",
            "Iteration 431, loss = 0.36369151\n",
            "Iteration 432, loss = 0.36359057\n",
            "Iteration 433, loss = 0.36339373\n",
            "Iteration 434, loss = 0.36338185\n",
            "Iteration 435, loss = 0.36324831\n",
            "Iteration 436, loss = 0.36325728\n",
            "Iteration 437, loss = 0.36316297\n",
            "Iteration 438, loss = 0.36304543\n",
            "Iteration 439, loss = 0.36298951\n",
            "Iteration 440, loss = 0.36281508\n",
            "Iteration 441, loss = 0.36270824\n",
            "Iteration 442, loss = 0.36272876\n",
            "Iteration 443, loss = 0.36252435\n",
            "Iteration 444, loss = 0.36251220\n",
            "Iteration 445, loss = 0.36242847\n",
            "Iteration 446, loss = 0.36232863\n",
            "Iteration 447, loss = 0.36234650\n",
            "Iteration 448, loss = 0.36228303\n",
            "Iteration 449, loss = 0.36225837\n",
            "Iteration 450, loss = 0.36226299\n",
            "Iteration 451, loss = 0.36219773\n",
            "Iteration 452, loss = 0.36204212\n",
            "Iteration 453, loss = 0.36203168\n",
            "Iteration 454, loss = 0.36188391\n",
            "Iteration 455, loss = 0.36172085\n",
            "Iteration 456, loss = 0.36171757\n",
            "Iteration 457, loss = 0.36149350\n",
            "Iteration 458, loss = 0.36143884\n",
            "Iteration 459, loss = 0.36148679\n",
            "Iteration 460, loss = 0.36138740\n",
            "Iteration 461, loss = 0.36143932\n",
            "Iteration 462, loss = 0.36127773\n",
            "Iteration 463, loss = 0.36155175\n",
            "Iteration 464, loss = 0.36099484\n",
            "Iteration 465, loss = 0.36102578\n",
            "Iteration 466, loss = 0.36101354\n",
            "Iteration 467, loss = 0.36090075\n",
            "Iteration 468, loss = 0.36084935\n",
            "Iteration 469, loss = 0.36072028\n",
            "Iteration 470, loss = 0.36061143\n",
            "Iteration 471, loss = 0.36057453\n",
            "Iteration 472, loss = 0.36079777\n",
            "Iteration 473, loss = 0.36059097\n",
            "Iteration 474, loss = 0.36055175\n",
            "Iteration 475, loss = 0.36036173\n",
            "Iteration 476, loss = 0.36028758\n",
            "Iteration 477, loss = 0.36025219\n",
            "Iteration 478, loss = 0.36024896\n",
            "Iteration 479, loss = 0.36002751\n",
            "Iteration 480, loss = 0.36002590\n",
            "Iteration 481, loss = 0.35999372\n",
            "Iteration 482, loss = 0.35992014\n",
            "Iteration 483, loss = 0.35986938\n",
            "Iteration 484, loss = 0.35981122\n",
            "Iteration 485, loss = 0.35976781\n",
            "Iteration 486, loss = 0.35971851\n",
            "Iteration 487, loss = 0.35962370\n",
            "Iteration 488, loss = 0.35949768\n",
            "Iteration 489, loss = 0.35950272\n",
            "Iteration 490, loss = 0.35947128\n",
            "Iteration 491, loss = 0.35950886\n",
            "Iteration 492, loss = 0.35928530\n",
            "Iteration 493, loss = 0.35944061\n",
            "Iteration 494, loss = 0.35925113\n",
            "Iteration 495, loss = 0.35912877\n",
            "Iteration 496, loss = 0.35901879\n",
            "Iteration 497, loss = 0.35911829\n",
            "Iteration 498, loss = 0.35908129\n",
            "Iteration 499, loss = 0.35884042\n",
            "Iteration 500, loss = 0.35924387\n",
            "Iteration 501, loss = 0.35886470\n",
            "Iteration 502, loss = 0.35888612\n",
            "Iteration 503, loss = 0.35883778\n",
            "Iteration 504, loss = 0.35859664\n",
            "Iteration 505, loss = 0.35851058\n",
            "Iteration 506, loss = 0.35871688\n",
            "Iteration 507, loss = 0.35861454\n",
            "Iteration 508, loss = 0.35837120\n",
            "Iteration 509, loss = 0.35829980\n",
            "Iteration 510, loss = 0.35835788\n",
            "Iteration 511, loss = 0.35820877\n",
            "Iteration 512, loss = 0.35818152\n",
            "Iteration 513, loss = 0.35806764\n",
            "Iteration 514, loss = 0.35803209\n",
            "Iteration 515, loss = 0.35790476\n",
            "Iteration 516, loss = 0.35785626\n",
            "Iteration 517, loss = 0.35797739\n",
            "Iteration 518, loss = 0.35803622\n",
            "Iteration 519, loss = 0.35785410\n",
            "Iteration 520, loss = 0.35779263\n",
            "Iteration 521, loss = 0.35783913\n",
            "Iteration 522, loss = 0.35756830\n",
            "Iteration 523, loss = 0.35754629\n",
            "Iteration 524, loss = 0.35751971\n",
            "Iteration 525, loss = 0.35741213\n",
            "Iteration 526, loss = 0.35734365\n",
            "Iteration 527, loss = 0.35726459\n",
            "Iteration 528, loss = 0.35719783\n",
            "Iteration 529, loss = 0.35721544\n",
            "Iteration 530, loss = 0.35725368\n",
            "Iteration 531, loss = 0.35717234\n",
            "Iteration 532, loss = 0.35699554\n",
            "Iteration 533, loss = 0.35719398\n",
            "Iteration 534, loss = 0.35716806\n",
            "Iteration 535, loss = 0.35697025\n",
            "Iteration 536, loss = 0.35695325\n",
            "Iteration 537, loss = 0.35685418\n",
            "Iteration 538, loss = 0.35678222\n",
            "Iteration 539, loss = 0.35674961\n",
            "Iteration 540, loss = 0.35668559\n",
            "Iteration 541, loss = 0.35663668\n",
            "Iteration 542, loss = 0.35654575\n",
            "Iteration 543, loss = 0.35643394\n",
            "Iteration 544, loss = 0.35641249\n",
            "Iteration 545, loss = 0.35638360\n",
            "Iteration 546, loss = 0.35639513\n",
            "Iteration 547, loss = 0.35637039\n",
            "Iteration 548, loss = 0.35629022\n",
            "Iteration 549, loss = 0.35621850\n",
            "Iteration 550, loss = 0.35652066\n",
            "Iteration 551, loss = 0.35618051\n",
            "Iteration 552, loss = 0.35612157\n",
            "Iteration 553, loss = 0.35599494\n",
            "Iteration 554, loss = 0.35588789\n",
            "Iteration 555, loss = 0.35602672\n",
            "Iteration 556, loss = 0.35593101\n",
            "Iteration 557, loss = 0.35595582\n",
            "Iteration 558, loss = 0.35571686\n",
            "Iteration 559, loss = 0.35592597\n",
            "Iteration 560, loss = 0.35587294\n",
            "Iteration 561, loss = 0.35560458\n",
            "Iteration 562, loss = 0.35556000\n",
            "Iteration 563, loss = 0.35555389\n",
            "Iteration 564, loss = 0.35551317\n",
            "Iteration 565, loss = 0.35535634\n",
            "Iteration 566, loss = 0.35535486\n",
            "Iteration 567, loss = 0.35533685\n",
            "Iteration 568, loss = 0.35533593\n",
            "Iteration 569, loss = 0.35539648\n",
            "Iteration 570, loss = 0.35522078\n",
            "Iteration 571, loss = 0.35517441\n",
            "Iteration 572, loss = 0.35506482\n",
            "Iteration 573, loss = 0.35505762\n",
            "Iteration 574, loss = 0.35496925\n",
            "Iteration 575, loss = 0.35490703\n",
            "Iteration 576, loss = 0.35488080\n",
            "Iteration 577, loss = 0.35484707\n",
            "Iteration 578, loss = 0.35477356\n",
            "Iteration 579, loss = 0.35465820\n",
            "Iteration 580, loss = 0.35456560\n",
            "Iteration 581, loss = 0.35472922\n",
            "Iteration 582, loss = 0.35450280\n",
            "Iteration 583, loss = 0.35456238\n",
            "Iteration 584, loss = 0.35451257\n",
            "Iteration 585, loss = 0.35467341\n",
            "Iteration 586, loss = 0.35450850\n",
            "Iteration 587, loss = 0.35434296\n",
            "Iteration 588, loss = 0.35432138\n",
            "Iteration 589, loss = 0.35425997\n",
            "Iteration 590, loss = 0.35417607\n",
            "Iteration 591, loss = 0.35424880\n",
            "Iteration 592, loss = 0.35440541\n",
            "Iteration 593, loss = 0.35435908\n",
            "Iteration 594, loss = 0.35418071\n",
            "Iteration 595, loss = 0.35395270\n",
            "Iteration 596, loss = 0.35381883\n",
            "Iteration 597, loss = 0.35376554\n",
            "Iteration 598, loss = 0.35381671\n",
            "Iteration 599, loss = 0.35374883\n",
            "Iteration 600, loss = 0.35411547\n",
            "Iteration 601, loss = 0.35399544\n",
            "Iteration 602, loss = 0.35372490\n",
            "Iteration 603, loss = 0.35367262\n",
            "Iteration 604, loss = 0.35358497\n",
            "Iteration 605, loss = 0.35344156\n",
            "Iteration 606, loss = 0.35362965\n",
            "Iteration 607, loss = 0.35346972\n",
            "Iteration 608, loss = 0.35348015\n",
            "Iteration 609, loss = 0.35358706\n",
            "Iteration 610, loss = 0.35353907\n",
            "Iteration 611, loss = 0.35347196\n",
            "Iteration 612, loss = 0.35318601\n",
            "Iteration 613, loss = 0.35343896\n",
            "Iteration 614, loss = 0.35321710\n",
            "Iteration 615, loss = 0.35315057\n",
            "Iteration 616, loss = 0.35298283\n",
            "Iteration 617, loss = 0.35298647\n",
            "Iteration 618, loss = 0.35301506\n",
            "Iteration 619, loss = 0.35281403\n",
            "Iteration 620, loss = 0.35278454\n",
            "Iteration 621, loss = 0.35281651\n",
            "Iteration 622, loss = 0.35284523\n",
            "Iteration 623, loss = 0.35266212\n",
            "Iteration 624, loss = 0.35263804\n",
            "Iteration 625, loss = 0.35264178\n",
            "Iteration 626, loss = 0.35259599\n",
            "Iteration 627, loss = 0.35251454\n",
            "Iteration 628, loss = 0.35245041\n",
            "Iteration 629, loss = 0.35242870\n",
            "Iteration 630, loss = 0.35232428\n",
            "Iteration 631, loss = 0.35232790\n",
            "Iteration 632, loss = 0.35232145\n",
            "Iteration 633, loss = 0.35219928\n",
            "Iteration 634, loss = 0.35250025\n",
            "Iteration 635, loss = 0.35222842\n",
            "Iteration 636, loss = 0.35209715\n",
            "Iteration 637, loss = 0.35202568\n",
            "Iteration 638, loss = 0.35205497\n",
            "Iteration 639, loss = 0.35217499\n",
            "Iteration 640, loss = 0.35215801\n",
            "Iteration 641, loss = 0.35186023\n",
            "Iteration 642, loss = 0.35197476\n",
            "Iteration 643, loss = 0.35189578\n",
            "Iteration 644, loss = 0.35181667\n",
            "Iteration 645, loss = 0.35174199\n",
            "Iteration 646, loss = 0.35174208\n",
            "Iteration 647, loss = 0.35168775\n",
            "Iteration 648, loss = 0.35170411\n",
            "Iteration 649, loss = 0.35160497\n",
            "Iteration 650, loss = 0.35152421\n",
            "Iteration 651, loss = 0.35158604\n",
            "Iteration 652, loss = 0.35143976\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.76058778\n",
            "Iteration 2, loss = 0.75507986\n",
            "Iteration 3, loss = 0.75025602\n",
            "Iteration 4, loss = 0.74522093\n",
            "Iteration 5, loss = 0.74068733\n",
            "Iteration 6, loss = 0.73663146\n",
            "Iteration 7, loss = 0.73248556\n",
            "Iteration 8, loss = 0.72871671\n",
            "Iteration 9, loss = 0.72514066\n",
            "Iteration 10, loss = 0.72144522\n",
            "Iteration 11, loss = 0.71845660\n",
            "Iteration 12, loss = 0.71538132\n",
            "Iteration 13, loss = 0.71235231\n",
            "Iteration 14, loss = 0.70962974\n",
            "Iteration 15, loss = 0.70700934\n",
            "Iteration 16, loss = 0.70434934\n",
            "Iteration 17, loss = 0.70199086\n",
            "Iteration 18, loss = 0.69984498\n",
            "Iteration 19, loss = 0.69764991\n",
            "Iteration 20, loss = 0.69565748\n",
            "Iteration 21, loss = 0.69369294\n",
            "Iteration 22, loss = 0.69171519\n",
            "Iteration 23, loss = 0.69005662\n",
            "Iteration 24, loss = 0.68823147\n",
            "Iteration 25, loss = 0.68658151\n",
            "Iteration 26, loss = 0.68493504\n",
            "Iteration 27, loss = 0.68317996\n",
            "Iteration 28, loss = 0.68161154\n",
            "Iteration 29, loss = 0.68004968\n",
            "Iteration 30, loss = 0.67851250\n",
            "Iteration 31, loss = 0.67689211\n",
            "Iteration 32, loss = 0.67531274\n",
            "Iteration 33, loss = 0.67373546\n",
            "Iteration 34, loss = 0.67226887\n",
            "Iteration 35, loss = 0.67066120\n",
            "Iteration 36, loss = 0.66903910\n",
            "Iteration 37, loss = 0.66754631\n",
            "Iteration 38, loss = 0.66595037\n",
            "Iteration 39, loss = 0.66446290\n",
            "Iteration 40, loss = 0.66269075\n",
            "Iteration 41, loss = 0.66117340\n",
            "Iteration 42, loss = 0.65949663\n",
            "Iteration 43, loss = 0.65787337\n",
            "Iteration 44, loss = 0.65616877\n",
            "Iteration 45, loss = 0.65456583\n",
            "Iteration 46, loss = 0.65286949\n",
            "Iteration 47, loss = 0.65129850\n",
            "Iteration 48, loss = 0.64952027\n",
            "Iteration 49, loss = 0.64779723\n",
            "Iteration 50, loss = 0.64613641\n",
            "Iteration 51, loss = 0.64442511\n",
            "Iteration 52, loss = 0.64266095\n",
            "Iteration 53, loss = 0.64086752\n",
            "Iteration 54, loss = 0.63922277\n",
            "Iteration 55, loss = 0.63730772\n",
            "Iteration 56, loss = 0.63552507\n",
            "Iteration 57, loss = 0.63358714\n",
            "Iteration 58, loss = 0.63178580\n",
            "Iteration 59, loss = 0.62988393\n",
            "Iteration 60, loss = 0.62785405\n",
            "Iteration 61, loss = 0.62595778\n",
            "Iteration 62, loss = 0.62402838\n",
            "Iteration 63, loss = 0.62208391\n",
            "Iteration 64, loss = 0.62016001\n",
            "Iteration 65, loss = 0.61818697\n",
            "Iteration 66, loss = 0.61624511\n",
            "Iteration 67, loss = 0.61421879\n",
            "Iteration 68, loss = 0.61223356\n",
            "Iteration 69, loss = 0.61019615\n",
            "Iteration 70, loss = 0.60812785\n",
            "Iteration 71, loss = 0.60605098\n",
            "Iteration 72, loss = 0.60397939\n",
            "Iteration 73, loss = 0.60185931\n",
            "Iteration 74, loss = 0.59957460\n",
            "Iteration 75, loss = 0.59740907\n",
            "Iteration 76, loss = 0.59523747\n",
            "Iteration 77, loss = 0.59296187\n",
            "Iteration 78, loss = 0.59062545\n",
            "Iteration 79, loss = 0.58846589\n",
            "Iteration 80, loss = 0.58605842\n",
            "Iteration 81, loss = 0.58371066\n",
            "Iteration 82, loss = 0.58136939\n",
            "Iteration 83, loss = 0.57882217\n",
            "Iteration 84, loss = 0.57645429\n",
            "Iteration 85, loss = 0.57394681\n",
            "Iteration 86, loss = 0.57118675\n",
            "Iteration 87, loss = 0.56859749\n",
            "Iteration 88, loss = 0.56607496\n",
            "Iteration 89, loss = 0.56329273\n",
            "Iteration 90, loss = 0.56061401\n",
            "Iteration 91, loss = 0.55791634\n",
            "Iteration 92, loss = 0.55524934\n",
            "Iteration 93, loss = 0.55244713\n",
            "Iteration 94, loss = 0.54980285\n",
            "Iteration 95, loss = 0.54681206\n",
            "Iteration 96, loss = 0.54397313\n",
            "Iteration 97, loss = 0.54105365\n",
            "Iteration 98, loss = 0.53800898\n",
            "Iteration 99, loss = 0.53525073\n",
            "Iteration 100, loss = 0.53236641\n",
            "Iteration 101, loss = 0.52959359\n",
            "Iteration 102, loss = 0.52667312\n",
            "Iteration 103, loss = 0.52388526\n",
            "Iteration 104, loss = 0.52112741\n",
            "Iteration 105, loss = 0.51829733\n",
            "Iteration 106, loss = 0.51555991\n",
            "Iteration 107, loss = 0.51293480\n",
            "Iteration 108, loss = 0.51016301\n",
            "Iteration 109, loss = 0.50754844\n",
            "Iteration 110, loss = 0.50489325\n",
            "Iteration 111, loss = 0.50234018\n",
            "Iteration 112, loss = 0.49971806\n",
            "Iteration 113, loss = 0.49741166\n",
            "Iteration 114, loss = 0.49484671\n",
            "Iteration 115, loss = 0.49254044\n",
            "Iteration 116, loss = 0.49012478\n",
            "Iteration 117, loss = 0.48788846\n",
            "Iteration 118, loss = 0.48577406\n",
            "Iteration 119, loss = 0.48376312\n",
            "Iteration 120, loss = 0.48156042\n",
            "Iteration 121, loss = 0.47943533\n",
            "Iteration 122, loss = 0.47740170\n",
            "Iteration 123, loss = 0.47545197\n",
            "Iteration 124, loss = 0.47355031\n",
            "Iteration 125, loss = 0.47195715\n",
            "Iteration 126, loss = 0.46998922\n",
            "Iteration 127, loss = 0.46824923\n",
            "Iteration 128, loss = 0.46650778\n",
            "Iteration 129, loss = 0.46483751\n",
            "Iteration 130, loss = 0.46330234\n",
            "Iteration 131, loss = 0.46183116\n",
            "Iteration 132, loss = 0.46031824\n",
            "Iteration 133, loss = 0.45898271\n",
            "Iteration 134, loss = 0.45768737\n",
            "Iteration 135, loss = 0.45624355\n",
            "Iteration 136, loss = 0.45503983\n",
            "Iteration 137, loss = 0.45393086\n",
            "Iteration 138, loss = 0.45274034\n",
            "Iteration 139, loss = 0.45161871\n",
            "Iteration 140, loss = 0.45052968\n",
            "Iteration 141, loss = 0.44947956\n",
            "Iteration 142, loss = 0.44850484\n",
            "Iteration 143, loss = 0.44765240\n",
            "Iteration 144, loss = 0.44686350\n",
            "Iteration 145, loss = 0.44585630\n",
            "Iteration 146, loss = 0.44503291\n",
            "Iteration 147, loss = 0.44441387\n",
            "Iteration 148, loss = 0.44371878\n",
            "Iteration 149, loss = 0.44301413\n",
            "Iteration 150, loss = 0.44233744\n",
            "Iteration 151, loss = 0.44150625\n",
            "Iteration 152, loss = 0.44090188\n",
            "Iteration 153, loss = 0.44036877\n",
            "Iteration 154, loss = 0.43977258\n",
            "Iteration 155, loss = 0.43924302\n",
            "Iteration 156, loss = 0.43871938\n",
            "Iteration 157, loss = 0.43819516\n",
            "Iteration 158, loss = 0.43765477\n",
            "Iteration 159, loss = 0.43716614\n",
            "Iteration 160, loss = 0.43660605\n",
            "Iteration 161, loss = 0.43601922\n",
            "Iteration 162, loss = 0.43557162\n",
            "Iteration 163, loss = 0.43526924\n",
            "Iteration 164, loss = 0.43469736\n",
            "Iteration 165, loss = 0.43450089\n",
            "Iteration 166, loss = 0.43380748\n",
            "Iteration 167, loss = 0.43333490\n",
            "Iteration 168, loss = 0.43296932\n",
            "Iteration 169, loss = 0.43256566\n",
            "Iteration 170, loss = 0.43214291\n",
            "Iteration 171, loss = 0.43186479\n",
            "Iteration 172, loss = 0.43138112\n",
            "Iteration 173, loss = 0.43095195\n",
            "Iteration 174, loss = 0.43062888\n",
            "Iteration 175, loss = 0.43035766\n",
            "Iteration 176, loss = 0.42981796\n",
            "Iteration 177, loss = 0.42939453\n",
            "Iteration 178, loss = 0.42891740\n",
            "Iteration 179, loss = 0.42856363\n",
            "Iteration 180, loss = 0.42831645\n",
            "Iteration 181, loss = 0.42778982\n",
            "Iteration 182, loss = 0.42728489\n",
            "Iteration 183, loss = 0.42691766\n",
            "Iteration 184, loss = 0.42652169\n",
            "Iteration 185, loss = 0.42615637\n",
            "Iteration 186, loss = 0.42569652\n",
            "Iteration 187, loss = 0.42531938\n",
            "Iteration 188, loss = 0.42479768\n",
            "Iteration 189, loss = 0.42436767\n",
            "Iteration 190, loss = 0.42410494\n",
            "Iteration 191, loss = 0.42389119\n",
            "Iteration 192, loss = 0.42358537\n",
            "Iteration 193, loss = 0.42301538\n",
            "Iteration 194, loss = 0.42258359\n",
            "Iteration 195, loss = 0.42219114\n",
            "Iteration 196, loss = 0.42229278\n",
            "Iteration 197, loss = 0.42179169\n",
            "Iteration 198, loss = 0.42144835\n",
            "Iteration 199, loss = 0.42096197\n",
            "Iteration 200, loss = 0.42057170\n",
            "Iteration 201, loss = 0.42020040\n",
            "Iteration 202, loss = 0.41975128\n",
            "Iteration 203, loss = 0.41942720\n",
            "Iteration 204, loss = 0.41940849\n",
            "Iteration 205, loss = 0.41883647\n",
            "Iteration 206, loss = 0.41845726\n",
            "Iteration 207, loss = 0.41805281\n",
            "Iteration 208, loss = 0.41785435\n",
            "Iteration 209, loss = 0.41722502\n",
            "Iteration 210, loss = 0.41682063\n",
            "Iteration 211, loss = 0.41653945\n",
            "Iteration 212, loss = 0.41655616\n",
            "Iteration 213, loss = 0.41591517\n",
            "Iteration 214, loss = 0.41551576\n",
            "Iteration 215, loss = 0.41527208\n",
            "Iteration 216, loss = 0.41487381\n",
            "Iteration 217, loss = 0.41475838\n",
            "Iteration 218, loss = 0.41421927\n",
            "Iteration 219, loss = 0.41399426\n",
            "Iteration 220, loss = 0.41368003\n",
            "Iteration 221, loss = 0.41355282\n",
            "Iteration 222, loss = 0.41309649\n",
            "Iteration 223, loss = 0.41277184\n",
            "Iteration 224, loss = 0.41245040\n",
            "Iteration 225, loss = 0.41203590\n",
            "Iteration 226, loss = 0.41183696\n",
            "Iteration 227, loss = 0.41154072\n",
            "Iteration 228, loss = 0.41118903\n",
            "Iteration 229, loss = 0.41085904\n",
            "Iteration 230, loss = 0.41073594\n",
            "Iteration 231, loss = 0.41032911\n",
            "Iteration 232, loss = 0.41002585\n",
            "Iteration 233, loss = 0.40979831\n",
            "Iteration 234, loss = 0.40956483\n",
            "Iteration 235, loss = 0.40937320\n",
            "Iteration 236, loss = 0.40918419\n",
            "Iteration 237, loss = 0.40871069\n",
            "Iteration 238, loss = 0.40846106\n",
            "Iteration 239, loss = 0.40812330\n",
            "Iteration 240, loss = 0.40801163\n",
            "Iteration 241, loss = 0.40775724\n",
            "Iteration 242, loss = 0.40733787\n",
            "Iteration 243, loss = 0.40700528\n",
            "Iteration 244, loss = 0.40681372\n",
            "Iteration 245, loss = 0.40677474\n",
            "Iteration 246, loss = 0.40664145\n",
            "Iteration 247, loss = 0.40636547\n",
            "Iteration 248, loss = 0.40599641\n",
            "Iteration 249, loss = 0.40539709\n",
            "Iteration 250, loss = 0.40542915\n",
            "Iteration 251, loss = 0.40514231\n",
            "Iteration 252, loss = 0.40504926\n",
            "Iteration 253, loss = 0.40488013\n",
            "Iteration 254, loss = 0.40443899\n",
            "Iteration 255, loss = 0.40421701\n",
            "Iteration 256, loss = 0.40398719\n",
            "Iteration 257, loss = 0.40360644\n",
            "Iteration 258, loss = 0.40340311\n",
            "Iteration 259, loss = 0.40318690\n",
            "Iteration 260, loss = 0.40310422\n",
            "Iteration 261, loss = 0.40276458\n",
            "Iteration 262, loss = 0.40255890\n",
            "Iteration 263, loss = 0.40236783\n",
            "Iteration 264, loss = 0.40203819\n",
            "Iteration 265, loss = 0.40169057\n",
            "Iteration 266, loss = 0.40181435\n",
            "Iteration 267, loss = 0.40139763\n",
            "Iteration 268, loss = 0.40113776\n",
            "Iteration 269, loss = 0.40116194\n",
            "Iteration 270, loss = 0.40072068\n",
            "Iteration 271, loss = 0.40043281\n",
            "Iteration 272, loss = 0.40029977\n",
            "Iteration 273, loss = 0.40005671\n",
            "Iteration 274, loss = 0.39989881\n",
            "Iteration 275, loss = 0.39970050\n",
            "Iteration 276, loss = 0.39960359\n",
            "Iteration 277, loss = 0.39929335\n",
            "Iteration 278, loss = 0.39908891\n",
            "Iteration 279, loss = 0.39892732\n",
            "Iteration 280, loss = 0.39871076\n",
            "Iteration 281, loss = 0.39856720\n",
            "Iteration 282, loss = 0.39831396\n",
            "Iteration 283, loss = 0.39817687\n",
            "Iteration 284, loss = 0.39791142\n",
            "Iteration 285, loss = 0.39780641\n",
            "Iteration 286, loss = 0.39752945\n",
            "Iteration 287, loss = 0.39733559\n",
            "Iteration 288, loss = 0.39732962\n",
            "Iteration 289, loss = 0.39692713\n",
            "Iteration 290, loss = 0.39676071\n",
            "Iteration 291, loss = 0.39660611\n",
            "Iteration 292, loss = 0.39642881\n",
            "Iteration 293, loss = 0.39631405\n",
            "Iteration 294, loss = 0.39603935\n",
            "Iteration 295, loss = 0.39582468\n",
            "Iteration 296, loss = 0.39566062\n",
            "Iteration 297, loss = 0.39550134\n",
            "Iteration 298, loss = 0.39540427\n",
            "Iteration 299, loss = 0.39527627\n",
            "Iteration 300, loss = 0.39480922\n",
            "Iteration 301, loss = 0.39503688\n",
            "Iteration 302, loss = 0.39467888\n",
            "Iteration 303, loss = 0.39461707\n",
            "Iteration 304, loss = 0.39439320\n",
            "Iteration 305, loss = 0.39428766\n",
            "Iteration 306, loss = 0.39393470\n",
            "Iteration 307, loss = 0.39377274\n",
            "Iteration 308, loss = 0.39368573\n",
            "Iteration 309, loss = 0.39346945\n",
            "Iteration 310, loss = 0.39341917\n",
            "Iteration 311, loss = 0.39308067\n",
            "Iteration 312, loss = 0.39294469\n",
            "Iteration 313, loss = 0.39271339\n",
            "Iteration 314, loss = 0.39264408\n",
            "Iteration 315, loss = 0.39248765\n",
            "Iteration 316, loss = 0.39226643\n",
            "Iteration 317, loss = 0.39223926\n",
            "Iteration 318, loss = 0.39193656\n",
            "Iteration 319, loss = 0.39177205\n",
            "Iteration 320, loss = 0.39159964\n",
            "Iteration 321, loss = 0.39150819\n",
            "Iteration 322, loss = 0.39132450\n",
            "Iteration 323, loss = 0.39118166\n",
            "Iteration 324, loss = 0.39102468\n",
            "Iteration 325, loss = 0.39086770\n",
            "Iteration 326, loss = 0.39068371\n",
            "Iteration 327, loss = 0.39057114\n",
            "Iteration 328, loss = 0.39037955\n",
            "Iteration 329, loss = 0.39035719\n",
            "Iteration 330, loss = 0.39034728\n",
            "Iteration 331, loss = 0.39016696\n",
            "Iteration 332, loss = 0.38986745\n",
            "Iteration 333, loss = 0.38997235\n",
            "Iteration 334, loss = 0.38955303\n",
            "Iteration 335, loss = 0.38937645\n",
            "Iteration 336, loss = 0.38933570\n",
            "Iteration 337, loss = 0.38914738\n",
            "Iteration 338, loss = 0.38903449\n",
            "Iteration 339, loss = 0.38872329\n",
            "Iteration 340, loss = 0.38874966\n",
            "Iteration 341, loss = 0.38856545\n",
            "Iteration 342, loss = 0.38852486\n",
            "Iteration 343, loss = 0.38829041\n",
            "Iteration 344, loss = 0.38806034\n",
            "Iteration 345, loss = 0.38803908\n",
            "Iteration 346, loss = 0.38787419\n",
            "Iteration 347, loss = 0.38769828\n",
            "Iteration 348, loss = 0.38752804\n",
            "Iteration 349, loss = 0.38753494\n",
            "Iteration 350, loss = 0.38729062\n",
            "Iteration 351, loss = 0.38715476\n",
            "Iteration 352, loss = 0.38721069\n",
            "Iteration 353, loss = 0.38693912\n",
            "Iteration 354, loss = 0.38679212\n",
            "Iteration 355, loss = 0.38671235\n",
            "Iteration 356, loss = 0.38664711\n",
            "Iteration 357, loss = 0.38646336\n",
            "Iteration 358, loss = 0.38634555\n",
            "Iteration 359, loss = 0.38616780\n",
            "Iteration 360, loss = 0.38614279\n",
            "Iteration 361, loss = 0.38591474\n",
            "Iteration 362, loss = 0.38585427\n",
            "Iteration 363, loss = 0.38583916\n",
            "Iteration 364, loss = 0.38565836\n",
            "Iteration 365, loss = 0.38555626\n",
            "Iteration 366, loss = 0.38535787\n",
            "Iteration 367, loss = 0.38524146\n",
            "Iteration 368, loss = 0.38513060\n",
            "Iteration 369, loss = 0.38504162\n",
            "Iteration 370, loss = 0.38495648\n",
            "Iteration 371, loss = 0.38489646\n",
            "Iteration 372, loss = 0.38480610\n",
            "Iteration 373, loss = 0.38469303\n",
            "Iteration 374, loss = 0.38447277\n",
            "Iteration 375, loss = 0.38442243\n",
            "Iteration 376, loss = 0.38424757\n",
            "Iteration 377, loss = 0.38417138\n",
            "Iteration 378, loss = 0.38421799\n",
            "Iteration 379, loss = 0.38409663\n",
            "Iteration 380, loss = 0.38382182\n",
            "Iteration 381, loss = 0.38376614\n",
            "Iteration 382, loss = 0.38360621\n",
            "Iteration 383, loss = 0.38351124\n",
            "Iteration 384, loss = 0.38342689\n",
            "Iteration 385, loss = 0.38324474\n",
            "Iteration 386, loss = 0.38327598\n",
            "Iteration 387, loss = 0.38317222\n",
            "Iteration 388, loss = 0.38306536\n",
            "Iteration 389, loss = 0.38293136\n",
            "Iteration 390, loss = 0.38280934\n",
            "Iteration 391, loss = 0.38269120\n",
            "Iteration 392, loss = 0.38275905\n",
            "Iteration 393, loss = 0.38270013\n",
            "Iteration 394, loss = 0.38251701\n",
            "Iteration 395, loss = 0.38236793\n",
            "Iteration 396, loss = 0.38235130\n",
            "Iteration 397, loss = 0.38218454\n",
            "Iteration 398, loss = 0.38227337\n",
            "Iteration 399, loss = 0.38217613\n",
            "Iteration 400, loss = 0.38184088\n",
            "Iteration 401, loss = 0.38202057\n",
            "Iteration 402, loss = 0.38182657\n",
            "Iteration 403, loss = 0.38175484\n",
            "Iteration 404, loss = 0.38152662\n",
            "Iteration 405, loss = 0.38141327\n",
            "Iteration 406, loss = 0.38126005\n",
            "Iteration 407, loss = 0.38171474\n",
            "Iteration 408, loss = 0.38133342\n",
            "Iteration 409, loss = 0.38114946\n",
            "Iteration 410, loss = 0.38093738\n",
            "Iteration 411, loss = 0.38109932\n",
            "Iteration 412, loss = 0.38097162\n",
            "Iteration 413, loss = 0.38086068\n",
            "Iteration 414, loss = 0.38066036\n",
            "Iteration 415, loss = 0.38056364\n",
            "Iteration 416, loss = 0.38051337\n",
            "Iteration 417, loss = 0.38053359\n",
            "Iteration 418, loss = 0.38040769\n",
            "Iteration 419, loss = 0.38036757\n",
            "Iteration 420, loss = 0.38019198\n",
            "Iteration 421, loss = 0.38007490\n",
            "Iteration 422, loss = 0.38005862\n",
            "Iteration 423, loss = 0.37994087\n",
            "Iteration 424, loss = 0.37986480\n",
            "Iteration 425, loss = 0.37982605\n",
            "Iteration 426, loss = 0.37972158\n",
            "Iteration 427, loss = 0.37972642\n",
            "Iteration 428, loss = 0.37962385\n",
            "Iteration 429, loss = 0.37948901\n",
            "Iteration 430, loss = 0.37943386\n",
            "Iteration 431, loss = 0.37950194\n",
            "Iteration 432, loss = 0.37955119\n",
            "Iteration 433, loss = 0.37932658\n",
            "Iteration 434, loss = 0.37923365\n",
            "Iteration 435, loss = 0.37888432\n",
            "Iteration 436, loss = 0.37898975\n",
            "Iteration 437, loss = 0.37887656\n",
            "Iteration 438, loss = 0.37877283\n",
            "Iteration 439, loss = 0.37877804\n",
            "Iteration 440, loss = 0.37860782\n",
            "Iteration 441, loss = 0.37856504\n",
            "Iteration 442, loss = 0.37861361\n",
            "Iteration 443, loss = 0.37837917\n",
            "Iteration 444, loss = 0.37845800\n",
            "Iteration 445, loss = 0.37826030\n",
            "Iteration 446, loss = 0.37811334\n",
            "Iteration 447, loss = 0.37806983\n",
            "Iteration 448, loss = 0.37798841\n",
            "Iteration 449, loss = 0.37796810\n",
            "Iteration 450, loss = 0.37792929\n",
            "Iteration 451, loss = 0.37791684\n",
            "Iteration 452, loss = 0.37777934\n",
            "Iteration 453, loss = 0.37779028\n",
            "Iteration 454, loss = 0.37772302\n",
            "Iteration 455, loss = 0.37779513\n",
            "Iteration 456, loss = 0.37750131\n",
            "Iteration 457, loss = 0.37737794\n",
            "Iteration 458, loss = 0.37732019\n",
            "Iteration 459, loss = 0.37731823\n",
            "Iteration 460, loss = 0.37733124\n",
            "Iteration 461, loss = 0.37718935\n",
            "Iteration 462, loss = 0.37715670\n",
            "Iteration 463, loss = 0.37719850\n",
            "Iteration 464, loss = 0.37706713\n",
            "Iteration 465, loss = 0.37701802\n",
            "Iteration 466, loss = 0.37685054\n",
            "Iteration 467, loss = 0.37678246\n",
            "Iteration 468, loss = 0.37679928\n",
            "Iteration 469, loss = 0.37665313\n",
            "Iteration 470, loss = 0.37660086\n",
            "Iteration 471, loss = 0.37658906\n",
            "Iteration 472, loss = 0.37657964\n",
            "Iteration 473, loss = 0.37641649\n",
            "Iteration 474, loss = 0.37633650\n",
            "Iteration 475, loss = 0.37637845\n",
            "Iteration 476, loss = 0.37621112\n",
            "Iteration 477, loss = 0.37621163\n",
            "Iteration 478, loss = 0.37616149\n",
            "Iteration 479, loss = 0.37607443\n",
            "Iteration 480, loss = 0.37600771\n",
            "Iteration 481, loss = 0.37604802\n",
            "Iteration 482, loss = 0.37592698\n",
            "Iteration 483, loss = 0.37587349\n",
            "Iteration 484, loss = 0.37578153\n",
            "Iteration 485, loss = 0.37571892\n",
            "Iteration 486, loss = 0.37570288\n",
            "Iteration 487, loss = 0.37564361\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.75779631\n",
            "Iteration 2, loss = 0.75240383\n",
            "Iteration 3, loss = 0.74749813\n",
            "Iteration 4, loss = 0.74267498\n",
            "Iteration 5, loss = 0.73820095\n",
            "Iteration 6, loss = 0.73410299\n",
            "Iteration 7, loss = 0.73010241\n",
            "Iteration 8, loss = 0.72631787\n",
            "Iteration 9, loss = 0.72291995\n",
            "Iteration 10, loss = 0.71918914\n",
            "Iteration 11, loss = 0.71611632\n",
            "Iteration 12, loss = 0.71322858\n",
            "Iteration 13, loss = 0.71026386\n",
            "Iteration 14, loss = 0.70737242\n",
            "Iteration 15, loss = 0.70496315\n",
            "Iteration 16, loss = 0.70223407\n",
            "Iteration 17, loss = 0.70002558\n",
            "Iteration 18, loss = 0.69769211\n",
            "Iteration 19, loss = 0.69561203\n",
            "Iteration 20, loss = 0.69358145\n",
            "Iteration 21, loss = 0.69166693\n",
            "Iteration 22, loss = 0.68980242\n",
            "Iteration 23, loss = 0.68808049\n",
            "Iteration 24, loss = 0.68645314\n",
            "Iteration 25, loss = 0.68470545\n",
            "Iteration 26, loss = 0.68312144\n",
            "Iteration 27, loss = 0.68145126\n",
            "Iteration 28, loss = 0.67989803\n",
            "Iteration 29, loss = 0.67837809\n",
            "Iteration 30, loss = 0.67684987\n",
            "Iteration 31, loss = 0.67527589\n",
            "Iteration 32, loss = 0.67377180\n",
            "Iteration 33, loss = 0.67218679\n",
            "Iteration 34, loss = 0.67072992\n",
            "Iteration 35, loss = 0.66924414\n",
            "Iteration 36, loss = 0.66760006\n",
            "Iteration 37, loss = 0.66605267\n",
            "Iteration 38, loss = 0.66451173\n",
            "Iteration 39, loss = 0.66302294\n",
            "Iteration 40, loss = 0.66125214\n",
            "Iteration 41, loss = 0.65978461\n",
            "Iteration 42, loss = 0.65814698\n",
            "Iteration 43, loss = 0.65655785\n",
            "Iteration 44, loss = 0.65491839\n",
            "Iteration 45, loss = 0.65333278\n",
            "Iteration 46, loss = 0.65165971\n",
            "Iteration 47, loss = 0.65008737\n",
            "Iteration 48, loss = 0.64837287\n",
            "Iteration 49, loss = 0.64670175\n",
            "Iteration 50, loss = 0.64502097\n",
            "Iteration 51, loss = 0.64336950\n",
            "Iteration 52, loss = 0.64164304\n",
            "Iteration 53, loss = 0.63995356\n",
            "Iteration 54, loss = 0.63827438\n",
            "Iteration 55, loss = 0.63652251\n",
            "Iteration 56, loss = 0.63475391\n",
            "Iteration 57, loss = 0.63295407\n",
            "Iteration 58, loss = 0.63115929\n",
            "Iteration 59, loss = 0.62935431\n",
            "Iteration 60, loss = 0.62748335\n",
            "Iteration 61, loss = 0.62562071\n",
            "Iteration 62, loss = 0.62389688\n",
            "Iteration 63, loss = 0.62198718\n",
            "Iteration 64, loss = 0.62013754\n",
            "Iteration 65, loss = 0.61830645\n",
            "Iteration 66, loss = 0.61636304\n",
            "Iteration 67, loss = 0.61449548\n",
            "Iteration 68, loss = 0.61257982\n",
            "Iteration 69, loss = 0.61059985\n",
            "Iteration 70, loss = 0.60859097\n",
            "Iteration 71, loss = 0.60662091\n",
            "Iteration 72, loss = 0.60466471\n",
            "Iteration 73, loss = 0.60252255\n",
            "Iteration 74, loss = 0.60042944\n",
            "Iteration 75, loss = 0.59828358\n",
            "Iteration 76, loss = 0.59618178\n",
            "Iteration 77, loss = 0.59392548\n",
            "Iteration 78, loss = 0.59163232\n",
            "Iteration 79, loss = 0.58955239\n",
            "Iteration 80, loss = 0.58711462\n",
            "Iteration 81, loss = 0.58470137\n",
            "Iteration 82, loss = 0.58238225\n",
            "Iteration 83, loss = 0.57977099\n",
            "Iteration 84, loss = 0.57733040\n",
            "Iteration 85, loss = 0.57480996\n",
            "Iteration 86, loss = 0.57214922\n",
            "Iteration 87, loss = 0.56948650\n",
            "Iteration 88, loss = 0.56690234\n",
            "Iteration 89, loss = 0.56401376\n",
            "Iteration 90, loss = 0.56130818\n",
            "Iteration 91, loss = 0.55862007\n",
            "Iteration 92, loss = 0.55586163\n",
            "Iteration 93, loss = 0.55294021\n",
            "Iteration 94, loss = 0.55029755\n",
            "Iteration 95, loss = 0.54729310\n",
            "Iteration 96, loss = 0.54434687\n",
            "Iteration 97, loss = 0.54148311\n",
            "Iteration 98, loss = 0.53834990\n",
            "Iteration 99, loss = 0.53564029\n",
            "Iteration 100, loss = 0.53276903\n",
            "Iteration 101, loss = 0.52995052\n",
            "Iteration 102, loss = 0.52718483\n",
            "Iteration 103, loss = 0.52446250\n",
            "Iteration 104, loss = 0.52161353\n",
            "Iteration 105, loss = 0.51890074\n",
            "Iteration 106, loss = 0.51625466\n",
            "Iteration 107, loss = 0.51355848\n",
            "Iteration 108, loss = 0.51086315\n",
            "Iteration 109, loss = 0.50814060\n",
            "Iteration 110, loss = 0.50547422\n",
            "Iteration 111, loss = 0.50287025\n",
            "Iteration 112, loss = 0.50031573\n",
            "Iteration 113, loss = 0.49773656\n",
            "Iteration 114, loss = 0.49522131\n",
            "Iteration 115, loss = 0.49264665\n",
            "Iteration 116, loss = 0.49020256\n",
            "Iteration 117, loss = 0.48801099\n",
            "Iteration 118, loss = 0.48557274\n",
            "Iteration 119, loss = 0.48333303\n",
            "Iteration 120, loss = 0.48089994\n",
            "Iteration 121, loss = 0.47868457\n",
            "Iteration 122, loss = 0.47657849\n",
            "Iteration 123, loss = 0.47445921\n",
            "Iteration 124, loss = 0.47251091\n",
            "Iteration 125, loss = 0.47042285\n",
            "Iteration 126, loss = 0.46825379\n",
            "Iteration 127, loss = 0.46631401\n",
            "Iteration 128, loss = 0.46432822\n",
            "Iteration 129, loss = 0.46250481\n",
            "Iteration 130, loss = 0.46072313\n",
            "Iteration 131, loss = 0.45903452\n",
            "Iteration 132, loss = 0.45745114\n",
            "Iteration 133, loss = 0.45601218\n",
            "Iteration 134, loss = 0.45421004\n",
            "Iteration 135, loss = 0.45264666\n",
            "Iteration 136, loss = 0.45125924\n",
            "Iteration 137, loss = 0.44991341\n",
            "Iteration 138, loss = 0.44854708\n",
            "Iteration 139, loss = 0.44727700\n",
            "Iteration 140, loss = 0.44612024\n",
            "Iteration 141, loss = 0.44490149\n",
            "Iteration 142, loss = 0.44386733\n",
            "Iteration 143, loss = 0.44276411\n",
            "Iteration 144, loss = 0.44189042\n",
            "Iteration 145, loss = 0.44086481\n",
            "Iteration 146, loss = 0.44000075\n",
            "Iteration 147, loss = 0.43908069\n",
            "Iteration 148, loss = 0.43822479\n",
            "Iteration 149, loss = 0.43743920\n",
            "Iteration 150, loss = 0.43664044\n",
            "Iteration 151, loss = 0.43586215\n",
            "Iteration 152, loss = 0.43509092\n",
            "Iteration 153, loss = 0.43453114\n",
            "Iteration 154, loss = 0.43373983\n",
            "Iteration 155, loss = 0.43297950\n",
            "Iteration 156, loss = 0.43237230\n",
            "Iteration 157, loss = 0.43170105\n",
            "Iteration 158, loss = 0.43096232\n",
            "Iteration 159, loss = 0.43040358\n",
            "Iteration 160, loss = 0.42972379\n",
            "Iteration 161, loss = 0.42904742\n",
            "Iteration 162, loss = 0.42858387\n",
            "Iteration 163, loss = 0.42806517\n",
            "Iteration 164, loss = 0.42756298\n",
            "Iteration 165, loss = 0.42696515\n",
            "Iteration 166, loss = 0.42639693\n",
            "Iteration 167, loss = 0.42569955\n",
            "Iteration 168, loss = 0.42527644\n",
            "Iteration 169, loss = 0.42474953\n",
            "Iteration 170, loss = 0.42433574\n",
            "Iteration 171, loss = 0.42375666\n",
            "Iteration 172, loss = 0.42321240\n",
            "Iteration 173, loss = 0.42261952\n",
            "Iteration 174, loss = 0.42203533\n",
            "Iteration 175, loss = 0.42159925\n",
            "Iteration 176, loss = 0.42105418\n",
            "Iteration 177, loss = 0.42054506\n",
            "Iteration 178, loss = 0.42011282\n",
            "Iteration 179, loss = 0.41952902\n",
            "Iteration 180, loss = 0.41921228\n",
            "Iteration 181, loss = 0.41852357\n",
            "Iteration 182, loss = 0.41809111\n",
            "Iteration 183, loss = 0.41770223\n",
            "Iteration 184, loss = 0.41729436\n",
            "Iteration 185, loss = 0.41678532\n",
            "Iteration 186, loss = 0.41626005\n",
            "Iteration 187, loss = 0.41576744\n",
            "Iteration 188, loss = 0.41538917\n",
            "Iteration 189, loss = 0.41497732\n",
            "Iteration 190, loss = 0.41470767\n",
            "Iteration 191, loss = 0.41418556\n",
            "Iteration 192, loss = 0.41368992\n",
            "Iteration 193, loss = 0.41310520\n",
            "Iteration 194, loss = 0.41282655\n",
            "Iteration 195, loss = 0.41244248\n",
            "Iteration 196, loss = 0.41220682\n",
            "Iteration 197, loss = 0.41171539\n",
            "Iteration 198, loss = 0.41115102\n",
            "Iteration 199, loss = 0.41075801\n",
            "Iteration 200, loss = 0.41039396\n",
            "Iteration 201, loss = 0.41009177\n",
            "Iteration 202, loss = 0.40952476\n",
            "Iteration 203, loss = 0.40917326\n",
            "Iteration 204, loss = 0.40914794\n",
            "Iteration 205, loss = 0.40849331\n",
            "Iteration 206, loss = 0.40807231\n",
            "Iteration 207, loss = 0.40774061\n",
            "Iteration 208, loss = 0.40753354\n",
            "Iteration 209, loss = 0.40710363\n",
            "Iteration 210, loss = 0.40655730\n",
            "Iteration 211, loss = 0.40620549\n",
            "Iteration 212, loss = 0.40622341\n",
            "Iteration 213, loss = 0.40549876\n",
            "Iteration 214, loss = 0.40522348\n",
            "Iteration 215, loss = 0.40491081\n",
            "Iteration 216, loss = 0.40450521\n",
            "Iteration 217, loss = 0.40420952\n",
            "Iteration 218, loss = 0.40373739\n",
            "Iteration 219, loss = 0.40348032\n",
            "Iteration 220, loss = 0.40314421\n",
            "Iteration 221, loss = 0.40287862\n",
            "Iteration 222, loss = 0.40249350\n",
            "Iteration 223, loss = 0.40215133\n",
            "Iteration 224, loss = 0.40190162\n",
            "Iteration 225, loss = 0.40140293\n",
            "Iteration 226, loss = 0.40123071\n",
            "Iteration 227, loss = 0.40087782\n",
            "Iteration 228, loss = 0.40060609\n",
            "Iteration 229, loss = 0.40023993\n",
            "Iteration 230, loss = 0.39988922\n",
            "Iteration 231, loss = 0.39949384\n",
            "Iteration 232, loss = 0.39929848\n",
            "Iteration 233, loss = 0.39898967\n",
            "Iteration 234, loss = 0.39875544\n",
            "Iteration 235, loss = 0.39844986\n",
            "Iteration 236, loss = 0.39834881\n",
            "Iteration 237, loss = 0.39792599\n",
            "Iteration 238, loss = 0.39761402\n",
            "Iteration 239, loss = 0.39721540\n",
            "Iteration 240, loss = 0.39704832\n",
            "Iteration 241, loss = 0.39679638\n",
            "Iteration 242, loss = 0.39642441\n",
            "Iteration 243, loss = 0.39606183\n",
            "Iteration 244, loss = 0.39583931\n",
            "Iteration 245, loss = 0.39563589\n",
            "Iteration 246, loss = 0.39538913\n",
            "Iteration 247, loss = 0.39505601\n",
            "Iteration 248, loss = 0.39481936\n",
            "Iteration 249, loss = 0.39460312\n",
            "Iteration 250, loss = 0.39423032\n",
            "Iteration 251, loss = 0.39405425\n",
            "Iteration 252, loss = 0.39407410\n",
            "Iteration 253, loss = 0.39381856\n",
            "Iteration 254, loss = 0.39327412\n",
            "Iteration 255, loss = 0.39307341\n",
            "Iteration 256, loss = 0.39282625\n",
            "Iteration 257, loss = 0.39270346\n",
            "Iteration 258, loss = 0.39246199\n",
            "Iteration 259, loss = 0.39223723\n",
            "Iteration 260, loss = 0.39192048\n",
            "Iteration 261, loss = 0.39180659\n",
            "Iteration 262, loss = 0.39152989\n",
            "Iteration 263, loss = 0.39130980\n",
            "Iteration 264, loss = 0.39107482\n",
            "Iteration 265, loss = 0.39078030\n",
            "Iteration 266, loss = 0.39069040\n",
            "Iteration 267, loss = 0.39029516\n",
            "Iteration 268, loss = 0.39010391\n",
            "Iteration 269, loss = 0.39005137\n",
            "Iteration 270, loss = 0.38969963\n",
            "Iteration 271, loss = 0.38950809\n",
            "Iteration 272, loss = 0.38934932\n",
            "Iteration 273, loss = 0.38905468\n",
            "Iteration 274, loss = 0.38888279\n",
            "Iteration 275, loss = 0.38869621\n",
            "Iteration 276, loss = 0.38846632\n",
            "Iteration 277, loss = 0.38829849\n",
            "Iteration 278, loss = 0.38800600\n",
            "Iteration 279, loss = 0.38787677\n",
            "Iteration 280, loss = 0.38764019\n",
            "Iteration 281, loss = 0.38759260\n",
            "Iteration 282, loss = 0.38730266\n",
            "Iteration 283, loss = 0.38716000\n",
            "Iteration 284, loss = 0.38682448\n",
            "Iteration 285, loss = 0.38667111\n",
            "Iteration 286, loss = 0.38647930\n",
            "Iteration 287, loss = 0.38633909\n",
            "Iteration 288, loss = 0.38618840\n",
            "Iteration 289, loss = 0.38597238\n",
            "Iteration 290, loss = 0.38591605\n",
            "Iteration 291, loss = 0.38571334\n",
            "Iteration 292, loss = 0.38556622\n",
            "Iteration 293, loss = 0.38536899\n",
            "Iteration 294, loss = 0.38511748\n",
            "Iteration 295, loss = 0.38487608\n",
            "Iteration 296, loss = 0.38483376\n",
            "Iteration 297, loss = 0.38465299\n",
            "Iteration 298, loss = 0.38453624\n",
            "Iteration 299, loss = 0.38445353\n",
            "Iteration 300, loss = 0.38404117\n",
            "Iteration 301, loss = 0.38401480\n",
            "Iteration 302, loss = 0.38384206\n",
            "Iteration 303, loss = 0.38388278\n",
            "Iteration 304, loss = 0.38360879\n",
            "Iteration 305, loss = 0.38340098\n",
            "Iteration 306, loss = 0.38300169\n",
            "Iteration 307, loss = 0.38294385\n",
            "Iteration 308, loss = 0.38315279\n",
            "Iteration 309, loss = 0.38283700\n",
            "Iteration 310, loss = 0.38270056\n",
            "Iteration 311, loss = 0.38229751\n",
            "Iteration 312, loss = 0.38224181\n",
            "Iteration 313, loss = 0.38197481\n",
            "Iteration 314, loss = 0.38186756\n",
            "Iteration 315, loss = 0.38179086\n",
            "Iteration 316, loss = 0.38170259\n",
            "Iteration 317, loss = 0.38164865\n",
            "Iteration 318, loss = 0.38139359\n",
            "Iteration 319, loss = 0.38109632\n",
            "Iteration 320, loss = 0.38093923\n",
            "Iteration 321, loss = 0.38081512\n",
            "Iteration 322, loss = 0.38068049\n",
            "Iteration 323, loss = 0.38064760\n",
            "Iteration 324, loss = 0.38045280\n",
            "Iteration 325, loss = 0.38027385\n",
            "Iteration 326, loss = 0.38006380\n",
            "Iteration 327, loss = 0.38002397\n",
            "Iteration 328, loss = 0.37990821\n",
            "Iteration 329, loss = 0.37974382\n",
            "Iteration 330, loss = 0.37958712\n",
            "Iteration 331, loss = 0.37964727\n",
            "Iteration 332, loss = 0.37929290\n",
            "Iteration 333, loss = 0.37957595\n",
            "Iteration 334, loss = 0.37921100\n",
            "Iteration 335, loss = 0.37912919\n",
            "Iteration 336, loss = 0.37902057\n",
            "Iteration 337, loss = 0.37893291\n",
            "Iteration 338, loss = 0.37880773\n",
            "Iteration 339, loss = 0.37857329\n",
            "Iteration 340, loss = 0.37855461\n",
            "Iteration 341, loss = 0.37840670\n",
            "Iteration 342, loss = 0.37826831\n",
            "Iteration 343, loss = 0.37809986\n",
            "Iteration 344, loss = 0.37785577\n",
            "Iteration 345, loss = 0.37794122\n",
            "Iteration 346, loss = 0.37782422\n",
            "Iteration 347, loss = 0.37758213\n",
            "Iteration 348, loss = 0.37747487\n",
            "Iteration 349, loss = 0.37732233\n",
            "Iteration 350, loss = 0.37713968\n",
            "Iteration 351, loss = 0.37714149\n",
            "Iteration 352, loss = 0.37716120\n",
            "Iteration 353, loss = 0.37687414\n",
            "Iteration 354, loss = 0.37691879\n",
            "Iteration 355, loss = 0.37695948\n",
            "Iteration 356, loss = 0.37663246\n",
            "Iteration 357, loss = 0.37659671\n",
            "Iteration 358, loss = 0.37640981\n",
            "Iteration 359, loss = 0.37637430\n",
            "Iteration 360, loss = 0.37634842\n",
            "Iteration 361, loss = 0.37614402\n",
            "Iteration 362, loss = 0.37607405\n",
            "Iteration 363, loss = 0.37591483\n",
            "Iteration 364, loss = 0.37584817\n",
            "Iteration 365, loss = 0.37583428\n",
            "Iteration 366, loss = 0.37573683\n",
            "Iteration 367, loss = 0.37554355\n",
            "Iteration 368, loss = 0.37545657\n",
            "Iteration 369, loss = 0.37543548\n",
            "Iteration 370, loss = 0.37551759\n",
            "Iteration 371, loss = 0.37535252\n",
            "Iteration 372, loss = 0.37501654\n",
            "Iteration 373, loss = 0.37497575\n",
            "Iteration 374, loss = 0.37479467\n",
            "Iteration 375, loss = 0.37480824\n",
            "Iteration 376, loss = 0.37460572\n",
            "Iteration 377, loss = 0.37456545\n",
            "Iteration 378, loss = 0.37464087\n",
            "Iteration 379, loss = 0.37465425\n",
            "Iteration 380, loss = 0.37442494\n",
            "Iteration 381, loss = 0.37434321\n",
            "Iteration 382, loss = 0.37420644\n",
            "Iteration 383, loss = 0.37405511\n",
            "Iteration 384, loss = 0.37407407\n",
            "Iteration 385, loss = 0.37390851\n",
            "Iteration 386, loss = 0.37412942\n",
            "Iteration 387, loss = 0.37394801\n",
            "Iteration 388, loss = 0.37375406\n",
            "Iteration 389, loss = 0.37363387\n",
            "Iteration 390, loss = 0.37359774\n",
            "Iteration 391, loss = 0.37341495\n",
            "Iteration 392, loss = 0.37338075\n",
            "Iteration 393, loss = 0.37343065\n",
            "Iteration 394, loss = 0.37333317\n",
            "Iteration 395, loss = 0.37325224\n",
            "Iteration 396, loss = 0.37313463\n",
            "Iteration 397, loss = 0.37300574\n",
            "Iteration 398, loss = 0.37302577\n",
            "Iteration 399, loss = 0.37294204\n",
            "Iteration 400, loss = 0.37283438\n",
            "Iteration 401, loss = 0.37291247\n",
            "Iteration 402, loss = 0.37283492\n",
            "Iteration 403, loss = 0.37257213\n",
            "Iteration 404, loss = 0.37253893\n",
            "Iteration 405, loss = 0.37261025\n",
            "Iteration 406, loss = 0.37240126\n",
            "Iteration 407, loss = 0.37242672\n",
            "Iteration 408, loss = 0.37232488\n",
            "Iteration 409, loss = 0.37228062\n",
            "Iteration 410, loss = 0.37206256\n",
            "Iteration 411, loss = 0.37226649\n",
            "Iteration 412, loss = 0.37242447\n",
            "Iteration 413, loss = 0.37206985\n",
            "Iteration 414, loss = 0.37178704\n",
            "Iteration 415, loss = 0.37187681\n",
            "Iteration 416, loss = 0.37193317\n",
            "Iteration 417, loss = 0.37188549\n",
            "Iteration 418, loss = 0.37172097\n",
            "Iteration 419, loss = 0.37164089\n",
            "Iteration 420, loss = 0.37152542\n",
            "Iteration 421, loss = 0.37151602\n",
            "Iteration 422, loss = 0.37147199\n",
            "Iteration 423, loss = 0.37140800\n",
            "Iteration 424, loss = 0.37136759\n",
            "Iteration 425, loss = 0.37124337\n",
            "Iteration 426, loss = 0.37121869\n",
            "Iteration 427, loss = 0.37117701\n",
            "Iteration 428, loss = 0.37106622\n",
            "Iteration 429, loss = 0.37097079\n",
            "Iteration 430, loss = 0.37104366\n",
            "Iteration 431, loss = 0.37125182\n",
            "Iteration 432, loss = 0.37131663\n",
            "Iteration 433, loss = 0.37102092\n",
            "Iteration 434, loss = 0.37080255\n",
            "Iteration 435, loss = 0.37048880\n",
            "Iteration 436, loss = 0.37088591\n",
            "Iteration 437, loss = 0.37100524\n",
            "Iteration 438, loss = 0.37081060\n",
            "Iteration 439, loss = 0.37050668\n",
            "Iteration 440, loss = 0.37035214\n",
            "Iteration 441, loss = 0.37034343\n",
            "Iteration 442, loss = 0.37038094\n",
            "Iteration 443, loss = 0.37025119\n",
            "Iteration 444, loss = 0.37042335\n",
            "Iteration 445, loss = 0.37015133\n",
            "Iteration 446, loss = 0.37008619\n",
            "Iteration 447, loss = 0.37010768\n",
            "Iteration 448, loss = 0.37002241\n",
            "Iteration 449, loss = 0.37001965\n",
            "Iteration 450, loss = 0.36997512\n",
            "Iteration 451, loss = 0.36985139\n",
            "Iteration 452, loss = 0.36988338\n",
            "Iteration 453, loss = 0.36989095\n",
            "Iteration 454, loss = 0.36983766\n",
            "Iteration 455, loss = 0.36984070\n",
            "Iteration 456, loss = 0.36967582\n",
            "Iteration 457, loss = 0.36957730\n",
            "Iteration 458, loss = 0.36952059\n",
            "Iteration 459, loss = 0.36951000\n",
            "Iteration 460, loss = 0.36956679\n",
            "Iteration 461, loss = 0.36946603\n",
            "Iteration 462, loss = 0.36951445\n",
            "Iteration 463, loss = 0.36942232\n",
            "Iteration 464, loss = 0.36926157\n",
            "Iteration 465, loss = 0.36935486\n",
            "Iteration 466, loss = 0.36936940\n",
            "Iteration 467, loss = 0.36923714\n",
            "Iteration 468, loss = 0.36915693\n",
            "Iteration 469, loss = 0.36905235\n",
            "Iteration 470, loss = 0.36900408\n",
            "Iteration 471, loss = 0.36898013\n",
            "Iteration 472, loss = 0.36894207\n",
            "Iteration 473, loss = 0.36891052\n",
            "Iteration 474, loss = 0.36892096\n",
            "Iteration 475, loss = 0.36884534\n",
            "Iteration 476, loss = 0.36868080\n",
            "Iteration 477, loss = 0.36883995\n",
            "Iteration 478, loss = 0.36884691\n",
            "Iteration 479, loss = 0.36874164\n",
            "Iteration 480, loss = 0.36863577\n",
            "Iteration 481, loss = 0.36868628\n",
            "Iteration 482, loss = 0.36858035\n",
            "Iteration 483, loss = 0.36851764\n",
            "Iteration 484, loss = 0.36844669\n",
            "Iteration 485, loss = 0.36844349\n",
            "Iteration 486, loss = 0.36854075\n",
            "Iteration 487, loss = 0.36838241\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.75913425\n",
            "Iteration 2, loss = 0.75388585\n",
            "Iteration 3, loss = 0.74881340\n",
            "Iteration 4, loss = 0.74413510\n",
            "Iteration 5, loss = 0.73940839\n",
            "Iteration 6, loss = 0.73523402\n",
            "Iteration 7, loss = 0.73104754\n",
            "Iteration 8, loss = 0.72704380\n",
            "Iteration 9, loss = 0.72357753\n",
            "Iteration 10, loss = 0.71966952\n",
            "Iteration 11, loss = 0.71650654\n",
            "Iteration 12, loss = 0.71345175\n",
            "Iteration 13, loss = 0.71041614\n",
            "Iteration 14, loss = 0.70749506\n",
            "Iteration 15, loss = 0.70490016\n",
            "Iteration 16, loss = 0.70212025\n",
            "Iteration 17, loss = 0.69975824\n",
            "Iteration 18, loss = 0.69745541\n",
            "Iteration 19, loss = 0.69513718\n",
            "Iteration 20, loss = 0.69310528\n",
            "Iteration 21, loss = 0.69099671\n",
            "Iteration 22, loss = 0.68898554\n",
            "Iteration 23, loss = 0.68708495\n",
            "Iteration 24, loss = 0.68543631\n",
            "Iteration 25, loss = 0.68350654\n",
            "Iteration 26, loss = 0.68173033\n",
            "Iteration 27, loss = 0.67996747\n",
            "Iteration 28, loss = 0.67824814\n",
            "Iteration 29, loss = 0.67656494\n",
            "Iteration 30, loss = 0.67486414\n",
            "Iteration 31, loss = 0.67313249\n",
            "Iteration 32, loss = 0.67139967\n",
            "Iteration 33, loss = 0.66964318\n",
            "Iteration 34, loss = 0.66800071\n",
            "Iteration 35, loss = 0.66639216\n",
            "Iteration 36, loss = 0.66455460\n",
            "Iteration 37, loss = 0.66288916\n",
            "Iteration 38, loss = 0.66121399\n",
            "Iteration 39, loss = 0.65961219\n",
            "Iteration 40, loss = 0.65774576\n",
            "Iteration 41, loss = 0.65613232\n",
            "Iteration 42, loss = 0.65447698\n",
            "Iteration 43, loss = 0.65276631\n",
            "Iteration 44, loss = 0.65103287\n",
            "Iteration 45, loss = 0.64935963\n",
            "Iteration 46, loss = 0.64762847\n",
            "Iteration 47, loss = 0.64598099\n",
            "Iteration 48, loss = 0.64418326\n",
            "Iteration 49, loss = 0.64248950\n",
            "Iteration 50, loss = 0.64071371\n",
            "Iteration 51, loss = 0.63901786\n",
            "Iteration 52, loss = 0.63731572\n",
            "Iteration 53, loss = 0.63560108\n",
            "Iteration 54, loss = 0.63385417\n",
            "Iteration 55, loss = 0.63205942\n",
            "Iteration 56, loss = 0.63033657\n",
            "Iteration 57, loss = 0.62850993\n",
            "Iteration 58, loss = 0.62672534\n",
            "Iteration 59, loss = 0.62496716\n",
            "Iteration 60, loss = 0.62315569\n",
            "Iteration 61, loss = 0.62139426\n",
            "Iteration 62, loss = 0.61970842\n",
            "Iteration 63, loss = 0.61778775\n",
            "Iteration 64, loss = 0.61609866\n",
            "Iteration 65, loss = 0.61429004\n",
            "Iteration 66, loss = 0.61243045\n",
            "Iteration 67, loss = 0.61069917\n",
            "Iteration 68, loss = 0.60885757\n",
            "Iteration 69, loss = 0.60700774\n",
            "Iteration 70, loss = 0.60512929\n",
            "Iteration 71, loss = 0.60324028\n",
            "Iteration 72, loss = 0.60148282\n",
            "Iteration 73, loss = 0.59946162\n",
            "Iteration 74, loss = 0.59755990\n",
            "Iteration 75, loss = 0.59551262\n",
            "Iteration 76, loss = 0.59355823\n",
            "Iteration 77, loss = 0.59146277\n",
            "Iteration 78, loss = 0.58934573\n",
            "Iteration 79, loss = 0.58737504\n",
            "Iteration 80, loss = 0.58509332\n",
            "Iteration 81, loss = 0.58284369\n",
            "Iteration 82, loss = 0.58058069\n",
            "Iteration 83, loss = 0.57826768\n",
            "Iteration 84, loss = 0.57596965\n",
            "Iteration 85, loss = 0.57357555\n",
            "Iteration 86, loss = 0.57098751\n",
            "Iteration 87, loss = 0.56854294\n",
            "Iteration 88, loss = 0.56604295\n",
            "Iteration 89, loss = 0.56336907\n",
            "Iteration 90, loss = 0.56073663\n",
            "Iteration 91, loss = 0.55813754\n",
            "Iteration 92, loss = 0.55540124\n",
            "Iteration 93, loss = 0.55253558\n",
            "Iteration 94, loss = 0.54993259\n",
            "Iteration 95, loss = 0.54686754\n",
            "Iteration 96, loss = 0.54399795\n",
            "Iteration 97, loss = 0.54111467\n",
            "Iteration 98, loss = 0.53809132\n",
            "Iteration 99, loss = 0.53515544\n",
            "Iteration 100, loss = 0.53227137\n",
            "Iteration 101, loss = 0.52942367\n",
            "Iteration 102, loss = 0.52661173\n",
            "Iteration 103, loss = 0.52366608\n",
            "Iteration 104, loss = 0.52079070\n",
            "Iteration 105, loss = 0.51795096\n",
            "Iteration 106, loss = 0.51514237\n",
            "Iteration 107, loss = 0.51234273\n",
            "Iteration 108, loss = 0.50953491\n",
            "Iteration 109, loss = 0.50667134\n",
            "Iteration 110, loss = 0.50388159\n",
            "Iteration 111, loss = 0.50120815\n",
            "Iteration 112, loss = 0.49854336\n",
            "Iteration 113, loss = 0.49586308\n",
            "Iteration 114, loss = 0.49325471\n",
            "Iteration 115, loss = 0.49059716\n",
            "Iteration 116, loss = 0.48840432\n",
            "Iteration 117, loss = 0.48581760\n",
            "Iteration 118, loss = 0.48323896\n",
            "Iteration 119, loss = 0.48091794\n",
            "Iteration 120, loss = 0.47853823\n",
            "Iteration 121, loss = 0.47623041\n",
            "Iteration 122, loss = 0.47396840\n",
            "Iteration 123, loss = 0.47179691\n",
            "Iteration 124, loss = 0.46966454\n",
            "Iteration 125, loss = 0.46776013\n",
            "Iteration 126, loss = 0.46572926\n",
            "Iteration 127, loss = 0.46374543\n",
            "Iteration 128, loss = 0.46165701\n",
            "Iteration 129, loss = 0.45984864\n",
            "Iteration 130, loss = 0.45816679\n",
            "Iteration 131, loss = 0.45645128\n",
            "Iteration 132, loss = 0.45485856\n",
            "Iteration 133, loss = 0.45331653\n",
            "Iteration 134, loss = 0.45177847\n",
            "Iteration 135, loss = 0.45035390\n",
            "Iteration 136, loss = 0.44883037\n",
            "Iteration 137, loss = 0.44755713\n",
            "Iteration 138, loss = 0.44633782\n",
            "Iteration 139, loss = 0.44516235\n",
            "Iteration 140, loss = 0.44406517\n",
            "Iteration 141, loss = 0.44296358\n",
            "Iteration 142, loss = 0.44202267\n",
            "Iteration 143, loss = 0.44107271\n",
            "Iteration 144, loss = 0.44016126\n",
            "Iteration 145, loss = 0.43928773\n",
            "Iteration 146, loss = 0.43866228\n",
            "Iteration 147, loss = 0.43773348\n",
            "Iteration 148, loss = 0.43723502\n",
            "Iteration 149, loss = 0.43656757\n",
            "Iteration 150, loss = 0.43596681\n",
            "Iteration 151, loss = 0.43525559\n",
            "Iteration 152, loss = 0.43463538\n",
            "Iteration 153, loss = 0.43442132\n",
            "Iteration 154, loss = 0.43388479\n",
            "Iteration 155, loss = 0.43324600\n",
            "Iteration 156, loss = 0.43273933\n",
            "Iteration 157, loss = 0.43210907\n",
            "Iteration 158, loss = 0.43179266\n",
            "Iteration 159, loss = 0.43128062\n",
            "Iteration 160, loss = 0.43076069\n",
            "Iteration 161, loss = 0.43027949\n",
            "Iteration 162, loss = 0.42987821\n",
            "Iteration 163, loss = 0.42958634\n",
            "Iteration 164, loss = 0.42896300\n",
            "Iteration 165, loss = 0.42871801\n",
            "Iteration 166, loss = 0.42817190\n",
            "Iteration 167, loss = 0.42792416\n",
            "Iteration 168, loss = 0.42740033\n",
            "Iteration 169, loss = 0.42710161\n",
            "Iteration 170, loss = 0.42681921\n",
            "Iteration 171, loss = 0.42642503\n",
            "Iteration 172, loss = 0.42619536\n",
            "Iteration 173, loss = 0.42580645\n",
            "Iteration 174, loss = 0.42545227\n",
            "Iteration 175, loss = 0.42513869\n",
            "Iteration 176, loss = 0.42475958\n",
            "Iteration 177, loss = 0.42452269\n",
            "Iteration 178, loss = 0.42417723\n",
            "Iteration 179, loss = 0.42392640\n",
            "Iteration 180, loss = 0.42377126\n",
            "Iteration 181, loss = 0.42323203\n",
            "Iteration 182, loss = 0.42299506\n",
            "Iteration 183, loss = 0.42268485\n",
            "Iteration 184, loss = 0.42235718\n",
            "Iteration 185, loss = 0.42216214\n",
            "Iteration 186, loss = 0.42173649\n",
            "Iteration 187, loss = 0.42140911\n",
            "Iteration 188, loss = 0.42112272\n",
            "Iteration 189, loss = 0.42081467\n",
            "Iteration 190, loss = 0.42059790\n",
            "Iteration 191, loss = 0.42036730\n",
            "Iteration 192, loss = 0.41996922\n",
            "Iteration 193, loss = 0.41954895\n",
            "Iteration 194, loss = 0.41940060\n",
            "Iteration 195, loss = 0.41929290\n",
            "Iteration 196, loss = 0.41911578\n",
            "Iteration 197, loss = 0.41878778\n",
            "Iteration 198, loss = 0.41834764\n",
            "Iteration 199, loss = 0.41805162\n",
            "Iteration 200, loss = 0.41784333\n",
            "Iteration 201, loss = 0.41763443\n",
            "Iteration 202, loss = 0.41718799\n",
            "Iteration 203, loss = 0.41709003\n",
            "Iteration 204, loss = 0.41709947\n",
            "Iteration 205, loss = 0.41669065\n",
            "Iteration 206, loss = 0.41629704\n",
            "Iteration 207, loss = 0.41585925\n",
            "Iteration 208, loss = 0.41570174\n",
            "Iteration 209, loss = 0.41516726\n",
            "Iteration 210, loss = 0.41486792\n",
            "Iteration 211, loss = 0.41457636\n",
            "Iteration 212, loss = 0.41459233\n",
            "Iteration 213, loss = 0.41413548\n",
            "Iteration 214, loss = 0.41379207\n",
            "Iteration 215, loss = 0.41354429\n",
            "Iteration 216, loss = 0.41315507\n",
            "Iteration 217, loss = 0.41309326\n",
            "Iteration 218, loss = 0.41249496\n",
            "Iteration 219, loss = 0.41227729\n",
            "Iteration 220, loss = 0.41193256\n",
            "Iteration 221, loss = 0.41167203\n",
            "Iteration 222, loss = 0.41141509\n",
            "Iteration 223, loss = 0.41104258\n",
            "Iteration 224, loss = 0.41078901\n",
            "Iteration 225, loss = 0.41028249\n",
            "Iteration 226, loss = 0.41011773\n",
            "Iteration 227, loss = 0.40985509\n",
            "Iteration 228, loss = 0.40950543\n",
            "Iteration 229, loss = 0.40917133\n",
            "Iteration 230, loss = 0.40895582\n",
            "Iteration 231, loss = 0.40867912\n",
            "Iteration 232, loss = 0.40826451\n",
            "Iteration 233, loss = 0.40800105\n",
            "Iteration 234, loss = 0.40779495\n",
            "Iteration 235, loss = 0.40753260\n",
            "Iteration 236, loss = 0.40733858\n",
            "Iteration 237, loss = 0.40695441\n",
            "Iteration 238, loss = 0.40656394\n",
            "Iteration 239, loss = 0.40614588\n",
            "Iteration 240, loss = 0.40599875\n",
            "Iteration 241, loss = 0.40584784\n",
            "Iteration 242, loss = 0.40539541\n",
            "Iteration 243, loss = 0.40513451\n",
            "Iteration 244, loss = 0.40480089\n",
            "Iteration 245, loss = 0.40446809\n",
            "Iteration 246, loss = 0.40430346\n",
            "Iteration 247, loss = 0.40402724\n",
            "Iteration 248, loss = 0.40379132\n",
            "Iteration 249, loss = 0.40349669\n",
            "Iteration 250, loss = 0.40312854\n",
            "Iteration 251, loss = 0.40287065\n",
            "Iteration 252, loss = 0.40272013\n",
            "Iteration 253, loss = 0.40248045\n",
            "Iteration 254, loss = 0.40207191\n",
            "Iteration 255, loss = 0.40181534\n",
            "Iteration 256, loss = 0.40150227\n",
            "Iteration 257, loss = 0.40134351\n",
            "Iteration 258, loss = 0.40104022\n",
            "Iteration 259, loss = 0.40084237\n",
            "Iteration 260, loss = 0.40054702\n",
            "Iteration 261, loss = 0.40033718\n",
            "Iteration 262, loss = 0.39997720\n",
            "Iteration 263, loss = 0.39979423\n",
            "Iteration 264, loss = 0.39937475\n",
            "Iteration 265, loss = 0.39915914\n",
            "Iteration 266, loss = 0.39905546\n",
            "Iteration 267, loss = 0.39860165\n",
            "Iteration 268, loss = 0.39827352\n",
            "Iteration 269, loss = 0.39821593\n",
            "Iteration 270, loss = 0.39803313\n",
            "Iteration 271, loss = 0.39759486\n",
            "Iteration 272, loss = 0.39748607\n",
            "Iteration 273, loss = 0.39717779\n",
            "Iteration 274, loss = 0.39690939\n",
            "Iteration 275, loss = 0.39665941\n",
            "Iteration 276, loss = 0.39652471\n",
            "Iteration 277, loss = 0.39618451\n",
            "Iteration 278, loss = 0.39586544\n",
            "Iteration 279, loss = 0.39568090\n",
            "Iteration 280, loss = 0.39551370\n",
            "Iteration 281, loss = 0.39533502\n",
            "Iteration 282, loss = 0.39512964\n",
            "Iteration 283, loss = 0.39478441\n",
            "Iteration 284, loss = 0.39450709\n",
            "Iteration 285, loss = 0.39436079\n",
            "Iteration 286, loss = 0.39410646\n",
            "Iteration 287, loss = 0.39391263\n",
            "Iteration 288, loss = 0.39370551\n",
            "Iteration 289, loss = 0.39345842\n",
            "Iteration 290, loss = 0.39330510\n",
            "Iteration 291, loss = 0.39307513\n",
            "Iteration 292, loss = 0.39285317\n",
            "Iteration 293, loss = 0.39265373\n",
            "Iteration 294, loss = 0.39240830\n",
            "Iteration 295, loss = 0.39206723\n",
            "Iteration 296, loss = 0.39188490\n",
            "Iteration 297, loss = 0.39186432\n",
            "Iteration 298, loss = 0.39165194\n",
            "Iteration 299, loss = 0.39135734\n",
            "Iteration 300, loss = 0.39104981\n",
            "Iteration 301, loss = 0.39095405\n",
            "Iteration 302, loss = 0.39069572\n",
            "Iteration 303, loss = 0.39097389\n",
            "Iteration 304, loss = 0.39054265\n",
            "Iteration 305, loss = 0.39008031\n",
            "Iteration 306, loss = 0.38963351\n",
            "Iteration 307, loss = 0.38945391\n",
            "Iteration 308, loss = 0.38980448\n",
            "Iteration 309, loss = 0.38926415\n",
            "Iteration 310, loss = 0.38920209\n",
            "Iteration 311, loss = 0.38870869\n",
            "Iteration 312, loss = 0.38860546\n",
            "Iteration 313, loss = 0.38831415\n",
            "Iteration 314, loss = 0.38811733\n",
            "Iteration 315, loss = 0.38797918\n",
            "Iteration 316, loss = 0.38785051\n",
            "Iteration 317, loss = 0.38753339\n",
            "Iteration 318, loss = 0.38724770\n",
            "Iteration 319, loss = 0.38716912\n",
            "Iteration 320, loss = 0.38696406\n",
            "Iteration 321, loss = 0.38678004\n",
            "Iteration 322, loss = 0.38650161\n",
            "Iteration 323, loss = 0.38633084\n",
            "Iteration 324, loss = 0.38639235\n",
            "Iteration 325, loss = 0.38601915\n",
            "Iteration 326, loss = 0.38569086\n",
            "Iteration 327, loss = 0.38570982\n",
            "Iteration 328, loss = 0.38549290\n",
            "Iteration 329, loss = 0.38537352\n",
            "Iteration 330, loss = 0.38519827\n",
            "Iteration 331, loss = 0.38486871\n",
            "Iteration 332, loss = 0.38465589\n",
            "Iteration 333, loss = 0.38453242\n",
            "Iteration 334, loss = 0.38438713\n",
            "Iteration 335, loss = 0.38412401\n",
            "Iteration 336, loss = 0.38402514\n",
            "Iteration 337, loss = 0.38376217\n",
            "Iteration 338, loss = 0.38374143\n",
            "Iteration 339, loss = 0.38340973\n",
            "Iteration 340, loss = 0.38330757\n",
            "Iteration 341, loss = 0.38304240\n",
            "Iteration 342, loss = 0.38284698\n",
            "Iteration 343, loss = 0.38281933\n",
            "Iteration 344, loss = 0.38236435\n",
            "Iteration 345, loss = 0.38244376\n",
            "Iteration 346, loss = 0.38221742\n",
            "Iteration 347, loss = 0.38200876\n",
            "Iteration 348, loss = 0.38176592\n",
            "Iteration 349, loss = 0.38179097\n",
            "Iteration 350, loss = 0.38146889\n",
            "Iteration 351, loss = 0.38132825\n",
            "Iteration 352, loss = 0.38108295\n",
            "Iteration 353, loss = 0.38100353\n",
            "Iteration 354, loss = 0.38085568\n",
            "Iteration 355, loss = 0.38106170\n",
            "Iteration 356, loss = 0.38065721\n",
            "Iteration 357, loss = 0.38032376\n",
            "Iteration 358, loss = 0.38020946\n",
            "Iteration 359, loss = 0.38025530\n",
            "Iteration 360, loss = 0.37999445\n",
            "Iteration 361, loss = 0.37960933\n",
            "Iteration 362, loss = 0.37957039\n",
            "Iteration 363, loss = 0.37947003\n",
            "Iteration 364, loss = 0.37964045\n",
            "Iteration 365, loss = 0.37946092\n",
            "Iteration 366, loss = 0.37915244\n",
            "Iteration 367, loss = 0.37909945\n",
            "Iteration 368, loss = 0.37869767\n",
            "Iteration 369, loss = 0.37858744\n",
            "Iteration 370, loss = 0.37843425\n",
            "Iteration 371, loss = 0.37828115\n",
            "Iteration 372, loss = 0.37810981\n",
            "Iteration 373, loss = 0.37804080\n",
            "Iteration 374, loss = 0.37793431\n",
            "Iteration 375, loss = 0.37774100\n",
            "Iteration 376, loss = 0.37757434\n",
            "Iteration 377, loss = 0.37746556\n",
            "Iteration 378, loss = 0.37738788\n",
            "Iteration 379, loss = 0.37744631\n",
            "Iteration 380, loss = 0.37724881\n",
            "Iteration 381, loss = 0.37707311\n",
            "Iteration 382, loss = 0.37690636\n",
            "Iteration 383, loss = 0.37668835\n",
            "Iteration 384, loss = 0.37652676\n",
            "Iteration 385, loss = 0.37626268\n",
            "Iteration 386, loss = 0.37647455\n",
            "Iteration 387, loss = 0.37612479\n",
            "Iteration 388, loss = 0.37614712\n",
            "Iteration 389, loss = 0.37583943\n",
            "Iteration 390, loss = 0.37570055\n",
            "Iteration 391, loss = 0.37584478\n",
            "Iteration 392, loss = 0.37569269\n",
            "Iteration 393, loss = 0.37533661\n",
            "Iteration 394, loss = 0.37515752\n",
            "Iteration 395, loss = 0.37511949\n",
            "Iteration 396, loss = 0.37502702\n",
            "Iteration 397, loss = 0.37498633\n",
            "Iteration 398, loss = 0.37510207\n",
            "Iteration 399, loss = 0.37496208\n",
            "Iteration 400, loss = 0.37463379\n",
            "Iteration 401, loss = 0.37462231\n",
            "Iteration 402, loss = 0.37450883\n",
            "Iteration 403, loss = 0.37432529\n",
            "Iteration 404, loss = 0.37425060\n",
            "Iteration 405, loss = 0.37416435\n",
            "Iteration 406, loss = 0.37399358\n",
            "Iteration 407, loss = 0.37385789\n",
            "Iteration 408, loss = 0.37367645\n",
            "Iteration 409, loss = 0.37358885\n",
            "Iteration 410, loss = 0.37349484\n",
            "Iteration 411, loss = 0.37348458\n",
            "Iteration 412, loss = 0.37331770\n",
            "Iteration 413, loss = 0.37321465\n",
            "Iteration 414, loss = 0.37300837\n",
            "Iteration 415, loss = 0.37309963\n",
            "Iteration 416, loss = 0.37288858\n",
            "Iteration 417, loss = 0.37284959\n",
            "Iteration 418, loss = 0.37269421\n",
            "Iteration 419, loss = 0.37263501\n",
            "Iteration 420, loss = 0.37239180\n",
            "Iteration 421, loss = 0.37223337\n",
            "Iteration 422, loss = 0.37214152\n",
            "Iteration 423, loss = 0.37202423\n",
            "Iteration 424, loss = 0.37191848\n",
            "Iteration 425, loss = 0.37184519\n",
            "Iteration 426, loss = 0.37173935\n",
            "Iteration 427, loss = 0.37160200\n",
            "Iteration 428, loss = 0.37182522\n",
            "Iteration 429, loss = 0.37154917\n",
            "Iteration 430, loss = 0.37131625\n",
            "Iteration 431, loss = 0.37128930\n",
            "Iteration 432, loss = 0.37132355\n",
            "Iteration 433, loss = 0.37116394\n",
            "Iteration 434, loss = 0.37098437\n",
            "Iteration 435, loss = 0.37076405\n",
            "Iteration 436, loss = 0.37095060\n",
            "Iteration 437, loss = 0.37104700\n",
            "Iteration 438, loss = 0.37095325\n",
            "Iteration 439, loss = 0.37076853\n",
            "Iteration 440, loss = 0.37039444\n",
            "Iteration 441, loss = 0.37034607\n",
            "Iteration 442, loss = 0.37026767\n",
            "Iteration 443, loss = 0.37017036\n",
            "Iteration 444, loss = 0.37003955\n",
            "Iteration 445, loss = 0.36994875\n",
            "Iteration 446, loss = 0.36982110\n",
            "Iteration 447, loss = 0.36986889\n",
            "Iteration 448, loss = 0.36974121\n",
            "Iteration 449, loss = 0.36971837\n",
            "Iteration 450, loss = 0.36983006\n",
            "Iteration 451, loss = 0.36972069\n",
            "Iteration 452, loss = 0.36937505\n",
            "Iteration 453, loss = 0.36919273\n",
            "Iteration 454, loss = 0.36920392\n",
            "Iteration 455, loss = 0.36920769\n",
            "Iteration 456, loss = 0.36922082\n",
            "Iteration 457, loss = 0.36911199\n",
            "Iteration 458, loss = 0.36889749\n",
            "Iteration 459, loss = 0.36879856\n",
            "Iteration 460, loss = 0.36894219\n",
            "Iteration 461, loss = 0.36862439\n",
            "Iteration 462, loss = 0.36853275\n",
            "Iteration 463, loss = 0.36844312\n",
            "Iteration 464, loss = 0.36838265\n",
            "Iteration 465, loss = 0.36838903\n",
            "Iteration 466, loss = 0.36817345\n",
            "Iteration 467, loss = 0.36841416\n",
            "Iteration 468, loss = 0.36810627\n",
            "Iteration 469, loss = 0.36799419\n",
            "Iteration 470, loss = 0.36789484\n",
            "Iteration 471, loss = 0.36787295\n",
            "Iteration 472, loss = 0.36774810\n",
            "Iteration 473, loss = 0.36778423\n",
            "Iteration 474, loss = 0.36766794\n",
            "Iteration 475, loss = 0.36755850\n",
            "Iteration 476, loss = 0.36745598\n",
            "Iteration 477, loss = 0.36750154\n",
            "Iteration 478, loss = 0.36745262\n",
            "Iteration 479, loss = 0.36732538\n",
            "Iteration 480, loss = 0.36716159\n",
            "Iteration 481, loss = 0.36707259\n",
            "Iteration 482, loss = 0.36709430\n",
            "Iteration 483, loss = 0.36703610\n",
            "Iteration 484, loss = 0.36695218\n",
            "Iteration 485, loss = 0.36689074\n",
            "Iteration 486, loss = 0.36694518\n",
            "Iteration 487, loss = 0.36675178\n",
            "Iteration 488, loss = 0.36683617\n",
            "Iteration 489, loss = 0.36673876\n",
            "Iteration 490, loss = 0.36659264\n",
            "Iteration 491, loss = 0.36653090\n",
            "Iteration 492, loss = 0.36644490\n",
            "Iteration 493, loss = 0.36628975\n",
            "Iteration 494, loss = 0.36631167\n",
            "Iteration 495, loss = 0.36640718\n",
            "Iteration 496, loss = 0.36649241\n",
            "Iteration 497, loss = 0.36619036\n",
            "Iteration 498, loss = 0.36601496\n",
            "Iteration 499, loss = 0.36596798\n",
            "Iteration 500, loss = 0.36591929\n",
            "Iteration 501, loss = 0.36603792\n",
            "Iteration 502, loss = 0.36591533\n",
            "Iteration 503, loss = 0.36573875\n",
            "Iteration 504, loss = 0.36573381\n",
            "Iteration 505, loss = 0.36563403\n",
            "Iteration 506, loss = 0.36569247\n",
            "Iteration 507, loss = 0.36546927\n",
            "Iteration 508, loss = 0.36563148\n",
            "Iteration 509, loss = 0.36556484\n",
            "Iteration 510, loss = 0.36524532\n",
            "Iteration 511, loss = 0.36535080\n",
            "Iteration 512, loss = 0.36550057\n",
            "Iteration 513, loss = 0.36534408\n",
            "Iteration 514, loss = 0.36571781\n",
            "Iteration 515, loss = 0.36518220\n",
            "Iteration 516, loss = 0.36509005\n",
            "Iteration 517, loss = 0.36500167\n",
            "Iteration 518, loss = 0.36497338\n",
            "Iteration 519, loss = 0.36493240\n",
            "Iteration 520, loss = 0.36482638\n",
            "Iteration 521, loss = 0.36479515\n",
            "Iteration 522, loss = 0.36464808\n",
            "Iteration 523, loss = 0.36468346\n",
            "Iteration 524, loss = 0.36461635\n",
            "Iteration 525, loss = 0.36453250\n",
            "Iteration 526, loss = 0.36452642\n",
            "Iteration 527, loss = 0.36466685\n",
            "Iteration 528, loss = 0.36436623\n",
            "Iteration 529, loss = 0.36426497\n",
            "Iteration 530, loss = 0.36420208\n",
            "Iteration 531, loss = 0.36423601\n",
            "Iteration 532, loss = 0.36421952\n",
            "Iteration 533, loss = 0.36403963\n",
            "Iteration 534, loss = 0.36401999\n",
            "Iteration 535, loss = 0.36394887\n",
            "Iteration 536, loss = 0.36395485\n",
            "Iteration 537, loss = 0.36388832\n",
            "Iteration 538, loss = 0.36398785\n",
            "Iteration 539, loss = 0.36378197\n",
            "Iteration 540, loss = 0.36366088\n",
            "Iteration 541, loss = 0.36374376\n",
            "Iteration 542, loss = 0.36366529\n",
            "Iteration 543, loss = 0.36368497\n",
            "Iteration 544, loss = 0.36367193\n",
            "Iteration 545, loss = 0.36362493\n",
            "Iteration 546, loss = 0.36335668\n",
            "Iteration 547, loss = 0.36330108\n",
            "Iteration 548, loss = 0.36353620\n",
            "Iteration 549, loss = 0.36336700\n",
            "Iteration 550, loss = 0.36351317\n",
            "Iteration 551, loss = 0.36336045\n",
            "Iteration 552, loss = 0.36316830\n",
            "Iteration 553, loss = 0.36303104\n",
            "Iteration 554, loss = 0.36342305\n",
            "Iteration 555, loss = 0.36327116\n",
            "Iteration 556, loss = 0.36311986\n",
            "Iteration 557, loss = 0.36306219\n",
            "Iteration 558, loss = 0.36291240\n",
            "Iteration 559, loss = 0.36288871\n",
            "Iteration 560, loss = 0.36274710\n",
            "Iteration 561, loss = 0.36271716\n",
            "Iteration 562, loss = 0.36281027\n",
            "Iteration 563, loss = 0.36281579\n",
            "Iteration 564, loss = 0.36268511\n",
            "Iteration 565, loss = 0.36244658\n",
            "Iteration 566, loss = 0.36260140\n",
            "Iteration 567, loss = 0.36252705\n",
            "Iteration 568, loss = 0.36251683\n",
            "Iteration 569, loss = 0.36230929\n",
            "Iteration 570, loss = 0.36240583\n",
            "Iteration 571, loss = 0.36231286\n",
            "Iteration 572, loss = 0.36249428\n",
            "Iteration 573, loss = 0.36223353\n",
            "Iteration 574, loss = 0.36223038\n",
            "Iteration 575, loss = 0.36219588\n",
            "Iteration 576, loss = 0.36205622\n",
            "Iteration 577, loss = 0.36202936\n",
            "Iteration 578, loss = 0.36206298\n",
            "Iteration 579, loss = 0.36186705\n",
            "Iteration 580, loss = 0.36214080\n",
            "Iteration 581, loss = 0.36183334\n",
            "Iteration 582, loss = 0.36187849\n",
            "Iteration 583, loss = 0.36187181\n",
            "Iteration 584, loss = 0.36185437\n",
            "Iteration 585, loss = 0.36172902\n",
            "Iteration 586, loss = 0.36193470\n",
            "Iteration 587, loss = 0.36160350\n",
            "Iteration 588, loss = 0.36159573\n",
            "Iteration 589, loss = 0.36184176\n",
            "Iteration 590, loss = 0.36137987\n",
            "Iteration 591, loss = 0.36140927\n",
            "Iteration 592, loss = 0.36141491\n",
            "Iteration 593, loss = 0.36169598\n",
            "Iteration 594, loss = 0.36136967\n",
            "Iteration 595, loss = 0.36131222\n",
            "Iteration 596, loss = 0.36126181\n",
            "Iteration 597, loss = 0.36117276\n",
            "Iteration 598, loss = 0.36110409\n",
            "Iteration 599, loss = 0.36105394\n",
            "Iteration 600, loss = 0.36132306\n",
            "Iteration 601, loss = 0.36113830\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.75775441\n",
            "Iteration 2, loss = 0.75243319\n",
            "Iteration 3, loss = 0.74755596\n",
            "Iteration 4, loss = 0.74297823\n",
            "Iteration 5, loss = 0.73815897\n",
            "Iteration 6, loss = 0.73416311\n",
            "Iteration 7, loss = 0.73015614\n",
            "Iteration 8, loss = 0.72625260\n",
            "Iteration 9, loss = 0.72275917\n",
            "Iteration 10, loss = 0.71901404\n",
            "Iteration 11, loss = 0.71595904\n",
            "Iteration 12, loss = 0.71299836\n",
            "Iteration 13, loss = 0.70993720\n",
            "Iteration 14, loss = 0.70713516\n",
            "Iteration 15, loss = 0.70464491\n",
            "Iteration 16, loss = 0.70191857\n",
            "Iteration 17, loss = 0.69963407\n",
            "Iteration 18, loss = 0.69741324\n",
            "Iteration 19, loss = 0.69514889\n",
            "Iteration 20, loss = 0.69315202\n",
            "Iteration 21, loss = 0.69123284\n",
            "Iteration 22, loss = 0.68915827\n",
            "Iteration 23, loss = 0.68738344\n",
            "Iteration 24, loss = 0.68572038\n",
            "Iteration 25, loss = 0.68388599\n",
            "Iteration 26, loss = 0.68221573\n",
            "Iteration 27, loss = 0.68048597\n",
            "Iteration 28, loss = 0.67877032\n",
            "Iteration 29, loss = 0.67713401\n",
            "Iteration 30, loss = 0.67546863\n",
            "Iteration 31, loss = 0.67382019\n",
            "Iteration 32, loss = 0.67217660\n",
            "Iteration 33, loss = 0.67042438\n",
            "Iteration 34, loss = 0.66885723\n",
            "Iteration 35, loss = 0.66730639\n",
            "Iteration 36, loss = 0.66549610\n",
            "Iteration 37, loss = 0.66388493\n",
            "Iteration 38, loss = 0.66222263\n",
            "Iteration 39, loss = 0.66069245\n",
            "Iteration 40, loss = 0.65889109\n",
            "Iteration 41, loss = 0.65727268\n",
            "Iteration 42, loss = 0.65560200\n",
            "Iteration 43, loss = 0.65386598\n",
            "Iteration 44, loss = 0.65217357\n",
            "Iteration 45, loss = 0.65044596\n",
            "Iteration 46, loss = 0.64874690\n",
            "Iteration 47, loss = 0.64700074\n",
            "Iteration 48, loss = 0.64522705\n",
            "Iteration 49, loss = 0.64344682\n",
            "Iteration 50, loss = 0.64168966\n",
            "Iteration 51, loss = 0.63996914\n",
            "Iteration 52, loss = 0.63817945\n",
            "Iteration 53, loss = 0.63642003\n",
            "Iteration 54, loss = 0.63466680\n",
            "Iteration 55, loss = 0.63271936\n",
            "Iteration 56, loss = 0.63097261\n",
            "Iteration 57, loss = 0.62905551\n",
            "Iteration 58, loss = 0.62723813\n",
            "Iteration 59, loss = 0.62531559\n",
            "Iteration 60, loss = 0.62340245\n",
            "Iteration 61, loss = 0.62152098\n",
            "Iteration 62, loss = 0.61967004\n",
            "Iteration 63, loss = 0.61762063\n",
            "Iteration 64, loss = 0.61575842\n",
            "Iteration 65, loss = 0.61383621\n",
            "Iteration 66, loss = 0.61173805\n",
            "Iteration 67, loss = 0.60997245\n",
            "Iteration 68, loss = 0.60779381\n",
            "Iteration 69, loss = 0.60583214\n",
            "Iteration 70, loss = 0.60374580\n",
            "Iteration 71, loss = 0.60163866\n",
            "Iteration 72, loss = 0.59966571\n",
            "Iteration 73, loss = 0.59750662\n",
            "Iteration 74, loss = 0.59528949\n",
            "Iteration 75, loss = 0.59313778\n",
            "Iteration 76, loss = 0.59097556\n",
            "Iteration 77, loss = 0.58873114\n",
            "Iteration 78, loss = 0.58634454\n",
            "Iteration 79, loss = 0.58419775\n",
            "Iteration 80, loss = 0.58183084\n",
            "Iteration 81, loss = 0.57934689\n",
            "Iteration 82, loss = 0.57696989\n",
            "Iteration 83, loss = 0.57449515\n",
            "Iteration 84, loss = 0.57212327\n",
            "Iteration 85, loss = 0.56959007\n",
            "Iteration 86, loss = 0.56689814\n",
            "Iteration 87, loss = 0.56420756\n",
            "Iteration 88, loss = 0.56167700\n",
            "Iteration 89, loss = 0.55892367\n",
            "Iteration 90, loss = 0.55630738\n",
            "Iteration 91, loss = 0.55359331\n",
            "Iteration 92, loss = 0.55082783\n",
            "Iteration 93, loss = 0.54798011\n",
            "Iteration 94, loss = 0.54535255\n",
            "Iteration 95, loss = 0.54233338\n",
            "Iteration 96, loss = 0.53952895\n",
            "Iteration 97, loss = 0.53675512\n",
            "Iteration 98, loss = 0.53382467\n",
            "Iteration 99, loss = 0.53098228\n",
            "Iteration 100, loss = 0.52823432\n",
            "Iteration 101, loss = 0.52553160\n",
            "Iteration 102, loss = 0.52273950\n",
            "Iteration 103, loss = 0.51980486\n",
            "Iteration 104, loss = 0.51716745\n",
            "Iteration 105, loss = 0.51440409\n",
            "Iteration 106, loss = 0.51165515\n",
            "Iteration 107, loss = 0.50889971\n",
            "Iteration 108, loss = 0.50630421\n",
            "Iteration 109, loss = 0.50366709\n",
            "Iteration 110, loss = 0.50099868\n",
            "Iteration 111, loss = 0.49842258\n",
            "Iteration 112, loss = 0.49594701\n",
            "Iteration 113, loss = 0.49339031\n",
            "Iteration 114, loss = 0.49102036\n",
            "Iteration 115, loss = 0.48842099\n",
            "Iteration 116, loss = 0.48617745\n",
            "Iteration 117, loss = 0.48382533\n",
            "Iteration 118, loss = 0.48131578\n",
            "Iteration 119, loss = 0.47929971\n",
            "Iteration 120, loss = 0.47716317\n",
            "Iteration 121, loss = 0.47506929\n",
            "Iteration 122, loss = 0.47283125\n",
            "Iteration 123, loss = 0.47071701\n",
            "Iteration 124, loss = 0.46891169\n",
            "Iteration 125, loss = 0.46706329\n",
            "Iteration 126, loss = 0.46529456\n",
            "Iteration 127, loss = 0.46335648\n",
            "Iteration 128, loss = 0.46165091\n",
            "Iteration 129, loss = 0.45998577\n",
            "Iteration 130, loss = 0.45847807\n",
            "Iteration 131, loss = 0.45707797\n",
            "Iteration 132, loss = 0.45566218\n",
            "Iteration 133, loss = 0.45427286\n",
            "Iteration 134, loss = 0.45309202\n",
            "Iteration 135, loss = 0.45168669\n",
            "Iteration 136, loss = 0.45062935\n",
            "Iteration 137, loss = 0.44936977\n",
            "Iteration 138, loss = 0.44842834\n",
            "Iteration 139, loss = 0.44733582\n",
            "Iteration 140, loss = 0.44629093\n",
            "Iteration 141, loss = 0.44546860\n",
            "Iteration 142, loss = 0.44467088\n",
            "Iteration 143, loss = 0.44393659\n",
            "Iteration 144, loss = 0.44303001\n",
            "Iteration 145, loss = 0.44228344\n",
            "Iteration 146, loss = 0.44177255\n",
            "Iteration 147, loss = 0.44100423\n",
            "Iteration 148, loss = 0.44051086\n",
            "Iteration 149, loss = 0.43978707\n",
            "Iteration 150, loss = 0.43924049\n",
            "Iteration 151, loss = 0.43865340\n",
            "Iteration 152, loss = 0.43811186\n",
            "Iteration 153, loss = 0.43767764\n",
            "Iteration 154, loss = 0.43713466\n",
            "Iteration 155, loss = 0.43666986\n",
            "Iteration 156, loss = 0.43613272\n",
            "Iteration 157, loss = 0.43562746\n",
            "Iteration 158, loss = 0.43516609\n",
            "Iteration 159, loss = 0.43478809\n",
            "Iteration 160, loss = 0.43425693\n",
            "Iteration 161, loss = 0.43370725\n",
            "Iteration 162, loss = 0.43327199\n",
            "Iteration 163, loss = 0.43289668\n",
            "Iteration 164, loss = 0.43237702\n",
            "Iteration 165, loss = 0.43215449\n",
            "Iteration 166, loss = 0.43172685\n",
            "Iteration 167, loss = 0.43122827\n",
            "Iteration 168, loss = 0.43075805\n",
            "Iteration 169, loss = 0.43040496\n",
            "Iteration 170, loss = 0.42997758\n",
            "Iteration 171, loss = 0.42956404\n",
            "Iteration 172, loss = 0.42917291\n",
            "Iteration 173, loss = 0.42876734\n",
            "Iteration 174, loss = 0.42836567\n",
            "Iteration 175, loss = 0.42802979\n",
            "Iteration 176, loss = 0.42753134\n",
            "Iteration 177, loss = 0.42713083\n",
            "Iteration 178, loss = 0.42669955\n",
            "Iteration 179, loss = 0.42635622\n",
            "Iteration 180, loss = 0.42600568\n",
            "Iteration 181, loss = 0.42547500\n",
            "Iteration 182, loss = 0.42504622\n",
            "Iteration 183, loss = 0.42458578\n",
            "Iteration 184, loss = 0.42412306\n",
            "Iteration 185, loss = 0.42389544\n",
            "Iteration 186, loss = 0.42311642\n",
            "Iteration 187, loss = 0.42286213\n",
            "Iteration 188, loss = 0.42267818\n",
            "Iteration 189, loss = 0.42202157\n",
            "Iteration 190, loss = 0.42153513\n",
            "Iteration 191, loss = 0.42119108\n",
            "Iteration 192, loss = 0.42074959\n",
            "Iteration 193, loss = 0.42006962\n",
            "Iteration 194, loss = 0.41954627\n",
            "Iteration 195, loss = 0.41901224\n",
            "Iteration 196, loss = 0.41898050\n",
            "Iteration 197, loss = 0.41829542\n",
            "Iteration 198, loss = 0.41782643\n",
            "Iteration 199, loss = 0.41727848\n",
            "Iteration 200, loss = 0.41696524\n",
            "Iteration 201, loss = 0.41645002\n",
            "Iteration 202, loss = 0.41587922\n",
            "Iteration 203, loss = 0.41570762\n",
            "Iteration 204, loss = 0.41532069\n",
            "Iteration 205, loss = 0.41475990\n",
            "Iteration 206, loss = 0.41412969\n",
            "Iteration 207, loss = 0.41363051\n",
            "Iteration 208, loss = 0.41337796\n",
            "Iteration 209, loss = 0.41282195\n",
            "Iteration 210, loss = 0.41239031\n",
            "Iteration 211, loss = 0.41189547\n",
            "Iteration 212, loss = 0.41178243\n",
            "Iteration 213, loss = 0.41122208\n",
            "Iteration 214, loss = 0.41079374\n",
            "Iteration 215, loss = 0.41038847\n",
            "Iteration 216, loss = 0.40998945\n",
            "Iteration 217, loss = 0.40977026\n",
            "Iteration 218, loss = 0.40925401\n",
            "Iteration 219, loss = 0.40899540\n",
            "Iteration 220, loss = 0.40864896\n",
            "Iteration 221, loss = 0.40815837\n",
            "Iteration 222, loss = 0.40779421\n",
            "Iteration 223, loss = 0.40747358\n",
            "Iteration 224, loss = 0.40709912\n",
            "Iteration 225, loss = 0.40672093\n",
            "Iteration 226, loss = 0.40634438\n",
            "Iteration 227, loss = 0.40609502\n",
            "Iteration 228, loss = 0.40559849\n",
            "Iteration 229, loss = 0.40532670\n",
            "Iteration 230, loss = 0.40507164\n",
            "Iteration 231, loss = 0.40454763\n",
            "Iteration 232, loss = 0.40429171\n",
            "Iteration 233, loss = 0.40424026\n",
            "Iteration 234, loss = 0.40386101\n",
            "Iteration 235, loss = 0.40332283\n",
            "Iteration 236, loss = 0.40323986\n",
            "Iteration 237, loss = 0.40300281\n",
            "Iteration 238, loss = 0.40251322\n",
            "Iteration 239, loss = 0.40192163\n",
            "Iteration 240, loss = 0.40231351\n",
            "Iteration 241, loss = 0.40184847\n",
            "Iteration 242, loss = 0.40147315\n",
            "Iteration 243, loss = 0.40099117\n",
            "Iteration 244, loss = 0.40074298\n",
            "Iteration 245, loss = 0.40048792\n",
            "Iteration 246, loss = 0.40032779\n",
            "Iteration 247, loss = 0.40001090\n",
            "Iteration 248, loss = 0.39967072\n",
            "Iteration 249, loss = 0.39930393\n",
            "Iteration 250, loss = 0.39912403\n",
            "Iteration 251, loss = 0.39898676\n",
            "Iteration 252, loss = 0.39859586\n",
            "Iteration 253, loss = 0.39837422\n",
            "Iteration 254, loss = 0.39799497\n",
            "Iteration 255, loss = 0.39769622\n",
            "Iteration 256, loss = 0.39762218\n",
            "Iteration 257, loss = 0.39725461\n",
            "Iteration 258, loss = 0.39703363\n",
            "Iteration 259, loss = 0.39686199\n",
            "Iteration 260, loss = 0.39675721\n",
            "Iteration 261, loss = 0.39631386\n",
            "Iteration 262, loss = 0.39612610\n",
            "Iteration 263, loss = 0.39582736\n",
            "Iteration 264, loss = 0.39555020\n",
            "Iteration 265, loss = 0.39530623\n",
            "Iteration 266, loss = 0.39513102\n",
            "Iteration 267, loss = 0.39484775\n",
            "Iteration 268, loss = 0.39446703\n",
            "Iteration 269, loss = 0.39438880\n",
            "Iteration 270, loss = 0.39424151\n",
            "Iteration 271, loss = 0.39389927\n",
            "Iteration 272, loss = 0.39374042\n",
            "Iteration 273, loss = 0.39343917\n",
            "Iteration 274, loss = 0.39324501\n",
            "Iteration 275, loss = 0.39294663\n",
            "Iteration 276, loss = 0.39281896\n",
            "Iteration 277, loss = 0.39257565\n",
            "Iteration 278, loss = 0.39230216\n",
            "Iteration 279, loss = 0.39213691\n",
            "Iteration 280, loss = 0.39194063\n",
            "Iteration 281, loss = 0.39169938\n",
            "Iteration 282, loss = 0.39181965\n",
            "Iteration 283, loss = 0.39143052\n",
            "Iteration 284, loss = 0.39105377\n",
            "Iteration 285, loss = 0.39094776\n",
            "Iteration 286, loss = 0.39073855\n",
            "Iteration 287, loss = 0.39051886\n",
            "Iteration 288, loss = 0.39035613\n",
            "Iteration 289, loss = 0.39018365\n",
            "Iteration 290, loss = 0.38998509\n",
            "Iteration 291, loss = 0.38969925\n",
            "Iteration 292, loss = 0.38955513\n",
            "Iteration 293, loss = 0.38956787\n",
            "Iteration 294, loss = 0.38917415\n",
            "Iteration 295, loss = 0.38889759\n",
            "Iteration 296, loss = 0.38887967\n",
            "Iteration 297, loss = 0.38875644\n",
            "Iteration 298, loss = 0.38854260\n",
            "Iteration 299, loss = 0.38839606\n",
            "Iteration 300, loss = 0.38814826\n",
            "Iteration 301, loss = 0.38820448\n",
            "Iteration 302, loss = 0.38796889\n",
            "Iteration 303, loss = 0.38773423\n",
            "Iteration 304, loss = 0.38736030\n",
            "Iteration 305, loss = 0.38714409\n",
            "Iteration 306, loss = 0.38689655\n",
            "Iteration 307, loss = 0.38688715\n",
            "Iteration 308, loss = 0.38715927\n",
            "Iteration 309, loss = 0.38672759\n",
            "Iteration 310, loss = 0.38640534\n",
            "Iteration 311, loss = 0.38608358\n",
            "Iteration 312, loss = 0.38602004\n",
            "Iteration 313, loss = 0.38587992\n",
            "Iteration 314, loss = 0.38570889\n",
            "Iteration 315, loss = 0.38552486\n",
            "Iteration 316, loss = 0.38537704\n",
            "Iteration 317, loss = 0.38517362\n",
            "Iteration 318, loss = 0.38501637\n",
            "Iteration 319, loss = 0.38476051\n",
            "Iteration 320, loss = 0.38468379\n",
            "Iteration 321, loss = 0.38448975\n",
            "Iteration 322, loss = 0.38433819\n",
            "Iteration 323, loss = 0.38406740\n",
            "Iteration 324, loss = 0.38410427\n",
            "Iteration 325, loss = 0.38399967\n",
            "Iteration 326, loss = 0.38370686\n",
            "Iteration 327, loss = 0.38364859\n",
            "Iteration 328, loss = 0.38341009\n",
            "Iteration 329, loss = 0.38338004\n",
            "Iteration 330, loss = 0.38318731\n",
            "Iteration 331, loss = 0.38303318\n",
            "Iteration 332, loss = 0.38283432\n",
            "Iteration 333, loss = 0.38285000\n",
            "Iteration 334, loss = 0.38265255\n",
            "Iteration 335, loss = 0.38241604\n",
            "Iteration 336, loss = 0.38234899\n",
            "Iteration 337, loss = 0.38214153\n",
            "Iteration 338, loss = 0.38206390\n",
            "Iteration 339, loss = 0.38185461\n",
            "Iteration 340, loss = 0.38192880\n",
            "Iteration 341, loss = 0.38158867\n",
            "Iteration 342, loss = 0.38139982\n",
            "Iteration 343, loss = 0.38128750\n",
            "Iteration 344, loss = 0.38102805\n",
            "Iteration 345, loss = 0.38122186\n",
            "Iteration 346, loss = 0.38104960\n",
            "Iteration 347, loss = 0.38078406\n",
            "Iteration 348, loss = 0.38064125\n",
            "Iteration 349, loss = 0.38054335\n",
            "Iteration 350, loss = 0.38042236\n",
            "Iteration 351, loss = 0.38030473\n",
            "Iteration 352, loss = 0.38027903\n",
            "Iteration 353, loss = 0.38000215\n",
            "Iteration 354, loss = 0.37982132\n",
            "Iteration 355, loss = 0.37993582\n",
            "Iteration 356, loss = 0.37973652\n",
            "Iteration 357, loss = 0.37951751\n",
            "Iteration 358, loss = 0.37945299\n",
            "Iteration 359, loss = 0.37942404\n",
            "Iteration 360, loss = 0.37932529\n",
            "Iteration 361, loss = 0.37904880\n",
            "Iteration 362, loss = 0.37907114\n",
            "Iteration 363, loss = 0.37885030\n",
            "Iteration 364, loss = 0.37886664\n",
            "Iteration 365, loss = 0.37876618\n",
            "Iteration 366, loss = 0.37853055\n",
            "Iteration 367, loss = 0.37866681\n",
            "Iteration 368, loss = 0.37829862\n",
            "Iteration 369, loss = 0.37822546\n",
            "Iteration 370, loss = 0.37798493\n",
            "Iteration 371, loss = 0.37806950\n",
            "Iteration 372, loss = 0.37773337\n",
            "Iteration 373, loss = 0.37769943\n",
            "Iteration 374, loss = 0.37759882\n",
            "Iteration 375, loss = 0.37755632\n",
            "Iteration 376, loss = 0.37757971\n",
            "Iteration 377, loss = 0.37752316\n",
            "Iteration 378, loss = 0.37740093\n",
            "Iteration 379, loss = 0.37712646\n",
            "Iteration 380, loss = 0.37700758\n",
            "Iteration 381, loss = 0.37694299\n",
            "Iteration 382, loss = 0.37681516\n",
            "Iteration 383, loss = 0.37668831\n",
            "Iteration 384, loss = 0.37694825\n",
            "Iteration 385, loss = 0.37644819\n",
            "Iteration 386, loss = 0.37643820\n",
            "Iteration 387, loss = 0.37643425\n",
            "Iteration 388, loss = 0.37658914\n",
            "Iteration 389, loss = 0.37617940\n",
            "Iteration 390, loss = 0.37603819\n",
            "Iteration 391, loss = 0.37615588\n",
            "Iteration 392, loss = 0.37602511\n",
            "Iteration 393, loss = 0.37577572\n",
            "Iteration 394, loss = 0.37552126\n",
            "Iteration 395, loss = 0.37561016\n",
            "Iteration 396, loss = 0.37551785\n",
            "Iteration 397, loss = 0.37561492\n",
            "Iteration 398, loss = 0.37558379\n",
            "Iteration 399, loss = 0.37557297\n",
            "Iteration 400, loss = 0.37536811\n",
            "Iteration 401, loss = 0.37520904\n",
            "Iteration 402, loss = 0.37513946\n",
            "Iteration 403, loss = 0.37497830\n",
            "Iteration 404, loss = 0.37490658\n",
            "Iteration 405, loss = 0.37483761\n",
            "Iteration 406, loss = 0.37471615\n",
            "Iteration 407, loss = 0.37467207\n",
            "Iteration 408, loss = 0.37458062\n",
            "Iteration 409, loss = 0.37457774\n",
            "Iteration 410, loss = 0.37432362\n",
            "Iteration 411, loss = 0.37464855\n",
            "Iteration 412, loss = 0.37440566\n",
            "Iteration 413, loss = 0.37445371\n",
            "Iteration 414, loss = 0.37408415\n",
            "Iteration 415, loss = 0.37403779\n",
            "Iteration 416, loss = 0.37385151\n",
            "Iteration 417, loss = 0.37389413\n",
            "Iteration 418, loss = 0.37395406\n",
            "Iteration 419, loss = 0.37396731\n",
            "Iteration 420, loss = 0.37372685\n",
            "Iteration 421, loss = 0.37362856\n",
            "Iteration 422, loss = 0.37346445\n",
            "Iteration 423, loss = 0.37343436\n",
            "Iteration 424, loss = 0.37325637\n",
            "Iteration 425, loss = 0.37326933\n",
            "Iteration 426, loss = 0.37325173\n",
            "Iteration 427, loss = 0.37314795\n",
            "Iteration 428, loss = 0.37319366\n",
            "Iteration 429, loss = 0.37292708\n",
            "Iteration 430, loss = 0.37281924\n",
            "Iteration 431, loss = 0.37324860\n",
            "Iteration 432, loss = 0.37307251\n",
            "Iteration 433, loss = 0.37283299\n",
            "Iteration 434, loss = 0.37265649\n",
            "Iteration 435, loss = 0.37234174\n",
            "Iteration 436, loss = 0.37249712\n",
            "Iteration 437, loss = 0.37258644\n",
            "Iteration 438, loss = 0.37256408\n",
            "Iteration 439, loss = 0.37240264\n",
            "Iteration 440, loss = 0.37208523\n",
            "Iteration 441, loss = 0.37202773\n",
            "Iteration 442, loss = 0.37200754\n",
            "Iteration 443, loss = 0.37199732\n",
            "Iteration 444, loss = 0.37183676\n",
            "Iteration 445, loss = 0.37174773\n",
            "Iteration 446, loss = 0.37172378\n",
            "Iteration 447, loss = 0.37161781\n",
            "Iteration 448, loss = 0.37164010\n",
            "Iteration 449, loss = 0.37163287\n",
            "Iteration 450, loss = 0.37169560\n",
            "Iteration 451, loss = 0.37153692\n",
            "Iteration 452, loss = 0.37148732\n",
            "Iteration 453, loss = 0.37127983\n",
            "Iteration 454, loss = 0.37118394\n",
            "Iteration 455, loss = 0.37120059\n",
            "Iteration 456, loss = 0.37118906\n",
            "Iteration 457, loss = 0.37117926\n",
            "Iteration 458, loss = 0.37112378\n",
            "Iteration 459, loss = 0.37101292\n",
            "Iteration 460, loss = 0.37138769\n",
            "Iteration 461, loss = 0.37075924\n",
            "Iteration 462, loss = 0.37075149\n",
            "Iteration 463, loss = 0.37062399\n",
            "Iteration 464, loss = 0.37066864\n",
            "Iteration 465, loss = 0.37056873\n",
            "Iteration 466, loss = 0.37059289\n",
            "Iteration 467, loss = 0.37050713\n",
            "Iteration 468, loss = 0.37041962\n",
            "Iteration 469, loss = 0.37037592\n",
            "Iteration 470, loss = 0.37032125\n",
            "Iteration 471, loss = 0.37036465\n",
            "Iteration 472, loss = 0.37026273\n",
            "Iteration 473, loss = 0.37034263\n",
            "Iteration 474, loss = 0.37014440\n",
            "Iteration 475, loss = 0.36999281\n",
            "Iteration 476, loss = 0.36986443\n",
            "Iteration 477, loss = 0.37008731\n",
            "Iteration 478, loss = 0.37020902\n",
            "Iteration 479, loss = 0.37007104\n",
            "Iteration 480, loss = 0.36972098\n",
            "Iteration 481, loss = 0.36967144\n",
            "Iteration 482, loss = 0.36962035\n",
            "Iteration 483, loss = 0.36963914\n",
            "Iteration 484, loss = 0.36952405\n",
            "Iteration 485, loss = 0.36947696\n",
            "Iteration 486, loss = 0.36955265\n",
            "Iteration 487, loss = 0.36937959\n",
            "Iteration 488, loss = 0.36942060\n",
            "Iteration 489, loss = 0.36927600\n",
            "Iteration 490, loss = 0.36927324\n",
            "Iteration 491, loss = 0.36919786\n",
            "Iteration 492, loss = 0.36909163\n",
            "Iteration 493, loss = 0.36906974\n",
            "Iteration 494, loss = 0.36905380\n",
            "Iteration 495, loss = 0.36917400\n",
            "Iteration 496, loss = 0.36924389\n",
            "Iteration 497, loss = 0.36906317\n",
            "Iteration 498, loss = 0.36883365\n",
            "Iteration 499, loss = 0.36880508\n",
            "Iteration 500, loss = 0.36892193\n",
            "Iteration 501, loss = 0.36873072\n",
            "Iteration 502, loss = 0.36876820\n",
            "Iteration 503, loss = 0.36871531\n",
            "Iteration 504, loss = 0.36864154\n",
            "Iteration 505, loss = 0.36849499\n",
            "Iteration 506, loss = 0.36851674\n",
            "Iteration 507, loss = 0.36826259\n",
            "Iteration 508, loss = 0.36840352\n",
            "Iteration 509, loss = 0.36851573\n",
            "Iteration 510, loss = 0.36830495\n",
            "Iteration 511, loss = 0.36818705\n",
            "Iteration 512, loss = 0.36829507\n",
            "Iteration 513, loss = 0.36821352\n",
            "Iteration 514, loss = 0.36838079\n",
            "Iteration 515, loss = 0.36819273\n",
            "Iteration 516, loss = 0.36810404\n",
            "Iteration 517, loss = 0.36800431\n",
            "Iteration 518, loss = 0.36798437\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.75881546\n",
            "Iteration 2, loss = 0.75346722\n",
            "Iteration 3, loss = 0.74839520\n",
            "Iteration 4, loss = 0.74370273\n",
            "Iteration 5, loss = 0.73880438\n",
            "Iteration 6, loss = 0.73463356\n",
            "Iteration 7, loss = 0.73044612\n",
            "Iteration 8, loss = 0.72639426\n",
            "Iteration 9, loss = 0.72273030\n",
            "Iteration 10, loss = 0.71881835\n",
            "Iteration 11, loss = 0.71561209\n",
            "Iteration 12, loss = 0.71253459\n",
            "Iteration 13, loss = 0.70933421\n",
            "Iteration 14, loss = 0.70643104\n",
            "Iteration 15, loss = 0.70383568\n",
            "Iteration 16, loss = 0.70094925\n",
            "Iteration 17, loss = 0.69860265\n",
            "Iteration 18, loss = 0.69627842\n",
            "Iteration 19, loss = 0.69386896\n",
            "Iteration 20, loss = 0.69177832\n",
            "Iteration 21, loss = 0.68975885\n",
            "Iteration 22, loss = 0.68765178\n",
            "Iteration 23, loss = 0.68576690\n",
            "Iteration 24, loss = 0.68393863\n",
            "Iteration 25, loss = 0.68206593\n",
            "Iteration 26, loss = 0.68031263\n",
            "Iteration 27, loss = 0.67849115\n",
            "Iteration 28, loss = 0.67665292\n",
            "Iteration 29, loss = 0.67491960\n",
            "Iteration 30, loss = 0.67317453\n",
            "Iteration 31, loss = 0.67143114\n",
            "Iteration 32, loss = 0.66973743\n",
            "Iteration 33, loss = 0.66793570\n",
            "Iteration 34, loss = 0.66629663\n",
            "Iteration 35, loss = 0.66470663\n",
            "Iteration 36, loss = 0.66291448\n",
            "Iteration 37, loss = 0.66124130\n",
            "Iteration 38, loss = 0.65954948\n",
            "Iteration 39, loss = 0.65797177\n",
            "Iteration 40, loss = 0.65614870\n",
            "Iteration 41, loss = 0.65452935\n",
            "Iteration 42, loss = 0.65282210\n",
            "Iteration 43, loss = 0.65112667\n",
            "Iteration 44, loss = 0.64947194\n",
            "Iteration 45, loss = 0.64773789\n",
            "Iteration 46, loss = 0.64612235\n",
            "Iteration 47, loss = 0.64440125\n",
            "Iteration 48, loss = 0.64270067\n",
            "Iteration 49, loss = 0.64091470\n",
            "Iteration 50, loss = 0.63926965\n",
            "Iteration 51, loss = 0.63755701\n",
            "Iteration 52, loss = 0.63591244\n",
            "Iteration 53, loss = 0.63416663\n",
            "Iteration 54, loss = 0.63254602\n",
            "Iteration 55, loss = 0.63068683\n",
            "Iteration 56, loss = 0.62903625\n",
            "Iteration 57, loss = 0.62723641\n",
            "Iteration 58, loss = 0.62552816\n",
            "Iteration 59, loss = 0.62373396\n",
            "Iteration 60, loss = 0.62194768\n",
            "Iteration 61, loss = 0.62031078\n",
            "Iteration 62, loss = 0.61858647\n",
            "Iteration 63, loss = 0.61668939\n",
            "Iteration 64, loss = 0.61502946\n",
            "Iteration 65, loss = 0.61338180\n",
            "Iteration 66, loss = 0.61148210\n",
            "Iteration 67, loss = 0.60998413\n",
            "Iteration 68, loss = 0.60806165\n",
            "Iteration 69, loss = 0.60635721\n",
            "Iteration 70, loss = 0.60459760\n",
            "Iteration 71, loss = 0.60280133\n",
            "Iteration 72, loss = 0.60112619\n",
            "Iteration 73, loss = 0.59933919\n",
            "Iteration 74, loss = 0.59746808\n",
            "Iteration 75, loss = 0.59569626\n",
            "Iteration 76, loss = 0.59384612\n",
            "Iteration 77, loss = 0.59198095\n",
            "Iteration 78, loss = 0.58999706\n",
            "Iteration 79, loss = 0.58818952\n",
            "Iteration 80, loss = 0.58616389\n",
            "Iteration 81, loss = 0.58409001\n",
            "Iteration 82, loss = 0.58211640\n",
            "Iteration 83, loss = 0.58002250\n",
            "Iteration 84, loss = 0.57795645\n",
            "Iteration 85, loss = 0.57582792\n",
            "Iteration 86, loss = 0.57356636\n",
            "Iteration 87, loss = 0.57125110\n",
            "Iteration 88, loss = 0.56900466\n",
            "Iteration 89, loss = 0.56656855\n",
            "Iteration 90, loss = 0.56420606\n",
            "Iteration 91, loss = 0.56185288\n",
            "Iteration 92, loss = 0.55923808\n",
            "Iteration 93, loss = 0.55661283\n",
            "Iteration 94, loss = 0.55411199\n",
            "Iteration 95, loss = 0.55133412\n",
            "Iteration 96, loss = 0.54877348\n",
            "Iteration 97, loss = 0.54608377\n",
            "Iteration 98, loss = 0.54332552\n",
            "Iteration 99, loss = 0.54058762\n",
            "Iteration 100, loss = 0.53797760\n",
            "Iteration 101, loss = 0.53534111\n",
            "Iteration 102, loss = 0.53262467\n",
            "Iteration 103, loss = 0.52972567\n",
            "Iteration 104, loss = 0.52711472\n",
            "Iteration 105, loss = 0.52432578\n",
            "Iteration 106, loss = 0.52160870\n",
            "Iteration 107, loss = 0.51894308\n",
            "Iteration 108, loss = 0.51626896\n",
            "Iteration 109, loss = 0.51356002\n",
            "Iteration 110, loss = 0.51090625\n",
            "Iteration 111, loss = 0.50827815\n",
            "Iteration 112, loss = 0.50570397\n",
            "Iteration 113, loss = 0.50312069\n",
            "Iteration 114, loss = 0.50064213\n",
            "Iteration 115, loss = 0.49806757\n",
            "Iteration 116, loss = 0.49568199\n",
            "Iteration 117, loss = 0.49332204\n",
            "Iteration 118, loss = 0.49070561\n",
            "Iteration 119, loss = 0.48850597\n",
            "Iteration 120, loss = 0.48626717\n",
            "Iteration 121, loss = 0.48395552\n",
            "Iteration 122, loss = 0.48157425\n",
            "Iteration 123, loss = 0.47935214\n",
            "Iteration 124, loss = 0.47730095\n",
            "Iteration 125, loss = 0.47512980\n",
            "Iteration 126, loss = 0.47325677\n",
            "Iteration 127, loss = 0.47097955\n",
            "Iteration 128, loss = 0.46900698\n",
            "Iteration 129, loss = 0.46708058\n",
            "Iteration 130, loss = 0.46525817\n",
            "Iteration 131, loss = 0.46346294\n",
            "Iteration 132, loss = 0.46184875\n",
            "Iteration 133, loss = 0.46012884\n",
            "Iteration 134, loss = 0.45862115\n",
            "Iteration 135, loss = 0.45708762\n",
            "Iteration 136, loss = 0.45573716\n",
            "Iteration 137, loss = 0.45430643\n",
            "Iteration 138, loss = 0.45316990\n",
            "Iteration 139, loss = 0.45183603\n",
            "Iteration 140, loss = 0.45065616\n",
            "Iteration 141, loss = 0.44972114\n",
            "Iteration 142, loss = 0.44874211\n",
            "Iteration 143, loss = 0.44783894\n",
            "Iteration 144, loss = 0.44693500\n",
            "Iteration 145, loss = 0.44599648\n",
            "Iteration 146, loss = 0.44538765\n",
            "Iteration 147, loss = 0.44456735\n",
            "Iteration 148, loss = 0.44401268\n",
            "Iteration 149, loss = 0.44333060\n",
            "Iteration 150, loss = 0.44264835\n",
            "Iteration 151, loss = 0.44206976\n",
            "Iteration 152, loss = 0.44149101\n",
            "Iteration 153, loss = 0.44108274\n",
            "Iteration 154, loss = 0.44059317\n",
            "Iteration 155, loss = 0.44002392\n",
            "Iteration 156, loss = 0.43950808\n",
            "Iteration 157, loss = 0.43904998\n",
            "Iteration 158, loss = 0.43855160\n",
            "Iteration 159, loss = 0.43817401\n",
            "Iteration 160, loss = 0.43769081\n",
            "Iteration 161, loss = 0.43739959\n",
            "Iteration 162, loss = 0.43689781\n",
            "Iteration 163, loss = 0.43642292\n",
            "Iteration 164, loss = 0.43600780\n",
            "Iteration 165, loss = 0.43581373\n",
            "Iteration 166, loss = 0.43544728\n",
            "Iteration 167, loss = 0.43509750\n",
            "Iteration 168, loss = 0.43463653\n",
            "Iteration 169, loss = 0.43441117\n",
            "Iteration 170, loss = 0.43403418\n",
            "Iteration 171, loss = 0.43369451\n",
            "Iteration 172, loss = 0.43336359\n",
            "Iteration 173, loss = 0.43309793\n",
            "Iteration 174, loss = 0.43278168\n",
            "Iteration 175, loss = 0.43246897\n",
            "Iteration 176, loss = 0.43213476\n",
            "Iteration 177, loss = 0.43184768\n",
            "Iteration 178, loss = 0.43150758\n",
            "Iteration 179, loss = 0.43131653\n",
            "Iteration 180, loss = 0.43108506\n",
            "Iteration 181, loss = 0.43073355\n",
            "Iteration 182, loss = 0.43039641\n",
            "Iteration 183, loss = 0.43011256\n",
            "Iteration 184, loss = 0.42984029\n",
            "Iteration 185, loss = 0.42974289\n",
            "Iteration 186, loss = 0.42912354\n",
            "Iteration 187, loss = 0.42891259\n",
            "Iteration 188, loss = 0.42872864\n",
            "Iteration 189, loss = 0.42835200\n",
            "Iteration 190, loss = 0.42806992\n",
            "Iteration 191, loss = 0.42785907\n",
            "Iteration 192, loss = 0.42758480\n",
            "Iteration 193, loss = 0.42719870\n",
            "Iteration 194, loss = 0.42677574\n",
            "Iteration 195, loss = 0.42637745\n",
            "Iteration 196, loss = 0.42626194\n",
            "Iteration 197, loss = 0.42587926\n",
            "Iteration 198, loss = 0.42561166\n",
            "Iteration 199, loss = 0.42525631\n",
            "Iteration 200, loss = 0.42511957\n",
            "Iteration 201, loss = 0.42467339\n",
            "Iteration 202, loss = 0.42409326\n",
            "Iteration 203, loss = 0.42397234\n",
            "Iteration 204, loss = 0.42385035\n",
            "Iteration 205, loss = 0.42347282\n",
            "Iteration 206, loss = 0.42304493\n",
            "Iteration 207, loss = 0.42251823\n",
            "Iteration 208, loss = 0.42225042\n",
            "Iteration 209, loss = 0.42160719\n",
            "Iteration 210, loss = 0.42130207\n",
            "Iteration 211, loss = 0.42097265\n",
            "Iteration 212, loss = 0.42085755\n",
            "Iteration 213, loss = 0.42035607\n",
            "Iteration 214, loss = 0.41996521\n",
            "Iteration 215, loss = 0.41951494\n",
            "Iteration 216, loss = 0.41919946\n",
            "Iteration 217, loss = 0.41900308\n",
            "Iteration 218, loss = 0.41855920\n",
            "Iteration 219, loss = 0.41822386\n",
            "Iteration 220, loss = 0.41796683\n",
            "Iteration 221, loss = 0.41756722\n",
            "Iteration 222, loss = 0.41728740\n",
            "Iteration 223, loss = 0.41703025\n",
            "Iteration 224, loss = 0.41670435\n",
            "Iteration 225, loss = 0.41638740\n",
            "Iteration 226, loss = 0.41608388\n",
            "Iteration 227, loss = 0.41584354\n",
            "Iteration 228, loss = 0.41547699\n",
            "Iteration 229, loss = 0.41521500\n",
            "Iteration 230, loss = 0.41500812\n",
            "Iteration 231, loss = 0.41464922\n",
            "Iteration 232, loss = 0.41437591\n",
            "Iteration 233, loss = 0.41426399\n",
            "Iteration 234, loss = 0.41392161\n",
            "Iteration 235, loss = 0.41358265\n",
            "Iteration 236, loss = 0.41345913\n",
            "Iteration 237, loss = 0.41334140\n",
            "Iteration 238, loss = 0.41274667\n",
            "Iteration 239, loss = 0.41237674\n",
            "Iteration 240, loss = 0.41268323\n",
            "Iteration 241, loss = 0.41251700\n",
            "Iteration 242, loss = 0.41220721\n",
            "Iteration 243, loss = 0.41172840\n",
            "Iteration 244, loss = 0.41148258\n",
            "Iteration 245, loss = 0.41114487\n",
            "Iteration 246, loss = 0.41103407\n",
            "Iteration 247, loss = 0.41077195\n",
            "Iteration 248, loss = 0.41057311\n",
            "Iteration 249, loss = 0.41024411\n",
            "Iteration 250, loss = 0.41006235\n",
            "Iteration 251, loss = 0.40991478\n",
            "Iteration 252, loss = 0.40966687\n",
            "Iteration 253, loss = 0.40951804\n",
            "Iteration 254, loss = 0.40920016\n",
            "Iteration 255, loss = 0.40903168\n",
            "Iteration 256, loss = 0.40893780\n",
            "Iteration 257, loss = 0.40868431\n",
            "Iteration 258, loss = 0.40838289\n",
            "Iteration 259, loss = 0.40816410\n",
            "Iteration 260, loss = 0.40834273\n",
            "Iteration 261, loss = 0.40784961\n",
            "Iteration 262, loss = 0.40758198\n",
            "Iteration 263, loss = 0.40730243\n",
            "Iteration 264, loss = 0.40704665\n",
            "Iteration 265, loss = 0.40684231\n",
            "Iteration 266, loss = 0.40678757\n",
            "Iteration 267, loss = 0.40646355\n",
            "Iteration 268, loss = 0.40612789\n",
            "Iteration 269, loss = 0.40596817\n",
            "Iteration 270, loss = 0.40598174\n",
            "Iteration 271, loss = 0.40563370\n",
            "Iteration 272, loss = 0.40554930\n",
            "Iteration 273, loss = 0.40525556\n",
            "Iteration 274, loss = 0.40504273\n",
            "Iteration 275, loss = 0.40474254\n",
            "Iteration 276, loss = 0.40457390\n",
            "Iteration 277, loss = 0.40443388\n",
            "Iteration 278, loss = 0.40423636\n",
            "Iteration 279, loss = 0.40404818\n",
            "Iteration 280, loss = 0.40383694\n",
            "Iteration 281, loss = 0.40365074\n",
            "Iteration 282, loss = 0.40368490\n",
            "Iteration 283, loss = 0.40341216\n",
            "Iteration 284, loss = 0.40309163\n",
            "Iteration 285, loss = 0.40296713\n",
            "Iteration 286, loss = 0.40275490\n",
            "Iteration 287, loss = 0.40257933\n",
            "Iteration 288, loss = 0.40246747\n",
            "Iteration 289, loss = 0.40225584\n",
            "Iteration 290, loss = 0.40208289\n",
            "Iteration 291, loss = 0.40189350\n",
            "Iteration 292, loss = 0.40175437\n",
            "Iteration 293, loss = 0.40155058\n",
            "Iteration 294, loss = 0.40124003\n",
            "Iteration 295, loss = 0.40110156\n",
            "Iteration 296, loss = 0.40098876\n",
            "Iteration 297, loss = 0.40075378\n",
            "Iteration 298, loss = 0.40060685\n",
            "Iteration 299, loss = 0.40045674\n",
            "Iteration 300, loss = 0.40023559\n",
            "Iteration 301, loss = 0.40015045\n",
            "Iteration 302, loss = 0.39995632\n",
            "Iteration 303, loss = 0.39981255\n",
            "Iteration 304, loss = 0.39969835\n",
            "Iteration 305, loss = 0.39935756\n",
            "Iteration 306, loss = 0.39916540\n",
            "Iteration 307, loss = 0.39915023\n",
            "Iteration 308, loss = 0.39955199\n",
            "Iteration 309, loss = 0.39910755\n",
            "Iteration 310, loss = 0.39888647\n",
            "Iteration 311, loss = 0.39848519\n",
            "Iteration 312, loss = 0.39840837\n",
            "Iteration 313, loss = 0.39839583\n",
            "Iteration 314, loss = 0.39823759\n",
            "Iteration 315, loss = 0.39796511\n",
            "Iteration 316, loss = 0.39779104\n",
            "Iteration 317, loss = 0.39754414\n",
            "Iteration 318, loss = 0.39744635\n",
            "Iteration 319, loss = 0.39715064\n",
            "Iteration 320, loss = 0.39705692\n",
            "Iteration 321, loss = 0.39692000\n",
            "Iteration 322, loss = 0.39683632\n",
            "Iteration 323, loss = 0.39654663\n",
            "Iteration 324, loss = 0.39659001\n",
            "Iteration 325, loss = 0.39636620\n",
            "Iteration 326, loss = 0.39614698\n",
            "Iteration 327, loss = 0.39607878\n",
            "Iteration 328, loss = 0.39589051\n",
            "Iteration 329, loss = 0.39582997\n",
            "Iteration 330, loss = 0.39566793\n",
            "Iteration 331, loss = 0.39552015\n",
            "Iteration 332, loss = 0.39531619\n",
            "Iteration 333, loss = 0.39537217\n",
            "Iteration 334, loss = 0.39512223\n",
            "Iteration 335, loss = 0.39489772\n",
            "Iteration 336, loss = 0.39476334\n",
            "Iteration 337, loss = 0.39459803\n",
            "Iteration 338, loss = 0.39451009\n",
            "Iteration 339, loss = 0.39427898\n",
            "Iteration 340, loss = 0.39426945\n",
            "Iteration 341, loss = 0.39400666\n",
            "Iteration 342, loss = 0.39385419\n",
            "Iteration 343, loss = 0.39374604\n",
            "Iteration 344, loss = 0.39351793\n",
            "Iteration 345, loss = 0.39361119\n",
            "Iteration 346, loss = 0.39360829\n",
            "Iteration 347, loss = 0.39322471\n",
            "Iteration 348, loss = 0.39311489\n",
            "Iteration 349, loss = 0.39301627\n",
            "Iteration 350, loss = 0.39300256\n",
            "Iteration 351, loss = 0.39282305\n",
            "Iteration 352, loss = 0.39261799\n",
            "Iteration 353, loss = 0.39247043\n",
            "Iteration 354, loss = 0.39222303\n",
            "Iteration 355, loss = 0.39227224\n",
            "Iteration 356, loss = 0.39207704\n",
            "Iteration 357, loss = 0.39199045\n",
            "Iteration 358, loss = 0.39177953\n",
            "Iteration 359, loss = 0.39177893\n",
            "Iteration 360, loss = 0.39177138\n",
            "Iteration 361, loss = 0.39150021\n",
            "Iteration 362, loss = 0.39144238\n",
            "Iteration 363, loss = 0.39127243\n",
            "Iteration 364, loss = 0.39121650\n",
            "Iteration 365, loss = 0.39107009\n",
            "Iteration 366, loss = 0.39087917\n",
            "Iteration 367, loss = 0.39091188\n",
            "Iteration 368, loss = 0.39078459\n",
            "Iteration 369, loss = 0.39076519\n",
            "Iteration 370, loss = 0.39056323\n",
            "Iteration 371, loss = 0.39052317\n",
            "Iteration 372, loss = 0.39012668\n",
            "Iteration 373, loss = 0.39010826\n",
            "Iteration 374, loss = 0.39007324\n",
            "Iteration 375, loss = 0.38997671\n",
            "Iteration 376, loss = 0.38983485\n",
            "Iteration 377, loss = 0.38982957\n",
            "Iteration 378, loss = 0.38956044\n",
            "Iteration 379, loss = 0.38940026\n",
            "Iteration 380, loss = 0.38937671\n",
            "Iteration 381, loss = 0.38926040\n",
            "Iteration 382, loss = 0.38915183\n",
            "Iteration 383, loss = 0.38904930\n",
            "Iteration 384, loss = 0.38920056\n",
            "Iteration 385, loss = 0.38884657\n",
            "Iteration 386, loss = 0.38883437\n",
            "Iteration 387, loss = 0.38877303\n",
            "Iteration 388, loss = 0.38881915\n",
            "Iteration 389, loss = 0.38848636\n",
            "Iteration 390, loss = 0.38837402\n",
            "Iteration 391, loss = 0.38825848\n",
            "Iteration 392, loss = 0.38831483\n",
            "Iteration 393, loss = 0.38814406\n",
            "Iteration 394, loss = 0.38792330\n",
            "Iteration 395, loss = 0.38785551\n",
            "Iteration 396, loss = 0.38770862\n",
            "Iteration 397, loss = 0.38785428\n",
            "Iteration 398, loss = 0.38789920\n",
            "Iteration 399, loss = 0.38803086\n",
            "Iteration 400, loss = 0.38788236\n",
            "Iteration 401, loss = 0.38761503\n",
            "Iteration 402, loss = 0.38757720\n",
            "Iteration 403, loss = 0.38726832\n",
            "Iteration 404, loss = 0.38718484\n",
            "Iteration 405, loss = 0.38709113\n",
            "Iteration 406, loss = 0.38697708\n",
            "Iteration 407, loss = 0.38691084\n",
            "Iteration 408, loss = 0.38678775\n",
            "Iteration 409, loss = 0.38679306\n",
            "Iteration 410, loss = 0.38655677\n",
            "Iteration 411, loss = 0.38690668\n",
            "Iteration 412, loss = 0.38653502\n",
            "Iteration 413, loss = 0.38659635\n",
            "Iteration 414, loss = 0.38622133\n",
            "Iteration 415, loss = 0.38618545\n",
            "Iteration 416, loss = 0.38611743\n",
            "Iteration 417, loss = 0.38617907\n",
            "Iteration 418, loss = 0.38606209\n",
            "Iteration 419, loss = 0.38592044\n",
            "Iteration 420, loss = 0.38577807\n",
            "Iteration 421, loss = 0.38562515\n",
            "Iteration 422, loss = 0.38555994\n",
            "Iteration 423, loss = 0.38548264\n",
            "Iteration 424, loss = 0.38541572\n",
            "Iteration 425, loss = 0.38539925\n",
            "Iteration 426, loss = 0.38531421\n",
            "Iteration 427, loss = 0.38514423\n",
            "Iteration 428, loss = 0.38514550\n",
            "Iteration 429, loss = 0.38500099\n",
            "Iteration 430, loss = 0.38495629\n",
            "Iteration 431, loss = 0.38520192\n",
            "Iteration 432, loss = 0.38498805\n",
            "Iteration 433, loss = 0.38481987\n",
            "Iteration 434, loss = 0.38469226\n",
            "Iteration 435, loss = 0.38450187\n",
            "Iteration 436, loss = 0.38462054\n",
            "Iteration 437, loss = 0.38459317\n",
            "Iteration 438, loss = 0.38462100\n",
            "Iteration 439, loss = 0.38439336\n",
            "Iteration 440, loss = 0.38422635\n",
            "Iteration 441, loss = 0.38429337\n",
            "Iteration 442, loss = 0.38410829\n",
            "Iteration 443, loss = 0.38412764\n",
            "Iteration 444, loss = 0.38404698\n",
            "Iteration 445, loss = 0.38392714\n",
            "Iteration 446, loss = 0.38392171\n",
            "Iteration 447, loss = 0.38382876\n",
            "Iteration 448, loss = 0.38373472\n",
            "Iteration 449, loss = 0.38370066\n",
            "Iteration 450, loss = 0.38366131\n",
            "Iteration 451, loss = 0.38355311\n",
            "Iteration 452, loss = 0.38356126\n",
            "Iteration 453, loss = 0.38348558\n",
            "Iteration 454, loss = 0.38337428\n",
            "Iteration 455, loss = 0.38341389\n",
            "Iteration 456, loss = 0.38329504\n",
            "Iteration 457, loss = 0.38318683\n",
            "Iteration 458, loss = 0.38317622\n",
            "Iteration 459, loss = 0.38305945\n",
            "Iteration 460, loss = 0.38331386\n",
            "Iteration 461, loss = 0.38294645\n",
            "Iteration 462, loss = 0.38292096\n",
            "Iteration 463, loss = 0.38280668\n",
            "Iteration 464, loss = 0.38285864\n",
            "Iteration 465, loss = 0.38275244\n",
            "Iteration 466, loss = 0.38272896\n",
            "Iteration 467, loss = 0.38263072\n",
            "Iteration 468, loss = 0.38260133\n",
            "Iteration 469, loss = 0.38253103\n",
            "Iteration 470, loss = 0.38255123\n",
            "Iteration 471, loss = 0.38244408\n",
            "Iteration 472, loss = 0.38248170\n",
            "Iteration 473, loss = 0.38261436\n",
            "Iteration 474, loss = 0.38224897\n",
            "Iteration 475, loss = 0.38217483\n",
            "Iteration 476, loss = 0.38207505\n",
            "Iteration 477, loss = 0.38226676\n",
            "Iteration 478, loss = 0.38221110\n",
            "Iteration 479, loss = 0.38211225\n",
            "Iteration 480, loss = 0.38185749\n",
            "Iteration 481, loss = 0.38182298\n",
            "Iteration 482, loss = 0.38168020\n",
            "Iteration 483, loss = 0.38162220\n",
            "Iteration 484, loss = 0.38156514\n",
            "Iteration 485, loss = 0.38153073\n",
            "Iteration 486, loss = 0.38164617\n",
            "Iteration 487, loss = 0.38146356\n",
            "Iteration 488, loss = 0.38157674\n",
            "Iteration 489, loss = 0.38135991\n",
            "Iteration 490, loss = 0.38131489\n",
            "Iteration 491, loss = 0.38131130\n",
            "Iteration 492, loss = 0.38122239\n",
            "Iteration 493, loss = 0.38118257\n",
            "Iteration 494, loss = 0.38112773\n",
            "Iteration 495, loss = 0.38131913\n",
            "Iteration 496, loss = 0.38131464\n",
            "Iteration 497, loss = 0.38103095\n",
            "Iteration 498, loss = 0.38094762\n",
            "Iteration 499, loss = 0.38083520\n",
            "Iteration 500, loss = 0.38083296\n",
            "Iteration 501, loss = 0.38088889\n",
            "Iteration 502, loss = 0.38086292\n",
            "Iteration 503, loss = 0.38093126\n",
            "Iteration 504, loss = 0.38076417\n",
            "Iteration 505, loss = 0.38067627\n",
            "Iteration 506, loss = 0.38061221\n",
            "Iteration 507, loss = 0.38043552\n",
            "Iteration 508, loss = 0.38048483\n",
            "Iteration 509, loss = 0.38067247\n",
            "Iteration 510, loss = 0.38039629\n",
            "Iteration 511, loss = 0.38035462\n",
            "Iteration 512, loss = 0.38045155\n",
            "Iteration 513, loss = 0.38028511\n",
            "Iteration 514, loss = 0.38047378\n",
            "Iteration 515, loss = 0.38038259\n",
            "Iteration 516, loss = 0.38026637\n",
            "Iteration 517, loss = 0.38014990\n",
            "Iteration 518, loss = 0.38020874\n",
            "Iteration 519, loss = 0.38009115\n",
            "Iteration 520, loss = 0.37995786\n",
            "Iteration 521, loss = 0.38004218\n",
            "Iteration 522, loss = 0.37997374\n",
            "Iteration 523, loss = 0.37996273\n",
            "Iteration 524, loss = 0.37987380\n",
            "Iteration 525, loss = 0.37984791\n",
            "Iteration 526, loss = 0.37977001\n",
            "Iteration 527, loss = 0.38010814\n",
            "Iteration 528, loss = 0.37973828\n",
            "Iteration 529, loss = 0.37967613\n",
            "Iteration 530, loss = 0.37965742\n",
            "Iteration 531, loss = 0.37959053\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.75891501\n",
            "Iteration 2, loss = 0.75357549\n",
            "Iteration 3, loss = 0.74848304\n",
            "Iteration 4, loss = 0.74386753\n",
            "Iteration 5, loss = 0.73910571\n",
            "Iteration 6, loss = 0.73485746\n",
            "Iteration 7, loss = 0.73093887\n",
            "Iteration 8, loss = 0.72698997\n",
            "Iteration 9, loss = 0.72335252\n",
            "Iteration 10, loss = 0.71964206\n",
            "Iteration 11, loss = 0.71660568\n",
            "Iteration 12, loss = 0.71367950\n",
            "Iteration 13, loss = 0.71057258\n",
            "Iteration 14, loss = 0.70784096\n",
            "Iteration 15, loss = 0.70539138\n",
            "Iteration 16, loss = 0.70264305\n",
            "Iteration 17, loss = 0.70047959\n",
            "Iteration 18, loss = 0.69825224\n",
            "Iteration 19, loss = 0.69607673\n",
            "Iteration 20, loss = 0.69403587\n",
            "Iteration 21, loss = 0.69213976\n",
            "Iteration 22, loss = 0.69014389\n",
            "Iteration 23, loss = 0.68831953\n",
            "Iteration 24, loss = 0.68660000\n",
            "Iteration 25, loss = 0.68480604\n",
            "Iteration 26, loss = 0.68313158\n",
            "Iteration 27, loss = 0.68133000\n",
            "Iteration 28, loss = 0.67964709\n",
            "Iteration 29, loss = 0.67799500\n",
            "Iteration 30, loss = 0.67633717\n",
            "Iteration 31, loss = 0.67465897\n",
            "Iteration 32, loss = 0.67310877\n",
            "Iteration 33, loss = 0.67138547\n",
            "Iteration 34, loss = 0.66981162\n",
            "Iteration 35, loss = 0.66824772\n",
            "Iteration 36, loss = 0.66656635\n",
            "Iteration 37, loss = 0.66499178\n",
            "Iteration 38, loss = 0.66339096\n",
            "Iteration 39, loss = 0.66184958\n",
            "Iteration 40, loss = 0.66008675\n",
            "Iteration 41, loss = 0.65854082\n",
            "Iteration 42, loss = 0.65690076\n",
            "Iteration 43, loss = 0.65521692\n",
            "Iteration 44, loss = 0.65365277\n",
            "Iteration 45, loss = 0.65188264\n",
            "Iteration 46, loss = 0.65030073\n",
            "Iteration 47, loss = 0.64861109\n",
            "Iteration 48, loss = 0.64686204\n",
            "Iteration 49, loss = 0.64516030\n",
            "Iteration 50, loss = 0.64343757\n",
            "Iteration 51, loss = 0.64169007\n",
            "Iteration 52, loss = 0.64006290\n",
            "Iteration 53, loss = 0.63826896\n",
            "Iteration 54, loss = 0.63660636\n",
            "Iteration 55, loss = 0.63477768\n",
            "Iteration 56, loss = 0.63309106\n",
            "Iteration 57, loss = 0.63116532\n",
            "Iteration 58, loss = 0.62946298\n",
            "Iteration 59, loss = 0.62756410\n",
            "Iteration 60, loss = 0.62572838\n",
            "Iteration 61, loss = 0.62392302\n",
            "Iteration 62, loss = 0.62214239\n",
            "Iteration 63, loss = 0.62016912\n",
            "Iteration 64, loss = 0.61840499\n",
            "Iteration 65, loss = 0.61655710\n",
            "Iteration 66, loss = 0.61462462\n",
            "Iteration 67, loss = 0.61283788\n",
            "Iteration 68, loss = 0.61083037\n",
            "Iteration 69, loss = 0.60888330\n",
            "Iteration 70, loss = 0.60692498\n",
            "Iteration 71, loss = 0.60491944\n",
            "Iteration 72, loss = 0.60297183\n",
            "Iteration 73, loss = 0.60097165\n",
            "Iteration 74, loss = 0.59881139\n",
            "Iteration 75, loss = 0.59665839\n",
            "Iteration 76, loss = 0.59459505\n",
            "Iteration 77, loss = 0.59240985\n",
            "Iteration 78, loss = 0.59011447\n",
            "Iteration 79, loss = 0.58792103\n",
            "Iteration 80, loss = 0.58563842\n",
            "Iteration 81, loss = 0.58308211\n",
            "Iteration 82, loss = 0.58077269\n",
            "Iteration 83, loss = 0.57829259\n",
            "Iteration 84, loss = 0.57581176\n",
            "Iteration 85, loss = 0.57322825\n",
            "Iteration 86, loss = 0.57060236\n",
            "Iteration 87, loss = 0.56789429\n",
            "Iteration 88, loss = 0.56517055\n",
            "Iteration 89, loss = 0.56236263\n",
            "Iteration 90, loss = 0.55963609\n",
            "Iteration 91, loss = 0.55689754\n",
            "Iteration 92, loss = 0.55404813\n",
            "Iteration 93, loss = 0.55109009\n",
            "Iteration 94, loss = 0.54832429\n",
            "Iteration 95, loss = 0.54527681\n",
            "Iteration 96, loss = 0.54237435\n",
            "Iteration 97, loss = 0.53958529\n",
            "Iteration 98, loss = 0.53650121\n",
            "Iteration 99, loss = 0.53365602\n",
            "Iteration 100, loss = 0.53088189\n",
            "Iteration 101, loss = 0.52811993\n",
            "Iteration 102, loss = 0.52531555\n",
            "Iteration 103, loss = 0.52234889\n",
            "Iteration 104, loss = 0.51965208\n",
            "Iteration 105, loss = 0.51688184\n",
            "Iteration 106, loss = 0.51412214\n",
            "Iteration 107, loss = 0.51144409\n",
            "Iteration 108, loss = 0.50879039\n",
            "Iteration 109, loss = 0.50609311\n",
            "Iteration 110, loss = 0.50352818\n",
            "Iteration 111, loss = 0.50096214\n",
            "Iteration 112, loss = 0.49842328\n",
            "Iteration 113, loss = 0.49602347\n",
            "Iteration 114, loss = 0.49363792\n",
            "Iteration 115, loss = 0.49116419\n",
            "Iteration 116, loss = 0.48888992\n",
            "Iteration 117, loss = 0.48671773\n",
            "Iteration 118, loss = 0.48430754\n",
            "Iteration 119, loss = 0.48226264\n",
            "Iteration 120, loss = 0.48009054\n",
            "Iteration 121, loss = 0.47802306\n",
            "Iteration 122, loss = 0.47588566\n",
            "Iteration 123, loss = 0.47382427\n",
            "Iteration 124, loss = 0.47194404\n",
            "Iteration 125, loss = 0.46987601\n",
            "Iteration 126, loss = 0.46814481\n",
            "Iteration 127, loss = 0.46628554\n",
            "Iteration 128, loss = 0.46442588\n",
            "Iteration 129, loss = 0.46267142\n",
            "Iteration 130, loss = 0.46110279\n",
            "Iteration 131, loss = 0.45960581\n",
            "Iteration 132, loss = 0.45811835\n",
            "Iteration 133, loss = 0.45651780\n",
            "Iteration 134, loss = 0.45513667\n",
            "Iteration 135, loss = 0.45381530\n",
            "Iteration 136, loss = 0.45251396\n",
            "Iteration 137, loss = 0.45122778\n",
            "Iteration 138, loss = 0.45013480\n",
            "Iteration 139, loss = 0.44907003\n",
            "Iteration 140, loss = 0.44788750\n",
            "Iteration 141, loss = 0.44700922\n",
            "Iteration 142, loss = 0.44601768\n",
            "Iteration 143, loss = 0.44521837\n",
            "Iteration 144, loss = 0.44429559\n",
            "Iteration 145, loss = 0.44333132\n",
            "Iteration 146, loss = 0.44260441\n",
            "Iteration 147, loss = 0.44182101\n",
            "Iteration 148, loss = 0.44111550\n",
            "Iteration 149, loss = 0.44055289\n",
            "Iteration 150, loss = 0.43966835\n",
            "Iteration 151, loss = 0.43903853\n",
            "Iteration 152, loss = 0.43823640\n",
            "Iteration 153, loss = 0.43770500\n",
            "Iteration 154, loss = 0.43701863\n",
            "Iteration 155, loss = 0.43635833\n",
            "Iteration 156, loss = 0.43574533\n",
            "Iteration 157, loss = 0.43517155\n",
            "Iteration 158, loss = 0.43457084\n",
            "Iteration 159, loss = 0.43413200\n",
            "Iteration 160, loss = 0.43348814\n",
            "Iteration 161, loss = 0.43302813\n",
            "Iteration 162, loss = 0.43253019\n",
            "Iteration 163, loss = 0.43187292\n",
            "Iteration 164, loss = 0.43132804\n",
            "Iteration 165, loss = 0.43089026\n",
            "Iteration 166, loss = 0.43040767\n",
            "Iteration 167, loss = 0.42995401\n",
            "Iteration 168, loss = 0.42950530\n",
            "Iteration 169, loss = 0.42913511\n",
            "Iteration 170, loss = 0.42858716\n",
            "Iteration 171, loss = 0.42810167\n",
            "Iteration 172, loss = 0.42770054\n",
            "Iteration 173, loss = 0.42720806\n",
            "Iteration 174, loss = 0.42683993\n",
            "Iteration 175, loss = 0.42638144\n",
            "Iteration 176, loss = 0.42590922\n",
            "Iteration 177, loss = 0.42547660\n",
            "Iteration 178, loss = 0.42508928\n",
            "Iteration 179, loss = 0.42473013\n",
            "Iteration 180, loss = 0.42433813\n",
            "Iteration 181, loss = 0.42394645\n",
            "Iteration 182, loss = 0.42357796\n",
            "Iteration 183, loss = 0.42314575\n",
            "Iteration 184, loss = 0.42279758\n",
            "Iteration 185, loss = 0.42248452\n",
            "Iteration 186, loss = 0.42192139\n",
            "Iteration 187, loss = 0.42167409\n",
            "Iteration 188, loss = 0.42140023\n",
            "Iteration 189, loss = 0.42113146\n",
            "Iteration 190, loss = 0.42078810\n",
            "Iteration 191, loss = 0.42037563\n",
            "Iteration 192, loss = 0.42000933\n",
            "Iteration 193, loss = 0.41959756\n",
            "Iteration 194, loss = 0.41919127\n",
            "Iteration 195, loss = 0.41885078\n",
            "Iteration 196, loss = 0.41860748\n",
            "Iteration 197, loss = 0.41829591\n",
            "Iteration 198, loss = 0.41799181\n",
            "Iteration 199, loss = 0.41767722\n",
            "Iteration 200, loss = 0.41743821\n",
            "Iteration 201, loss = 0.41704574\n",
            "Iteration 202, loss = 0.41671920\n",
            "Iteration 203, loss = 0.41653777\n",
            "Iteration 204, loss = 0.41640798\n",
            "Iteration 205, loss = 0.41613606\n",
            "Iteration 206, loss = 0.41574928\n",
            "Iteration 207, loss = 0.41522326\n",
            "Iteration 208, loss = 0.41480814\n",
            "Iteration 209, loss = 0.41453834\n",
            "Iteration 210, loss = 0.41428673\n",
            "Iteration 211, loss = 0.41400864\n",
            "Iteration 212, loss = 0.41392615\n",
            "Iteration 213, loss = 0.41358034\n",
            "Iteration 214, loss = 0.41318028\n",
            "Iteration 215, loss = 0.41279250\n",
            "Iteration 216, loss = 0.41249491\n",
            "Iteration 217, loss = 0.41241021\n",
            "Iteration 218, loss = 0.41196700\n",
            "Iteration 219, loss = 0.41171822\n",
            "Iteration 220, loss = 0.41147163\n",
            "Iteration 221, loss = 0.41139686\n",
            "Iteration 222, loss = 0.41094497\n",
            "Iteration 223, loss = 0.41066859\n",
            "Iteration 224, loss = 0.41037977\n",
            "Iteration 225, loss = 0.41005562\n",
            "Iteration 226, loss = 0.40978740\n",
            "Iteration 227, loss = 0.40956576\n",
            "Iteration 228, loss = 0.40931538\n",
            "Iteration 229, loss = 0.40903407\n",
            "Iteration 230, loss = 0.40879861\n",
            "Iteration 231, loss = 0.40854696\n",
            "Iteration 232, loss = 0.40834347\n",
            "Iteration 233, loss = 0.40813051\n",
            "Iteration 234, loss = 0.40781033\n",
            "Iteration 235, loss = 0.40751626\n",
            "Iteration 236, loss = 0.40739724\n",
            "Iteration 237, loss = 0.40717406\n",
            "Iteration 238, loss = 0.40673296\n",
            "Iteration 239, loss = 0.40635516\n",
            "Iteration 240, loss = 0.40648584\n",
            "Iteration 241, loss = 0.40656320\n",
            "Iteration 242, loss = 0.40635368\n",
            "Iteration 243, loss = 0.40591085\n",
            "Iteration 244, loss = 0.40541204\n",
            "Iteration 245, loss = 0.40513934\n",
            "Iteration 246, loss = 0.40505175\n",
            "Iteration 247, loss = 0.40482978\n",
            "Iteration 248, loss = 0.40453675\n",
            "Iteration 249, loss = 0.40423425\n",
            "Iteration 250, loss = 0.40405055\n",
            "Iteration 251, loss = 0.40397774\n",
            "Iteration 252, loss = 0.40378744\n",
            "Iteration 253, loss = 0.40361323\n",
            "Iteration 254, loss = 0.40328940\n",
            "Iteration 255, loss = 0.40308372\n",
            "Iteration 256, loss = 0.40286379\n",
            "Iteration 257, loss = 0.40267230\n",
            "Iteration 258, loss = 0.40248148\n",
            "Iteration 259, loss = 0.40231786\n",
            "Iteration 260, loss = 0.40232455\n",
            "Iteration 261, loss = 0.40191992\n",
            "Iteration 262, loss = 0.40172183\n",
            "Iteration 263, loss = 0.40150667\n",
            "Iteration 264, loss = 0.40130265\n",
            "Iteration 265, loss = 0.40099819\n",
            "Iteration 266, loss = 0.40099595\n",
            "Iteration 267, loss = 0.40070726\n",
            "Iteration 268, loss = 0.40054277\n",
            "Iteration 269, loss = 0.40033671\n",
            "Iteration 270, loss = 0.40017382\n",
            "Iteration 271, loss = 0.39991786\n",
            "Iteration 272, loss = 0.39983913\n",
            "Iteration 273, loss = 0.39963590\n",
            "Iteration 274, loss = 0.39935312\n",
            "Iteration 275, loss = 0.39912647\n",
            "Iteration 276, loss = 0.39904703\n",
            "Iteration 277, loss = 0.39879639\n",
            "Iteration 278, loss = 0.39861150\n",
            "Iteration 279, loss = 0.39843111\n",
            "Iteration 280, loss = 0.39833373\n",
            "Iteration 281, loss = 0.39813198\n",
            "Iteration 282, loss = 0.39807013\n",
            "Iteration 283, loss = 0.39781774\n",
            "Iteration 284, loss = 0.39758641\n",
            "Iteration 285, loss = 0.39737408\n",
            "Iteration 286, loss = 0.39717288\n",
            "Iteration 287, loss = 0.39705001\n",
            "Iteration 288, loss = 0.39701008\n",
            "Iteration 289, loss = 0.39670965\n",
            "Iteration 290, loss = 0.39662282\n",
            "Iteration 291, loss = 0.39641863\n",
            "Iteration 292, loss = 0.39623202\n",
            "Iteration 293, loss = 0.39598834\n",
            "Iteration 294, loss = 0.39576308\n",
            "Iteration 295, loss = 0.39562212\n",
            "Iteration 296, loss = 0.39546587\n",
            "Iteration 297, loss = 0.39529826\n",
            "Iteration 298, loss = 0.39521820\n",
            "Iteration 299, loss = 0.39501319\n",
            "Iteration 300, loss = 0.39481535\n",
            "Iteration 301, loss = 0.39481952\n",
            "Iteration 302, loss = 0.39451841\n",
            "Iteration 303, loss = 0.39444321\n",
            "Iteration 304, loss = 0.39425135\n",
            "Iteration 305, loss = 0.39388333\n",
            "Iteration 306, loss = 0.39372089\n",
            "Iteration 307, loss = 0.39364600\n",
            "Iteration 308, loss = 0.39391852\n",
            "Iteration 309, loss = 0.39337975\n",
            "Iteration 310, loss = 0.39314521\n",
            "Iteration 311, loss = 0.39300556\n",
            "Iteration 312, loss = 0.39290668\n",
            "Iteration 313, loss = 0.39289405\n",
            "Iteration 314, loss = 0.39271221\n",
            "Iteration 315, loss = 0.39256000\n",
            "Iteration 316, loss = 0.39221282\n",
            "Iteration 317, loss = 0.39196190\n",
            "Iteration 318, loss = 0.39184897\n",
            "Iteration 319, loss = 0.39169451\n",
            "Iteration 320, loss = 0.39151084\n",
            "Iteration 321, loss = 0.39143963\n",
            "Iteration 322, loss = 0.39133034\n",
            "Iteration 323, loss = 0.39102797\n",
            "Iteration 324, loss = 0.39105765\n",
            "Iteration 325, loss = 0.39078211\n",
            "Iteration 326, loss = 0.39053996\n",
            "Iteration 327, loss = 0.39037871\n",
            "Iteration 328, loss = 0.39042585\n",
            "Iteration 329, loss = 0.39030834\n",
            "Iteration 330, loss = 0.39007518\n",
            "Iteration 331, loss = 0.38997019\n",
            "Iteration 332, loss = 0.38970265\n",
            "Iteration 333, loss = 0.38970703\n",
            "Iteration 334, loss = 0.38943835\n",
            "Iteration 335, loss = 0.38926140\n",
            "Iteration 336, loss = 0.38908774\n",
            "Iteration 337, loss = 0.38912494\n",
            "Iteration 338, loss = 0.38901906\n",
            "Iteration 339, loss = 0.38875745\n",
            "Iteration 340, loss = 0.38857496\n",
            "Iteration 341, loss = 0.38831772\n",
            "Iteration 342, loss = 0.38817508\n",
            "Iteration 343, loss = 0.38818144\n",
            "Iteration 344, loss = 0.38784656\n",
            "Iteration 345, loss = 0.38806391\n",
            "Iteration 346, loss = 0.38773829\n",
            "Iteration 347, loss = 0.38757054\n",
            "Iteration 348, loss = 0.38737761\n",
            "Iteration 349, loss = 0.38723694\n",
            "Iteration 350, loss = 0.38728072\n",
            "Iteration 351, loss = 0.38698771\n",
            "Iteration 352, loss = 0.38686072\n",
            "Iteration 353, loss = 0.38662917\n",
            "Iteration 354, loss = 0.38661470\n",
            "Iteration 355, loss = 0.38658426\n",
            "Iteration 356, loss = 0.38627630\n",
            "Iteration 357, loss = 0.38618452\n",
            "Iteration 358, loss = 0.38603546\n",
            "Iteration 359, loss = 0.38595838\n",
            "Iteration 360, loss = 0.38585804\n",
            "Iteration 361, loss = 0.38568214\n",
            "Iteration 362, loss = 0.38558670\n",
            "Iteration 363, loss = 0.38543423\n",
            "Iteration 364, loss = 0.38536469\n",
            "Iteration 365, loss = 0.38522567\n",
            "Iteration 366, loss = 0.38506207\n",
            "Iteration 367, loss = 0.38534136\n",
            "Iteration 368, loss = 0.38480777\n",
            "Iteration 369, loss = 0.38479395\n",
            "Iteration 370, loss = 0.38452364\n",
            "Iteration 371, loss = 0.38439048\n",
            "Iteration 372, loss = 0.38427313\n",
            "Iteration 373, loss = 0.38419314\n",
            "Iteration 374, loss = 0.38417830\n",
            "Iteration 375, loss = 0.38401749\n",
            "Iteration 376, loss = 0.38381224\n",
            "Iteration 377, loss = 0.38379907\n",
            "Iteration 378, loss = 0.38364251\n",
            "Iteration 379, loss = 0.38337748\n",
            "Iteration 380, loss = 0.38327055\n",
            "Iteration 381, loss = 0.38315077\n",
            "Iteration 382, loss = 0.38299568\n",
            "Iteration 383, loss = 0.38285075\n",
            "Iteration 384, loss = 0.38280294\n",
            "Iteration 385, loss = 0.38271964\n",
            "Iteration 386, loss = 0.38262545\n",
            "Iteration 387, loss = 0.38246151\n",
            "Iteration 388, loss = 0.38244068\n",
            "Iteration 389, loss = 0.38236176\n",
            "Iteration 390, loss = 0.38218291\n",
            "Iteration 391, loss = 0.38198254\n",
            "Iteration 392, loss = 0.38186463\n",
            "Iteration 393, loss = 0.38180429\n",
            "Iteration 394, loss = 0.38168300\n",
            "Iteration 395, loss = 0.38155162\n",
            "Iteration 396, loss = 0.38139067\n",
            "Iteration 397, loss = 0.38120477\n",
            "Iteration 398, loss = 0.38118729\n",
            "Iteration 399, loss = 0.38135566\n",
            "Iteration 400, loss = 0.38116670\n",
            "Iteration 401, loss = 0.38099418\n",
            "Iteration 402, loss = 0.38082326\n",
            "Iteration 403, loss = 0.38066401\n",
            "Iteration 404, loss = 0.38071928\n",
            "Iteration 405, loss = 0.38056860\n",
            "Iteration 406, loss = 0.38043890\n",
            "Iteration 407, loss = 0.38035866\n",
            "Iteration 408, loss = 0.38024725\n",
            "Iteration 409, loss = 0.38010143\n",
            "Iteration 410, loss = 0.37986265\n",
            "Iteration 411, loss = 0.38009928\n",
            "Iteration 412, loss = 0.38005333\n",
            "Iteration 413, loss = 0.37989193\n",
            "Iteration 414, loss = 0.37951625\n",
            "Iteration 415, loss = 0.37936991\n",
            "Iteration 416, loss = 0.37945809\n",
            "Iteration 417, loss = 0.37945220\n",
            "Iteration 418, loss = 0.37934250\n",
            "Iteration 419, loss = 0.37922303\n",
            "Iteration 420, loss = 0.37890751\n",
            "Iteration 421, loss = 0.37882570\n",
            "Iteration 422, loss = 0.37874942\n",
            "Iteration 423, loss = 0.37875031\n",
            "Iteration 424, loss = 0.37848189\n",
            "Iteration 425, loss = 0.37847618\n",
            "Iteration 426, loss = 0.37841198\n",
            "Iteration 427, loss = 0.37829777\n",
            "Iteration 428, loss = 0.37833986\n",
            "Iteration 429, loss = 0.37807879\n",
            "Iteration 430, loss = 0.37795503\n",
            "Iteration 431, loss = 0.37807031\n",
            "Iteration 432, loss = 0.37791736\n",
            "Iteration 433, loss = 0.37775940\n",
            "Iteration 434, loss = 0.37755950\n",
            "Iteration 435, loss = 0.37743115\n",
            "Iteration 436, loss = 0.37790752\n",
            "Iteration 437, loss = 0.37776453\n",
            "Iteration 438, loss = 0.37744844\n",
            "Iteration 439, loss = 0.37719671\n",
            "Iteration 440, loss = 0.37706167\n",
            "Iteration 441, loss = 0.37717318\n",
            "Iteration 442, loss = 0.37698737\n",
            "Iteration 443, loss = 0.37686234\n",
            "Iteration 444, loss = 0.37676375\n",
            "Iteration 445, loss = 0.37663140\n",
            "Iteration 446, loss = 0.37653465\n",
            "Iteration 447, loss = 0.37649393\n",
            "Iteration 448, loss = 0.37642081\n",
            "Iteration 449, loss = 0.37627600\n",
            "Iteration 450, loss = 0.37623919\n",
            "Iteration 451, loss = 0.37614368\n",
            "Iteration 452, loss = 0.37615625\n",
            "Iteration 453, loss = 0.37598128\n",
            "Iteration 454, loss = 0.37596200\n",
            "Iteration 455, loss = 0.37589792\n",
            "Iteration 456, loss = 0.37580826\n",
            "Iteration 457, loss = 0.37574972\n",
            "Iteration 458, loss = 0.37561235\n",
            "Iteration 459, loss = 0.37550635\n",
            "Iteration 460, loss = 0.37554358\n",
            "Iteration 461, loss = 0.37539407\n",
            "Iteration 462, loss = 0.37530249\n",
            "Iteration 463, loss = 0.37514822\n",
            "Iteration 464, loss = 0.37512553\n",
            "Iteration 465, loss = 0.37515553\n",
            "Iteration 466, loss = 0.37509476\n",
            "Iteration 467, loss = 0.37519446\n",
            "Iteration 468, loss = 0.37484573\n",
            "Iteration 469, loss = 0.37475386\n",
            "Iteration 470, loss = 0.37472461\n",
            "Iteration 471, loss = 0.37456378\n",
            "Iteration 472, loss = 0.37454416\n",
            "Iteration 473, loss = 0.37462115\n",
            "Iteration 474, loss = 0.37451157\n",
            "Iteration 475, loss = 0.37428963\n",
            "Iteration 476, loss = 0.37414993\n",
            "Iteration 477, loss = 0.37414363\n",
            "Iteration 478, loss = 0.37402836\n",
            "Iteration 479, loss = 0.37400430\n",
            "Iteration 480, loss = 0.37384864\n",
            "Iteration 481, loss = 0.37393627\n",
            "Iteration 482, loss = 0.37378672\n",
            "Iteration 483, loss = 0.37372371\n",
            "Iteration 484, loss = 0.37356888\n",
            "Iteration 485, loss = 0.37364834\n",
            "Iteration 486, loss = 0.37352271\n",
            "Iteration 487, loss = 0.37342825\n",
            "Iteration 488, loss = 0.37330227\n",
            "Iteration 489, loss = 0.37320940\n",
            "Iteration 490, loss = 0.37323183\n",
            "Iteration 491, loss = 0.37326717\n",
            "Iteration 492, loss = 0.37317021\n",
            "Iteration 493, loss = 0.37299172\n",
            "Iteration 494, loss = 0.37297301\n",
            "Iteration 495, loss = 0.37303251\n",
            "Iteration 496, loss = 0.37312295\n",
            "Iteration 497, loss = 0.37285650\n",
            "Iteration 498, loss = 0.37281857\n",
            "Iteration 499, loss = 0.37259643\n",
            "Iteration 500, loss = 0.37255520\n",
            "Iteration 501, loss = 0.37257961\n",
            "Iteration 502, loss = 0.37246377\n",
            "Iteration 503, loss = 0.37243633\n",
            "Iteration 504, loss = 0.37234788\n",
            "Iteration 505, loss = 0.37237636\n",
            "Iteration 506, loss = 0.37227215\n",
            "Iteration 507, loss = 0.37212943\n",
            "Iteration 508, loss = 0.37201375\n",
            "Iteration 509, loss = 0.37238692\n",
            "Iteration 510, loss = 0.37198246\n",
            "Iteration 511, loss = 0.37206500\n",
            "Iteration 512, loss = 0.37187530\n",
            "Iteration 513, loss = 0.37174772\n",
            "Iteration 514, loss = 0.37187984\n",
            "Iteration 515, loss = 0.37184566\n",
            "Iteration 516, loss = 0.37175288\n",
            "Iteration 517, loss = 0.37173071\n",
            "Iteration 518, loss = 0.37160442\n",
            "Iteration 519, loss = 0.37160660\n",
            "Iteration 520, loss = 0.37134139\n",
            "Iteration 521, loss = 0.37131138\n",
            "Iteration 522, loss = 0.37124192\n",
            "Iteration 523, loss = 0.37114687\n",
            "Iteration 524, loss = 0.37107851\n",
            "Iteration 525, loss = 0.37117618\n",
            "Iteration 526, loss = 0.37099140\n",
            "Iteration 527, loss = 0.37108347\n",
            "Iteration 528, loss = 0.37087588\n",
            "Iteration 529, loss = 0.37081459\n",
            "Iteration 530, loss = 0.37083729\n",
            "Iteration 531, loss = 0.37065057\n",
            "Iteration 532, loss = 0.37076808\n",
            "Iteration 533, loss = 0.37059155\n",
            "Iteration 534, loss = 0.37051141\n",
            "Iteration 535, loss = 0.37041471\n",
            "Iteration 536, loss = 0.37039135\n",
            "Iteration 537, loss = 0.37034971\n",
            "Iteration 538, loss = 0.37033742\n",
            "Iteration 539, loss = 0.37020422\n",
            "Iteration 540, loss = 0.37016736\n",
            "Iteration 541, loss = 0.37018146\n",
            "Iteration 542, loss = 0.37007711\n",
            "Iteration 543, loss = 0.37014500\n",
            "Iteration 544, loss = 0.37006365\n",
            "Iteration 545, loss = 0.36999640\n",
            "Iteration 546, loss = 0.36989798\n",
            "Iteration 547, loss = 0.36990305\n",
            "Iteration 548, loss = 0.36996159\n",
            "Iteration 549, loss = 0.36972745\n",
            "Iteration 550, loss = 0.37031777\n",
            "Iteration 551, loss = 0.37009285\n",
            "Iteration 552, loss = 0.36985261\n",
            "Iteration 553, loss = 0.36964593\n",
            "Iteration 554, loss = 0.36973986\n",
            "Iteration 555, loss = 0.36964112\n",
            "Iteration 556, loss = 0.36944649\n",
            "Iteration 557, loss = 0.36948361\n",
            "Iteration 558, loss = 0.36940999\n",
            "Iteration 559, loss = 0.36938816\n",
            "Iteration 560, loss = 0.36922899\n",
            "Iteration 561, loss = 0.36921335\n",
            "Iteration 562, loss = 0.36921194\n",
            "Iteration 563, loss = 0.36910988\n",
            "Iteration 564, loss = 0.36906817\n",
            "Iteration 565, loss = 0.36901868\n",
            "Iteration 566, loss = 0.36893562\n",
            "Iteration 567, loss = 0.36870316\n",
            "Iteration 568, loss = 0.36876984\n",
            "Iteration 569, loss = 0.36870776\n",
            "Iteration 570, loss = 0.36875407\n",
            "Iteration 571, loss = 0.36864077\n",
            "Iteration 572, loss = 0.36858637\n",
            "Iteration 573, loss = 0.36854823\n",
            "Iteration 574, loss = 0.36854329\n",
            "Iteration 575, loss = 0.36841208\n",
            "Iteration 576, loss = 0.36836818\n",
            "Iteration 577, loss = 0.36825788\n",
            "Iteration 578, loss = 0.36834204\n",
            "Iteration 579, loss = 0.36823302\n",
            "Iteration 580, loss = 0.36822846\n",
            "Iteration 581, loss = 0.36814002\n",
            "Iteration 582, loss = 0.36809733\n",
            "Iteration 583, loss = 0.36803015\n",
            "Iteration 584, loss = 0.36803742\n",
            "Iteration 585, loss = 0.36790552\n",
            "Iteration 586, loss = 0.36812345\n",
            "Iteration 587, loss = 0.36790784\n",
            "Iteration 588, loss = 0.36780879\n",
            "Iteration 589, loss = 0.36815529\n",
            "Iteration 590, loss = 0.36768497\n",
            "Iteration 591, loss = 0.36778252\n",
            "Iteration 592, loss = 0.36766629\n",
            "Iteration 593, loss = 0.36754168\n",
            "Iteration 594, loss = 0.36752965\n",
            "Iteration 595, loss = 0.36752354\n",
            "Iteration 596, loss = 0.36750377\n",
            "Iteration 597, loss = 0.36740324\n",
            "Iteration 598, loss = 0.36734653\n",
            "Iteration 599, loss = 0.36729875\n",
            "Iteration 600, loss = 0.36761466\n",
            "Iteration 601, loss = 0.36730529\n",
            "Iteration 602, loss = 0.36736658\n",
            "Iteration 603, loss = 0.36747148\n",
            "Iteration 604, loss = 0.36734220\n",
            "Iteration 605, loss = 0.36718596\n",
            "Iteration 606, loss = 0.36700838\n",
            "Iteration 607, loss = 0.36692714\n",
            "Iteration 608, loss = 0.36709297\n",
            "Iteration 609, loss = 0.36701126\n",
            "Iteration 610, loss = 0.36689802\n",
            "Iteration 611, loss = 0.36677255\n",
            "Iteration 612, loss = 0.36674491\n",
            "Iteration 613, loss = 0.36675348\n",
            "Iteration 614, loss = 0.36662728\n",
            "Iteration 615, loss = 0.36660954\n",
            "Iteration 616, loss = 0.36656594\n",
            "Iteration 617, loss = 0.36656181\n",
            "Iteration 618, loss = 0.36653228\n",
            "Iteration 619, loss = 0.36646671\n",
            "Iteration 620, loss = 0.36643882\n",
            "Iteration 621, loss = 0.36642423\n",
            "Iteration 622, loss = 0.36664122\n",
            "Iteration 623, loss = 0.36662544\n",
            "Iteration 624, loss = 0.36644020\n",
            "Iteration 625, loss = 0.36634812\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.71460790\n",
            "Iteration 2, loss = 0.71321656\n",
            "Iteration 3, loss = 0.71202615\n",
            "Iteration 4, loss = 0.71066814\n",
            "Iteration 5, loss = 0.70950879\n",
            "Iteration 6, loss = 0.70837923\n",
            "Iteration 7, loss = 0.70720465\n",
            "Iteration 8, loss = 0.70622172\n",
            "Iteration 9, loss = 0.70519022\n",
            "Iteration 10, loss = 0.70401408\n",
            "Iteration 11, loss = 0.70307496\n",
            "Iteration 12, loss = 0.70215674\n",
            "Iteration 13, loss = 0.70115987\n",
            "Iteration 14, loss = 0.70024175\n",
            "Iteration 15, loss = 0.69937607\n",
            "Iteration 16, loss = 0.69841076\n",
            "Iteration 17, loss = 0.69754830\n",
            "Iteration 18, loss = 0.69675189\n",
            "Iteration 19, loss = 0.69594477\n",
            "Iteration 20, loss = 0.69511773\n",
            "Iteration 21, loss = 0.69441202\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.71427190\n",
            "Iteration 2, loss = 0.71287735\n",
            "Iteration 3, loss = 0.71173028\n",
            "Iteration 4, loss = 0.71045029\n",
            "Iteration 5, loss = 0.70937067\n",
            "Iteration 6, loss = 0.70826990\n",
            "Iteration 7, loss = 0.70715679\n",
            "Iteration 8, loss = 0.70618200\n",
            "Iteration 9, loss = 0.70519379\n",
            "Iteration 10, loss = 0.70400992\n",
            "Iteration 11, loss = 0.70310960\n",
            "Iteration 12, loss = 0.70222772\n",
            "Iteration 13, loss = 0.70124792\n",
            "Iteration 14, loss = 0.70037953\n",
            "Iteration 15, loss = 0.69949421\n",
            "Iteration 16, loss = 0.69855637\n",
            "Iteration 17, loss = 0.69775625\n",
            "Iteration 18, loss = 0.69697965\n",
            "Iteration 19, loss = 0.69620383\n",
            "Iteration 20, loss = 0.69538844\n",
            "Iteration 21, loss = 0.69460777\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.71449371\n",
            "Iteration 2, loss = 0.71301648\n",
            "Iteration 3, loss = 0.71180783\n",
            "Iteration 4, loss = 0.71041331\n",
            "Iteration 5, loss = 0.70928395\n",
            "Iteration 6, loss = 0.70810140\n",
            "Iteration 7, loss = 0.70698230\n",
            "Iteration 8, loss = 0.70592961\n",
            "Iteration 9, loss = 0.70489770\n",
            "Iteration 10, loss = 0.70376009\n",
            "Iteration 11, loss = 0.70288977\n",
            "Iteration 12, loss = 0.70192434\n",
            "Iteration 13, loss = 0.70093200\n",
            "Iteration 14, loss = 0.70006661\n",
            "Iteration 15, loss = 0.69923660\n",
            "Iteration 16, loss = 0.69826933\n",
            "Iteration 17, loss = 0.69743146\n",
            "Iteration 18, loss = 0.69662922\n",
            "Iteration 19, loss = 0.69583402\n",
            "Iteration 20, loss = 0.69506586\n",
            "Iteration 21, loss = 0.69431339\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.71458690\n",
            "Iteration 2, loss = 0.71314999\n",
            "Iteration 3, loss = 0.71189265\n",
            "Iteration 4, loss = 0.71049195\n",
            "Iteration 5, loss = 0.70932018\n",
            "Iteration 6, loss = 0.70809452\n",
            "Iteration 7, loss = 0.70698360\n",
            "Iteration 8, loss = 0.70581521\n",
            "Iteration 9, loss = 0.70474494\n",
            "Iteration 10, loss = 0.70357536\n",
            "Iteration 11, loss = 0.70264636\n",
            "Iteration 12, loss = 0.70154443\n",
            "Iteration 13, loss = 0.70062935\n",
            "Iteration 14, loss = 0.69967624\n",
            "Iteration 15, loss = 0.69873009\n",
            "Iteration 16, loss = 0.69774861\n",
            "Iteration 17, loss = 0.69692779\n",
            "Iteration 18, loss = 0.69612323\n",
            "Iteration 19, loss = 0.69524071\n",
            "Iteration 20, loss = 0.69450821\n",
            "Iteration 21, loss = 0.69373566\n",
            "Iteration 22, loss = 0.69297688\n",
            "Iteration 23, loss = 0.69228793\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.71493598\n",
            "Iteration 2, loss = 0.71343795\n",
            "Iteration 3, loss = 0.71214249\n",
            "Iteration 4, loss = 0.71073453\n",
            "Iteration 5, loss = 0.70944430\n",
            "Iteration 6, loss = 0.70831324\n",
            "Iteration 7, loss = 0.70709049\n",
            "Iteration 8, loss = 0.70596166\n",
            "Iteration 9, loss = 0.70486352\n",
            "Iteration 10, loss = 0.70369251\n",
            "Iteration 11, loss = 0.70276137\n",
            "Iteration 12, loss = 0.70175067\n",
            "Iteration 13, loss = 0.70073800\n",
            "Iteration 14, loss = 0.69979383\n",
            "Iteration 15, loss = 0.69887642\n",
            "Iteration 16, loss = 0.69790312\n",
            "Iteration 17, loss = 0.69704668\n",
            "Iteration 18, loss = 0.69625401\n",
            "Iteration 19, loss = 0.69544107\n",
            "Iteration 20, loss = 0.69468035\n",
            "Iteration 21, loss = 0.69397118\n",
            "Iteration 22, loss = 0.69320416\n",
            "Iteration 23, loss = 0.69260096\n",
            "Iteration 24, loss = 0.69187435\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.71388902\n",
            "Iteration 2, loss = 0.71246620\n",
            "Iteration 3, loss = 0.71116484\n",
            "Iteration 4, loss = 0.70986986\n",
            "Iteration 5, loss = 0.70862687\n",
            "Iteration 6, loss = 0.70749312\n",
            "Iteration 7, loss = 0.70635438\n",
            "Iteration 8, loss = 0.70524572\n",
            "Iteration 9, loss = 0.70422616\n",
            "Iteration 10, loss = 0.70305828\n",
            "Iteration 11, loss = 0.70208973\n",
            "Iteration 12, loss = 0.70118331\n",
            "Iteration 13, loss = 0.70021256\n",
            "Iteration 14, loss = 0.69920311\n",
            "Iteration 15, loss = 0.69839975\n",
            "Iteration 16, loss = 0.69744246\n",
            "Iteration 17, loss = 0.69665226\n",
            "Iteration 18, loss = 0.69577030\n",
            "Iteration 19, loss = 0.69501081\n",
            "Iteration 20, loss = 0.69425616\n",
            "Iteration 21, loss = 0.69352031\n",
            "Iteration 22, loss = 0.69277485\n",
            "Iteration 23, loss = 0.69210269\n",
            "Iteration 24, loss = 0.69145563\n",
            "Iteration 25, loss = 0.69075085\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.71451868\n",
            "Iteration 2, loss = 0.71318634\n",
            "Iteration 3, loss = 0.71186370\n",
            "Iteration 4, loss = 0.71064069\n",
            "Iteration 5, loss = 0.70931891\n",
            "Iteration 6, loss = 0.70817023\n",
            "Iteration 7, loss = 0.70697103\n",
            "Iteration 8, loss = 0.70578794\n",
            "Iteration 9, loss = 0.70477062\n",
            "Iteration 10, loss = 0.70352847\n",
            "Iteration 11, loss = 0.70253855\n",
            "Iteration 12, loss = 0.70156273\n",
            "Iteration 13, loss = 0.70056387\n",
            "Iteration 14, loss = 0.69955579\n",
            "Iteration 15, loss = 0.69868071\n",
            "Iteration 16, loss = 0.69769488\n",
            "Iteration 17, loss = 0.69686448\n",
            "Iteration 18, loss = 0.69601987\n",
            "Iteration 19, loss = 0.69518709\n",
            "Iteration 20, loss = 0.69444571\n",
            "Iteration 21, loss = 0.69366142\n",
            "Iteration 22, loss = 0.69289654\n",
            "Iteration 23, loss = 0.69216318\n",
            "Iteration 24, loss = 0.69153034\n",
            "Iteration 25, loss = 0.69075449\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.71403741\n",
            "Iteration 2, loss = 0.71269964\n",
            "Iteration 3, loss = 0.71143934\n",
            "Iteration 4, loss = 0.71025484\n",
            "Iteration 5, loss = 0.70889951\n",
            "Iteration 6, loss = 0.70782782\n",
            "Iteration 7, loss = 0.70670017\n",
            "Iteration 8, loss = 0.70555029\n",
            "Iteration 9, loss = 0.70452060\n",
            "Iteration 10, loss = 0.70334897\n",
            "Iteration 11, loss = 0.70238655\n",
            "Iteration 12, loss = 0.70144902\n",
            "Iteration 13, loss = 0.70042921\n",
            "Iteration 14, loss = 0.69946018\n",
            "Iteration 15, loss = 0.69859853\n",
            "Iteration 16, loss = 0.69762388\n",
            "Iteration 17, loss = 0.69679012\n",
            "Iteration 18, loss = 0.69596500\n",
            "Iteration 19, loss = 0.69512730\n",
            "Iteration 20, loss = 0.69435592\n",
            "Iteration 21, loss = 0.69362788\n",
            "Iteration 22, loss = 0.69280903\n",
            "Iteration 23, loss = 0.69210865\n",
            "Iteration 24, loss = 0.69143733\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.71440751\n",
            "Iteration 2, loss = 0.71303346\n",
            "Iteration 3, loss = 0.71172299\n",
            "Iteration 4, loss = 0.71049695\n",
            "Iteration 5, loss = 0.70913999\n",
            "Iteration 6, loss = 0.70801497\n",
            "Iteration 7, loss = 0.70685038\n",
            "Iteration 8, loss = 0.70567383\n",
            "Iteration 9, loss = 0.70459744\n",
            "Iteration 10, loss = 0.70340106\n",
            "Iteration 11, loss = 0.70241555\n",
            "Iteration 12, loss = 0.70145888\n",
            "Iteration 13, loss = 0.70042492\n",
            "Iteration 14, loss = 0.69946569\n",
            "Iteration 15, loss = 0.69859249\n",
            "Iteration 16, loss = 0.69758675\n",
            "Iteration 17, loss = 0.69677907\n",
            "Iteration 18, loss = 0.69594834\n",
            "Iteration 19, loss = 0.69508358\n",
            "Iteration 20, loss = 0.69431278\n",
            "Iteration 21, loss = 0.69358849\n",
            "Iteration 22, loss = 0.69278887\n",
            "Iteration 23, loss = 0.69206887\n",
            "Iteration 24, loss = 0.69135984\n",
            "Iteration 25, loss = 0.69062547\n",
            "Iteration 26, loss = 0.68991423\n",
            "Iteration 27, loss = 0.68914758\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.71463623\n",
            "Iteration 2, loss = 0.71320153\n",
            "Iteration 3, loss = 0.71183347\n",
            "Iteration 4, loss = 0.71058774\n",
            "Iteration 5, loss = 0.70921297\n",
            "Iteration 6, loss = 0.70801347\n",
            "Iteration 7, loss = 0.70688365\n",
            "Iteration 8, loss = 0.70568512\n",
            "Iteration 9, loss = 0.70456983\n",
            "Iteration 10, loss = 0.70337597\n",
            "Iteration 11, loss = 0.70242730\n",
            "Iteration 12, loss = 0.70150802\n",
            "Iteration 13, loss = 0.70046148\n",
            "Iteration 14, loss = 0.69953761\n",
            "Iteration 15, loss = 0.69872994\n",
            "Iteration 16, loss = 0.69777361\n",
            "Iteration 17, loss = 0.69701080\n",
            "Iteration 18, loss = 0.69619613\n",
            "Iteration 19, loss = 0.69540845\n",
            "Iteration 20, loss = 0.69462257\n",
            "Iteration 21, loss = 0.69391451\n",
            "Iteration 22, loss = 0.69313295\n",
            "Iteration 23, loss = 0.69241579\n",
            "Iteration 24, loss = 0.69172551\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.71436866\n",
            "Iteration 2, loss = 0.71298470\n",
            "Iteration 3, loss = 0.71155144\n",
            "Iteration 4, loss = 0.71028647\n",
            "Iteration 5, loss = 0.70896064\n",
            "Iteration 6, loss = 0.70770178\n",
            "Iteration 7, loss = 0.70647573\n",
            "Iteration 8, loss = 0.70547062\n",
            "Iteration 9, loss = 0.70419805\n",
            "Iteration 10, loss = 0.70317954\n",
            "Iteration 11, loss = 0.70219892\n",
            "Iteration 12, loss = 0.70127937\n",
            "Iteration 13, loss = 0.70023070\n",
            "Iteration 14, loss = 0.69931209\n",
            "Iteration 15, loss = 0.69836777\n",
            "Iteration 16, loss = 0.69755130\n",
            "Iteration 17, loss = 0.69672000\n",
            "Iteration 18, loss = 0.69581642\n",
            "Iteration 19, loss = 0.69502530\n",
            "Iteration 20, loss = 0.69427321\n",
            "Iteration 21, loss = 0.69345774\n",
            "Iteration 22, loss = 0.69281167\n",
            "Iteration 23, loss = 0.69196743\n",
            "Iteration 24, loss = 0.69121860\n",
            "Iteration 25, loss = 0.69052842\n",
            "Iteration 26, loss = 0.68972307\n",
            "Iteration 27, loss = 0.68896869\n",
            "Iteration 28, loss = 0.68812995\n",
            "Iteration 29, loss = 0.68729404\n",
            "Iteration 30, loss = 0.68637688\n",
            "Iteration 31, loss = 0.68542227\n",
            "Iteration 32, loss = 0.68434155\n",
            "Iteration 33, loss = 0.68325400\n",
            "Iteration 34, loss = 0.68191939\n",
            "Iteration 35, loss = 0.68054939\n",
            "Iteration 36, loss = 0.67885404\n",
            "Iteration 37, loss = 0.67713631\n",
            "Iteration 38, loss = 0.67521563\n",
            "Iteration 39, loss = 0.67334233\n",
            "Iteration 40, loss = 0.67145680\n",
            "Iteration 41, loss = 0.66941992\n",
            "Iteration 42, loss = 0.66731086\n",
            "Iteration 43, loss = 0.66514131\n",
            "Iteration 44, loss = 0.66281762\n",
            "Iteration 45, loss = 0.66068185\n",
            "Iteration 46, loss = 0.65833686\n",
            "Iteration 47, loss = 0.65609875\n",
            "Iteration 48, loss = 0.65392692\n",
            "Iteration 49, loss = 0.65194546\n",
            "Iteration 50, loss = 0.64994150\n",
            "Iteration 51, loss = 0.64751793\n",
            "Iteration 52, loss = 0.64532684\n",
            "Iteration 53, loss = 0.64301287\n",
            "Iteration 54, loss = 0.64097946\n",
            "Iteration 55, loss = 0.63905572\n",
            "Iteration 56, loss = 0.63735068\n",
            "Iteration 57, loss = 0.63561865\n",
            "Iteration 58, loss = 0.63404152\n",
            "Iteration 59, loss = 0.63244765\n",
            "Iteration 60, loss = 0.63104276\n",
            "Iteration 61, loss = 0.62953840\n",
            "Iteration 62, loss = 0.62806525\n",
            "Iteration 63, loss = 0.62652560\n",
            "Iteration 64, loss = 0.62491959\n",
            "Iteration 65, loss = 0.62319919\n",
            "Iteration 66, loss = 0.62159656\n",
            "Iteration 67, loss = 0.61998672\n",
            "Iteration 68, loss = 0.61822360\n",
            "Iteration 69, loss = 0.61651340\n",
            "Iteration 70, loss = 0.61471815\n",
            "Iteration 71, loss = 0.61302066\n",
            "Iteration 72, loss = 0.61140251\n",
            "Iteration 73, loss = 0.60980674\n",
            "Iteration 74, loss = 0.60817142\n",
            "Iteration 75, loss = 0.60647817\n",
            "Iteration 76, loss = 0.60475580\n",
            "Iteration 77, loss = 0.60298172\n",
            "Iteration 78, loss = 0.60122843\n",
            "Iteration 79, loss = 0.59939725\n",
            "Iteration 80, loss = 0.59740746\n",
            "Iteration 81, loss = 0.59545157\n",
            "Iteration 82, loss = 0.59348525\n",
            "Iteration 83, loss = 0.59138442\n",
            "Iteration 84, loss = 0.58909578\n",
            "Iteration 85, loss = 0.58642418\n",
            "Iteration 86, loss = 0.58367510\n",
            "Iteration 87, loss = 0.58086702\n",
            "Iteration 88, loss = 0.57778830\n",
            "Iteration 89, loss = 0.57483844\n",
            "Iteration 90, loss = 0.57177864\n",
            "Iteration 91, loss = 0.56874960\n",
            "Iteration 92, loss = 0.56552271\n",
            "Iteration 93, loss = 0.56244126\n",
            "Iteration 94, loss = 0.55902497\n",
            "Iteration 95, loss = 0.55612706\n",
            "Iteration 96, loss = 0.55274194\n",
            "Iteration 97, loss = 0.54944077\n",
            "Iteration 98, loss = 0.54638804\n",
            "Iteration 99, loss = 0.54312252\n",
            "Iteration 100, loss = 0.53979725\n",
            "Iteration 101, loss = 0.53662470\n",
            "Iteration 102, loss = 0.53347158\n",
            "Iteration 103, loss = 0.53066218\n",
            "Iteration 104, loss = 0.52784879\n",
            "Iteration 105, loss = 0.52513184\n",
            "Iteration 106, loss = 0.52293509\n",
            "Iteration 107, loss = 0.52025176\n",
            "Iteration 108, loss = 0.51802391\n",
            "Iteration 109, loss = 0.51583627\n",
            "Iteration 110, loss = 0.51390106\n",
            "Iteration 111, loss = 0.51186364\n",
            "Iteration 112, loss = 0.50976400\n",
            "Iteration 113, loss = 0.50788703\n",
            "Iteration 114, loss = 0.50602841\n",
            "Iteration 115, loss = 0.50426711\n",
            "Iteration 116, loss = 0.50255519\n",
            "Iteration 117, loss = 0.50089726\n",
            "Iteration 118, loss = 0.49925036\n",
            "Iteration 119, loss = 0.49760243\n",
            "Iteration 120, loss = 0.49604437\n",
            "Iteration 121, loss = 0.49455642\n",
            "Iteration 122, loss = 0.49310537\n",
            "Iteration 123, loss = 0.49169696\n",
            "Iteration 124, loss = 0.49020270\n",
            "Iteration 125, loss = 0.48880190\n",
            "Iteration 126, loss = 0.48757120\n",
            "Iteration 127, loss = 0.48622578\n",
            "Iteration 128, loss = 0.48488589\n",
            "Iteration 129, loss = 0.48357933\n",
            "Iteration 130, loss = 0.48227846\n",
            "Iteration 131, loss = 0.48106975\n",
            "Iteration 132, loss = 0.47985496\n",
            "Iteration 133, loss = 0.47874135\n",
            "Iteration 134, loss = 0.47742241\n",
            "Iteration 135, loss = 0.47630752\n",
            "Iteration 136, loss = 0.47513997\n",
            "Iteration 137, loss = 0.47392190\n",
            "Iteration 138, loss = 0.47287117\n",
            "Iteration 139, loss = 0.47173976\n",
            "Iteration 140, loss = 0.47061560\n",
            "Iteration 141, loss = 0.46963345\n",
            "Iteration 142, loss = 0.46854473\n",
            "Iteration 143, loss = 0.46755233\n",
            "Iteration 144, loss = 0.46650889\n",
            "Iteration 145, loss = 0.46547016\n",
            "Iteration 146, loss = 0.46455294\n",
            "Iteration 147, loss = 0.46358548\n",
            "Iteration 148, loss = 0.46263003\n",
            "Iteration 149, loss = 0.46183301\n",
            "Iteration 150, loss = 0.46069190\n",
            "Iteration 151, loss = 0.45978193\n",
            "Iteration 152, loss = 0.45889381\n",
            "Iteration 153, loss = 0.45801085\n",
            "Iteration 154, loss = 0.45704234\n",
            "Iteration 155, loss = 0.45616448\n",
            "Iteration 156, loss = 0.45535535\n",
            "Iteration 157, loss = 0.45451620\n",
            "Iteration 158, loss = 0.45380139\n",
            "Iteration 159, loss = 0.45299652\n",
            "Iteration 160, loss = 0.45217335\n",
            "Iteration 161, loss = 0.45140140\n",
            "Iteration 162, loss = 0.45065064\n",
            "Iteration 163, loss = 0.44998447\n",
            "Iteration 164, loss = 0.44920413\n",
            "Iteration 165, loss = 0.44859533\n",
            "Iteration 166, loss = 0.44782006\n",
            "Iteration 167, loss = 0.44710385\n",
            "Iteration 168, loss = 0.44639360\n",
            "Iteration 169, loss = 0.44571490\n",
            "Iteration 170, loss = 0.44514482\n",
            "Iteration 171, loss = 0.44445105\n",
            "Iteration 172, loss = 0.44381936\n",
            "Iteration 173, loss = 0.44316721\n",
            "Iteration 174, loss = 0.44256493\n",
            "Iteration 175, loss = 0.44196929\n",
            "Iteration 176, loss = 0.44124637\n",
            "Iteration 177, loss = 0.44081203\n",
            "Iteration 178, loss = 0.44037001\n",
            "Iteration 179, loss = 0.43950856\n",
            "Iteration 180, loss = 0.43889165\n",
            "Iteration 181, loss = 0.43848815\n",
            "Iteration 182, loss = 0.43781310\n",
            "Iteration 183, loss = 0.43718786\n",
            "Iteration 184, loss = 0.43664502\n",
            "Iteration 185, loss = 0.43624056\n",
            "Iteration 186, loss = 0.43563571\n",
            "Iteration 187, loss = 0.43509901\n",
            "Iteration 188, loss = 0.43458097\n",
            "Iteration 189, loss = 0.43407435\n",
            "Iteration 190, loss = 0.43352046\n",
            "Iteration 191, loss = 0.43308542\n",
            "Iteration 192, loss = 0.43255121\n",
            "Iteration 193, loss = 0.43206666\n",
            "Iteration 194, loss = 0.43164378\n",
            "Iteration 195, loss = 0.43115643\n",
            "Iteration 196, loss = 0.43067406\n",
            "Iteration 197, loss = 0.43018084\n",
            "Iteration 198, loss = 0.42975603\n",
            "Iteration 199, loss = 0.42938489\n",
            "Iteration 200, loss = 0.42883304\n",
            "Iteration 201, loss = 0.42829885\n",
            "Iteration 202, loss = 0.42787333\n",
            "Iteration 203, loss = 0.42746061\n",
            "Iteration 204, loss = 0.42706963\n",
            "Iteration 205, loss = 0.42657284\n",
            "Iteration 206, loss = 0.42617682\n",
            "Iteration 207, loss = 0.42577324\n",
            "Iteration 208, loss = 0.42519410\n",
            "Iteration 209, loss = 0.42484251\n",
            "Iteration 210, loss = 0.42435014\n",
            "Iteration 211, loss = 0.42402041\n",
            "Iteration 212, loss = 0.42348916\n",
            "Iteration 213, loss = 0.42311840\n",
            "Iteration 214, loss = 0.42265312\n",
            "Iteration 215, loss = 0.42221378\n",
            "Iteration 216, loss = 0.42190195\n",
            "Iteration 217, loss = 0.42145215\n",
            "Iteration 218, loss = 0.42121295\n",
            "Iteration 219, loss = 0.42080155\n",
            "Iteration 220, loss = 0.42034705\n",
            "Iteration 221, loss = 0.41999610\n",
            "Iteration 222, loss = 0.41964580\n",
            "Iteration 223, loss = 0.41934934\n",
            "Iteration 224, loss = 0.41916394\n",
            "Iteration 225, loss = 0.41862046\n",
            "Iteration 226, loss = 0.41850487\n",
            "Iteration 227, loss = 0.41808654\n",
            "Iteration 228, loss = 0.41769004\n",
            "Iteration 229, loss = 0.41739748\n",
            "Iteration 230, loss = 0.41702452\n",
            "Iteration 231, loss = 0.41676242\n",
            "Iteration 232, loss = 0.41649931\n",
            "Iteration 233, loss = 0.41603762\n",
            "Iteration 234, loss = 0.41572983\n",
            "Iteration 235, loss = 0.41543584\n",
            "Iteration 236, loss = 0.41511029\n",
            "Iteration 237, loss = 0.41480372\n",
            "Iteration 238, loss = 0.41458596\n",
            "Iteration 239, loss = 0.41443819\n",
            "Iteration 240, loss = 0.41397656\n",
            "Iteration 241, loss = 0.41368089\n",
            "Iteration 242, loss = 0.41335850\n",
            "Iteration 243, loss = 0.41305139\n",
            "Iteration 244, loss = 0.41263849\n",
            "Iteration 245, loss = 0.41255489\n",
            "Iteration 246, loss = 0.41217171\n",
            "Iteration 247, loss = 0.41173162\n",
            "Iteration 248, loss = 0.41145007\n",
            "Iteration 249, loss = 0.41115669\n",
            "Iteration 250, loss = 0.41092875\n",
            "Iteration 251, loss = 0.41059179\n",
            "Iteration 252, loss = 0.41040227\n",
            "Iteration 253, loss = 0.41008763\n",
            "Iteration 254, loss = 0.40978392\n",
            "Iteration 255, loss = 0.40951109\n",
            "Iteration 256, loss = 0.40929860\n",
            "Iteration 257, loss = 0.40925924\n",
            "Iteration 258, loss = 0.40878167\n",
            "Iteration 259, loss = 0.40850066\n",
            "Iteration 260, loss = 0.40828421\n",
            "Iteration 261, loss = 0.40820107\n",
            "Iteration 262, loss = 0.40776081\n",
            "Iteration 263, loss = 0.40745116\n",
            "Iteration 264, loss = 0.40734584\n",
            "Iteration 265, loss = 0.40705954\n",
            "Iteration 266, loss = 0.40679379\n",
            "Iteration 267, loss = 0.40652085\n",
            "Iteration 268, loss = 0.40634489\n",
            "Iteration 269, loss = 0.40628220\n",
            "Iteration 270, loss = 0.40584693\n",
            "Iteration 271, loss = 0.40562984\n",
            "Iteration 272, loss = 0.40533394\n",
            "Iteration 273, loss = 0.40524570\n",
            "Iteration 274, loss = 0.40492586\n",
            "Iteration 275, loss = 0.40465904\n",
            "Iteration 276, loss = 0.40453719\n",
            "Iteration 277, loss = 0.40429081\n",
            "Iteration 278, loss = 0.40410599\n",
            "Iteration 279, loss = 0.40385187\n",
            "Iteration 280, loss = 0.40365186\n",
            "Iteration 281, loss = 0.40339188\n",
            "Iteration 282, loss = 0.40324111\n",
            "Iteration 283, loss = 0.40312098\n",
            "Iteration 284, loss = 0.40280962\n",
            "Iteration 285, loss = 0.40259551\n",
            "Iteration 286, loss = 0.40241629\n",
            "Iteration 287, loss = 0.40226383\n",
            "Iteration 288, loss = 0.40218468\n",
            "Iteration 289, loss = 0.40216453\n",
            "Iteration 290, loss = 0.40174757\n",
            "Iteration 291, loss = 0.40143134\n",
            "Iteration 292, loss = 0.40123658\n",
            "Iteration 293, loss = 0.40116061\n",
            "Iteration 294, loss = 0.40096650\n",
            "Iteration 295, loss = 0.40072234\n",
            "Iteration 296, loss = 0.40058093\n",
            "Iteration 297, loss = 0.40033974\n",
            "Iteration 298, loss = 0.40043088\n",
            "Iteration 299, loss = 0.40007507\n",
            "Iteration 300, loss = 0.39987545\n",
            "Iteration 301, loss = 0.39967260\n",
            "Iteration 302, loss = 0.39952394\n",
            "Iteration 303, loss = 0.39936403\n",
            "Iteration 304, loss = 0.39925167\n",
            "Iteration 305, loss = 0.39907246\n",
            "Iteration 306, loss = 0.39888978\n",
            "Iteration 307, loss = 0.39875946\n",
            "Iteration 308, loss = 0.39874608\n",
            "Iteration 309, loss = 0.39844630\n",
            "Iteration 310, loss = 0.39833908\n",
            "Iteration 311, loss = 0.39819530\n",
            "Iteration 312, loss = 0.39797390\n",
            "Iteration 313, loss = 0.39785572\n",
            "Iteration 314, loss = 0.39771523\n",
            "Iteration 315, loss = 0.39785598\n",
            "Iteration 316, loss = 0.39750317\n",
            "Iteration 317, loss = 0.39738370\n",
            "Iteration 318, loss = 0.39724919\n",
            "Iteration 319, loss = 0.39709356\n",
            "Iteration 320, loss = 0.39698929\n",
            "Iteration 321, loss = 0.39682182\n",
            "Iteration 322, loss = 0.39666256\n",
            "Iteration 323, loss = 0.39656251\n",
            "Iteration 324, loss = 0.39641515\n",
            "Iteration 325, loss = 0.39628335\n",
            "Iteration 326, loss = 0.39618216\n",
            "Iteration 327, loss = 0.39604716\n",
            "Iteration 328, loss = 0.39592786\n",
            "Iteration 329, loss = 0.39585543\n",
            "Iteration 330, loss = 0.39568447\n",
            "Iteration 331, loss = 0.39556756\n",
            "Iteration 332, loss = 0.39560163\n",
            "Iteration 333, loss = 0.39532767\n",
            "Iteration 334, loss = 0.39521951\n",
            "Iteration 335, loss = 0.39520398\n",
            "Iteration 336, loss = 0.39508849\n",
            "Iteration 337, loss = 0.39490839\n",
            "Iteration 338, loss = 0.39485525\n",
            "Iteration 339, loss = 0.39480839\n",
            "Iteration 340, loss = 0.39457939\n",
            "Iteration 341, loss = 0.39464878\n",
            "Iteration 342, loss = 0.39458420\n",
            "Iteration 343, loss = 0.39434598\n",
            "Iteration 344, loss = 0.39417486\n",
            "Iteration 345, loss = 0.39409409\n",
            "Iteration 346, loss = 0.39413964\n",
            "Iteration 347, loss = 0.39390737\n",
            "Iteration 348, loss = 0.39377362\n",
            "Iteration 349, loss = 0.39379667\n",
            "Iteration 350, loss = 0.39376375\n",
            "Iteration 351, loss = 0.39359107\n",
            "Iteration 352, loss = 0.39343511\n",
            "Iteration 353, loss = 0.39362844\n",
            "Iteration 354, loss = 0.39332694\n",
            "Iteration 355, loss = 0.39325414\n",
            "Iteration 356, loss = 0.39314084\n",
            "Iteration 357, loss = 0.39302489\n",
            "Iteration 358, loss = 0.39288193\n",
            "Iteration 359, loss = 0.39280935\n",
            "Iteration 360, loss = 0.39272524\n",
            "Iteration 361, loss = 0.39265029\n",
            "Iteration 362, loss = 0.39254886\n",
            "Iteration 363, loss = 0.39256692\n",
            "Iteration 364, loss = 0.39237287\n",
            "Iteration 365, loss = 0.39233960\n",
            "Iteration 366, loss = 0.39222598\n",
            "Iteration 367, loss = 0.39225019\n",
            "Iteration 368, loss = 0.39214527\n",
            "Iteration 369, loss = 0.39206407\n",
            "Iteration 370, loss = 0.39191243\n",
            "Iteration 371, loss = 0.39188876\n",
            "Iteration 372, loss = 0.39183156\n",
            "Iteration 373, loss = 0.39168040\n",
            "Iteration 374, loss = 0.39163261\n",
            "Iteration 375, loss = 0.39162150\n",
            "Iteration 376, loss = 0.39145090\n",
            "Iteration 377, loss = 0.39147066\n",
            "Iteration 378, loss = 0.39133228\n",
            "Iteration 379, loss = 0.39155573\n",
            "Iteration 380, loss = 0.39118617\n",
            "Iteration 381, loss = 0.39131336\n",
            "Iteration 382, loss = 0.39113404\n",
            "Iteration 383, loss = 0.39104528\n",
            "Iteration 384, loss = 0.39089439\n",
            "Iteration 385, loss = 0.39100460\n",
            "Iteration 386, loss = 0.39081413\n",
            "Iteration 387, loss = 0.39079279\n",
            "Iteration 388, loss = 0.39066933\n",
            "Iteration 389, loss = 0.39058647\n",
            "Iteration 390, loss = 0.39051401\n",
            "Iteration 391, loss = 0.39050502\n",
            "Iteration 392, loss = 0.39041573\n",
            "Iteration 393, loss = 0.39050190\n",
            "Iteration 394, loss = 0.39033883\n",
            "Iteration 395, loss = 0.39023212\n",
            "Iteration 396, loss = 0.39020753\n",
            "Iteration 397, loss = 0.39019807\n",
            "Iteration 398, loss = 0.39006689\n",
            "Iteration 399, loss = 0.39000814\n",
            "Iteration 400, loss = 0.38996926\n",
            "Iteration 401, loss = 0.38997485\n",
            "Iteration 402, loss = 0.38978884\n",
            "Iteration 403, loss = 0.38974648\n",
            "Iteration 404, loss = 0.38965462\n",
            "Iteration 405, loss = 0.38971995\n",
            "Iteration 406, loss = 0.38960030\n",
            "Iteration 407, loss = 0.38951848\n",
            "Iteration 408, loss = 0.38949947\n",
            "Iteration 409, loss = 0.38953193\n",
            "Iteration 410, loss = 0.38945962\n",
            "Iteration 411, loss = 0.38924015\n",
            "Iteration 412, loss = 0.38926658\n",
            "Iteration 413, loss = 0.38913009\n",
            "Iteration 414, loss = 0.38923163\n",
            "Iteration 415, loss = 0.38915300\n",
            "Iteration 416, loss = 0.38903943\n",
            "Iteration 417, loss = 0.38900036\n",
            "Iteration 418, loss = 0.38889484\n",
            "Iteration 419, loss = 0.38897103\n",
            "Iteration 420, loss = 0.38878258\n",
            "Iteration 421, loss = 0.38873612\n",
            "Iteration 422, loss = 0.38865695\n",
            "Iteration 423, loss = 0.38870450\n",
            "Iteration 424, loss = 0.38873018\n",
            "Iteration 425, loss = 0.38870221\n",
            "Iteration 426, loss = 0.38856109\n",
            "Iteration 427, loss = 0.38853617\n",
            "Iteration 428, loss = 0.38841887\n",
            "Iteration 429, loss = 0.38832437\n",
            "Iteration 430, loss = 0.38826946\n",
            "Iteration 431, loss = 0.38821906\n",
            "Iteration 432, loss = 0.38830331\n",
            "Iteration 433, loss = 0.38816586\n",
            "Iteration 434, loss = 0.38824081\n",
            "Iteration 435, loss = 0.38810695\n",
            "Iteration 436, loss = 0.38812757\n",
            "Iteration 437, loss = 0.38801699\n",
            "Iteration 438, loss = 0.38800108\n",
            "Iteration 439, loss = 0.38796410\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Melhores hiperparâmetros encontrados através do Random Search:\n",
            "{'tol': 0.0001, 'solver': 'adam', 'max_iter': 1000, 'activation': 'relu'}\n",
            "Melhor pontuação (acurácia) encontrada através do Random Search: 0.8223684210526315\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ajuste do modelo MLP aos dados de treinamento\n",
        "rna3_best_random = MLPClassifier(**best_params_random)\n",
        "rna3_best_random.fit(X_train_oversampled, y_train_oversampled)\n",
        "\n",
        "# Predições nos dados de teste usando o modelo com melhores hiperparâmetros encontrados pelo Random Search\n",
        "pred_random3 = rna3_best_random.predict(X_test)"
      ],
      "metadata": {
        "id": "wwHVYvs7XZrA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cálculo e impressão da acurácia nos dados de teste\n",
        "accuracy_random = accuracy_score(y_test, pred_random3)\n",
        "print(\"Acurácia do modelo MLP com melhores hiperparâmetros pelo Random Search:\", accuracy_random)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e17ae7e-4ea2-480e-8c5a-3e2f0e6ce04b",
        "id": "BH5p94jhXZrA"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acurácia do modelo MLP com melhores hiperparâmetros pelo Random Search: 0.7574626865671642\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Matriz de confusão para o modelo com melhores hiperparâmetros pelo Random Search\n",
        "print(\"Matriz de Confusão - Random Search\")\n",
        "cm_rna3_random = confusion_matrix(y_test, pred_random3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3415fb3d-4cd8-4d3c-efab-fe9f4ea43f80",
        "id": "WGY5bsFVXZrA"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matriz de Confusão - Random Search\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtém a média das acurácias (10 folds) referente ao conjunto treino\n",
        "rna3_random = r_results.loc[r_search.best_index_,'mean_test_score']"
      ],
      "metadata": {
        "id": "MzisErhQXZrA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmCmWSvctwDY"
      },
      "source": [
        "## Rede Neural (Keras)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define the keras model\n",
        "model = Sequential()\n",
        "model.add(Dense(100, input_shape=(len(df.columns)-1,), activation='relu'))\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhc6A3kbtqnF",
        "outputId": "558f2a9f-4091-43ae-9182-798e1dd55e8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_3 (Dense)             (None, 100)               2000      \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 8)                 808       \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 1)                 9         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,817\n",
            "Trainable params: 2,817\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# adam optimizer works pretty well for\n",
        "# all kinds of problems and is a good starting point\n",
        "model.compile(optimizer='adam',\n",
        "\n",
        "\t\t\t# MAE error is good for\n",
        "\t\t\t# numerical predictions\n",
        "\t\t\tloss='mae',\n",
        "             metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "V2X1K5KSuWVW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "losses = model.fit(X_train, y_train,\n",
        "\n",
        "\t\t\t\tvalidation_data=(X_test, y_test),\n",
        "\n",
        "\t\t\t\t# it will use 'batch_size' number\n",
        "\t\t\t\t# of examples per example\n",
        "\t\t\t\tbatch_size=50,\n",
        "\t\t\t\tepochs=100, # total epoch\n",
        "\n",
        "\t\t\t\t)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IfwH_xwwuX_4",
        "outputId": "6cdaf23d-7937-4fbe-bbf1-48b13b071158"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "13/13 [==============================] - 1s 20ms/step - loss: 0.4803 - accuracy: 0.6613 - val_loss: 0.4590 - val_accuracy: 0.6903\n",
            "Epoch 2/100\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4400 - accuracy: 0.6581 - val_loss: 0.4102 - val_accuracy: 0.7090\n",
            "Epoch 3/100\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4004 - accuracy: 0.6693 - val_loss: 0.3655 - val_accuracy: 0.7090\n",
            "Epoch 4/100\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.3644 - accuracy: 0.6790 - val_loss: 0.3280 - val_accuracy: 0.7276\n",
            "Epoch 5/100\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.3352 - accuracy: 0.6966 - val_loss: 0.2962 - val_accuracy: 0.7612\n",
            "Epoch 6/100\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.3071 - accuracy: 0.7608 - val_loss: 0.2715 - val_accuracy: 0.7836\n",
            "Epoch 7/100\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.2836 - accuracy: 0.7769 - val_loss: 0.2519 - val_accuracy: 0.7836\n",
            "Epoch 8/100\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.2650 - accuracy: 0.7753 - val_loss: 0.2387 - val_accuracy: 0.7910\n",
            "Epoch 9/100\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.2478 - accuracy: 0.8010 - val_loss: 0.2277 - val_accuracy: 0.7948\n",
            "Epoch 10/100\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.2356 - accuracy: 0.8074 - val_loss: 0.2196 - val_accuracy: 0.8097\n",
            "Epoch 11/100\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.2251 - accuracy: 0.8090 - val_loss: 0.2171 - val_accuracy: 0.7948\n",
            "Epoch 12/100\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.2177 - accuracy: 0.8154 - val_loss: 0.2123 - val_accuracy: 0.8060\n",
            "Epoch 13/100\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.2095 - accuracy: 0.8154 - val_loss: 0.2112 - val_accuracy: 0.7873\n",
            "Epoch 14/100\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.2063 - accuracy: 0.8234 - val_loss: 0.2113 - val_accuracy: 0.7873\n",
            "Epoch 15/100\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.2015 - accuracy: 0.8250 - val_loss: 0.2088 - val_accuracy: 0.7873\n",
            "Epoch 16/100\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.1981 - accuracy: 0.8250 - val_loss: 0.2077 - val_accuracy: 0.7836\n",
            "Epoch 17/100\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.1958 - accuracy: 0.8250 - val_loss: 0.2064 - val_accuracy: 0.7873\n",
            "Epoch 18/100\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.1929 - accuracy: 0.8299 - val_loss: 0.2066 - val_accuracy: 0.7873\n",
            "Epoch 19/100\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.1899 - accuracy: 0.8299 - val_loss: 0.2050 - val_accuracy: 0.7873\n",
            "Epoch 20/100\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.1876 - accuracy: 0.8331 - val_loss: 0.2039 - val_accuracy: 0.7910\n",
            "Epoch 21/100\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.1857 - accuracy: 0.8331 - val_loss: 0.2034 - val_accuracy: 0.7910\n",
            "Epoch 22/100\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.1844 - accuracy: 0.8331 - val_loss: 0.2018 - val_accuracy: 0.7910\n",
            "Epoch 23/100\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.1827 - accuracy: 0.8331 - val_loss: 0.2027 - val_accuracy: 0.7910\n",
            "Epoch 24/100\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.1817 - accuracy: 0.8331 - val_loss: 0.2026 - val_accuracy: 0.7910\n",
            "Epoch 25/100\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.1805 - accuracy: 0.8331 - val_loss: 0.2027 - val_accuracy: 0.7910\n",
            "Epoch 26/100\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.1794 - accuracy: 0.8347 - val_loss: 0.2033 - val_accuracy: 0.7910\n",
            "Epoch 27/100\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.1785 - accuracy: 0.8347 - val_loss: 0.2021 - val_accuracy: 0.7910\n",
            "Epoch 28/100\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.1771 - accuracy: 0.8363 - val_loss: 0.2008 - val_accuracy: 0.7910\n",
            "Epoch 29/100\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.1764 - accuracy: 0.8363 - val_loss: 0.2013 - val_accuracy: 0.7910\n",
            "Epoch 30/100\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.1749 - accuracy: 0.8347 - val_loss: 0.2028 - val_accuracy: 0.7948\n",
            "Epoch 31/100\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.1744 - accuracy: 0.8379 - val_loss: 0.2012 - val_accuracy: 0.7948\n",
            "Epoch 32/100\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.1740 - accuracy: 0.8379 - val_loss: 0.2000 - val_accuracy: 0.7948\n",
            "Epoch 33/100\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.1723 - accuracy: 0.8395 - val_loss: 0.2021 - val_accuracy: 0.7948\n",
            "Epoch 34/100\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.1716 - accuracy: 0.8379 - val_loss: 0.2001 - val_accuracy: 0.7948\n",
            "Epoch 35/100\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.1717 - accuracy: 0.8395 - val_loss: 0.1992 - val_accuracy: 0.7948\n",
            "Epoch 36/100\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.1702 - accuracy: 0.8395 - val_loss: 0.2014 - val_accuracy: 0.7948\n",
            "Epoch 37/100\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.1697 - accuracy: 0.8379 - val_loss: 0.2002 - val_accuracy: 0.7948\n",
            "Epoch 38/100\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.1689 - accuracy: 0.8395 - val_loss: 0.1995 - val_accuracy: 0.7948\n",
            "Epoch 39/100\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.1682 - accuracy: 0.8395 - val_loss: 0.2014 - val_accuracy: 0.7948\n",
            "Epoch 40/100\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.1675 - accuracy: 0.8395 - val_loss: 0.2013 - val_accuracy: 0.7948\n",
            "Epoch 41/100\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.1667 - accuracy: 0.8411 - val_loss: 0.2002 - val_accuracy: 0.7948\n",
            "Epoch 42/100\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.1667 - accuracy: 0.8411 - val_loss: 0.2006 - val_accuracy: 0.7948\n",
            "Epoch 43/100\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.1670 - accuracy: 0.8443 - val_loss: 0.2013 - val_accuracy: 0.7948\n",
            "Epoch 44/100\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.1661 - accuracy: 0.8411 - val_loss: 0.1992 - val_accuracy: 0.7948\n",
            "Epoch 45/100\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.1649 - accuracy: 0.8443 - val_loss: 0.2009 - val_accuracy: 0.7948\n",
            "Epoch 46/100\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.1645 - accuracy: 0.8427 - val_loss: 0.2007 - val_accuracy: 0.7948\n",
            "Epoch 47/100\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.1647 - accuracy: 0.8443 - val_loss: 0.1996 - val_accuracy: 0.7948\n",
            "Epoch 48/100\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.1637 - accuracy: 0.8459 - val_loss: 0.2004 - val_accuracy: 0.7985\n",
            "Epoch 49/100\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.1638 - accuracy: 0.8443 - val_loss: 0.2010 - val_accuracy: 0.7985\n",
            "Epoch 50/100\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.1625 - accuracy: 0.8459 - val_loss: 0.1998 - val_accuracy: 0.7985\n",
            "Epoch 51/100\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.1637 - accuracy: 0.8427 - val_loss: 0.2000 - val_accuracy: 0.7985\n",
            "Epoch 52/100\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.1625 - accuracy: 0.8443 - val_loss: 0.2012 - val_accuracy: 0.7985\n",
            "Epoch 53/100\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.1622 - accuracy: 0.8459 - val_loss: 0.2001 - val_accuracy: 0.7985\n",
            "Epoch 54/100\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.1618 - accuracy: 0.8443 - val_loss: 0.1998 - val_accuracy: 0.7985\n",
            "Epoch 55/100\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.1611 - accuracy: 0.8459 - val_loss: 0.2001 - val_accuracy: 0.7985\n",
            "Epoch 56/100\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.1610 - accuracy: 0.8459 - val_loss: 0.2002 - val_accuracy: 0.7985\n",
            "Epoch 57/100\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.1610 - accuracy: 0.8459 - val_loss: 0.2003 - val_accuracy: 0.7985\n",
            "Epoch 58/100\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.1604 - accuracy: 0.8459 - val_loss: 0.2002 - val_accuracy: 0.7985\n",
            "Epoch 59/100\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.1604 - accuracy: 0.8459 - val_loss: 0.1994 - val_accuracy: 0.7985\n",
            "Epoch 60/100\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.1604 - accuracy: 0.8459 - val_loss: 0.2003 - val_accuracy: 0.7985\n",
            "Epoch 61/100\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.1600 - accuracy: 0.8459 - val_loss: 0.1991 - val_accuracy: 0.7985\n",
            "Epoch 62/100\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.1597 - accuracy: 0.8459 - val_loss: 0.2007 - val_accuracy: 0.7985\n",
            "Epoch 63/100\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.1592 - accuracy: 0.8459 - val_loss: 0.1996 - val_accuracy: 0.7985\n",
            "Epoch 64/100\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.1591 - accuracy: 0.8459 - val_loss: 0.2001 - val_accuracy: 0.7985\n",
            "Epoch 65/100\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.1589 - accuracy: 0.8459 - val_loss: 0.1998 - val_accuracy: 0.7985\n",
            "Epoch 66/100\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.1587 - accuracy: 0.8459 - val_loss: 0.2003 - val_accuracy: 0.7985\n",
            "Epoch 67/100\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.1587 - accuracy: 0.8475 - val_loss: 0.1989 - val_accuracy: 0.8022\n",
            "Epoch 68/100\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.1589 - accuracy: 0.8459 - val_loss: 0.2001 - val_accuracy: 0.7985\n",
            "Epoch 69/100\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.1587 - accuracy: 0.8475 - val_loss: 0.1987 - val_accuracy: 0.8022\n",
            "Epoch 70/100\n",
            "13/13 [==============================] - 0s 8ms/step - loss: 0.1581 - accuracy: 0.8459 - val_loss: 0.2005 - val_accuracy: 0.7985\n",
            "Epoch 71/100\n",
            "13/13 [==============================] - 0s 8ms/step - loss: 0.1579 - accuracy: 0.8459 - val_loss: 0.2004 - val_accuracy: 0.7985\n",
            "Epoch 72/100\n",
            "13/13 [==============================] - 0s 8ms/step - loss: 0.1577 - accuracy: 0.8475 - val_loss: 0.1996 - val_accuracy: 0.7985\n",
            "Epoch 73/100\n",
            "13/13 [==============================] - 0s 8ms/step - loss: 0.1575 - accuracy: 0.8475 - val_loss: 0.2002 - val_accuracy: 0.7985\n",
            "Epoch 74/100\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.1575 - accuracy: 0.8475 - val_loss: 0.2005 - val_accuracy: 0.7985\n",
            "Epoch 75/100\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.1573 - accuracy: 0.8475 - val_loss: 0.1996 - val_accuracy: 0.8022\n",
            "Epoch 76/100\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.1570 - accuracy: 0.8475 - val_loss: 0.2000 - val_accuracy: 0.8022\n",
            "Epoch 77/100\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.1575 - accuracy: 0.8475 - val_loss: 0.1990 - val_accuracy: 0.8022\n",
            "Epoch 78/100\n",
            "13/13 [==============================] - 0s 8ms/step - loss: 0.1570 - accuracy: 0.8475 - val_loss: 0.2000 - val_accuracy: 0.8022\n",
            "Epoch 79/100\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.1569 - accuracy: 0.8475 - val_loss: 0.1990 - val_accuracy: 0.8022\n",
            "Epoch 80/100\n",
            "13/13 [==============================] - 0s 10ms/step - loss: 0.1566 - accuracy: 0.8475 - val_loss: 0.1993 - val_accuracy: 0.8022\n",
            "Epoch 81/100\n",
            "13/13 [==============================] - 0s 8ms/step - loss: 0.1565 - accuracy: 0.8475 - val_loss: 0.1992 - val_accuracy: 0.8022\n",
            "Epoch 82/100\n",
            "13/13 [==============================] - 0s 8ms/step - loss: 0.1564 - accuracy: 0.8475 - val_loss: 0.1989 - val_accuracy: 0.8022\n",
            "Epoch 83/100\n",
            "13/13 [==============================] - 0s 9ms/step - loss: 0.1565 - accuracy: 0.8475 - val_loss: 0.1988 - val_accuracy: 0.8022\n",
            "Epoch 84/100\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.1564 - accuracy: 0.8475 - val_loss: 0.2006 - val_accuracy: 0.7985\n",
            "Epoch 85/100\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.1563 - accuracy: 0.8475 - val_loss: 0.2003 - val_accuracy: 0.7985\n",
            "Epoch 86/100\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.1561 - accuracy: 0.8475 - val_loss: 0.2002 - val_accuracy: 0.8022\n",
            "Epoch 87/100\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.1557 - accuracy: 0.8475 - val_loss: 0.2008 - val_accuracy: 0.8022\n",
            "Epoch 88/100\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.1560 - accuracy: 0.8491 - val_loss: 0.2002 - val_accuracy: 0.8022\n",
            "Epoch 89/100\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.1556 - accuracy: 0.8475 - val_loss: 0.1996 - val_accuracy: 0.8022\n",
            "Epoch 90/100\n",
            "13/13 [==============================] - 0s 9ms/step - loss: 0.1557 - accuracy: 0.8475 - val_loss: 0.1995 - val_accuracy: 0.8022\n",
            "Epoch 91/100\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.1557 - accuracy: 0.8475 - val_loss: 0.2000 - val_accuracy: 0.8022\n",
            "Epoch 92/100\n",
            "13/13 [==============================] - 0s 8ms/step - loss: 0.1557 - accuracy: 0.8475 - val_loss: 0.1994 - val_accuracy: 0.8022\n",
            "Epoch 93/100\n",
            "13/13 [==============================] - 0s 10ms/step - loss: 0.1558 - accuracy: 0.8475 - val_loss: 0.2000 - val_accuracy: 0.8022\n",
            "Epoch 94/100\n",
            "13/13 [==============================] - 0s 8ms/step - loss: 0.1551 - accuracy: 0.8475 - val_loss: 0.1994 - val_accuracy: 0.8022\n",
            "Epoch 95/100\n",
            "13/13 [==============================] - 0s 8ms/step - loss: 0.1551 - accuracy: 0.8475 - val_loss: 0.1996 - val_accuracy: 0.8022\n",
            "Epoch 96/100\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.1549 - accuracy: 0.8475 - val_loss: 0.1992 - val_accuracy: 0.8022\n",
            "Epoch 97/100\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.1549 - accuracy: 0.8475 - val_loss: 0.2000 - val_accuracy: 0.8022\n",
            "Epoch 98/100\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.1545 - accuracy: 0.8491 - val_loss: 0.1994 - val_accuracy: 0.8022\n",
            "Epoch 99/100\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.1544 - accuracy: 0.8491 - val_loss: 0.1999 - val_accuracy: 0.8022\n",
            "Epoch 100/100\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.1545 - accuracy: 0.8491 - val_loss: 0.1994 - val_accuracy: 0.8022\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UO3WQGmFu_UE",
        "outputId": "c73ae07b-1964-4c60-fec9-d3a79b50628e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9/9 [==============================] - 0s 2ms/step - loss: 0.1994 - accuracy: 0.8022\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.1993560492992401, 0.8022388219833374]"
            ]
          },
          "metadata": {},
          "execution_count": 176
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# this will pass the first 3 rows of features\n",
        "# of our data as input to make predictions\n",
        "y_pred = (model.predict(X_test) > 0.5).astype(int)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-8m9Uf77fPh",
        "outputId": "1e9df4ff-cfaf-4be1-e086-965aa3c17d49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9/9 [==============================] - 0s 2ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "acuracia_keras = accuracy_score(y_test,y_pred)"
      ],
      "metadata": {
        "id": "v4_W89IbvABt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_df = pd.DataFrame(losses.history)"
      ],
      "metadata": {
        "id": "8b8mv4R29u5h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# history stores the loss/val\n",
        "# loss in each epoch\n",
        "\n",
        "# loss_df is a dataframe which\n",
        "# contains the losses so we can\n",
        "# plot it to visualize our model training\n",
        "loss_df.loc[:,['accuracy','val_accuracy']].plot()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "id": "sug8E3fivEXP",
        "outputId": "be380cf4-56ae-4e5e-c4b6-9e2830791720"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: >"
            ]
          },
          "metadata": {},
          "execution_count": 181
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNwgQDGVl-5O"
      },
      "source": [
        "## REPETINDO O EXPERIMENTO, AGORA, SEM A PRESENÇA DE \"CABIN\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_re_teste = df_re_teste.drop([\"Cabin\"],axis=1)"
      ],
      "metadata": {
        "id": "lZMlhwtP7hFR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_re_teste.isna().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v95h5KIM88id",
        "outputId": "f072ec93-e9f3-4cb4-ecbe-c9d55e317515"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Survived      0\n",
              "Sex           0\n",
              "Age           0\n",
              "SibSp         0\n",
              "Parch         0\n",
              "Fare          0\n",
              "Embarked_C    0\n",
              "Embarked_Q    0\n",
              "Embarked_S    0\n",
              "Pclass_1      0\n",
              "Pclass_2      0\n",
              "Pclass_3      0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Transformando a coluna \"Sex\" em numérico (note que,\n",
        "# devido a natureza da coluna, seus dados se comportarão\n",
        "# de forma binária).\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "labelencoder = LabelEncoder()\n",
        "df[[\"Sex\"]] = \\\n",
        "df[[\"Sex\"]].apply(labelencoder.fit_transform)"
      ],
      "metadata": {
        "id": "5urzqRjw9HPE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_re_teste.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sdDEuQLU9NTF",
        "outputId": "678a9994-4c71-4132-cd88-69d43501af29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 891 entries, 0 to 890\n",
            "Data columns (total 12 columns):\n",
            " #   Column      Non-Null Count  Dtype  \n",
            "---  ------      --------------  -----  \n",
            " 0   Survived    891 non-null    int64  \n",
            " 1   Sex         891 non-null    object \n",
            " 2   Age         891 non-null    float64\n",
            " 3   SibSp       891 non-null    int64  \n",
            " 4   Parch       891 non-null    int64  \n",
            " 5   Fare        891 non-null    float64\n",
            " 6   Embarked_C  891 non-null    uint8  \n",
            " 7   Embarked_Q  891 non-null    uint8  \n",
            " 8   Embarked_S  891 non-null    uint8  \n",
            " 9   Pclass_1    891 non-null    uint8  \n",
            " 10  Pclass_2    891 non-null    uint8  \n",
            " 11  Pclass_3    891 non-null    uint8  \n",
            "dtypes: float64(2), int64(3), object(1), uint8(6)\n",
            "memory usage: 47.1+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_re_teste"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "gPCyuakN9ire",
        "outputId": "b13d516d-01a2-43c9-e493-7945389b9068"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     Survived     Sex   Age  SibSp  Parch     Fare  Embarked_C  Embarked_Q  \\\n",
              "0           0    male  22.0      1      0   7.2500           0           0   \n",
              "1           1  female  38.0      1      0  71.2833           1           0   \n",
              "2           1  female  26.0      0      0   7.9250           0           0   \n",
              "3           1  female  35.0      1      0  53.1000           0           0   \n",
              "4           0    male  35.0      0      0   8.0500           0           0   \n",
              "..        ...     ...   ...    ...    ...      ...         ...         ...   \n",
              "886         0    male  27.0      0      0  13.0000           0           0   \n",
              "887         1  female  19.0      0      0  30.0000           0           0   \n",
              "888         0  female  28.0      1      2  23.4500           0           0   \n",
              "889         1    male  26.0      0      0  30.0000           1           0   \n",
              "890         0    male  32.0      0      0   7.7500           0           1   \n",
              "\n",
              "     Embarked_S  Pclass_1  Pclass_2  Pclass_3  \n",
              "0             1         0         0         1  \n",
              "1             0         1         0         0  \n",
              "2             1         0         0         1  \n",
              "3             1         1         0         0  \n",
              "4             1         0         0         1  \n",
              "..          ...       ...       ...       ...  \n",
              "886           1         0         1         0  \n",
              "887           1         1         0         0  \n",
              "888           1         0         0         1  \n",
              "889           0         1         0         0  \n",
              "890           0         0         0         1  \n",
              "\n",
              "[891 rows x 12 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a4f14ef3-c9ad-4b6f-b99b-343b466d0cfa\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Survived</th>\n",
              "      <th>Sex</th>\n",
              "      <th>Age</th>\n",
              "      <th>SibSp</th>\n",
              "      <th>Parch</th>\n",
              "      <th>Fare</th>\n",
              "      <th>Embarked_C</th>\n",
              "      <th>Embarked_Q</th>\n",
              "      <th>Embarked_S</th>\n",
              "      <th>Pclass_1</th>\n",
              "      <th>Pclass_2</th>\n",
              "      <th>Pclass_3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>male</td>\n",
              "      <td>22.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>7.2500</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>female</td>\n",
              "      <td>38.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>71.2833</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>female</td>\n",
              "      <td>26.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>7.9250</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>female</td>\n",
              "      <td>35.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>53.1000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>male</td>\n",
              "      <td>35.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>8.0500</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>886</th>\n",
              "      <td>0</td>\n",
              "      <td>male</td>\n",
              "      <td>27.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>13.0000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>887</th>\n",
              "      <td>1</td>\n",
              "      <td>female</td>\n",
              "      <td>19.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>30.0000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>888</th>\n",
              "      <td>0</td>\n",
              "      <td>female</td>\n",
              "      <td>28.0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>23.4500</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>889</th>\n",
              "      <td>1</td>\n",
              "      <td>male</td>\n",
              "      <td>26.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>30.0000</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>890</th>\n",
              "      <td>0</td>\n",
              "      <td>male</td>\n",
              "      <td>32.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>7.7500</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>891 rows × 12 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a4f14ef3-c9ad-4b6f-b99b-343b466d0cfa')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a4f14ef3-c9ad-4b6f-b99b-343b466d0cfa button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a4f14ef3-c9ad-4b6f-b99b-343b466d0cfa');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#mulheres = 0 ; homens = 1\n",
        "df_re_teste[\"Sex\"].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wDvKY6tA9-bt",
        "outputId": "fe21541a-c1ee-458d-b6d5-67ba4bb59205"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "male      577\n",
              "female    314\n",
              "Name: Sex, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Normalização de Age e Fare, que possuem discrepância alta em distribuição de dados.\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "df_re_teste[['Age', 'Fare']] = scaler.fit_transform(df_re_teste[['Age', 'Fare']])"
      ],
      "metadata": {
        "id": "FS1fDJii-N4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_re_teste"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "XbParixc_R7t",
        "outputId": "20c250dd-a9e6-4dff-8bb7-79828257e192"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     Survived     Sex       Age  SibSp  Parch      Fare  Embarked_C  \\\n",
              "0           0    male  0.271174      1      0  0.014151           0   \n",
              "1           1  female  0.472229      1      0  0.139136           1   \n",
              "2           1  female  0.321438      0      0  0.015469           0   \n",
              "3           1  female  0.434531      1      0  0.103644           0   \n",
              "4           0    male  0.434531      0      0  0.015713           0   \n",
              "..        ...     ...       ...    ...    ...       ...         ...   \n",
              "886         0    male  0.334004      0      0  0.025374           0   \n",
              "887         1  female  0.233476      0      0  0.058556           0   \n",
              "888         0  female  0.346569      1      2  0.045771           0   \n",
              "889         1    male  0.321438      0      0  0.058556           1   \n",
              "890         0    male  0.396833      0      0  0.015127           0   \n",
              "\n",
              "     Embarked_Q  Embarked_S  Pclass_1  Pclass_2  Pclass_3  \n",
              "0             0           1         0         0         1  \n",
              "1             0           0         1         0         0  \n",
              "2             0           1         0         0         1  \n",
              "3             0           1         1         0         0  \n",
              "4             0           1         0         0         1  \n",
              "..          ...         ...       ...       ...       ...  \n",
              "886           0           1         0         1         0  \n",
              "887           0           1         1         0         0  \n",
              "888           0           1         0         0         1  \n",
              "889           0           0         1         0         0  \n",
              "890           1           0         0         0         1  \n",
              "\n",
              "[891 rows x 12 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e1de4e3d-283e-4c26-b006-f051d76797e8\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Survived</th>\n",
              "      <th>Sex</th>\n",
              "      <th>Age</th>\n",
              "      <th>SibSp</th>\n",
              "      <th>Parch</th>\n",
              "      <th>Fare</th>\n",
              "      <th>Embarked_C</th>\n",
              "      <th>Embarked_Q</th>\n",
              "      <th>Embarked_S</th>\n",
              "      <th>Pclass_1</th>\n",
              "      <th>Pclass_2</th>\n",
              "      <th>Pclass_3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>male</td>\n",
              "      <td>0.271174</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.014151</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>female</td>\n",
              "      <td>0.472229</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.139136</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>female</td>\n",
              "      <td>0.321438</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.015469</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>female</td>\n",
              "      <td>0.434531</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.103644</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>male</td>\n",
              "      <td>0.434531</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.015713</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>886</th>\n",
              "      <td>0</td>\n",
              "      <td>male</td>\n",
              "      <td>0.334004</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.025374</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>887</th>\n",
              "      <td>1</td>\n",
              "      <td>female</td>\n",
              "      <td>0.233476</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.058556</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>888</th>\n",
              "      <td>0</td>\n",
              "      <td>female</td>\n",
              "      <td>0.346569</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0.045771</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>889</th>\n",
              "      <td>1</td>\n",
              "      <td>male</td>\n",
              "      <td>0.321438</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.058556</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>890</th>\n",
              "      <td>0</td>\n",
              "      <td>male</td>\n",
              "      <td>0.396833</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.015127</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>891 rows × 12 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e1de4e3d-283e-4c26-b006-f051d76797e8')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e1de4e3d-283e-4c26-b006-f051d76797e8 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e1de4e3d-283e-4c26-b006-f051d76797e8');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.drop('Survived', axis=1)\n",
        "y = df['Survived']"
      ],
      "metadata": {
        "id": "epco5syETCKf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dividir os dados em conjunto de treinamento e teste\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=10)"
      ],
      "metadata": {
        "id": "YW9Unz-kAAdo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aplicar balanceamento nas classes usando SMOTE\n",
        "sm = SMOTE()\n",
        "X_train_oversampled, y_train_oversampled = sm.fit_resample(X_train, y_train)"
      ],
      "metadata": {
        "id": "a5cxZ3ckAKis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train_oversampled.shape)\n",
        "print(X_train.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yc6JrS3MAPrL",
        "outputId": "cf45a073-a021-4e19-857a-849da1081068"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(750, 19)\n",
            "(623, 19)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ibk_9FslzW44"
      },
      "source": [
        "## Rede Neural (Scikit) - 2 camadas\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observação: dentre os classificadores, nos testes, a rede neural por Scikit com duas camadas se saiu melhor no geral."
      ],
      "metadata": {
        "id": "nc1Ap4vNzgpR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rnaf = MLPClassifier(hidden_layer_sizes=(3,3), activation='relu', solver='sgd', max_iter =1000,\n",
        "                              tol=0.0001, random_state = 3, verbose = True)"
      ],
      "metadata": {
        "id": "o0F2uYdnz3p5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rnaf.fit(X_train_oversampled, y_train_oversampled)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2a643dd1-875d-491c-f82c-eb7e154746e0",
        "id": "HXcrgqDRz3p5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 0.81059396\n",
            "Iteration 2, loss = 0.80947781\n",
            "Iteration 3, loss = 0.80772490\n",
            "Iteration 4, loss = 0.80559245\n",
            "Iteration 5, loss = 0.80320885\n",
            "Iteration 6, loss = 0.80073822\n",
            "Iteration 7, loss = 0.79831709\n",
            "Iteration 8, loss = 0.79566829\n",
            "Iteration 9, loss = 0.79314901\n",
            "Iteration 10, loss = 0.79052377\n",
            "Iteration 11, loss = 0.78798018\n",
            "Iteration 12, loss = 0.78545524\n",
            "Iteration 13, loss = 0.78303482\n",
            "Iteration 14, loss = 0.78061982\n",
            "Iteration 15, loss = 0.77815844\n",
            "Iteration 16, loss = 0.77580966\n",
            "Iteration 17, loss = 0.77367297\n",
            "Iteration 18, loss = 0.77137954\n",
            "Iteration 19, loss = 0.76915508\n",
            "Iteration 20, loss = 0.76697622\n",
            "Iteration 21, loss = 0.76479181\n",
            "Iteration 22, loss = 0.76276593\n",
            "Iteration 23, loss = 0.76074046\n",
            "Iteration 24, loss = 0.75882433\n",
            "Iteration 25, loss = 0.75680439\n",
            "Iteration 26, loss = 0.75495299\n",
            "Iteration 27, loss = 0.75304353\n",
            "Iteration 28, loss = 0.75133965\n",
            "Iteration 29, loss = 0.74941787\n",
            "Iteration 30, loss = 0.74775859\n",
            "Iteration 31, loss = 0.74598920\n",
            "Iteration 32, loss = 0.74431791\n",
            "Iteration 33, loss = 0.74272727\n",
            "Iteration 34, loss = 0.74108175\n",
            "Iteration 35, loss = 0.73950682\n",
            "Iteration 36, loss = 0.73793804\n",
            "Iteration 37, loss = 0.73640962\n",
            "Iteration 38, loss = 0.73496498\n",
            "Iteration 39, loss = 0.73355344\n",
            "Iteration 40, loss = 0.73206265\n",
            "Iteration 41, loss = 0.73063771\n",
            "Iteration 42, loss = 0.72932165\n",
            "Iteration 43, loss = 0.72786582\n",
            "Iteration 44, loss = 0.72668935\n",
            "Iteration 45, loss = 0.72534010\n",
            "Iteration 46, loss = 0.72416583\n",
            "Iteration 47, loss = 0.72290570\n",
            "Iteration 48, loss = 0.72169007\n",
            "Iteration 49, loss = 0.72057663\n",
            "Iteration 50, loss = 0.71945553\n",
            "Iteration 51, loss = 0.71832054\n",
            "Iteration 52, loss = 0.71726250\n",
            "Iteration 53, loss = 0.71621480\n",
            "Iteration 54, loss = 0.71514747\n",
            "Iteration 55, loss = 0.71420619\n",
            "Iteration 56, loss = 0.71323766\n",
            "Iteration 57, loss = 0.71231156\n",
            "Iteration 58, loss = 0.71141527\n",
            "Iteration 59, loss = 0.71041918\n",
            "Iteration 60, loss = 0.70962512\n",
            "Iteration 61, loss = 0.70874544\n",
            "Iteration 62, loss = 0.70794062\n",
            "Iteration 63, loss = 0.70705091\n",
            "Iteration 64, loss = 0.70630716\n",
            "Iteration 65, loss = 0.70551470\n",
            "Iteration 66, loss = 0.70477311\n",
            "Iteration 67, loss = 0.70401869\n",
            "Iteration 68, loss = 0.70328557\n",
            "Iteration 69, loss = 0.70256086\n",
            "Iteration 70, loss = 0.70192119\n",
            "Iteration 71, loss = 0.70126641\n",
            "Iteration 72, loss = 0.70056272\n",
            "Iteration 73, loss = 0.69996793\n",
            "Iteration 74, loss = 0.69933505\n",
            "Iteration 75, loss = 0.69873728\n",
            "Iteration 76, loss = 0.69812743\n",
            "Iteration 77, loss = 0.69754379\n",
            "Iteration 78, loss = 0.69700411\n",
            "Iteration 79, loss = 0.69642284\n",
            "Iteration 80, loss = 0.69591109\n",
            "Iteration 81, loss = 0.69534092\n",
            "Iteration 82, loss = 0.69482910\n",
            "Iteration 83, loss = 0.69431794\n",
            "Iteration 84, loss = 0.69383298\n",
            "Iteration 85, loss = 0.69334945\n",
            "Iteration 86, loss = 0.69283728\n",
            "Iteration 87, loss = 0.69240772\n",
            "Iteration 88, loss = 0.69194827\n",
            "Iteration 89, loss = 0.69149210\n",
            "Iteration 90, loss = 0.69105950\n",
            "Iteration 91, loss = 0.69061818\n",
            "Iteration 92, loss = 0.69017087\n",
            "Iteration 93, loss = 0.68982034\n",
            "Iteration 94, loss = 0.68940824\n",
            "Iteration 95, loss = 0.68898536\n",
            "Iteration 96, loss = 0.68857253\n",
            "Iteration 97, loss = 0.68821509\n",
            "Iteration 98, loss = 0.68781055\n",
            "Iteration 99, loss = 0.68746762\n",
            "Iteration 100, loss = 0.68712164\n",
            "Iteration 101, loss = 0.68672461\n",
            "Iteration 102, loss = 0.68634595\n",
            "Iteration 103, loss = 0.68598454\n",
            "Iteration 104, loss = 0.68566814\n",
            "Iteration 105, loss = 0.68529516\n",
            "Iteration 106, loss = 0.68495195\n",
            "Iteration 107, loss = 0.68463611\n",
            "Iteration 108, loss = 0.68426193\n",
            "Iteration 109, loss = 0.68395278\n",
            "Iteration 110, loss = 0.68361494\n",
            "Iteration 111, loss = 0.68329764\n",
            "Iteration 112, loss = 0.68297143\n",
            "Iteration 113, loss = 0.68264024\n",
            "Iteration 114, loss = 0.68233426\n",
            "Iteration 115, loss = 0.68200655\n",
            "Iteration 116, loss = 0.68171389\n",
            "Iteration 117, loss = 0.68139209\n",
            "Iteration 118, loss = 0.68109237\n",
            "Iteration 119, loss = 0.68077061\n",
            "Iteration 120, loss = 0.68047026\n",
            "Iteration 121, loss = 0.68017066\n",
            "Iteration 122, loss = 0.67986731\n",
            "Iteration 123, loss = 0.67956891\n",
            "Iteration 124, loss = 0.67927297\n",
            "Iteration 125, loss = 0.67900593\n",
            "Iteration 126, loss = 0.67867023\n",
            "Iteration 127, loss = 0.67837558\n",
            "Iteration 128, loss = 0.67809149\n",
            "Iteration 129, loss = 0.67780275\n",
            "Iteration 130, loss = 0.67749527\n",
            "Iteration 131, loss = 0.67721986\n",
            "Iteration 132, loss = 0.67691570\n",
            "Iteration 133, loss = 0.67661248\n",
            "Iteration 134, loss = 0.67633363\n",
            "Iteration 135, loss = 0.67603716\n",
            "Iteration 136, loss = 0.67574882\n",
            "Iteration 137, loss = 0.67544021\n",
            "Iteration 138, loss = 0.67514979\n",
            "Iteration 139, loss = 0.67485153\n",
            "Iteration 140, loss = 0.67458581\n",
            "Iteration 141, loss = 0.67426964\n",
            "Iteration 142, loss = 0.67396245\n",
            "Iteration 143, loss = 0.67367986\n",
            "Iteration 144, loss = 0.67338164\n",
            "Iteration 145, loss = 0.67307127\n",
            "Iteration 146, loss = 0.67280023\n",
            "Iteration 147, loss = 0.67247641\n",
            "Iteration 148, loss = 0.67216745\n",
            "Iteration 149, loss = 0.67185824\n",
            "Iteration 150, loss = 0.67156342\n",
            "Iteration 151, loss = 0.67125271\n",
            "Iteration 152, loss = 0.67095216\n",
            "Iteration 153, loss = 0.67063168\n",
            "Iteration 154, loss = 0.67031587\n",
            "Iteration 155, loss = 0.67000412\n",
            "Iteration 156, loss = 0.66969848\n",
            "Iteration 157, loss = 0.66937672\n",
            "Iteration 158, loss = 0.66904960\n",
            "Iteration 159, loss = 0.66872528\n",
            "Iteration 160, loss = 0.66840592\n",
            "Iteration 161, loss = 0.66807552\n",
            "Iteration 162, loss = 0.66775977\n",
            "Iteration 163, loss = 0.66741724\n",
            "Iteration 164, loss = 0.66708146\n",
            "Iteration 165, loss = 0.66676370\n",
            "Iteration 166, loss = 0.66644075\n",
            "Iteration 167, loss = 0.66610752\n",
            "Iteration 168, loss = 0.66577046\n",
            "Iteration 169, loss = 0.66543408\n",
            "Iteration 170, loss = 0.66512061\n",
            "Iteration 171, loss = 0.66480762\n",
            "Iteration 172, loss = 0.66448486\n",
            "Iteration 173, loss = 0.66418769\n",
            "Iteration 174, loss = 0.66385439\n",
            "Iteration 175, loss = 0.66355917\n",
            "Iteration 176, loss = 0.66324964\n",
            "Iteration 177, loss = 0.66295336\n",
            "Iteration 178, loss = 0.66265022\n",
            "Iteration 179, loss = 0.66234364\n",
            "Iteration 180, loss = 0.66205382\n",
            "Iteration 181, loss = 0.66175723\n",
            "Iteration 182, loss = 0.66144865\n",
            "Iteration 183, loss = 0.66113742\n",
            "Iteration 184, loss = 0.66084933\n",
            "Iteration 185, loss = 0.66053439\n",
            "Iteration 186, loss = 0.66023432\n",
            "Iteration 187, loss = 0.65990254\n",
            "Iteration 188, loss = 0.65960478\n",
            "Iteration 189, loss = 0.65930537\n",
            "Iteration 190, loss = 0.65900663\n",
            "Iteration 191, loss = 0.65869727\n",
            "Iteration 192, loss = 0.65841615\n",
            "Iteration 193, loss = 0.65809593\n",
            "Iteration 194, loss = 0.65780307\n",
            "Iteration 195, loss = 0.65750764\n",
            "Iteration 196, loss = 0.65720868\n",
            "Iteration 197, loss = 0.65689272\n",
            "Iteration 198, loss = 0.65658584\n",
            "Iteration 199, loss = 0.65629207\n",
            "Iteration 200, loss = 0.65597635\n",
            "Iteration 201, loss = 0.65567106\n",
            "Iteration 202, loss = 0.65537064\n",
            "Iteration 203, loss = 0.65504950\n",
            "Iteration 204, loss = 0.65474583\n",
            "Iteration 205, loss = 0.65441790\n",
            "Iteration 206, loss = 0.65410490\n",
            "Iteration 207, loss = 0.65379478\n",
            "Iteration 208, loss = 0.65348850\n",
            "Iteration 209, loss = 0.65317982\n",
            "Iteration 210, loss = 0.65285541\n",
            "Iteration 211, loss = 0.65252238\n",
            "Iteration 212, loss = 0.65219647\n",
            "Iteration 213, loss = 0.65187779\n",
            "Iteration 214, loss = 0.65155218\n",
            "Iteration 215, loss = 0.65122684\n",
            "Iteration 216, loss = 0.65090710\n",
            "Iteration 217, loss = 0.65055566\n",
            "Iteration 218, loss = 0.65021428\n",
            "Iteration 219, loss = 0.64990996\n",
            "Iteration 220, loss = 0.64955212\n",
            "Iteration 221, loss = 0.64921266\n",
            "Iteration 222, loss = 0.64886987\n",
            "Iteration 223, loss = 0.64853322\n",
            "Iteration 224, loss = 0.64819082\n",
            "Iteration 225, loss = 0.64784320\n",
            "Iteration 226, loss = 0.64749197\n",
            "Iteration 227, loss = 0.64712873\n",
            "Iteration 228, loss = 0.64677404\n",
            "Iteration 229, loss = 0.64641405\n",
            "Iteration 230, loss = 0.64607861\n",
            "Iteration 231, loss = 0.64570352\n",
            "Iteration 232, loss = 0.64535019\n",
            "Iteration 233, loss = 0.64496808\n",
            "Iteration 234, loss = 0.64461339\n",
            "Iteration 235, loss = 0.64422982\n",
            "Iteration 236, loss = 0.64384983\n",
            "Iteration 237, loss = 0.64348667\n",
            "Iteration 238, loss = 0.64310001\n",
            "Iteration 239, loss = 0.64271693\n",
            "Iteration 240, loss = 0.64230695\n",
            "Iteration 241, loss = 0.64192167\n",
            "Iteration 242, loss = 0.64151198\n",
            "Iteration 243, loss = 0.64111513\n",
            "Iteration 244, loss = 0.64070241\n",
            "Iteration 245, loss = 0.64028073\n",
            "Iteration 246, loss = 0.63986397\n",
            "Iteration 247, loss = 0.63944628\n",
            "Iteration 248, loss = 0.63898903\n",
            "Iteration 249, loss = 0.63858379\n",
            "Iteration 250, loss = 0.63811070\n",
            "Iteration 251, loss = 0.63765526\n",
            "Iteration 252, loss = 0.63718647\n",
            "Iteration 253, loss = 0.63671348\n",
            "Iteration 254, loss = 0.63621961\n",
            "Iteration 255, loss = 0.63572778\n",
            "Iteration 256, loss = 0.63522628\n",
            "Iteration 257, loss = 0.63472775\n",
            "Iteration 258, loss = 0.63419666\n",
            "Iteration 259, loss = 0.63367278\n",
            "Iteration 260, loss = 0.63313342\n",
            "Iteration 261, loss = 0.63258849\n",
            "Iteration 262, loss = 0.63205979\n",
            "Iteration 263, loss = 0.63149927\n",
            "Iteration 264, loss = 0.63093387\n",
            "Iteration 265, loss = 0.63036351\n",
            "Iteration 266, loss = 0.62981816\n",
            "Iteration 267, loss = 0.62923631\n",
            "Iteration 268, loss = 0.62867086\n",
            "Iteration 269, loss = 0.62812831\n",
            "Iteration 270, loss = 0.62761187\n",
            "Iteration 271, loss = 0.62714314\n",
            "Iteration 272, loss = 0.62665846\n",
            "Iteration 273, loss = 0.62618707\n",
            "Iteration 274, loss = 0.62572836\n",
            "Iteration 275, loss = 0.62526519\n",
            "Iteration 276, loss = 0.62480022\n",
            "Iteration 277, loss = 0.62432947\n",
            "Iteration 278, loss = 0.62388643\n",
            "Iteration 279, loss = 0.62342207\n",
            "Iteration 280, loss = 0.62295955\n",
            "Iteration 281, loss = 0.62248407\n",
            "Iteration 282, loss = 0.62202655\n",
            "Iteration 283, loss = 0.62153923\n",
            "Iteration 284, loss = 0.62103447\n",
            "Iteration 285, loss = 0.62054210\n",
            "Iteration 286, loss = 0.62006550\n",
            "Iteration 287, loss = 0.61954691\n",
            "Iteration 288, loss = 0.61903387\n",
            "Iteration 289, loss = 0.61854360\n",
            "Iteration 290, loss = 0.61803600\n",
            "Iteration 291, loss = 0.61751082\n",
            "Iteration 292, loss = 0.61699394\n",
            "Iteration 293, loss = 0.61648514\n",
            "Iteration 294, loss = 0.61596649\n",
            "Iteration 295, loss = 0.61538896\n",
            "Iteration 296, loss = 0.61487253\n",
            "Iteration 297, loss = 0.61432024\n",
            "Iteration 298, loss = 0.61377266\n",
            "Iteration 299, loss = 0.61323040\n",
            "Iteration 300, loss = 0.61266289\n",
            "Iteration 301, loss = 0.61212582\n",
            "Iteration 302, loss = 0.61156817\n",
            "Iteration 303, loss = 0.61098849\n",
            "Iteration 304, loss = 0.61045004\n",
            "Iteration 305, loss = 0.60989648\n",
            "Iteration 306, loss = 0.60933405\n",
            "Iteration 307, loss = 0.60876267\n",
            "Iteration 308, loss = 0.60819469\n",
            "Iteration 309, loss = 0.60764563\n",
            "Iteration 310, loss = 0.60706105\n",
            "Iteration 311, loss = 0.60650790\n",
            "Iteration 312, loss = 0.60596443\n",
            "Iteration 313, loss = 0.60537359\n",
            "Iteration 314, loss = 0.60479552\n",
            "Iteration 315, loss = 0.60424583\n",
            "Iteration 316, loss = 0.60366519\n",
            "Iteration 317, loss = 0.60309475\n",
            "Iteration 318, loss = 0.60253974\n",
            "Iteration 319, loss = 0.60195609\n",
            "Iteration 320, loss = 0.60138160\n",
            "Iteration 321, loss = 0.60077830\n",
            "Iteration 322, loss = 0.60021451\n",
            "Iteration 323, loss = 0.59961276\n",
            "Iteration 324, loss = 0.59901928\n",
            "Iteration 325, loss = 0.59843353\n",
            "Iteration 326, loss = 0.59783897\n",
            "Iteration 327, loss = 0.59725841\n",
            "Iteration 328, loss = 0.59667762\n",
            "Iteration 329, loss = 0.59607661\n",
            "Iteration 330, loss = 0.59551559\n",
            "Iteration 331, loss = 0.59494173\n",
            "Iteration 332, loss = 0.59435928\n",
            "Iteration 333, loss = 0.59378261\n",
            "Iteration 334, loss = 0.59325945\n",
            "Iteration 335, loss = 0.59262347\n",
            "Iteration 336, loss = 0.59205568\n",
            "Iteration 337, loss = 0.59148250\n",
            "Iteration 338, loss = 0.59090395\n",
            "Iteration 339, loss = 0.59033704\n",
            "Iteration 340, loss = 0.58973496\n",
            "Iteration 341, loss = 0.58918083\n",
            "Iteration 342, loss = 0.58859930\n",
            "Iteration 343, loss = 0.58801930\n",
            "Iteration 344, loss = 0.58743556\n",
            "Iteration 345, loss = 0.58683363\n",
            "Iteration 346, loss = 0.58627005\n",
            "Iteration 347, loss = 0.58564770\n",
            "Iteration 348, loss = 0.58507804\n",
            "Iteration 349, loss = 0.58449178\n",
            "Iteration 350, loss = 0.58387017\n",
            "Iteration 351, loss = 0.58327895\n",
            "Iteration 352, loss = 0.58267058\n",
            "Iteration 353, loss = 0.58207295\n",
            "Iteration 354, loss = 0.58148985\n",
            "Iteration 355, loss = 0.58088353\n",
            "Iteration 356, loss = 0.58027730\n",
            "Iteration 357, loss = 0.57966514\n",
            "Iteration 358, loss = 0.57907434\n",
            "Iteration 359, loss = 0.57846724\n",
            "Iteration 360, loss = 0.57787481\n",
            "Iteration 361, loss = 0.57726619\n",
            "Iteration 362, loss = 0.57670232\n",
            "Iteration 363, loss = 0.57609725\n",
            "Iteration 364, loss = 0.57550342\n",
            "Iteration 365, loss = 0.57490089\n",
            "Iteration 366, loss = 0.57432488\n",
            "Iteration 367, loss = 0.57371537\n",
            "Iteration 368, loss = 0.57313668\n",
            "Iteration 369, loss = 0.57254734\n",
            "Iteration 370, loss = 0.57198505\n",
            "Iteration 371, loss = 0.57135712\n",
            "Iteration 372, loss = 0.57077577\n",
            "Iteration 373, loss = 0.57019773\n",
            "Iteration 374, loss = 0.56960418\n",
            "Iteration 375, loss = 0.56900718\n",
            "Iteration 376, loss = 0.56843427\n",
            "Iteration 377, loss = 0.56784206\n",
            "Iteration 378, loss = 0.56724014\n",
            "Iteration 379, loss = 0.56668322\n",
            "Iteration 380, loss = 0.56607164\n",
            "Iteration 381, loss = 0.56546830\n",
            "Iteration 382, loss = 0.56489633\n",
            "Iteration 383, loss = 0.56432391\n",
            "Iteration 384, loss = 0.56374122\n",
            "Iteration 385, loss = 0.56314306\n",
            "Iteration 386, loss = 0.56259415\n",
            "Iteration 387, loss = 0.56200897\n",
            "Iteration 388, loss = 0.56142824\n",
            "Iteration 389, loss = 0.56087130\n",
            "Iteration 390, loss = 0.56029739\n",
            "Iteration 391, loss = 0.55973972\n",
            "Iteration 392, loss = 0.55916147\n",
            "Iteration 393, loss = 0.55861784\n",
            "Iteration 394, loss = 0.55802648\n",
            "Iteration 395, loss = 0.55747745\n",
            "Iteration 396, loss = 0.55692130\n",
            "Iteration 397, loss = 0.55635100\n",
            "Iteration 398, loss = 0.55581457\n",
            "Iteration 399, loss = 0.55526367\n",
            "Iteration 400, loss = 0.55472520\n",
            "Iteration 401, loss = 0.55419258\n",
            "Iteration 402, loss = 0.55365884\n",
            "Iteration 403, loss = 0.55311832\n",
            "Iteration 404, loss = 0.55258684\n",
            "Iteration 405, loss = 0.55206596\n",
            "Iteration 406, loss = 0.55156736\n",
            "Iteration 407, loss = 0.55104235\n",
            "Iteration 408, loss = 0.55052947\n",
            "Iteration 409, loss = 0.55000935\n",
            "Iteration 410, loss = 0.54952058\n",
            "Iteration 411, loss = 0.54902029\n",
            "Iteration 412, loss = 0.54850746\n",
            "Iteration 413, loss = 0.54802595\n",
            "Iteration 414, loss = 0.54752786\n",
            "Iteration 415, loss = 0.54703961\n",
            "Iteration 416, loss = 0.54654923\n",
            "Iteration 417, loss = 0.54603577\n",
            "Iteration 418, loss = 0.54553689\n",
            "Iteration 419, loss = 0.54506004\n",
            "Iteration 420, loss = 0.54459713\n",
            "Iteration 421, loss = 0.54406497\n",
            "Iteration 422, loss = 0.54357600\n",
            "Iteration 423, loss = 0.54312561\n",
            "Iteration 424, loss = 0.54263669\n",
            "Iteration 425, loss = 0.54214686\n",
            "Iteration 426, loss = 0.54166955\n",
            "Iteration 427, loss = 0.54119188\n",
            "Iteration 428, loss = 0.54072265\n",
            "Iteration 429, loss = 0.54026983\n",
            "Iteration 430, loss = 0.53977325\n",
            "Iteration 431, loss = 0.53932611\n",
            "Iteration 432, loss = 0.53887102\n",
            "Iteration 433, loss = 0.53840376\n",
            "Iteration 434, loss = 0.53796097\n",
            "Iteration 435, loss = 0.53748159\n",
            "Iteration 436, loss = 0.53703123\n",
            "Iteration 437, loss = 0.53658075\n",
            "Iteration 438, loss = 0.53614042\n",
            "Iteration 439, loss = 0.53569694\n",
            "Iteration 440, loss = 0.53523950\n",
            "Iteration 441, loss = 0.53481672\n",
            "Iteration 442, loss = 0.53435794\n",
            "Iteration 443, loss = 0.53393306\n",
            "Iteration 444, loss = 0.53350805\n",
            "Iteration 445, loss = 0.53306313\n",
            "Iteration 446, loss = 0.53261971\n",
            "Iteration 447, loss = 0.53221110\n",
            "Iteration 448, loss = 0.53176440\n",
            "Iteration 449, loss = 0.53133343\n",
            "Iteration 450, loss = 0.53094709\n",
            "Iteration 451, loss = 0.53048612\n",
            "Iteration 452, loss = 0.53008339\n",
            "Iteration 453, loss = 0.52966966\n",
            "Iteration 454, loss = 0.52927867\n",
            "Iteration 455, loss = 0.52885008\n",
            "Iteration 456, loss = 0.52843526\n",
            "Iteration 457, loss = 0.52802994\n",
            "Iteration 458, loss = 0.52761134\n",
            "Iteration 459, loss = 0.52721770\n",
            "Iteration 460, loss = 0.52681052\n",
            "Iteration 461, loss = 0.52638304\n",
            "Iteration 462, loss = 0.52600237\n",
            "Iteration 463, loss = 0.52556211\n",
            "Iteration 464, loss = 0.52517241\n",
            "Iteration 465, loss = 0.52476478\n",
            "Iteration 466, loss = 0.52435140\n",
            "Iteration 467, loss = 0.52397307\n",
            "Iteration 468, loss = 0.52357507\n",
            "Iteration 469, loss = 0.52315038\n",
            "Iteration 470, loss = 0.52277085\n",
            "Iteration 471, loss = 0.52237426\n",
            "Iteration 472, loss = 0.52198123\n",
            "Iteration 473, loss = 0.52160292\n",
            "Iteration 474, loss = 0.52118280\n",
            "Iteration 475, loss = 0.52078186\n",
            "Iteration 476, loss = 0.52040086\n",
            "Iteration 477, loss = 0.52000949\n",
            "Iteration 478, loss = 0.51959773\n",
            "Iteration 479, loss = 0.51921002\n",
            "Iteration 480, loss = 0.51882291\n",
            "Iteration 481, loss = 0.51842885\n",
            "Iteration 482, loss = 0.51800849\n",
            "Iteration 483, loss = 0.51764141\n",
            "Iteration 484, loss = 0.51722780\n",
            "Iteration 485, loss = 0.51685193\n",
            "Iteration 486, loss = 0.51646288\n",
            "Iteration 487, loss = 0.51606446\n",
            "Iteration 488, loss = 0.51567903\n",
            "Iteration 489, loss = 0.51532384\n",
            "Iteration 490, loss = 0.51492511\n",
            "Iteration 491, loss = 0.51455895\n",
            "Iteration 492, loss = 0.51415981\n",
            "Iteration 493, loss = 0.51381369\n",
            "Iteration 494, loss = 0.51343305\n",
            "Iteration 495, loss = 0.51306913\n",
            "Iteration 496, loss = 0.51270476\n",
            "Iteration 497, loss = 0.51234250\n",
            "Iteration 498, loss = 0.51197136\n",
            "Iteration 499, loss = 0.51160400\n",
            "Iteration 500, loss = 0.51127200\n",
            "Iteration 501, loss = 0.51089761\n",
            "Iteration 502, loss = 0.51054631\n",
            "Iteration 503, loss = 0.51023094\n",
            "Iteration 504, loss = 0.50985023\n",
            "Iteration 505, loss = 0.50953318\n",
            "Iteration 506, loss = 0.50918067\n",
            "Iteration 507, loss = 0.50883327\n",
            "Iteration 508, loss = 0.50850449\n",
            "Iteration 509, loss = 0.50817185\n",
            "Iteration 510, loss = 0.50783965\n",
            "Iteration 511, loss = 0.50748678\n",
            "Iteration 512, loss = 0.50714973\n",
            "Iteration 513, loss = 0.50681824\n",
            "Iteration 514, loss = 0.50650988\n",
            "Iteration 515, loss = 0.50616897\n",
            "Iteration 516, loss = 0.50582891\n",
            "Iteration 517, loss = 0.50550905\n",
            "Iteration 518, loss = 0.50520326\n",
            "Iteration 519, loss = 0.50486100\n",
            "Iteration 520, loss = 0.50456466\n",
            "Iteration 521, loss = 0.50422936\n",
            "Iteration 522, loss = 0.50392506\n",
            "Iteration 523, loss = 0.50360677\n",
            "Iteration 524, loss = 0.50332133\n",
            "Iteration 525, loss = 0.50304103\n",
            "Iteration 526, loss = 0.50273369\n",
            "Iteration 527, loss = 0.50243917\n",
            "Iteration 528, loss = 0.50215350\n",
            "Iteration 529, loss = 0.50187229\n",
            "Iteration 530, loss = 0.50158246\n",
            "Iteration 531, loss = 0.50129363\n",
            "Iteration 532, loss = 0.50100645\n",
            "Iteration 533, loss = 0.50074758\n",
            "Iteration 534, loss = 0.50046335\n",
            "Iteration 535, loss = 0.50018317\n",
            "Iteration 536, loss = 0.49993018\n",
            "Iteration 537, loss = 0.49965395\n",
            "Iteration 538, loss = 0.49940750\n",
            "Iteration 539, loss = 0.49911756\n",
            "Iteration 540, loss = 0.49887202\n",
            "Iteration 541, loss = 0.49862800\n",
            "Iteration 542, loss = 0.49836900\n",
            "Iteration 543, loss = 0.49810096\n",
            "Iteration 544, loss = 0.49786172\n",
            "Iteration 545, loss = 0.49760697\n",
            "Iteration 546, loss = 0.49736720\n",
            "Iteration 547, loss = 0.49712759\n",
            "Iteration 548, loss = 0.49689197\n",
            "Iteration 549, loss = 0.49665518\n",
            "Iteration 550, loss = 0.49642210\n",
            "Iteration 551, loss = 0.49619318\n",
            "Iteration 552, loss = 0.49595861\n",
            "Iteration 553, loss = 0.49573226\n",
            "Iteration 554, loss = 0.49550629\n",
            "Iteration 555, loss = 0.49527971\n",
            "Iteration 556, loss = 0.49505968\n",
            "Iteration 557, loss = 0.49484034\n",
            "Iteration 558, loss = 0.49464588\n",
            "Iteration 559, loss = 0.49438437\n",
            "Iteration 560, loss = 0.49418394\n",
            "Iteration 561, loss = 0.49396448\n",
            "Iteration 562, loss = 0.49375965\n",
            "Iteration 563, loss = 0.49354909\n",
            "Iteration 564, loss = 0.49335044\n",
            "Iteration 565, loss = 0.49312393\n",
            "Iteration 566, loss = 0.49293303\n",
            "Iteration 567, loss = 0.49272174\n",
            "Iteration 568, loss = 0.49252643\n",
            "Iteration 569, loss = 0.49231129\n",
            "Iteration 570, loss = 0.49209725\n",
            "Iteration 571, loss = 0.49190959\n",
            "Iteration 572, loss = 0.49169315\n",
            "Iteration 573, loss = 0.49149424\n",
            "Iteration 574, loss = 0.49128146\n",
            "Iteration 575, loss = 0.49108373\n",
            "Iteration 576, loss = 0.49087245\n",
            "Iteration 577, loss = 0.49066949\n",
            "Iteration 578, loss = 0.49045311\n",
            "Iteration 579, loss = 0.49025568\n",
            "Iteration 580, loss = 0.49006631\n",
            "Iteration 581, loss = 0.48985928\n",
            "Iteration 582, loss = 0.48965575\n",
            "Iteration 583, loss = 0.48946654\n",
            "Iteration 584, loss = 0.48926922\n",
            "Iteration 585, loss = 0.48905396\n",
            "Iteration 586, loss = 0.48886113\n",
            "Iteration 587, loss = 0.48867456\n",
            "Iteration 588, loss = 0.48846755\n",
            "Iteration 589, loss = 0.48826911\n",
            "Iteration 590, loss = 0.48807883\n",
            "Iteration 591, loss = 0.48789282\n",
            "Iteration 592, loss = 0.48769149\n",
            "Iteration 593, loss = 0.48749946\n",
            "Iteration 594, loss = 0.48732391\n",
            "Iteration 595, loss = 0.48713995\n",
            "Iteration 596, loss = 0.48696153\n",
            "Iteration 597, loss = 0.48679024\n",
            "Iteration 598, loss = 0.48660179\n",
            "Iteration 599, loss = 0.48644268\n",
            "Iteration 600, loss = 0.48626930\n",
            "Iteration 601, loss = 0.48610423\n",
            "Iteration 602, loss = 0.48592328\n",
            "Iteration 603, loss = 0.48576350\n",
            "Iteration 604, loss = 0.48561425\n",
            "Iteration 605, loss = 0.48543925\n",
            "Iteration 606, loss = 0.48525687\n",
            "Iteration 607, loss = 0.48510970\n",
            "Iteration 608, loss = 0.48496286\n",
            "Iteration 609, loss = 0.48479446\n",
            "Iteration 610, loss = 0.48461577\n",
            "Iteration 611, loss = 0.48446939\n",
            "Iteration 612, loss = 0.48431782\n",
            "Iteration 613, loss = 0.48415616\n",
            "Iteration 614, loss = 0.48399162\n",
            "Iteration 615, loss = 0.48384329\n",
            "Iteration 616, loss = 0.48369915\n",
            "Iteration 617, loss = 0.48356131\n",
            "Iteration 618, loss = 0.48340273\n",
            "Iteration 619, loss = 0.48322989\n",
            "Iteration 620, loss = 0.48308708\n",
            "Iteration 621, loss = 0.48294533\n",
            "Iteration 622, loss = 0.48282787\n",
            "Iteration 623, loss = 0.48264909\n",
            "Iteration 624, loss = 0.48250776\n",
            "Iteration 625, loss = 0.48236940\n",
            "Iteration 626, loss = 0.48223237\n",
            "Iteration 627, loss = 0.48209433\n",
            "Iteration 628, loss = 0.48195370\n",
            "Iteration 629, loss = 0.48181047\n",
            "Iteration 630, loss = 0.48169250\n",
            "Iteration 631, loss = 0.48154210\n",
            "Iteration 632, loss = 0.48139148\n",
            "Iteration 633, loss = 0.48124171\n",
            "Iteration 634, loss = 0.48111635\n",
            "Iteration 635, loss = 0.48097042\n",
            "Iteration 636, loss = 0.48081594\n",
            "Iteration 637, loss = 0.48066418\n",
            "Iteration 638, loss = 0.48053846\n",
            "Iteration 639, loss = 0.48039858\n",
            "Iteration 640, loss = 0.48023093\n",
            "Iteration 641, loss = 0.48009323\n",
            "Iteration 642, loss = 0.47992189\n",
            "Iteration 643, loss = 0.47979718\n",
            "Iteration 644, loss = 0.47961141\n",
            "Iteration 645, loss = 0.47944844\n",
            "Iteration 646, loss = 0.47928148\n",
            "Iteration 647, loss = 0.47914535\n",
            "Iteration 648, loss = 0.47897962\n",
            "Iteration 649, loss = 0.47881806\n",
            "Iteration 650, loss = 0.47864459\n",
            "Iteration 651, loss = 0.47849400\n",
            "Iteration 652, loss = 0.47835885\n",
            "Iteration 653, loss = 0.47819256\n",
            "Iteration 654, loss = 0.47804943\n",
            "Iteration 655, loss = 0.47790200\n",
            "Iteration 656, loss = 0.47775748\n",
            "Iteration 657, loss = 0.47760442\n",
            "Iteration 658, loss = 0.47745613\n",
            "Iteration 659, loss = 0.47729839\n",
            "Iteration 660, loss = 0.47717598\n",
            "Iteration 661, loss = 0.47702871\n",
            "Iteration 662, loss = 0.47687853\n",
            "Iteration 663, loss = 0.47675267\n",
            "Iteration 664, loss = 0.47659670\n",
            "Iteration 665, loss = 0.47645629\n",
            "Iteration 666, loss = 0.47633543\n",
            "Iteration 667, loss = 0.47620751\n",
            "Iteration 668, loss = 0.47606093\n",
            "Iteration 669, loss = 0.47593101\n",
            "Iteration 670, loss = 0.47580660\n",
            "Iteration 671, loss = 0.47570032\n",
            "Iteration 672, loss = 0.47556302\n",
            "Iteration 673, loss = 0.47545561\n",
            "Iteration 674, loss = 0.47531895\n",
            "Iteration 675, loss = 0.47520507\n",
            "Iteration 676, loss = 0.47506834\n",
            "Iteration 677, loss = 0.47494130\n",
            "Iteration 678, loss = 0.47480636\n",
            "Iteration 679, loss = 0.47467506\n",
            "Iteration 680, loss = 0.47453525\n",
            "Iteration 681, loss = 0.47440548\n",
            "Iteration 682, loss = 0.47427383\n",
            "Iteration 683, loss = 0.47416572\n",
            "Iteration 684, loss = 0.47400508\n",
            "Iteration 685, loss = 0.47389143\n",
            "Iteration 686, loss = 0.47376668\n",
            "Iteration 687, loss = 0.47363944\n",
            "Iteration 688, loss = 0.47350409\n",
            "Iteration 689, loss = 0.47339327\n",
            "Iteration 690, loss = 0.47328050\n",
            "Iteration 691, loss = 0.47315515\n",
            "Iteration 692, loss = 0.47303440\n",
            "Iteration 693, loss = 0.47293134\n",
            "Iteration 694, loss = 0.47281517\n",
            "Iteration 695, loss = 0.47270113\n",
            "Iteration 696, loss = 0.47259780\n",
            "Iteration 697, loss = 0.47248056\n",
            "Iteration 698, loss = 0.47239366\n",
            "Iteration 699, loss = 0.47228990\n",
            "Iteration 700, loss = 0.47218718\n",
            "Iteration 701, loss = 0.47209523\n",
            "Iteration 702, loss = 0.47200041\n",
            "Iteration 703, loss = 0.47190125\n",
            "Iteration 704, loss = 0.47180769\n",
            "Iteration 705, loss = 0.47170593\n",
            "Iteration 706, loss = 0.47161434\n",
            "Iteration 707, loss = 0.47150477\n",
            "Iteration 708, loss = 0.47143425\n",
            "Iteration 709, loss = 0.47132517\n",
            "Iteration 710, loss = 0.47123815\n",
            "Iteration 711, loss = 0.47114573\n",
            "Iteration 712, loss = 0.47102559\n",
            "Iteration 713, loss = 0.47092450\n",
            "Iteration 714, loss = 0.47081160\n",
            "Iteration 715, loss = 0.47069771\n",
            "Iteration 716, loss = 0.47059704\n",
            "Iteration 717, loss = 0.47048530\n",
            "Iteration 718, loss = 0.47037101\n",
            "Iteration 719, loss = 0.47027655\n",
            "Iteration 720, loss = 0.47017063\n",
            "Iteration 721, loss = 0.47006355\n",
            "Iteration 722, loss = 0.46996091\n",
            "Iteration 723, loss = 0.46986193\n",
            "Iteration 724, loss = 0.46976351\n",
            "Iteration 725, loss = 0.46964993\n",
            "Iteration 726, loss = 0.46955218\n",
            "Iteration 727, loss = 0.46949191\n",
            "Iteration 728, loss = 0.46935047\n",
            "Iteration 729, loss = 0.46927019\n",
            "Iteration 730, loss = 0.46917984\n",
            "Iteration 731, loss = 0.46909378\n",
            "Iteration 732, loss = 0.46902184\n",
            "Iteration 733, loss = 0.46892383\n",
            "Iteration 734, loss = 0.46883810\n",
            "Iteration 735, loss = 0.46874936\n",
            "Iteration 736, loss = 0.46868269\n",
            "Iteration 737, loss = 0.46858138\n",
            "Iteration 738, loss = 0.46850104\n",
            "Iteration 739, loss = 0.46842280\n",
            "Iteration 740, loss = 0.46834177\n",
            "Iteration 741, loss = 0.46825852\n",
            "Iteration 742, loss = 0.46818473\n",
            "Iteration 743, loss = 0.46810228\n",
            "Iteration 744, loss = 0.46802210\n",
            "Iteration 745, loss = 0.46795476\n",
            "Iteration 746, loss = 0.46787213\n",
            "Iteration 747, loss = 0.46780187\n",
            "Iteration 748, loss = 0.46774099\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(hidden_layer_sizes=(3, 3), max_iter=1000, random_state=3,\n",
              "              solver='sgd', verbose=True)"
            ],
            "text/html": [
              "<style>#sk-container-id-6 {color: black;background-color: white;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(hidden_layer_sizes=(3, 3), max_iter=1000, random_state=3,\n",
              "              solver=&#x27;sgd&#x27;, verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" checked><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(hidden_layer_sizes=(3, 3), max_iter=1000, random_state=3,\n",
              "              solver=&#x27;sgd&#x27;, verbose=True)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "previsoesf = rnaf.predict(X_test)"
      ],
      "metadata": {
        "id": "1W1_Moc3z3p6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rnaf_teste = accuracy_score(y_test, previsoesf)"
      ],
      "metadata": {
        "id": "f-iNurn0z3p6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(rnaf_teste)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d77e24ab-c4ca-48cb-f53a-cb441961b5e6",
        "id": "ydPvdPtxz3p6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7947761194029851\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cm_rnaf_teste = confusion_matrix(y_test, previsoesf)"
      ],
      "metadata": {
        "id": "3UfZJk7Uz3p6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_test, previsoesf))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc994c92-4e22-4ce1-956b-0209dacafb66",
        "id": "p8EqFd63z3p6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.87      0.85       174\n",
            "           1       0.73      0.66      0.69        94\n",
            "\n",
            "    accuracy                           0.79       268\n",
            "   macro avg       0.78      0.76      0.77       268\n",
            "weighted avg       0.79      0.79      0.79       268\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "previsoes_treinof = rnaf.predict(X_train_oversampled)"
      ],
      "metadata": {
        "id": "E4nZj9e_z3p7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rnaf_treino = accuracy_score(y_train_oversampled, previsoes_treinof)"
      ],
      "metadata": {
        "id": "UkTpHfk3z3p7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm_rnaf_treino = confusion_matrix(y_train_oversampled, previsoes_treinof)"
      ],
      "metadata": {
        "id": "Zhp67WSUz3p7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GridSearch"
      ],
      "metadata": {
        "id": "KDV7JQ5tz3p7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Grid Search\n",
        "param_grid = {\n",
        "    'activation': ['relu', 'tanh', 'logistic'],\n",
        "    'solver': ['sgd', 'adam'],\n",
        "    'max_iter': [1000, 2000],\n",
        "    'tol': [0.0001, 0.001],\n",
        "}\n",
        "\n",
        "g_search = GridSearchCV(estimator=rnaf, param_grid=param_grid, cv=10)\n",
        "g_search.fit(X_train_oversampled, y_train_oversampled)\n",
        "\n",
        "best_params_gridf = g_search.best_params_\n",
        "best_score_gridf = g_search.best_score_\n",
        "\n",
        "print(\"Melhores hiperparâmetros encontrados através do Grid Search:\")\n",
        "print(best_params_gridf)\n",
        "print(\"Melhor pontuação (acurácia) encontrada através do Grid Search:\", best_score_gridf)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad767494-e38d-4424-e3e6-150bb5485ec9",
        "id": "nDryL1TGz3p7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mA saída de streaming foi truncada nas últimas 5000 linhas.\u001b[0m\n",
            "Iteration 205, loss = 0.62486398\n",
            "Iteration 206, loss = 0.62407569\n",
            "Iteration 207, loss = 0.62327356\n",
            "Iteration 208, loss = 0.62245669\n",
            "Iteration 209, loss = 0.62167399\n",
            "Iteration 210, loss = 0.62088268\n",
            "Iteration 211, loss = 0.62012609\n",
            "Iteration 212, loss = 0.61929912\n",
            "Iteration 213, loss = 0.61850635\n",
            "Iteration 214, loss = 0.61776799\n",
            "Iteration 215, loss = 0.61693847\n",
            "Iteration 216, loss = 0.61612989\n",
            "Iteration 217, loss = 0.61533264\n",
            "Iteration 218, loss = 0.61452399\n",
            "Iteration 219, loss = 0.61371583\n",
            "Iteration 220, loss = 0.61295434\n",
            "Iteration 221, loss = 0.61211752\n",
            "Iteration 222, loss = 0.61134373\n",
            "Iteration 223, loss = 0.61051655\n",
            "Iteration 224, loss = 0.60971689\n",
            "Iteration 225, loss = 0.60892812\n",
            "Iteration 226, loss = 0.60808444\n",
            "Iteration 227, loss = 0.60725820\n",
            "Iteration 228, loss = 0.60644400\n",
            "Iteration 229, loss = 0.60564659\n",
            "Iteration 230, loss = 0.60478509\n",
            "Iteration 231, loss = 0.60394986\n",
            "Iteration 232, loss = 0.60314434\n",
            "Iteration 233, loss = 0.60233547\n",
            "Iteration 234, loss = 0.60147380\n",
            "Iteration 235, loss = 0.60065214\n",
            "Iteration 236, loss = 0.59982404\n",
            "Iteration 237, loss = 0.59898224\n",
            "Iteration 238, loss = 0.59816935\n",
            "Iteration 239, loss = 0.59731606\n",
            "Iteration 240, loss = 0.59650353\n",
            "Iteration 241, loss = 0.59569043\n",
            "Iteration 242, loss = 0.59487686\n",
            "Iteration 243, loss = 0.59406963\n",
            "Iteration 244, loss = 0.59323330\n",
            "Iteration 245, loss = 0.59245585\n",
            "Iteration 246, loss = 0.59163439\n",
            "Iteration 247, loss = 0.59079627\n",
            "Iteration 248, loss = 0.58996367\n",
            "Iteration 249, loss = 0.58915033\n",
            "Iteration 250, loss = 0.58829625\n",
            "Iteration 251, loss = 0.58747711\n",
            "Iteration 252, loss = 0.58664778\n",
            "Iteration 253, loss = 0.58582865\n",
            "Iteration 254, loss = 0.58499513\n",
            "Iteration 255, loss = 0.58417312\n",
            "Iteration 256, loss = 0.58335111\n",
            "Iteration 257, loss = 0.58253585\n",
            "Iteration 258, loss = 0.58168673\n",
            "Iteration 259, loss = 0.58088234\n",
            "Iteration 260, loss = 0.58004529\n",
            "Iteration 261, loss = 0.57922375\n",
            "Iteration 262, loss = 0.57837860\n",
            "Iteration 263, loss = 0.57752102\n",
            "Iteration 264, loss = 0.57668260\n",
            "Iteration 265, loss = 0.57583276\n",
            "Iteration 266, loss = 0.57496958\n",
            "Iteration 267, loss = 0.57413057\n",
            "Iteration 268, loss = 0.57326616\n",
            "Iteration 269, loss = 0.57245604\n",
            "Iteration 270, loss = 0.57161447\n",
            "Iteration 271, loss = 0.57077777\n",
            "Iteration 272, loss = 0.56991921\n",
            "Iteration 273, loss = 0.56905700\n",
            "Iteration 274, loss = 0.56823668\n",
            "Iteration 275, loss = 0.56739290\n",
            "Iteration 276, loss = 0.56655510\n",
            "Iteration 277, loss = 0.56570433\n",
            "Iteration 278, loss = 0.56482786\n",
            "Iteration 279, loss = 0.56400324\n",
            "Iteration 280, loss = 0.56314111\n",
            "Iteration 281, loss = 0.56228585\n",
            "Iteration 282, loss = 0.56142781\n",
            "Iteration 283, loss = 0.56058011\n",
            "Iteration 284, loss = 0.55982714\n",
            "Iteration 285, loss = 0.55895422\n",
            "Iteration 286, loss = 0.55810083\n",
            "Iteration 287, loss = 0.55726021\n",
            "Iteration 288, loss = 0.55640408\n",
            "Iteration 289, loss = 0.55554787\n",
            "Iteration 290, loss = 0.55478012\n",
            "Iteration 291, loss = 0.55389350\n",
            "Iteration 292, loss = 0.55310170\n",
            "Iteration 293, loss = 0.55223184\n",
            "Iteration 294, loss = 0.55143636\n",
            "Iteration 295, loss = 0.55062531\n",
            "Iteration 296, loss = 0.54978241\n",
            "Iteration 297, loss = 0.54892336\n",
            "Iteration 298, loss = 0.54808481\n",
            "Iteration 299, loss = 0.54728911\n",
            "Iteration 300, loss = 0.54645999\n",
            "Iteration 301, loss = 0.54565898\n",
            "Iteration 302, loss = 0.54483244\n",
            "Iteration 303, loss = 0.54400527\n",
            "Iteration 304, loss = 0.54317832\n",
            "Iteration 305, loss = 0.54236911\n",
            "Iteration 306, loss = 0.54154734\n",
            "Iteration 307, loss = 0.54072243\n",
            "Iteration 308, loss = 0.53993387\n",
            "Iteration 309, loss = 0.53911404\n",
            "Iteration 310, loss = 0.53832077\n",
            "Iteration 311, loss = 0.53751320\n",
            "Iteration 312, loss = 0.53670136\n",
            "Iteration 313, loss = 0.53587212\n",
            "Iteration 314, loss = 0.53507810\n",
            "Iteration 315, loss = 0.53432273\n",
            "Iteration 316, loss = 0.53347147\n",
            "Iteration 317, loss = 0.53272155\n",
            "Iteration 318, loss = 0.53187641\n",
            "Iteration 319, loss = 0.53110250\n",
            "Iteration 320, loss = 0.53033179\n",
            "Iteration 321, loss = 0.52953834\n",
            "Iteration 322, loss = 0.52885381\n",
            "Iteration 323, loss = 0.52810246\n",
            "Iteration 324, loss = 0.52732648\n",
            "Iteration 325, loss = 0.52658003\n",
            "Iteration 326, loss = 0.52582630\n",
            "Iteration 327, loss = 0.52506652\n",
            "Iteration 328, loss = 0.52431020\n",
            "Iteration 329, loss = 0.52357000\n",
            "Iteration 330, loss = 0.52283928\n",
            "Iteration 331, loss = 0.52206015\n",
            "Iteration 332, loss = 0.52134570\n",
            "Iteration 333, loss = 0.52061034\n",
            "Iteration 334, loss = 0.51991311\n",
            "Iteration 335, loss = 0.51915823\n",
            "Iteration 336, loss = 0.51848262\n",
            "Iteration 337, loss = 0.51774997\n",
            "Iteration 338, loss = 0.51706915\n",
            "Iteration 339, loss = 0.51638445\n",
            "Iteration 340, loss = 0.51570405\n",
            "Iteration 341, loss = 0.51500406\n",
            "Iteration 342, loss = 0.51430560\n",
            "Iteration 343, loss = 0.51364376\n",
            "Iteration 344, loss = 0.51294007\n",
            "Iteration 345, loss = 0.51227681\n",
            "Iteration 346, loss = 0.51159202\n",
            "Iteration 347, loss = 0.51092343\n",
            "Iteration 348, loss = 0.51024792\n",
            "Iteration 349, loss = 0.50967949\n",
            "Iteration 350, loss = 0.50893867\n",
            "Iteration 351, loss = 0.50828829\n",
            "Iteration 352, loss = 0.50764284\n",
            "Iteration 353, loss = 0.50698929\n",
            "Iteration 354, loss = 0.50630375\n",
            "Iteration 355, loss = 0.50565196\n",
            "Iteration 356, loss = 0.50509049\n",
            "Iteration 357, loss = 0.50438879\n",
            "Iteration 358, loss = 0.50373689\n",
            "Iteration 359, loss = 0.50313118\n",
            "Iteration 360, loss = 0.50249664\n",
            "Iteration 361, loss = 0.50190006\n",
            "Iteration 362, loss = 0.50129548\n",
            "Iteration 363, loss = 0.50070440\n",
            "Iteration 364, loss = 0.50011814\n",
            "Iteration 365, loss = 0.49952435\n",
            "Iteration 366, loss = 0.49897055\n",
            "Iteration 367, loss = 0.49836901\n",
            "Iteration 368, loss = 0.49779809\n",
            "Iteration 369, loss = 0.49720765\n",
            "Iteration 370, loss = 0.49669087\n",
            "Iteration 371, loss = 0.49611916\n",
            "Iteration 372, loss = 0.49551523\n",
            "Iteration 373, loss = 0.49499147\n",
            "Iteration 374, loss = 0.49445904\n",
            "Iteration 375, loss = 0.49390725\n",
            "Iteration 376, loss = 0.49341153\n",
            "Iteration 377, loss = 0.49286969\n",
            "Iteration 378, loss = 0.49229780\n",
            "Iteration 379, loss = 0.49177338\n",
            "Iteration 380, loss = 0.49128786\n",
            "Iteration 381, loss = 0.49077257\n",
            "Iteration 382, loss = 0.49024718\n",
            "Iteration 383, loss = 0.48968567\n",
            "Iteration 384, loss = 0.48918337\n",
            "Iteration 385, loss = 0.48864710\n",
            "Iteration 386, loss = 0.48813019\n",
            "Iteration 387, loss = 0.48765422\n",
            "Iteration 388, loss = 0.48718711\n",
            "Iteration 389, loss = 0.48671223\n",
            "Iteration 390, loss = 0.48624411\n",
            "Iteration 391, loss = 0.48573759\n",
            "Iteration 392, loss = 0.48530187\n",
            "Iteration 393, loss = 0.48482670\n",
            "Iteration 394, loss = 0.48437764\n",
            "Iteration 395, loss = 0.48388272\n",
            "Iteration 396, loss = 0.48340447\n",
            "Iteration 397, loss = 0.48301554\n",
            "Iteration 398, loss = 0.48247557\n",
            "Iteration 399, loss = 0.48204053\n",
            "Iteration 400, loss = 0.48157424\n",
            "Iteration 401, loss = 0.48114900\n",
            "Iteration 402, loss = 0.48075411\n",
            "Iteration 403, loss = 0.48026516\n",
            "Iteration 404, loss = 0.47982726\n",
            "Iteration 405, loss = 0.47939800\n",
            "Iteration 406, loss = 0.47899263\n",
            "Iteration 407, loss = 0.47862135\n",
            "Iteration 408, loss = 0.47814344\n",
            "Iteration 409, loss = 0.47773723\n",
            "Iteration 410, loss = 0.47729767\n",
            "Iteration 411, loss = 0.47688526\n",
            "Iteration 412, loss = 0.47643458\n",
            "Iteration 413, loss = 0.47604883\n",
            "Iteration 414, loss = 0.47562056\n",
            "Iteration 415, loss = 0.47520125\n",
            "Iteration 416, loss = 0.47483574\n",
            "Iteration 417, loss = 0.47447635\n",
            "Iteration 418, loss = 0.47405963\n",
            "Iteration 419, loss = 0.47369645\n",
            "Iteration 420, loss = 0.47334224\n",
            "Iteration 421, loss = 0.47296692\n",
            "Iteration 422, loss = 0.47262372\n",
            "Iteration 423, loss = 0.47222022\n",
            "Iteration 424, loss = 0.47191442\n",
            "Iteration 425, loss = 0.47161523\n",
            "Iteration 426, loss = 0.47133919\n",
            "Iteration 427, loss = 0.47092777\n",
            "Iteration 428, loss = 0.47061403\n",
            "Iteration 429, loss = 0.47027963\n",
            "Iteration 430, loss = 0.46991431\n",
            "Iteration 431, loss = 0.46961527\n",
            "Iteration 432, loss = 0.46927686\n",
            "Iteration 433, loss = 0.46891917\n",
            "Iteration 434, loss = 0.46858689\n",
            "Iteration 435, loss = 0.46826322\n",
            "Iteration 436, loss = 0.46791713\n",
            "Iteration 437, loss = 0.46762480\n",
            "Iteration 438, loss = 0.46729862\n",
            "Iteration 439, loss = 0.46697641\n",
            "Iteration 440, loss = 0.46668172\n",
            "Iteration 441, loss = 0.46639971\n",
            "Iteration 442, loss = 0.46608961\n",
            "Iteration 443, loss = 0.46580152\n",
            "Iteration 444, loss = 0.46551435\n",
            "Iteration 445, loss = 0.46523471\n",
            "Iteration 446, loss = 0.46495647\n",
            "Iteration 447, loss = 0.46466904\n",
            "Iteration 448, loss = 0.46439515\n",
            "Iteration 449, loss = 0.46409748\n",
            "Iteration 450, loss = 0.46385249\n",
            "Iteration 451, loss = 0.46351677\n",
            "Iteration 452, loss = 0.46323329\n",
            "Iteration 453, loss = 0.46299487\n",
            "Iteration 454, loss = 0.46269000\n",
            "Iteration 455, loss = 0.46241417\n",
            "Iteration 456, loss = 0.46216652\n",
            "Iteration 457, loss = 0.46187174\n",
            "Iteration 458, loss = 0.46167540\n",
            "Iteration 459, loss = 0.46137810\n",
            "Iteration 460, loss = 0.46114846\n",
            "Iteration 461, loss = 0.46087516\n",
            "Iteration 462, loss = 0.46065768\n",
            "Iteration 463, loss = 0.46039675\n",
            "Iteration 464, loss = 0.46016557\n",
            "Iteration 465, loss = 0.45992405\n",
            "Iteration 466, loss = 0.45965287\n",
            "Iteration 467, loss = 0.45941760\n",
            "Iteration 468, loss = 0.45916860\n",
            "Iteration 469, loss = 0.45895045\n",
            "Iteration 470, loss = 0.45871774\n",
            "Iteration 471, loss = 0.45852153\n",
            "Iteration 472, loss = 0.45828700\n",
            "Iteration 473, loss = 0.45808993\n",
            "Iteration 474, loss = 0.45783906\n",
            "Iteration 475, loss = 0.45762185\n",
            "Iteration 476, loss = 0.45741754\n",
            "Iteration 477, loss = 0.45722502\n",
            "Iteration 478, loss = 0.45701519\n",
            "Iteration 479, loss = 0.45681970\n",
            "Iteration 480, loss = 0.45663945\n",
            "Iteration 481, loss = 0.45644016\n",
            "Iteration 482, loss = 0.45624603\n",
            "Iteration 483, loss = 0.45603545\n",
            "Iteration 484, loss = 0.45587355\n",
            "Iteration 485, loss = 0.45566697\n",
            "Iteration 486, loss = 0.45547778\n",
            "Iteration 487, loss = 0.45528140\n",
            "Iteration 488, loss = 0.45509451\n",
            "Iteration 489, loss = 0.45491946\n",
            "Iteration 490, loss = 0.45473155\n",
            "Iteration 491, loss = 0.45455367\n",
            "Iteration 492, loss = 0.45437920\n",
            "Iteration 493, loss = 0.45422151\n",
            "Iteration 494, loss = 0.45406947\n",
            "Iteration 495, loss = 0.45390729\n",
            "Iteration 496, loss = 0.45371428\n",
            "Iteration 497, loss = 0.45356341\n",
            "Iteration 498, loss = 0.45338609\n",
            "Iteration 499, loss = 0.45321403\n",
            "Iteration 500, loss = 0.45307621\n",
            "Iteration 501, loss = 0.45290746\n",
            "Iteration 502, loss = 0.45274879\n",
            "Iteration 503, loss = 0.45257885\n",
            "Iteration 504, loss = 0.45244966\n",
            "Iteration 505, loss = 0.45230164\n",
            "Iteration 506, loss = 0.45214444\n",
            "Iteration 507, loss = 0.45198218\n",
            "Iteration 508, loss = 0.45184730\n",
            "Iteration 509, loss = 0.45169223\n",
            "Iteration 510, loss = 0.45155044\n",
            "Iteration 511, loss = 0.45139751\n",
            "Iteration 512, loss = 0.45124619\n",
            "Iteration 513, loss = 0.45109181\n",
            "Iteration 514, loss = 0.45094419\n",
            "Iteration 515, loss = 0.45079749\n",
            "Iteration 516, loss = 0.45070690\n",
            "Iteration 517, loss = 0.45053925\n",
            "Iteration 518, loss = 0.45042661\n",
            "Iteration 519, loss = 0.45034253\n",
            "Iteration 520, loss = 0.45021884\n",
            "Iteration 521, loss = 0.45007915\n",
            "Iteration 522, loss = 0.44994531\n",
            "Iteration 523, loss = 0.44979771\n",
            "Iteration 524, loss = 0.44966641\n",
            "Iteration 525, loss = 0.44953175\n",
            "Iteration 526, loss = 0.44940535\n",
            "Iteration 527, loss = 0.44927594\n",
            "Iteration 528, loss = 0.44920019\n",
            "Iteration 529, loss = 0.44908123\n",
            "Iteration 530, loss = 0.44893706\n",
            "Iteration 531, loss = 0.44885024\n",
            "Iteration 532, loss = 0.44869894\n",
            "Iteration 533, loss = 0.44858370\n",
            "Iteration 534, loss = 0.44846083\n",
            "Iteration 535, loss = 0.44834772\n",
            "Iteration 536, loss = 0.44821908\n",
            "Iteration 537, loss = 0.44811731\n",
            "Iteration 538, loss = 0.44798712\n",
            "Iteration 539, loss = 0.44787226\n",
            "Iteration 540, loss = 0.44777081\n",
            "Iteration 541, loss = 0.44767516\n",
            "Iteration 542, loss = 0.44765375\n",
            "Iteration 543, loss = 0.44751899\n",
            "Iteration 544, loss = 0.44744736\n",
            "Iteration 545, loss = 0.44732889\n",
            "Iteration 546, loss = 0.44721222\n",
            "Iteration 547, loss = 0.44709091\n",
            "Iteration 548, loss = 0.44698411\n",
            "Iteration 549, loss = 0.44687434\n",
            "Iteration 550, loss = 0.44678364\n",
            "Iteration 551, loss = 0.44669813\n",
            "Iteration 552, loss = 0.44659014\n",
            "Iteration 553, loss = 0.44650560\n",
            "Iteration 554, loss = 0.44639785\n",
            "Iteration 555, loss = 0.44631352\n",
            "Iteration 556, loss = 0.44622402\n",
            "Iteration 557, loss = 0.44612905\n",
            "Iteration 558, loss = 0.44601360\n",
            "Iteration 559, loss = 0.44596741\n",
            "Iteration 560, loss = 0.44584822\n",
            "Iteration 561, loss = 0.44573565\n",
            "Iteration 562, loss = 0.44565632\n",
            "Iteration 563, loss = 0.44557302\n",
            "Iteration 564, loss = 0.44551331\n",
            "Iteration 565, loss = 0.44539903\n",
            "Iteration 566, loss = 0.44531895\n",
            "Iteration 567, loss = 0.44523811\n",
            "Iteration 568, loss = 0.44514321\n",
            "Iteration 569, loss = 0.44506557\n",
            "Iteration 570, loss = 0.44498703\n",
            "Iteration 571, loss = 0.44491897\n",
            "Iteration 572, loss = 0.44486140\n",
            "Iteration 573, loss = 0.44482063\n",
            "Iteration 574, loss = 0.44469494\n",
            "Iteration 575, loss = 0.44461677\n",
            "Iteration 576, loss = 0.44454306\n",
            "Iteration 577, loss = 0.44450157\n",
            "Iteration 578, loss = 0.44439800\n",
            "Iteration 579, loss = 0.44433276\n",
            "Iteration 580, loss = 0.44427647\n",
            "Iteration 581, loss = 0.44421121\n",
            "Iteration 582, loss = 0.44410410\n",
            "Iteration 583, loss = 0.44405498\n",
            "Iteration 584, loss = 0.44397829\n",
            "Iteration 585, loss = 0.44395536\n",
            "Iteration 586, loss = 0.44385053\n",
            "Iteration 587, loss = 0.44379607\n",
            "Iteration 588, loss = 0.44374199\n",
            "Iteration 589, loss = 0.44366329\n",
            "Iteration 590, loss = 0.44359692\n",
            "Iteration 591, loss = 0.44352951\n",
            "Iteration 592, loss = 0.44345611\n",
            "Iteration 593, loss = 0.44337994\n",
            "Iteration 594, loss = 0.44333207\n",
            "Iteration 595, loss = 0.44324338\n",
            "Iteration 596, loss = 0.44320480\n",
            "Iteration 597, loss = 0.44312466\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.71709503\n",
            "Iteration 2, loss = 0.71588915\n",
            "Iteration 3, loss = 0.71491711\n",
            "Iteration 4, loss = 0.71383716\n",
            "Iteration 5, loss = 0.71278470\n",
            "Iteration 6, loss = 0.71190597\n",
            "Iteration 7, loss = 0.71086813\n",
            "Iteration 8, loss = 0.70990736\n",
            "Iteration 9, loss = 0.70900638\n",
            "Iteration 10, loss = 0.70812529\n",
            "Iteration 11, loss = 0.70733261\n",
            "Iteration 12, loss = 0.70642457\n",
            "Iteration 13, loss = 0.70559984\n",
            "Iteration 14, loss = 0.70493451\n",
            "Iteration 15, loss = 0.70420525\n",
            "Iteration 16, loss = 0.70349177\n",
            "Iteration 17, loss = 0.70279350\n",
            "Iteration 18, loss = 0.70214295\n",
            "Iteration 19, loss = 0.70155952\n",
            "Iteration 20, loss = 0.70094256\n",
            "Iteration 21, loss = 0.70033388\n",
            "Iteration 22, loss = 0.69987426\n",
            "Iteration 23, loss = 0.69937259\n",
            "Iteration 24, loss = 0.69885218\n",
            "Iteration 25, loss = 0.69848067\n",
            "Iteration 26, loss = 0.69805078\n",
            "Iteration 27, loss = 0.69776580\n",
            "Iteration 28, loss = 0.69732732\n",
            "Iteration 29, loss = 0.69699824\n",
            "Iteration 30, loss = 0.69669938\n",
            "Iteration 31, loss = 0.69634035\n",
            "Iteration 32, loss = 0.69607697\n",
            "Iteration 33, loss = 0.69579139\n",
            "Iteration 34, loss = 0.69553678\n",
            "Iteration 35, loss = 0.69525142\n",
            "Iteration 36, loss = 0.69504044\n",
            "Iteration 37, loss = 0.69484110\n",
            "Iteration 38, loss = 0.69466199\n",
            "Iteration 39, loss = 0.69442802\n",
            "Iteration 40, loss = 0.69426884\n",
            "Iteration 41, loss = 0.69410583\n",
            "Iteration 42, loss = 0.69394531\n",
            "Iteration 43, loss = 0.69378780\n",
            "Iteration 44, loss = 0.69367487\n",
            "Iteration 45, loss = 0.69349281\n",
            "Iteration 46, loss = 0.69337411\n",
            "Iteration 47, loss = 0.69322160\n",
            "Iteration 48, loss = 0.69310087\n",
            "Iteration 49, loss = 0.69295351\n",
            "Iteration 50, loss = 0.69283192\n",
            "Iteration 51, loss = 0.69276215\n",
            "Iteration 52, loss = 0.69263000\n",
            "Iteration 53, loss = 0.69249072\n",
            "Iteration 54, loss = 0.69242373\n",
            "Iteration 55, loss = 0.69230021\n",
            "Iteration 56, loss = 0.69220561\n",
            "Iteration 57, loss = 0.69210125\n",
            "Iteration 58, loss = 0.69202725\n",
            "Iteration 59, loss = 0.69193065\n",
            "Iteration 60, loss = 0.69184331\n",
            "Iteration 61, loss = 0.69174483\n",
            "Iteration 62, loss = 0.69168680\n",
            "Iteration 63, loss = 0.69156889\n",
            "Iteration 64, loss = 0.69147250\n",
            "Iteration 65, loss = 0.69139636\n",
            "Iteration 66, loss = 0.69128475\n",
            "Iteration 67, loss = 0.69119082\n",
            "Iteration 68, loss = 0.69109263\n",
            "Iteration 69, loss = 0.69099410\n",
            "Iteration 70, loss = 0.69091013\n",
            "Iteration 71, loss = 0.69079234\n",
            "Iteration 72, loss = 0.69067855\n",
            "Iteration 73, loss = 0.69055733\n",
            "Iteration 74, loss = 0.69042918\n",
            "Iteration 75, loss = 0.69028014\n",
            "Iteration 76, loss = 0.69013344\n",
            "Iteration 77, loss = 0.69002190\n",
            "Iteration 78, loss = 0.68986487\n",
            "Iteration 79, loss = 0.68971770\n",
            "Iteration 80, loss = 0.68958128\n",
            "Iteration 81, loss = 0.68940890\n",
            "Iteration 82, loss = 0.68924899\n",
            "Iteration 83, loss = 0.68907853\n",
            "Iteration 84, loss = 0.68891006\n",
            "Iteration 85, loss = 0.68872892\n",
            "Iteration 86, loss = 0.68855462\n",
            "Iteration 87, loss = 0.68835149\n",
            "Iteration 88, loss = 0.68815524\n",
            "Iteration 89, loss = 0.68794644\n",
            "Iteration 90, loss = 0.68774280\n",
            "Iteration 91, loss = 0.68753789\n",
            "Iteration 92, loss = 0.68728976\n",
            "Iteration 93, loss = 0.68708437\n",
            "Iteration 94, loss = 0.68684839\n",
            "Iteration 95, loss = 0.68659973\n",
            "Iteration 96, loss = 0.68636282\n",
            "Iteration 97, loss = 0.68610860\n",
            "Iteration 98, loss = 0.68584152\n",
            "Iteration 99, loss = 0.68559064\n",
            "Iteration 100, loss = 0.68532624\n",
            "Iteration 101, loss = 0.68505984\n",
            "Iteration 102, loss = 0.68478434\n",
            "Iteration 103, loss = 0.68449727\n",
            "Iteration 104, loss = 0.68421877\n",
            "Iteration 105, loss = 0.68392144\n",
            "Iteration 106, loss = 0.68363894\n",
            "Iteration 107, loss = 0.68334481\n",
            "Iteration 108, loss = 0.68303775\n",
            "Iteration 109, loss = 0.68272982\n",
            "Iteration 110, loss = 0.68242040\n",
            "Iteration 111, loss = 0.68210580\n",
            "Iteration 112, loss = 0.68177655\n",
            "Iteration 113, loss = 0.68144791\n",
            "Iteration 114, loss = 0.68108245\n",
            "Iteration 115, loss = 0.68074851\n",
            "Iteration 116, loss = 0.68034997\n",
            "Iteration 117, loss = 0.67997625\n",
            "Iteration 118, loss = 0.67958124\n",
            "Iteration 119, loss = 0.67922226\n",
            "Iteration 120, loss = 0.67881133\n",
            "Iteration 121, loss = 0.67841081\n",
            "Iteration 122, loss = 0.67801199\n",
            "Iteration 123, loss = 0.67759795\n",
            "Iteration 124, loss = 0.67717582\n",
            "Iteration 125, loss = 0.67677570\n",
            "Iteration 126, loss = 0.67632289\n",
            "Iteration 127, loss = 0.67588769\n",
            "Iteration 128, loss = 0.67547960\n",
            "Iteration 129, loss = 0.67501707\n",
            "Iteration 130, loss = 0.67457217\n",
            "Iteration 131, loss = 0.67412157\n",
            "Iteration 132, loss = 0.67366355\n",
            "Iteration 133, loss = 0.67320387\n",
            "Iteration 134, loss = 0.67275069\n",
            "Iteration 135, loss = 0.67226527\n",
            "Iteration 136, loss = 0.67180136\n",
            "Iteration 137, loss = 0.67132840\n",
            "Iteration 138, loss = 0.67082838\n",
            "Iteration 139, loss = 0.67037003\n",
            "Iteration 140, loss = 0.66986935\n",
            "Iteration 141, loss = 0.66938709\n",
            "Iteration 142, loss = 0.66882030\n",
            "Iteration 143, loss = 0.66832216\n",
            "Iteration 144, loss = 0.66779092\n",
            "Iteration 145, loss = 0.66722909\n",
            "Iteration 146, loss = 0.66670592\n",
            "Iteration 147, loss = 0.66616838\n",
            "Iteration 148, loss = 0.66562768\n",
            "Iteration 149, loss = 0.66505357\n",
            "Iteration 150, loss = 0.66448977\n",
            "Iteration 151, loss = 0.66394107\n",
            "Iteration 152, loss = 0.66339035\n",
            "Iteration 153, loss = 0.66282388\n",
            "Iteration 154, loss = 0.66223445\n",
            "Iteration 155, loss = 0.66166067\n",
            "Iteration 156, loss = 0.66104557\n",
            "Iteration 157, loss = 0.66045899\n",
            "Iteration 158, loss = 0.65983864\n",
            "Iteration 159, loss = 0.65919216\n",
            "Iteration 160, loss = 0.65855953\n",
            "Iteration 161, loss = 0.65796367\n",
            "Iteration 162, loss = 0.65729545\n",
            "Iteration 163, loss = 0.65664842\n",
            "Iteration 164, loss = 0.65599416\n",
            "Iteration 165, loss = 0.65537038\n",
            "Iteration 166, loss = 0.65475167\n",
            "Iteration 167, loss = 0.65407252\n",
            "Iteration 168, loss = 0.65347636\n",
            "Iteration 169, loss = 0.65281318\n",
            "Iteration 170, loss = 0.65217609\n",
            "Iteration 171, loss = 0.65154939\n",
            "Iteration 172, loss = 0.65087386\n",
            "Iteration 173, loss = 0.65022188\n",
            "Iteration 174, loss = 0.64956976\n",
            "Iteration 175, loss = 0.64890358\n",
            "Iteration 176, loss = 0.64822737\n",
            "Iteration 177, loss = 0.64754387\n",
            "Iteration 178, loss = 0.64686323\n",
            "Iteration 179, loss = 0.64617554\n",
            "Iteration 180, loss = 0.64544710\n",
            "Iteration 181, loss = 0.64480024\n",
            "Iteration 182, loss = 0.64406391\n",
            "Iteration 183, loss = 0.64334932\n",
            "Iteration 184, loss = 0.64264390\n",
            "Iteration 185, loss = 0.64192650\n",
            "Iteration 186, loss = 0.64121041\n",
            "Iteration 187, loss = 0.64048924\n",
            "Iteration 188, loss = 0.63975535\n",
            "Iteration 189, loss = 0.63899949\n",
            "Iteration 190, loss = 0.63828627\n",
            "Iteration 191, loss = 0.63753337\n",
            "Iteration 192, loss = 0.63685406\n",
            "Iteration 193, loss = 0.63609241\n",
            "Iteration 194, loss = 0.63532793\n",
            "Iteration 195, loss = 0.63461029\n",
            "Iteration 196, loss = 0.63382162\n",
            "Iteration 197, loss = 0.63310272\n",
            "Iteration 198, loss = 0.63235253\n",
            "Iteration 199, loss = 0.63159774\n",
            "Iteration 200, loss = 0.63083705\n",
            "Iteration 201, loss = 0.63007178\n",
            "Iteration 202, loss = 0.62930737\n",
            "Iteration 203, loss = 0.62854589\n",
            "Iteration 204, loss = 0.62781209\n",
            "Iteration 205, loss = 0.62702437\n",
            "Iteration 206, loss = 0.62627639\n",
            "Iteration 207, loss = 0.62549022\n",
            "Iteration 208, loss = 0.62471954\n",
            "Iteration 209, loss = 0.62394651\n",
            "Iteration 210, loss = 0.62316946\n",
            "Iteration 211, loss = 0.62240507\n",
            "Iteration 212, loss = 0.62160664\n",
            "Iteration 213, loss = 0.62080245\n",
            "Iteration 214, loss = 0.62004101\n",
            "Iteration 215, loss = 0.61924759\n",
            "Iteration 216, loss = 0.61846456\n",
            "Iteration 217, loss = 0.61764929\n",
            "Iteration 218, loss = 0.61685760\n",
            "Iteration 219, loss = 0.61605540\n",
            "Iteration 220, loss = 0.61528954\n",
            "Iteration 221, loss = 0.61444921\n",
            "Iteration 222, loss = 0.61368557\n",
            "Iteration 223, loss = 0.61285608\n",
            "Iteration 224, loss = 0.61204904\n",
            "Iteration 225, loss = 0.61125171\n",
            "Iteration 226, loss = 0.61043642\n",
            "Iteration 227, loss = 0.60959088\n",
            "Iteration 228, loss = 0.60878848\n",
            "Iteration 229, loss = 0.60800384\n",
            "Iteration 230, loss = 0.60717365\n",
            "Iteration 231, loss = 0.60632251\n",
            "Iteration 232, loss = 0.60549950\n",
            "Iteration 233, loss = 0.60469159\n",
            "Iteration 234, loss = 0.60386790\n",
            "Iteration 235, loss = 0.60304153\n",
            "Iteration 236, loss = 0.60220077\n",
            "Iteration 237, loss = 0.60137909\n",
            "Iteration 238, loss = 0.60054807\n",
            "Iteration 239, loss = 0.59970086\n",
            "Iteration 240, loss = 0.59889176\n",
            "Iteration 241, loss = 0.59804810\n",
            "Iteration 242, loss = 0.59722875\n",
            "Iteration 243, loss = 0.59640747\n",
            "Iteration 244, loss = 0.59554830\n",
            "Iteration 245, loss = 0.59476313\n",
            "Iteration 246, loss = 0.59391843\n",
            "Iteration 247, loss = 0.59308963\n",
            "Iteration 248, loss = 0.59223807\n",
            "Iteration 249, loss = 0.59142215\n",
            "Iteration 250, loss = 0.59058292\n",
            "Iteration 251, loss = 0.58971046\n",
            "Iteration 252, loss = 0.58887710\n",
            "Iteration 253, loss = 0.58803970\n",
            "Iteration 254, loss = 0.58717479\n",
            "Iteration 255, loss = 0.58634586\n",
            "Iteration 256, loss = 0.58549997\n",
            "Iteration 257, loss = 0.58465003\n",
            "Iteration 258, loss = 0.58379329\n",
            "Iteration 259, loss = 0.58296452\n",
            "Iteration 260, loss = 0.58211631\n",
            "Iteration 261, loss = 0.58125229\n",
            "Iteration 262, loss = 0.58041811\n",
            "Iteration 263, loss = 0.57953595\n",
            "Iteration 264, loss = 0.57870083\n",
            "Iteration 265, loss = 0.57783879\n",
            "Iteration 266, loss = 0.57694780\n",
            "Iteration 267, loss = 0.57608876\n",
            "Iteration 268, loss = 0.57520858\n",
            "Iteration 269, loss = 0.57436008\n",
            "Iteration 270, loss = 0.57348742\n",
            "Iteration 271, loss = 0.57266540\n",
            "Iteration 272, loss = 0.57174912\n",
            "Iteration 273, loss = 0.57088304\n",
            "Iteration 274, loss = 0.57001845\n",
            "Iteration 275, loss = 0.56915703\n",
            "Iteration 276, loss = 0.56828403\n",
            "Iteration 277, loss = 0.56744322\n",
            "Iteration 278, loss = 0.56655154\n",
            "Iteration 279, loss = 0.56570418\n",
            "Iteration 280, loss = 0.56483915\n",
            "Iteration 281, loss = 0.56397269\n",
            "Iteration 282, loss = 0.56309476\n",
            "Iteration 283, loss = 0.56222245\n",
            "Iteration 284, loss = 0.56150634\n",
            "Iteration 285, loss = 0.56055933\n",
            "Iteration 286, loss = 0.55970250\n",
            "Iteration 287, loss = 0.55886698\n",
            "Iteration 288, loss = 0.55799748\n",
            "Iteration 289, loss = 0.55714283\n",
            "Iteration 290, loss = 0.55637218\n",
            "Iteration 291, loss = 0.55546543\n",
            "Iteration 292, loss = 0.55466837\n",
            "Iteration 293, loss = 0.55378695\n",
            "Iteration 294, loss = 0.55291325\n",
            "Iteration 295, loss = 0.55206768\n",
            "Iteration 296, loss = 0.55121429\n",
            "Iteration 297, loss = 0.55036284\n",
            "Iteration 298, loss = 0.54947631\n",
            "Iteration 299, loss = 0.54864314\n",
            "Iteration 300, loss = 0.54784307\n",
            "Iteration 301, loss = 0.54695746\n",
            "Iteration 302, loss = 0.54612631\n",
            "Iteration 303, loss = 0.54530070\n",
            "Iteration 304, loss = 0.54448199\n",
            "Iteration 305, loss = 0.54362869\n",
            "Iteration 306, loss = 0.54279772\n",
            "Iteration 307, loss = 0.54196321\n",
            "Iteration 308, loss = 0.54114332\n",
            "Iteration 309, loss = 0.54033479\n",
            "Iteration 310, loss = 0.53951952\n",
            "Iteration 311, loss = 0.53867486\n",
            "Iteration 312, loss = 0.53787541\n",
            "Iteration 313, loss = 0.53701985\n",
            "Iteration 314, loss = 0.53620009\n",
            "Iteration 315, loss = 0.53540465\n",
            "Iteration 316, loss = 0.53453474\n",
            "Iteration 317, loss = 0.53375915\n",
            "Iteration 318, loss = 0.53290696\n",
            "Iteration 319, loss = 0.53210739\n",
            "Iteration 320, loss = 0.53131692\n",
            "Iteration 321, loss = 0.53048672\n",
            "Iteration 322, loss = 0.52975319\n",
            "Iteration 323, loss = 0.52890682\n",
            "Iteration 324, loss = 0.52811165\n",
            "Iteration 325, loss = 0.52733594\n",
            "Iteration 326, loss = 0.52654200\n",
            "Iteration 327, loss = 0.52577058\n",
            "Iteration 328, loss = 0.52495329\n",
            "Iteration 329, loss = 0.52421546\n",
            "Iteration 330, loss = 0.52345064\n",
            "Iteration 331, loss = 0.52266994\n",
            "Iteration 332, loss = 0.52193614\n",
            "Iteration 333, loss = 0.52116645\n",
            "Iteration 334, loss = 0.52044252\n",
            "Iteration 335, loss = 0.51968180\n",
            "Iteration 336, loss = 0.51896930\n",
            "Iteration 337, loss = 0.51819077\n",
            "Iteration 338, loss = 0.51746112\n",
            "Iteration 339, loss = 0.51676971\n",
            "Iteration 340, loss = 0.51600698\n",
            "Iteration 341, loss = 0.51527190\n",
            "Iteration 342, loss = 0.51453978\n",
            "Iteration 343, loss = 0.51384723\n",
            "Iteration 344, loss = 0.51309591\n",
            "Iteration 345, loss = 0.51240128\n",
            "Iteration 346, loss = 0.51169218\n",
            "Iteration 347, loss = 0.51094902\n",
            "Iteration 348, loss = 0.51023617\n",
            "Iteration 349, loss = 0.50960908\n",
            "Iteration 350, loss = 0.50885210\n",
            "Iteration 351, loss = 0.50814052\n",
            "Iteration 352, loss = 0.50745419\n",
            "Iteration 353, loss = 0.50672144\n",
            "Iteration 354, loss = 0.50600625\n",
            "Iteration 355, loss = 0.50531679\n",
            "Iteration 356, loss = 0.50466747\n",
            "Iteration 357, loss = 0.50395157\n",
            "Iteration 358, loss = 0.50327333\n",
            "Iteration 359, loss = 0.50257914\n",
            "Iteration 360, loss = 0.50190146\n",
            "Iteration 361, loss = 0.50125693\n",
            "Iteration 362, loss = 0.50060455\n",
            "Iteration 363, loss = 0.49995559\n",
            "Iteration 364, loss = 0.49932279\n",
            "Iteration 365, loss = 0.49868538\n",
            "Iteration 366, loss = 0.49811411\n",
            "Iteration 367, loss = 0.49744351\n",
            "Iteration 368, loss = 0.49682643\n",
            "Iteration 369, loss = 0.49618988\n",
            "Iteration 370, loss = 0.49562293\n",
            "Iteration 371, loss = 0.49499797\n",
            "Iteration 372, loss = 0.49434181\n",
            "Iteration 373, loss = 0.49372864\n",
            "Iteration 374, loss = 0.49319774\n",
            "Iteration 375, loss = 0.49258571\n",
            "Iteration 376, loss = 0.49202940\n",
            "Iteration 377, loss = 0.49144334\n",
            "Iteration 378, loss = 0.49083113\n",
            "Iteration 379, loss = 0.49024967\n",
            "Iteration 380, loss = 0.48973521\n",
            "Iteration 381, loss = 0.48913375\n",
            "Iteration 382, loss = 0.48858343\n",
            "Iteration 383, loss = 0.48797845\n",
            "Iteration 384, loss = 0.48743739\n",
            "Iteration 385, loss = 0.48687647\n",
            "Iteration 386, loss = 0.48633523\n",
            "Iteration 387, loss = 0.48579361\n",
            "Iteration 388, loss = 0.48529855\n",
            "Iteration 389, loss = 0.48477548\n",
            "Iteration 390, loss = 0.48429751\n",
            "Iteration 391, loss = 0.48375261\n",
            "Iteration 392, loss = 0.48326420\n",
            "Iteration 393, loss = 0.48276972\n",
            "Iteration 394, loss = 0.48227577\n",
            "Iteration 395, loss = 0.48177449\n",
            "Iteration 396, loss = 0.48124897\n",
            "Iteration 397, loss = 0.48079838\n",
            "Iteration 398, loss = 0.48024446\n",
            "Iteration 399, loss = 0.47973898\n",
            "Iteration 400, loss = 0.47922503\n",
            "Iteration 401, loss = 0.47876177\n",
            "Iteration 402, loss = 0.47830678\n",
            "Iteration 403, loss = 0.47775076\n",
            "Iteration 404, loss = 0.47727193\n",
            "Iteration 405, loss = 0.47682141\n",
            "Iteration 406, loss = 0.47635467\n",
            "Iteration 407, loss = 0.47587678\n",
            "Iteration 408, loss = 0.47540232\n",
            "Iteration 409, loss = 0.47495522\n",
            "Iteration 410, loss = 0.47448642\n",
            "Iteration 411, loss = 0.47407127\n",
            "Iteration 412, loss = 0.47357498\n",
            "Iteration 413, loss = 0.47313872\n",
            "Iteration 414, loss = 0.47269525\n",
            "Iteration 415, loss = 0.47226805\n",
            "Iteration 416, loss = 0.47188656\n",
            "Iteration 417, loss = 0.47145228\n",
            "Iteration 418, loss = 0.47104904\n",
            "Iteration 419, loss = 0.47063760\n",
            "Iteration 420, loss = 0.47022581\n",
            "Iteration 421, loss = 0.46983273\n",
            "Iteration 422, loss = 0.46946331\n",
            "Iteration 423, loss = 0.46901962\n",
            "Iteration 424, loss = 0.46865666\n",
            "Iteration 425, loss = 0.46831118\n",
            "Iteration 426, loss = 0.46801795\n",
            "Iteration 427, loss = 0.46758887\n",
            "Iteration 428, loss = 0.46724242\n",
            "Iteration 429, loss = 0.46686647\n",
            "Iteration 430, loss = 0.46647848\n",
            "Iteration 431, loss = 0.46613043\n",
            "Iteration 432, loss = 0.46577823\n",
            "Iteration 433, loss = 0.46537863\n",
            "Iteration 434, loss = 0.46503460\n",
            "Iteration 435, loss = 0.46466282\n",
            "Iteration 436, loss = 0.46430576\n",
            "Iteration 437, loss = 0.46394129\n",
            "Iteration 438, loss = 0.46358894\n",
            "Iteration 439, loss = 0.46325287\n",
            "Iteration 440, loss = 0.46290771\n",
            "Iteration 441, loss = 0.46258788\n",
            "Iteration 442, loss = 0.46226245\n",
            "Iteration 443, loss = 0.46191306\n",
            "Iteration 444, loss = 0.46159329\n",
            "Iteration 445, loss = 0.46127566\n",
            "Iteration 446, loss = 0.46094947\n",
            "Iteration 447, loss = 0.46063835\n",
            "Iteration 448, loss = 0.46033928\n",
            "Iteration 449, loss = 0.46001736\n",
            "Iteration 450, loss = 0.45974252\n",
            "Iteration 451, loss = 0.45942306\n",
            "Iteration 452, loss = 0.45911469\n",
            "Iteration 453, loss = 0.45883411\n",
            "Iteration 454, loss = 0.45851065\n",
            "Iteration 455, loss = 0.45822774\n",
            "Iteration 456, loss = 0.45794183\n",
            "Iteration 457, loss = 0.45762600\n",
            "Iteration 458, loss = 0.45742149\n",
            "Iteration 459, loss = 0.45709642\n",
            "Iteration 460, loss = 0.45686214\n",
            "Iteration 461, loss = 0.45655704\n",
            "Iteration 462, loss = 0.45629894\n",
            "Iteration 463, loss = 0.45603142\n",
            "Iteration 464, loss = 0.45578584\n",
            "Iteration 465, loss = 0.45550098\n",
            "Iteration 466, loss = 0.45522624\n",
            "Iteration 467, loss = 0.45498236\n",
            "Iteration 468, loss = 0.45470592\n",
            "Iteration 469, loss = 0.45448938\n",
            "Iteration 470, loss = 0.45421660\n",
            "Iteration 471, loss = 0.45401555\n",
            "Iteration 472, loss = 0.45374387\n",
            "Iteration 473, loss = 0.45351849\n",
            "Iteration 474, loss = 0.45322741\n",
            "Iteration 475, loss = 0.45301080\n",
            "Iteration 476, loss = 0.45278171\n",
            "Iteration 477, loss = 0.45255046\n",
            "Iteration 478, loss = 0.45233931\n",
            "Iteration 479, loss = 0.45211361\n",
            "Iteration 480, loss = 0.45190800\n",
            "Iteration 481, loss = 0.45169800\n",
            "Iteration 482, loss = 0.45147326\n",
            "Iteration 483, loss = 0.45126754\n",
            "Iteration 484, loss = 0.45107092\n",
            "Iteration 485, loss = 0.45086166\n",
            "Iteration 486, loss = 0.45066466\n",
            "Iteration 487, loss = 0.45045869\n",
            "Iteration 488, loss = 0.45025932\n",
            "Iteration 489, loss = 0.45005053\n",
            "Iteration 490, loss = 0.44985980\n",
            "Iteration 491, loss = 0.44967932\n",
            "Iteration 492, loss = 0.44945447\n",
            "Iteration 493, loss = 0.44925092\n",
            "Iteration 494, loss = 0.44908792\n",
            "Iteration 495, loss = 0.44890150\n",
            "Iteration 496, loss = 0.44870480\n",
            "Iteration 497, loss = 0.44852704\n",
            "Iteration 498, loss = 0.44832631\n",
            "Iteration 499, loss = 0.44816279\n",
            "Iteration 500, loss = 0.44798647\n",
            "Iteration 501, loss = 0.44781054\n",
            "Iteration 502, loss = 0.44763133\n",
            "Iteration 503, loss = 0.44746377\n",
            "Iteration 504, loss = 0.44731465\n",
            "Iteration 505, loss = 0.44711276\n",
            "Iteration 506, loss = 0.44693425\n",
            "Iteration 507, loss = 0.44677282\n",
            "Iteration 508, loss = 0.44660229\n",
            "Iteration 509, loss = 0.44644707\n",
            "Iteration 510, loss = 0.44625512\n",
            "Iteration 511, loss = 0.44609228\n",
            "Iteration 512, loss = 0.44594415\n",
            "Iteration 513, loss = 0.44574569\n",
            "Iteration 514, loss = 0.44558560\n",
            "Iteration 515, loss = 0.44543188\n",
            "Iteration 516, loss = 0.44530735\n",
            "Iteration 517, loss = 0.44514278\n",
            "Iteration 518, loss = 0.44498814\n",
            "Iteration 519, loss = 0.44482672\n",
            "Iteration 520, loss = 0.44470224\n",
            "Iteration 521, loss = 0.44455141\n",
            "Iteration 522, loss = 0.44440516\n",
            "Iteration 523, loss = 0.44426758\n",
            "Iteration 524, loss = 0.44411199\n",
            "Iteration 525, loss = 0.44399643\n",
            "Iteration 526, loss = 0.44384195\n",
            "Iteration 527, loss = 0.44374387\n",
            "Iteration 528, loss = 0.44356460\n",
            "Iteration 529, loss = 0.44343959\n",
            "Iteration 530, loss = 0.44329924\n",
            "Iteration 531, loss = 0.44316235\n",
            "Iteration 532, loss = 0.44301187\n",
            "Iteration 533, loss = 0.44289537\n",
            "Iteration 534, loss = 0.44274251\n",
            "Iteration 535, loss = 0.44263405\n",
            "Iteration 536, loss = 0.44249290\n",
            "Iteration 537, loss = 0.44237214\n",
            "Iteration 538, loss = 0.44224254\n",
            "Iteration 539, loss = 0.44211555\n",
            "Iteration 540, loss = 0.44199365\n",
            "Iteration 541, loss = 0.44189996\n",
            "Iteration 542, loss = 0.44187757\n",
            "Iteration 543, loss = 0.44170658\n",
            "Iteration 544, loss = 0.44156212\n",
            "Iteration 545, loss = 0.44143135\n",
            "Iteration 546, loss = 0.44130686\n",
            "Iteration 547, loss = 0.44118191\n",
            "Iteration 548, loss = 0.44105267\n",
            "Iteration 549, loss = 0.44096574\n",
            "Iteration 550, loss = 0.44086042\n",
            "Iteration 551, loss = 0.44075906\n",
            "Iteration 552, loss = 0.44068326\n",
            "Iteration 553, loss = 0.44055941\n",
            "Iteration 554, loss = 0.44045119\n",
            "Iteration 555, loss = 0.44035550\n",
            "Iteration 556, loss = 0.44022600\n",
            "Iteration 557, loss = 0.44012015\n",
            "Iteration 558, loss = 0.44001516\n",
            "Iteration 559, loss = 0.43990530\n",
            "Iteration 560, loss = 0.43981028\n",
            "Iteration 561, loss = 0.43970588\n",
            "Iteration 562, loss = 0.43960413\n",
            "Iteration 563, loss = 0.43950832\n",
            "Iteration 564, loss = 0.43945094\n",
            "Iteration 565, loss = 0.43933176\n",
            "Iteration 566, loss = 0.43926363\n",
            "Iteration 567, loss = 0.43915593\n",
            "Iteration 568, loss = 0.43904759\n",
            "Iteration 569, loss = 0.43894906\n",
            "Iteration 570, loss = 0.43888922\n",
            "Iteration 571, loss = 0.43879938\n",
            "Iteration 572, loss = 0.43877172\n",
            "Iteration 573, loss = 0.43868215\n",
            "Iteration 574, loss = 0.43856208\n",
            "Iteration 575, loss = 0.43844789\n",
            "Iteration 576, loss = 0.43835330\n",
            "Iteration 577, loss = 0.43831755\n",
            "Iteration 578, loss = 0.43819960\n",
            "Iteration 579, loss = 0.43811732\n",
            "Iteration 580, loss = 0.43803600\n",
            "Iteration 581, loss = 0.43795992\n",
            "Iteration 582, loss = 0.43786884\n",
            "Iteration 583, loss = 0.43780425\n",
            "Iteration 584, loss = 0.43768663\n",
            "Iteration 585, loss = 0.43763490\n",
            "Iteration 586, loss = 0.43755416\n",
            "Iteration 587, loss = 0.43747796\n",
            "Iteration 588, loss = 0.43739695\n",
            "Iteration 589, loss = 0.43729862\n",
            "Iteration 590, loss = 0.43725281\n",
            "Iteration 591, loss = 0.43715699\n",
            "Iteration 592, loss = 0.43710787\n",
            "Iteration 593, loss = 0.43701951\n",
            "Iteration 594, loss = 0.43696684\n",
            "Iteration 595, loss = 0.43688068\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.71703974\n",
            "Iteration 2, loss = 0.71588542\n",
            "Iteration 3, loss = 0.71477858\n",
            "Iteration 4, loss = 0.71367209\n",
            "Iteration 5, loss = 0.71256737\n",
            "Iteration 6, loss = 0.71174364\n",
            "Iteration 7, loss = 0.71066989\n",
            "Iteration 8, loss = 0.70971830\n",
            "Iteration 9, loss = 0.70882839\n",
            "Iteration 10, loss = 0.70801967\n",
            "Iteration 11, loss = 0.70721694\n",
            "Iteration 12, loss = 0.70637381\n",
            "Iteration 13, loss = 0.70558334\n",
            "Iteration 14, loss = 0.70493271\n",
            "Iteration 15, loss = 0.70424014\n",
            "Iteration 16, loss = 0.70351352\n",
            "Iteration 17, loss = 0.70287053\n",
            "Iteration 18, loss = 0.70226272\n",
            "Iteration 19, loss = 0.70166645\n",
            "Iteration 20, loss = 0.70105734\n",
            "Iteration 21, loss = 0.70049027\n",
            "Iteration 22, loss = 0.69999710\n",
            "Iteration 23, loss = 0.69944714\n",
            "Iteration 24, loss = 0.69893722\n",
            "Iteration 25, loss = 0.69851433\n",
            "Iteration 26, loss = 0.69804269\n",
            "Iteration 27, loss = 0.69773397\n",
            "Iteration 28, loss = 0.69727951\n",
            "Iteration 29, loss = 0.69695062\n",
            "Iteration 30, loss = 0.69664737\n",
            "Iteration 31, loss = 0.69627290\n",
            "Iteration 32, loss = 0.69602746\n",
            "Iteration 33, loss = 0.69576770\n",
            "Iteration 34, loss = 0.69547389\n",
            "Iteration 35, loss = 0.69517507\n",
            "Iteration 36, loss = 0.69497490\n",
            "Iteration 37, loss = 0.69480229\n",
            "Iteration 38, loss = 0.69460306\n",
            "Iteration 39, loss = 0.69437476\n",
            "Iteration 40, loss = 0.69422099\n",
            "Iteration 41, loss = 0.69408024\n",
            "Iteration 42, loss = 0.69388533\n",
            "Iteration 43, loss = 0.69376341\n",
            "Iteration 44, loss = 0.69364641\n",
            "Iteration 45, loss = 0.69347552\n",
            "Iteration 46, loss = 0.69335214\n",
            "Iteration 47, loss = 0.69323969\n",
            "Iteration 48, loss = 0.69310702\n",
            "Iteration 49, loss = 0.69297950\n",
            "Iteration 50, loss = 0.69288835\n",
            "Iteration 51, loss = 0.69276451\n",
            "Iteration 52, loss = 0.69264328\n",
            "Iteration 53, loss = 0.69253373\n",
            "Iteration 54, loss = 0.69244000\n",
            "Iteration 55, loss = 0.69233164\n",
            "Iteration 56, loss = 0.69225382\n",
            "Iteration 57, loss = 0.69214145\n",
            "Iteration 58, loss = 0.69204058\n",
            "Iteration 59, loss = 0.69195680\n",
            "Iteration 60, loss = 0.69185516\n",
            "Iteration 61, loss = 0.69175938\n",
            "Iteration 62, loss = 0.69168164\n",
            "Iteration 63, loss = 0.69158573\n",
            "Iteration 64, loss = 0.69149376\n",
            "Iteration 65, loss = 0.69140361\n",
            "Iteration 66, loss = 0.69131042\n",
            "Iteration 67, loss = 0.69121807\n",
            "Iteration 68, loss = 0.69111091\n",
            "Iteration 69, loss = 0.69102385\n",
            "Iteration 70, loss = 0.69092932\n",
            "Iteration 71, loss = 0.69081469\n",
            "Iteration 72, loss = 0.69069802\n",
            "Iteration 73, loss = 0.69057089\n",
            "Iteration 74, loss = 0.69045839\n",
            "Iteration 75, loss = 0.69030770\n",
            "Iteration 76, loss = 0.69015758\n",
            "Iteration 77, loss = 0.69002934\n",
            "Iteration 78, loss = 0.68988288\n",
            "Iteration 79, loss = 0.68972993\n",
            "Iteration 80, loss = 0.68958228\n",
            "Iteration 81, loss = 0.68942173\n",
            "Iteration 82, loss = 0.68924911\n",
            "Iteration 83, loss = 0.68908467\n",
            "Iteration 84, loss = 0.68889899\n",
            "Iteration 85, loss = 0.68872586\n",
            "Iteration 86, loss = 0.68853289\n",
            "Iteration 87, loss = 0.68832329\n",
            "Iteration 88, loss = 0.68811889\n",
            "Iteration 89, loss = 0.68790803\n",
            "Iteration 90, loss = 0.68770314\n",
            "Iteration 91, loss = 0.68746314\n",
            "Iteration 92, loss = 0.68722590\n",
            "Iteration 93, loss = 0.68700188\n",
            "Iteration 94, loss = 0.68675560\n",
            "Iteration 95, loss = 0.68652381\n",
            "Iteration 96, loss = 0.68628186\n",
            "Iteration 97, loss = 0.68601817\n",
            "Iteration 98, loss = 0.68574370\n",
            "Iteration 99, loss = 0.68547789\n",
            "Iteration 100, loss = 0.68518666\n",
            "Iteration 101, loss = 0.68492982\n",
            "Iteration 102, loss = 0.68463029\n",
            "Iteration 103, loss = 0.68433051\n",
            "Iteration 104, loss = 0.68403742\n",
            "Iteration 105, loss = 0.68374974\n",
            "Iteration 106, loss = 0.68345826\n",
            "Iteration 107, loss = 0.68316090\n",
            "Iteration 108, loss = 0.68285572\n",
            "Iteration 109, loss = 0.68253134\n",
            "Iteration 110, loss = 0.68220063\n",
            "Iteration 111, loss = 0.68187229\n",
            "Iteration 112, loss = 0.68152442\n",
            "Iteration 113, loss = 0.68120536\n",
            "Iteration 114, loss = 0.68083296\n",
            "Iteration 115, loss = 0.68045696\n",
            "Iteration 116, loss = 0.68007300\n",
            "Iteration 117, loss = 0.67973556\n",
            "Iteration 118, loss = 0.67931017\n",
            "Iteration 119, loss = 0.67893332\n",
            "Iteration 120, loss = 0.67853975\n",
            "Iteration 121, loss = 0.67813153\n",
            "Iteration 122, loss = 0.67771615\n",
            "Iteration 123, loss = 0.67730027\n",
            "Iteration 124, loss = 0.67687590\n",
            "Iteration 125, loss = 0.67644925\n",
            "Iteration 126, loss = 0.67599807\n",
            "Iteration 127, loss = 0.67555927\n",
            "Iteration 128, loss = 0.67512793\n",
            "Iteration 129, loss = 0.67467620\n",
            "Iteration 130, loss = 0.67420435\n",
            "Iteration 131, loss = 0.67374673\n",
            "Iteration 132, loss = 0.67327271\n",
            "Iteration 133, loss = 0.67280885\n",
            "Iteration 134, loss = 0.67233831\n",
            "Iteration 135, loss = 0.67184265\n",
            "Iteration 136, loss = 0.67135787\n",
            "Iteration 137, loss = 0.67087231\n",
            "Iteration 138, loss = 0.67034644\n",
            "Iteration 139, loss = 0.66989313\n",
            "Iteration 140, loss = 0.66938089\n",
            "Iteration 141, loss = 0.66887752\n",
            "Iteration 142, loss = 0.66832524\n",
            "Iteration 143, loss = 0.66781158\n",
            "Iteration 144, loss = 0.66727153\n",
            "Iteration 145, loss = 0.66672988\n",
            "Iteration 146, loss = 0.66618402\n",
            "Iteration 147, loss = 0.66563075\n",
            "Iteration 148, loss = 0.66509886\n",
            "Iteration 149, loss = 0.66452254\n",
            "Iteration 150, loss = 0.66395337\n",
            "Iteration 151, loss = 0.66341874\n",
            "Iteration 152, loss = 0.66283403\n",
            "Iteration 153, loss = 0.66226937\n",
            "Iteration 154, loss = 0.66166007\n",
            "Iteration 155, loss = 0.66107738\n",
            "Iteration 156, loss = 0.66045348\n",
            "Iteration 157, loss = 0.65984675\n",
            "Iteration 158, loss = 0.65923159\n",
            "Iteration 159, loss = 0.65860592\n",
            "Iteration 160, loss = 0.65796308\n",
            "Iteration 161, loss = 0.65734752\n",
            "Iteration 162, loss = 0.65669180\n",
            "Iteration 163, loss = 0.65605137\n",
            "Iteration 164, loss = 0.65539016\n",
            "Iteration 165, loss = 0.65476725\n",
            "Iteration 166, loss = 0.65413788\n",
            "Iteration 167, loss = 0.65348944\n",
            "Iteration 168, loss = 0.65283977\n",
            "Iteration 169, loss = 0.65218949\n",
            "Iteration 170, loss = 0.65155358\n",
            "Iteration 171, loss = 0.65091533\n",
            "Iteration 172, loss = 0.65024336\n",
            "Iteration 173, loss = 0.64958410\n",
            "Iteration 174, loss = 0.64891324\n",
            "Iteration 175, loss = 0.64826627\n",
            "Iteration 176, loss = 0.64757555\n",
            "Iteration 177, loss = 0.64691700\n",
            "Iteration 178, loss = 0.64621707\n",
            "Iteration 179, loss = 0.64554144\n",
            "Iteration 180, loss = 0.64482324\n",
            "Iteration 181, loss = 0.64415058\n",
            "Iteration 182, loss = 0.64342951\n",
            "Iteration 183, loss = 0.64273280\n",
            "Iteration 184, loss = 0.64202603\n",
            "Iteration 185, loss = 0.64130732\n",
            "Iteration 186, loss = 0.64057907\n",
            "Iteration 187, loss = 0.63986084\n",
            "Iteration 188, loss = 0.63913217\n",
            "Iteration 189, loss = 0.63837644\n",
            "Iteration 190, loss = 0.63766000\n",
            "Iteration 191, loss = 0.63690317\n",
            "Iteration 192, loss = 0.63618114\n",
            "Iteration 193, loss = 0.63543220\n",
            "Iteration 194, loss = 0.63468640\n",
            "Iteration 195, loss = 0.63397762\n",
            "Iteration 196, loss = 0.63319581\n",
            "Iteration 197, loss = 0.63247025\n",
            "Iteration 198, loss = 0.63171562\n",
            "Iteration 199, loss = 0.63098772\n",
            "Iteration 200, loss = 0.63020625\n",
            "Iteration 201, loss = 0.62944560\n",
            "Iteration 202, loss = 0.62870656\n",
            "Iteration 203, loss = 0.62793217\n",
            "Iteration 204, loss = 0.62720227\n",
            "Iteration 205, loss = 0.62646498\n",
            "Iteration 206, loss = 0.62571727\n",
            "Iteration 207, loss = 0.62491372\n",
            "Iteration 208, loss = 0.62417605\n",
            "Iteration 209, loss = 0.62342111\n",
            "Iteration 210, loss = 0.62266025\n",
            "Iteration 211, loss = 0.62188562\n",
            "Iteration 212, loss = 0.62110405\n",
            "Iteration 213, loss = 0.62031904\n",
            "Iteration 214, loss = 0.61959141\n",
            "Iteration 215, loss = 0.61879276\n",
            "Iteration 216, loss = 0.61807755\n",
            "Iteration 217, loss = 0.61723205\n",
            "Iteration 218, loss = 0.61645462\n",
            "Iteration 219, loss = 0.61565977\n",
            "Iteration 220, loss = 0.61491636\n",
            "Iteration 221, loss = 0.61407273\n",
            "Iteration 222, loss = 0.61330445\n",
            "Iteration 223, loss = 0.61250715\n",
            "Iteration 224, loss = 0.61173968\n",
            "Iteration 225, loss = 0.61095991\n",
            "Iteration 226, loss = 0.61013441\n",
            "Iteration 227, loss = 0.60933168\n",
            "Iteration 228, loss = 0.60854876\n",
            "Iteration 229, loss = 0.60778132\n",
            "Iteration 230, loss = 0.60696272\n",
            "Iteration 231, loss = 0.60616050\n",
            "Iteration 232, loss = 0.60537763\n",
            "Iteration 233, loss = 0.60459858\n",
            "Iteration 234, loss = 0.60377776\n",
            "Iteration 235, loss = 0.60298427\n",
            "Iteration 236, loss = 0.60219497\n",
            "Iteration 237, loss = 0.60136712\n",
            "Iteration 238, loss = 0.60057259\n",
            "Iteration 239, loss = 0.59976985\n",
            "Iteration 240, loss = 0.59894038\n",
            "Iteration 241, loss = 0.59817100\n",
            "Iteration 242, loss = 0.59735337\n",
            "Iteration 243, loss = 0.59658031\n",
            "Iteration 244, loss = 0.59577016\n",
            "Iteration 245, loss = 0.59500620\n",
            "Iteration 246, loss = 0.59418719\n",
            "Iteration 247, loss = 0.59337000\n",
            "Iteration 248, loss = 0.59258117\n",
            "Iteration 249, loss = 0.59176177\n",
            "Iteration 250, loss = 0.59095840\n",
            "Iteration 251, loss = 0.59014038\n",
            "Iteration 252, loss = 0.58933058\n",
            "Iteration 253, loss = 0.58852157\n",
            "Iteration 254, loss = 0.58767788\n",
            "Iteration 255, loss = 0.58684523\n",
            "Iteration 256, loss = 0.58603151\n",
            "Iteration 257, loss = 0.58520548\n",
            "Iteration 258, loss = 0.58437734\n",
            "Iteration 259, loss = 0.58357675\n",
            "Iteration 260, loss = 0.58276364\n",
            "Iteration 261, loss = 0.58197457\n",
            "Iteration 262, loss = 0.58114576\n",
            "Iteration 263, loss = 0.58029244\n",
            "Iteration 264, loss = 0.57949673\n",
            "Iteration 265, loss = 0.57866215\n",
            "Iteration 266, loss = 0.57777314\n",
            "Iteration 267, loss = 0.57694993\n",
            "Iteration 268, loss = 0.57610846\n",
            "Iteration 269, loss = 0.57524433\n",
            "Iteration 270, loss = 0.57442435\n",
            "Iteration 271, loss = 0.57358349\n",
            "Iteration 272, loss = 0.57271747\n",
            "Iteration 273, loss = 0.57185642\n",
            "Iteration 274, loss = 0.57105491\n",
            "Iteration 275, loss = 0.57021957\n",
            "Iteration 276, loss = 0.56937715\n",
            "Iteration 277, loss = 0.56854616\n",
            "Iteration 278, loss = 0.56768101\n",
            "Iteration 279, loss = 0.56685633\n",
            "Iteration 280, loss = 0.56599232\n",
            "Iteration 281, loss = 0.56512218\n",
            "Iteration 282, loss = 0.56425740\n",
            "Iteration 283, loss = 0.56342500\n",
            "Iteration 284, loss = 0.56261314\n",
            "Iteration 285, loss = 0.56175428\n",
            "Iteration 286, loss = 0.56093552\n",
            "Iteration 287, loss = 0.56009744\n",
            "Iteration 288, loss = 0.55925046\n",
            "Iteration 289, loss = 0.55844619\n",
            "Iteration 290, loss = 0.55763745\n",
            "Iteration 291, loss = 0.55680836\n",
            "Iteration 292, loss = 0.55600958\n",
            "Iteration 293, loss = 0.55516148\n",
            "Iteration 294, loss = 0.55432427\n",
            "Iteration 295, loss = 0.55350698\n",
            "Iteration 296, loss = 0.55271508\n",
            "Iteration 297, loss = 0.55184390\n",
            "Iteration 298, loss = 0.55101746\n",
            "Iteration 299, loss = 0.55019926\n",
            "Iteration 300, loss = 0.54940876\n",
            "Iteration 301, loss = 0.54856475\n",
            "Iteration 302, loss = 0.54776762\n",
            "Iteration 303, loss = 0.54693875\n",
            "Iteration 304, loss = 0.54613425\n",
            "Iteration 305, loss = 0.54532010\n",
            "Iteration 306, loss = 0.54453408\n",
            "Iteration 307, loss = 0.54368355\n",
            "Iteration 308, loss = 0.54288744\n",
            "Iteration 309, loss = 0.54209351\n",
            "Iteration 310, loss = 0.54130166\n",
            "Iteration 311, loss = 0.54049683\n",
            "Iteration 312, loss = 0.53968995\n",
            "Iteration 313, loss = 0.53887148\n",
            "Iteration 314, loss = 0.53808435\n",
            "Iteration 315, loss = 0.53731267\n",
            "Iteration 316, loss = 0.53648914\n",
            "Iteration 317, loss = 0.53572269\n",
            "Iteration 318, loss = 0.53490516\n",
            "Iteration 319, loss = 0.53412233\n",
            "Iteration 320, loss = 0.53335124\n",
            "Iteration 321, loss = 0.53255328\n",
            "Iteration 322, loss = 0.53179961\n",
            "Iteration 323, loss = 0.53100228\n",
            "Iteration 324, loss = 0.53023200\n",
            "Iteration 325, loss = 0.52947249\n",
            "Iteration 326, loss = 0.52872345\n",
            "Iteration 327, loss = 0.52798138\n",
            "Iteration 328, loss = 0.52719094\n",
            "Iteration 329, loss = 0.52648140\n",
            "Iteration 330, loss = 0.52576093\n",
            "Iteration 331, loss = 0.52501455\n",
            "Iteration 332, loss = 0.52435510\n",
            "Iteration 333, loss = 0.52358918\n",
            "Iteration 334, loss = 0.52291607\n",
            "Iteration 335, loss = 0.52218258\n",
            "Iteration 336, loss = 0.52150273\n",
            "Iteration 337, loss = 0.52079401\n",
            "Iteration 338, loss = 0.52010839\n",
            "Iteration 339, loss = 0.51941901\n",
            "Iteration 340, loss = 0.51876068\n",
            "Iteration 341, loss = 0.51807388\n",
            "Iteration 342, loss = 0.51742065\n",
            "Iteration 343, loss = 0.51672537\n",
            "Iteration 344, loss = 0.51604055\n",
            "Iteration 345, loss = 0.51537349\n",
            "Iteration 346, loss = 0.51471203\n",
            "Iteration 347, loss = 0.51405762\n",
            "Iteration 348, loss = 0.51334410\n",
            "Iteration 349, loss = 0.51275512\n",
            "Iteration 350, loss = 0.51204768\n",
            "Iteration 351, loss = 0.51139909\n",
            "Iteration 352, loss = 0.51075440\n",
            "Iteration 353, loss = 0.51010297\n",
            "Iteration 354, loss = 0.50945688\n",
            "Iteration 355, loss = 0.50878546\n",
            "Iteration 356, loss = 0.50817504\n",
            "Iteration 357, loss = 0.50755668\n",
            "Iteration 358, loss = 0.50689598\n",
            "Iteration 359, loss = 0.50625998\n",
            "Iteration 360, loss = 0.50561814\n",
            "Iteration 361, loss = 0.50505753\n",
            "Iteration 362, loss = 0.50445511\n",
            "Iteration 363, loss = 0.50385117\n",
            "Iteration 364, loss = 0.50327477\n",
            "Iteration 365, loss = 0.50266959\n",
            "Iteration 366, loss = 0.50210029\n",
            "Iteration 367, loss = 0.50152314\n",
            "Iteration 368, loss = 0.50090027\n",
            "Iteration 369, loss = 0.50030658\n",
            "Iteration 370, loss = 0.49976260\n",
            "Iteration 371, loss = 0.49919588\n",
            "Iteration 372, loss = 0.49862054\n",
            "Iteration 373, loss = 0.49803447\n",
            "Iteration 374, loss = 0.49749360\n",
            "Iteration 375, loss = 0.49691544\n",
            "Iteration 376, loss = 0.49642432\n",
            "Iteration 377, loss = 0.49587969\n",
            "Iteration 378, loss = 0.49533198\n",
            "Iteration 379, loss = 0.49479318\n",
            "Iteration 380, loss = 0.49429188\n",
            "Iteration 381, loss = 0.49375616\n",
            "Iteration 382, loss = 0.49322570\n",
            "Iteration 383, loss = 0.49268216\n",
            "Iteration 384, loss = 0.49222698\n",
            "Iteration 385, loss = 0.49168260\n",
            "Iteration 386, loss = 0.49114408\n",
            "Iteration 387, loss = 0.49064380\n",
            "Iteration 388, loss = 0.49018611\n",
            "Iteration 389, loss = 0.48966305\n",
            "Iteration 390, loss = 0.48922831\n",
            "Iteration 391, loss = 0.48872522\n",
            "Iteration 392, loss = 0.48827424\n",
            "Iteration 393, loss = 0.48780769\n",
            "Iteration 394, loss = 0.48736892\n",
            "Iteration 395, loss = 0.48689664\n",
            "Iteration 396, loss = 0.48643922\n",
            "Iteration 397, loss = 0.48600623\n",
            "Iteration 398, loss = 0.48553348\n",
            "Iteration 399, loss = 0.48506300\n",
            "Iteration 400, loss = 0.48458374\n",
            "Iteration 401, loss = 0.48415536\n",
            "Iteration 402, loss = 0.48375844\n",
            "Iteration 403, loss = 0.48324482\n",
            "Iteration 404, loss = 0.48281684\n",
            "Iteration 405, loss = 0.48240212\n",
            "Iteration 406, loss = 0.48197274\n",
            "Iteration 407, loss = 0.48154159\n",
            "Iteration 408, loss = 0.48115223\n",
            "Iteration 409, loss = 0.48068835\n",
            "Iteration 410, loss = 0.48025113\n",
            "Iteration 411, loss = 0.47986604\n",
            "Iteration 412, loss = 0.47940742\n",
            "Iteration 413, loss = 0.47897703\n",
            "Iteration 414, loss = 0.47857855\n",
            "Iteration 415, loss = 0.47819088\n",
            "Iteration 416, loss = 0.47777874\n",
            "Iteration 417, loss = 0.47740298\n",
            "Iteration 418, loss = 0.47699294\n",
            "Iteration 419, loss = 0.47660581\n",
            "Iteration 420, loss = 0.47623058\n",
            "Iteration 421, loss = 0.47585474\n",
            "Iteration 422, loss = 0.47549119\n",
            "Iteration 423, loss = 0.47508144\n",
            "Iteration 424, loss = 0.47475477\n",
            "Iteration 425, loss = 0.47441940\n",
            "Iteration 426, loss = 0.47408165\n",
            "Iteration 427, loss = 0.47370742\n",
            "Iteration 428, loss = 0.47339437\n",
            "Iteration 429, loss = 0.47302159\n",
            "Iteration 430, loss = 0.47265976\n",
            "Iteration 431, loss = 0.47234855\n",
            "Iteration 432, loss = 0.47202602\n",
            "Iteration 433, loss = 0.47164629\n",
            "Iteration 434, loss = 0.47130700\n",
            "Iteration 435, loss = 0.47097141\n",
            "Iteration 436, loss = 0.47062764\n",
            "Iteration 437, loss = 0.47028398\n",
            "Iteration 438, loss = 0.46993239\n",
            "Iteration 439, loss = 0.46961506\n",
            "Iteration 440, loss = 0.46933120\n",
            "Iteration 441, loss = 0.46897696\n",
            "Iteration 442, loss = 0.46866901\n",
            "Iteration 443, loss = 0.46833802\n",
            "Iteration 444, loss = 0.46801931\n",
            "Iteration 445, loss = 0.46774175\n",
            "Iteration 446, loss = 0.46741003\n",
            "Iteration 447, loss = 0.46712410\n",
            "Iteration 448, loss = 0.46678853\n",
            "Iteration 449, loss = 0.46646952\n",
            "Iteration 450, loss = 0.46627425\n",
            "Iteration 451, loss = 0.46592761\n",
            "Iteration 452, loss = 0.46562123\n",
            "Iteration 453, loss = 0.46535507\n",
            "Iteration 454, loss = 0.46504904\n",
            "Iteration 455, loss = 0.46476931\n",
            "Iteration 456, loss = 0.46449817\n",
            "Iteration 457, loss = 0.46421983\n",
            "Iteration 458, loss = 0.46396557\n",
            "Iteration 459, loss = 0.46365539\n",
            "Iteration 460, loss = 0.46344273\n",
            "Iteration 461, loss = 0.46315262\n",
            "Iteration 462, loss = 0.46290350\n",
            "Iteration 463, loss = 0.46264736\n",
            "Iteration 464, loss = 0.46240175\n",
            "Iteration 465, loss = 0.46213475\n",
            "Iteration 466, loss = 0.46190348\n",
            "Iteration 467, loss = 0.46163057\n",
            "Iteration 468, loss = 0.46137024\n",
            "Iteration 469, loss = 0.46115268\n",
            "Iteration 470, loss = 0.46088346\n",
            "Iteration 471, loss = 0.46067727\n",
            "Iteration 472, loss = 0.46042300\n",
            "Iteration 473, loss = 0.46017204\n",
            "Iteration 474, loss = 0.45992760\n",
            "Iteration 475, loss = 0.45970127\n",
            "Iteration 476, loss = 0.45950193\n",
            "Iteration 477, loss = 0.45926893\n",
            "Iteration 478, loss = 0.45903931\n",
            "Iteration 479, loss = 0.45882237\n",
            "Iteration 480, loss = 0.45860055\n",
            "Iteration 481, loss = 0.45836077\n",
            "Iteration 482, loss = 0.45816879\n",
            "Iteration 483, loss = 0.45797648\n",
            "Iteration 484, loss = 0.45783721\n",
            "Iteration 485, loss = 0.45762795\n",
            "Iteration 486, loss = 0.45741714\n",
            "Iteration 487, loss = 0.45722081\n",
            "Iteration 488, loss = 0.45703024\n",
            "Iteration 489, loss = 0.45682875\n",
            "Iteration 490, loss = 0.45664184\n",
            "Iteration 491, loss = 0.45641769\n",
            "Iteration 492, loss = 0.45620204\n",
            "Iteration 493, loss = 0.45598366\n",
            "Iteration 494, loss = 0.45580349\n",
            "Iteration 495, loss = 0.45557780\n",
            "Iteration 496, loss = 0.45540520\n",
            "Iteration 497, loss = 0.45522027\n",
            "Iteration 498, loss = 0.45503575\n",
            "Iteration 499, loss = 0.45485637\n",
            "Iteration 500, loss = 0.45468558\n",
            "Iteration 501, loss = 0.45449338\n",
            "Iteration 502, loss = 0.45432500\n",
            "Iteration 503, loss = 0.45416174\n",
            "Iteration 504, loss = 0.45399214\n",
            "Iteration 505, loss = 0.45384383\n",
            "Iteration 506, loss = 0.45366895\n",
            "Iteration 507, loss = 0.45349031\n",
            "Iteration 508, loss = 0.45333190\n",
            "Iteration 509, loss = 0.45315219\n",
            "Iteration 510, loss = 0.45299479\n",
            "Iteration 511, loss = 0.45282071\n",
            "Iteration 512, loss = 0.45268915\n",
            "Iteration 513, loss = 0.45251146\n",
            "Iteration 514, loss = 0.45235587\n",
            "Iteration 515, loss = 0.45219698\n",
            "Iteration 516, loss = 0.45206861\n",
            "Iteration 517, loss = 0.45192484\n",
            "Iteration 518, loss = 0.45176603\n",
            "Iteration 519, loss = 0.45161595\n",
            "Iteration 520, loss = 0.45146429\n",
            "Iteration 521, loss = 0.45129320\n",
            "Iteration 522, loss = 0.45114601\n",
            "Iteration 523, loss = 0.45099686\n",
            "Iteration 524, loss = 0.45086007\n",
            "Iteration 525, loss = 0.45071606\n",
            "Iteration 526, loss = 0.45061968\n",
            "Iteration 527, loss = 0.45043270\n",
            "Iteration 528, loss = 0.45029823\n",
            "Iteration 529, loss = 0.45017497\n",
            "Iteration 530, loss = 0.45001978\n",
            "Iteration 531, loss = 0.44987261\n",
            "Iteration 532, loss = 0.44973393\n",
            "Iteration 533, loss = 0.44962666\n",
            "Iteration 534, loss = 0.44945324\n",
            "Iteration 535, loss = 0.44935495\n",
            "Iteration 536, loss = 0.44920644\n",
            "Iteration 537, loss = 0.44907750\n",
            "Iteration 538, loss = 0.44897200\n",
            "Iteration 539, loss = 0.44883681\n",
            "Iteration 540, loss = 0.44872348\n",
            "Iteration 541, loss = 0.44860359\n",
            "Iteration 542, loss = 0.44858339\n",
            "Iteration 543, loss = 0.44845700\n",
            "Iteration 544, loss = 0.44829714\n",
            "Iteration 545, loss = 0.44815089\n",
            "Iteration 546, loss = 0.44799108\n",
            "Iteration 547, loss = 0.44787818\n",
            "Iteration 548, loss = 0.44775391\n",
            "Iteration 549, loss = 0.44762774\n",
            "Iteration 550, loss = 0.44755571\n",
            "Iteration 551, loss = 0.44742948\n",
            "Iteration 552, loss = 0.44734102\n",
            "Iteration 553, loss = 0.44720657\n",
            "Iteration 554, loss = 0.44709285\n",
            "Iteration 555, loss = 0.44698968\n",
            "Iteration 556, loss = 0.44687034\n",
            "Iteration 557, loss = 0.44676531\n",
            "Iteration 558, loss = 0.44666169\n",
            "Iteration 559, loss = 0.44656235\n",
            "Iteration 560, loss = 0.44645127\n",
            "Iteration 561, loss = 0.44635923\n",
            "Iteration 562, loss = 0.44627552\n",
            "Iteration 563, loss = 0.44615855\n",
            "Iteration 564, loss = 0.44610583\n",
            "Iteration 565, loss = 0.44597519\n",
            "Iteration 566, loss = 0.44590234\n",
            "Iteration 567, loss = 0.44580707\n",
            "Iteration 568, loss = 0.44568760\n",
            "Iteration 569, loss = 0.44560059\n",
            "Iteration 570, loss = 0.44550526\n",
            "Iteration 571, loss = 0.44543106\n",
            "Iteration 572, loss = 0.44533115\n",
            "Iteration 573, loss = 0.44527171\n",
            "Iteration 574, loss = 0.44515715\n",
            "Iteration 575, loss = 0.44504413\n",
            "Iteration 576, loss = 0.44496083\n",
            "Iteration 577, loss = 0.44492602\n",
            "Iteration 578, loss = 0.44484363\n",
            "Iteration 579, loss = 0.44472914\n",
            "Iteration 580, loss = 0.44461585\n",
            "Iteration 581, loss = 0.44458929\n",
            "Iteration 582, loss = 0.44444287\n",
            "Iteration 583, loss = 0.44435564\n",
            "Iteration 584, loss = 0.44426174\n",
            "Iteration 585, loss = 0.44418356\n",
            "Iteration 586, loss = 0.44410350\n",
            "Iteration 587, loss = 0.44401341\n",
            "Iteration 588, loss = 0.44394408\n",
            "Iteration 589, loss = 0.44386764\n",
            "Iteration 590, loss = 0.44385524\n",
            "Iteration 591, loss = 0.44377562\n",
            "Iteration 592, loss = 0.44364410\n",
            "Iteration 593, loss = 0.44360331\n",
            "Iteration 594, loss = 0.44353284\n",
            "Iteration 595, loss = 0.44343996\n",
            "Iteration 596, loss = 0.44344304\n",
            "Iteration 597, loss = 0.44329918\n",
            "Iteration 598, loss = 0.44323881\n",
            "Iteration 599, loss = 0.44313116\n",
            "Iteration 600, loss = 0.44306866\n",
            "Iteration 601, loss = 0.44303160\n",
            "Iteration 602, loss = 0.44298957\n",
            "Iteration 603, loss = 0.44291913\n",
            "Iteration 604, loss = 0.44282025\n",
            "Iteration 605, loss = 0.44276137\n",
            "Iteration 606, loss = 0.44268545\n",
            "Iteration 607, loss = 0.44263652\n",
            "Iteration 608, loss = 0.44257782\n",
            "Iteration 609, loss = 0.44250490\n",
            "Iteration 610, loss = 0.44244509\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.71637661\n",
            "Iteration 2, loss = 0.71525936\n",
            "Iteration 3, loss = 0.71415897\n",
            "Iteration 4, loss = 0.71313144\n",
            "Iteration 5, loss = 0.71199846\n",
            "Iteration 6, loss = 0.71113812\n",
            "Iteration 7, loss = 0.71014127\n",
            "Iteration 8, loss = 0.70918612\n",
            "Iteration 9, loss = 0.70834956\n",
            "Iteration 10, loss = 0.70749947\n",
            "Iteration 11, loss = 0.70672606\n",
            "Iteration 12, loss = 0.70590015\n",
            "Iteration 13, loss = 0.70517982\n",
            "Iteration 14, loss = 0.70442197\n",
            "Iteration 15, loss = 0.70376690\n",
            "Iteration 16, loss = 0.70308132\n",
            "Iteration 17, loss = 0.70247998\n",
            "Iteration 18, loss = 0.70186933\n",
            "Iteration 19, loss = 0.70127853\n",
            "Iteration 20, loss = 0.70071136\n",
            "Iteration 21, loss = 0.70018869\n",
            "Iteration 22, loss = 0.69967939\n",
            "Iteration 23, loss = 0.69916429\n",
            "Iteration 24, loss = 0.69865833\n",
            "Iteration 25, loss = 0.69826048\n",
            "Iteration 26, loss = 0.69781063\n",
            "Iteration 27, loss = 0.69752975\n",
            "Iteration 28, loss = 0.69709022\n",
            "Iteration 29, loss = 0.69678907\n",
            "Iteration 30, loss = 0.69651861\n",
            "Iteration 31, loss = 0.69613202\n",
            "Iteration 32, loss = 0.69588240\n",
            "Iteration 33, loss = 0.69563310\n",
            "Iteration 34, loss = 0.69540632\n",
            "Iteration 35, loss = 0.69508047\n",
            "Iteration 36, loss = 0.69485925\n",
            "Iteration 37, loss = 0.69469843\n",
            "Iteration 38, loss = 0.69452017\n",
            "Iteration 39, loss = 0.69428261\n",
            "Iteration 40, loss = 0.69415627\n",
            "Iteration 41, loss = 0.69398247\n",
            "Iteration 42, loss = 0.69382825\n",
            "Iteration 43, loss = 0.69370580\n",
            "Iteration 44, loss = 0.69361459\n",
            "Iteration 45, loss = 0.69342142\n",
            "Iteration 46, loss = 0.69329187\n",
            "Iteration 47, loss = 0.69314335\n",
            "Iteration 48, loss = 0.69303929\n",
            "Iteration 49, loss = 0.69293207\n",
            "Iteration 50, loss = 0.69280278\n",
            "Iteration 51, loss = 0.69270929\n",
            "Iteration 52, loss = 0.69257624\n",
            "Iteration 53, loss = 0.69249431\n",
            "Iteration 54, loss = 0.69244301\n",
            "Iteration 55, loss = 0.69231403\n",
            "Iteration 56, loss = 0.69223098\n",
            "Iteration 57, loss = 0.69214366\n",
            "Iteration 58, loss = 0.69206594\n",
            "Iteration 59, loss = 0.69197781\n",
            "Iteration 60, loss = 0.69189152\n",
            "Iteration 61, loss = 0.69180394\n",
            "Iteration 62, loss = 0.69174704\n",
            "Iteration 63, loss = 0.69165286\n",
            "Iteration 64, loss = 0.69157287\n",
            "Iteration 65, loss = 0.69149153\n",
            "Iteration 66, loss = 0.69140438\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.71633851\n",
            "Iteration 2, loss = 0.71527056\n",
            "Iteration 3, loss = 0.71423059\n",
            "Iteration 4, loss = 0.71312904\n",
            "Iteration 5, loss = 0.71213935\n",
            "Iteration 6, loss = 0.71118687\n",
            "Iteration 7, loss = 0.71024315\n",
            "Iteration 8, loss = 0.70924614\n",
            "Iteration 9, loss = 0.70841515\n",
            "Iteration 10, loss = 0.70756432\n",
            "Iteration 11, loss = 0.70674513\n",
            "Iteration 12, loss = 0.70601579\n",
            "Iteration 13, loss = 0.70524084\n",
            "Iteration 14, loss = 0.70447708\n",
            "Iteration 15, loss = 0.70384692\n",
            "Iteration 16, loss = 0.70313558\n",
            "Iteration 17, loss = 0.70249838\n",
            "Iteration 18, loss = 0.70190971\n",
            "Iteration 19, loss = 0.70123939\n",
            "Iteration 20, loss = 0.70068754\n",
            "Iteration 21, loss = 0.70014997\n",
            "Iteration 22, loss = 0.69963863\n",
            "Iteration 23, loss = 0.69917193\n",
            "Iteration 24, loss = 0.69866799\n",
            "Iteration 25, loss = 0.69827541\n",
            "Iteration 26, loss = 0.69791729\n",
            "Iteration 27, loss = 0.69756122\n",
            "Iteration 28, loss = 0.69718694\n",
            "Iteration 29, loss = 0.69680417\n",
            "Iteration 30, loss = 0.69655919\n",
            "Iteration 31, loss = 0.69615343\n",
            "Iteration 32, loss = 0.69588704\n",
            "Iteration 33, loss = 0.69559949\n",
            "Iteration 34, loss = 0.69537344\n",
            "Iteration 35, loss = 0.69502795\n",
            "Iteration 36, loss = 0.69479819\n",
            "Iteration 37, loss = 0.69462570\n",
            "Iteration 38, loss = 0.69437955\n",
            "Iteration 39, loss = 0.69417017\n",
            "Iteration 40, loss = 0.69403629\n",
            "Iteration 41, loss = 0.69379458\n",
            "Iteration 42, loss = 0.69364034\n",
            "Iteration 43, loss = 0.69351159\n",
            "Iteration 44, loss = 0.69340368\n",
            "Iteration 45, loss = 0.69325122\n",
            "Iteration 46, loss = 0.69309853\n",
            "Iteration 47, loss = 0.69297274\n",
            "Iteration 48, loss = 0.69286744\n",
            "Iteration 49, loss = 0.69274640\n",
            "Iteration 50, loss = 0.69263884\n",
            "Iteration 51, loss = 0.69258255\n",
            "Iteration 52, loss = 0.69243774\n",
            "Iteration 53, loss = 0.69231933\n",
            "Iteration 54, loss = 0.69224417\n",
            "Iteration 55, loss = 0.69211999\n",
            "Iteration 56, loss = 0.69203603\n",
            "Iteration 57, loss = 0.69192206\n",
            "Iteration 58, loss = 0.69184715\n",
            "Iteration 59, loss = 0.69174119\n",
            "Iteration 60, loss = 0.69163508\n",
            "Iteration 61, loss = 0.69153871\n",
            "Iteration 62, loss = 0.69146830\n",
            "Iteration 63, loss = 0.69134317\n",
            "Iteration 64, loss = 0.69124418\n",
            "Iteration 65, loss = 0.69114007\n",
            "Iteration 66, loss = 0.69104140\n",
            "Iteration 67, loss = 0.69093161\n",
            "Iteration 68, loss = 0.69081233\n",
            "Iteration 69, loss = 0.69071347\n",
            "Iteration 70, loss = 0.69060610\n",
            "Iteration 71, loss = 0.69047875\n",
            "Iteration 72, loss = 0.69035125\n",
            "Iteration 73, loss = 0.69021605\n",
            "Iteration 74, loss = 0.69008564\n",
            "Iteration 75, loss = 0.68992693\n",
            "Iteration 76, loss = 0.68977250\n",
            "Iteration 77, loss = 0.68962505\n",
            "Iteration 78, loss = 0.68945896\n",
            "Iteration 79, loss = 0.68930835\n",
            "Iteration 80, loss = 0.68912568\n",
            "Iteration 81, loss = 0.68894796\n",
            "Iteration 82, loss = 0.68875471\n",
            "Iteration 83, loss = 0.68858616\n",
            "Iteration 84, loss = 0.68835693\n",
            "Iteration 85, loss = 0.68816689\n",
            "Iteration 86, loss = 0.68795134\n",
            "Iteration 87, loss = 0.68772737\n",
            "Iteration 88, loss = 0.68751370\n",
            "Iteration 89, loss = 0.68727393\n",
            "Iteration 90, loss = 0.68704531\n",
            "Iteration 91, loss = 0.68680008\n",
            "Iteration 92, loss = 0.68652092\n",
            "Iteration 93, loss = 0.68627805\n",
            "Iteration 94, loss = 0.68600147\n",
            "Iteration 95, loss = 0.68572967\n",
            "Iteration 96, loss = 0.68546203\n",
            "Iteration 97, loss = 0.68517022\n",
            "Iteration 98, loss = 0.68487802\n",
            "Iteration 99, loss = 0.68458453\n",
            "Iteration 100, loss = 0.68428461\n",
            "Iteration 101, loss = 0.68398535\n",
            "Iteration 102, loss = 0.68366478\n",
            "Iteration 103, loss = 0.68333541\n",
            "Iteration 104, loss = 0.68301172\n",
            "Iteration 105, loss = 0.68268785\n",
            "Iteration 106, loss = 0.68235433\n",
            "Iteration 107, loss = 0.68201189\n",
            "Iteration 108, loss = 0.68166072\n",
            "Iteration 109, loss = 0.68131089\n",
            "Iteration 110, loss = 0.68094970\n",
            "Iteration 111, loss = 0.68058515\n",
            "Iteration 112, loss = 0.68020857\n",
            "Iteration 113, loss = 0.67983788\n",
            "Iteration 114, loss = 0.67943165\n",
            "Iteration 115, loss = 0.67903417\n",
            "Iteration 116, loss = 0.67859422\n",
            "Iteration 117, loss = 0.67823642\n",
            "Iteration 118, loss = 0.67779588\n",
            "Iteration 119, loss = 0.67733614\n",
            "Iteration 120, loss = 0.67688533\n",
            "Iteration 121, loss = 0.67645259\n",
            "Iteration 122, loss = 0.67599447\n",
            "Iteration 123, loss = 0.67554380\n",
            "Iteration 124, loss = 0.67505629\n",
            "Iteration 125, loss = 0.67460804\n",
            "Iteration 126, loss = 0.67410819\n",
            "Iteration 127, loss = 0.67363655\n",
            "Iteration 128, loss = 0.67314260\n",
            "Iteration 129, loss = 0.67267092\n",
            "Iteration 130, loss = 0.67217973\n",
            "Iteration 131, loss = 0.67168577\n",
            "Iteration 132, loss = 0.67118218\n",
            "Iteration 133, loss = 0.67066646\n",
            "Iteration 134, loss = 0.67014245\n",
            "Iteration 135, loss = 0.66961903\n",
            "Iteration 136, loss = 0.66909768\n",
            "Iteration 137, loss = 0.66857232\n",
            "Iteration 138, loss = 0.66801412\n",
            "Iteration 139, loss = 0.66747383\n",
            "Iteration 140, loss = 0.66694335\n",
            "Iteration 141, loss = 0.66638437\n",
            "Iteration 142, loss = 0.66581466\n",
            "Iteration 143, loss = 0.66527162\n",
            "Iteration 144, loss = 0.66466616\n",
            "Iteration 145, loss = 0.66408006\n",
            "Iteration 146, loss = 0.66348284\n",
            "Iteration 147, loss = 0.66287665\n",
            "Iteration 148, loss = 0.66228411\n",
            "Iteration 149, loss = 0.66167209\n",
            "Iteration 150, loss = 0.66105868\n",
            "Iteration 151, loss = 0.66048183\n",
            "Iteration 152, loss = 0.65983584\n",
            "Iteration 153, loss = 0.65924478\n",
            "Iteration 154, loss = 0.65858487\n",
            "Iteration 155, loss = 0.65796033\n",
            "Iteration 156, loss = 0.65728628\n",
            "Iteration 157, loss = 0.65663969\n",
            "Iteration 158, loss = 0.65596174\n",
            "Iteration 159, loss = 0.65531461\n",
            "Iteration 160, loss = 0.65466034\n",
            "Iteration 161, loss = 0.65395165\n",
            "Iteration 162, loss = 0.65328036\n",
            "Iteration 163, loss = 0.65257896\n",
            "Iteration 164, loss = 0.65189100\n",
            "Iteration 165, loss = 0.65122694\n",
            "Iteration 166, loss = 0.65050708\n",
            "Iteration 167, loss = 0.64978061\n",
            "Iteration 168, loss = 0.64908441\n",
            "Iteration 169, loss = 0.64836566\n",
            "Iteration 170, loss = 0.64768133\n",
            "Iteration 171, loss = 0.64699541\n",
            "Iteration 172, loss = 0.64629047\n",
            "Iteration 173, loss = 0.64559681\n",
            "Iteration 174, loss = 0.64489685\n",
            "Iteration 175, loss = 0.64419317\n",
            "Iteration 176, loss = 0.64348473\n",
            "Iteration 177, loss = 0.64275600\n",
            "Iteration 178, loss = 0.64202220\n",
            "Iteration 179, loss = 0.64129805\n",
            "Iteration 180, loss = 0.64054376\n",
            "Iteration 181, loss = 0.63980261\n",
            "Iteration 182, loss = 0.63909638\n",
            "Iteration 183, loss = 0.63832177\n",
            "Iteration 184, loss = 0.63759187\n",
            "Iteration 185, loss = 0.63682889\n",
            "Iteration 186, loss = 0.63611055\n",
            "Iteration 187, loss = 0.63536689\n",
            "Iteration 188, loss = 0.63457471\n",
            "Iteration 189, loss = 0.63381797\n",
            "Iteration 190, loss = 0.63307050\n",
            "Iteration 191, loss = 0.63227714\n",
            "Iteration 192, loss = 0.63154057\n",
            "Iteration 193, loss = 0.63073118\n",
            "Iteration 194, loss = 0.62992309\n",
            "Iteration 195, loss = 0.62918208\n",
            "Iteration 196, loss = 0.62833826\n",
            "Iteration 197, loss = 0.62758051\n",
            "Iteration 198, loss = 0.62678217\n",
            "Iteration 199, loss = 0.62601159\n",
            "Iteration 200, loss = 0.62521324\n",
            "Iteration 201, loss = 0.62446901\n",
            "Iteration 202, loss = 0.62369415\n",
            "Iteration 203, loss = 0.62289279\n",
            "Iteration 204, loss = 0.62213547\n",
            "Iteration 205, loss = 0.62138720\n",
            "Iteration 206, loss = 0.62059445\n",
            "Iteration 207, loss = 0.61980896\n",
            "Iteration 208, loss = 0.61903195\n",
            "Iteration 209, loss = 0.61828930\n",
            "Iteration 210, loss = 0.61748224\n",
            "Iteration 211, loss = 0.61669499\n",
            "Iteration 212, loss = 0.61587767\n",
            "Iteration 213, loss = 0.61507194\n",
            "Iteration 214, loss = 0.61429470\n",
            "Iteration 215, loss = 0.61345524\n",
            "Iteration 216, loss = 0.61269291\n",
            "Iteration 217, loss = 0.61187095\n",
            "Iteration 218, loss = 0.61104068\n",
            "Iteration 219, loss = 0.61019950\n",
            "Iteration 220, loss = 0.60944060\n",
            "Iteration 221, loss = 0.60854482\n",
            "Iteration 222, loss = 0.60775176\n",
            "Iteration 223, loss = 0.60693023\n",
            "Iteration 224, loss = 0.60609481\n",
            "Iteration 225, loss = 0.60529588\n",
            "Iteration 226, loss = 0.60443523\n",
            "Iteration 227, loss = 0.60362445\n",
            "Iteration 228, loss = 0.60281302\n",
            "Iteration 229, loss = 0.60200914\n",
            "Iteration 230, loss = 0.60120032\n",
            "Iteration 231, loss = 0.60036797\n",
            "Iteration 232, loss = 0.59954220\n",
            "Iteration 233, loss = 0.59873698\n",
            "Iteration 234, loss = 0.59792993\n",
            "Iteration 235, loss = 0.59708538\n",
            "Iteration 236, loss = 0.59624315\n",
            "Iteration 237, loss = 0.59543469\n",
            "Iteration 238, loss = 0.59461948\n",
            "Iteration 239, loss = 0.59375842\n",
            "Iteration 240, loss = 0.59293663\n",
            "Iteration 241, loss = 0.59207440\n",
            "Iteration 242, loss = 0.59126196\n",
            "Iteration 243, loss = 0.59043119\n",
            "Iteration 244, loss = 0.58953712\n",
            "Iteration 245, loss = 0.58876140\n",
            "Iteration 246, loss = 0.58788202\n",
            "Iteration 247, loss = 0.58703553\n",
            "Iteration 248, loss = 0.58617482\n",
            "Iteration 249, loss = 0.58531415\n",
            "Iteration 250, loss = 0.58444811\n",
            "Iteration 251, loss = 0.58363700\n",
            "Iteration 252, loss = 0.58275648\n",
            "Iteration 253, loss = 0.58192981\n",
            "Iteration 254, loss = 0.58106943\n",
            "Iteration 255, loss = 0.58022313\n",
            "Iteration 256, loss = 0.57940104\n",
            "Iteration 257, loss = 0.57852361\n",
            "Iteration 258, loss = 0.57766812\n",
            "Iteration 259, loss = 0.57682303\n",
            "Iteration 260, loss = 0.57600512\n",
            "Iteration 261, loss = 0.57513854\n",
            "Iteration 262, loss = 0.57428774\n",
            "Iteration 263, loss = 0.57340944\n",
            "Iteration 264, loss = 0.57254597\n",
            "Iteration 265, loss = 0.57167662\n",
            "Iteration 266, loss = 0.57078834\n",
            "Iteration 267, loss = 0.56991416\n",
            "Iteration 268, loss = 0.56903765\n",
            "Iteration 269, loss = 0.56814611\n",
            "Iteration 270, loss = 0.56729727\n",
            "Iteration 271, loss = 0.56645018\n",
            "Iteration 272, loss = 0.56554657\n",
            "Iteration 273, loss = 0.56467526\n",
            "Iteration 274, loss = 0.56377000\n",
            "Iteration 275, loss = 0.56289582\n",
            "Iteration 276, loss = 0.56202272\n",
            "Iteration 277, loss = 0.56115780\n",
            "Iteration 278, loss = 0.56022406\n",
            "Iteration 279, loss = 0.55934787\n",
            "Iteration 280, loss = 0.55849151\n",
            "Iteration 281, loss = 0.55763633\n",
            "Iteration 282, loss = 0.55676741\n",
            "Iteration 283, loss = 0.55591401\n",
            "Iteration 284, loss = 0.55511427\n",
            "Iteration 285, loss = 0.55425169\n",
            "Iteration 286, loss = 0.55340345\n",
            "Iteration 287, loss = 0.55249361\n",
            "Iteration 288, loss = 0.55167320\n",
            "Iteration 289, loss = 0.55080111\n",
            "Iteration 290, loss = 0.54999837\n",
            "Iteration 291, loss = 0.54913531\n",
            "Iteration 292, loss = 0.54830255\n",
            "Iteration 293, loss = 0.54745767\n",
            "Iteration 294, loss = 0.54658889\n",
            "Iteration 295, loss = 0.54575480\n",
            "Iteration 296, loss = 0.54490097\n",
            "Iteration 297, loss = 0.54406155\n",
            "Iteration 298, loss = 0.54321565\n",
            "Iteration 299, loss = 0.54237136\n",
            "Iteration 300, loss = 0.54156438\n",
            "Iteration 301, loss = 0.54073122\n",
            "Iteration 302, loss = 0.53991558\n",
            "Iteration 303, loss = 0.53910337\n",
            "Iteration 304, loss = 0.53828608\n",
            "Iteration 305, loss = 0.53748239\n",
            "Iteration 306, loss = 0.53664557\n",
            "Iteration 307, loss = 0.53582138\n",
            "Iteration 308, loss = 0.53501461\n",
            "Iteration 309, loss = 0.53422809\n",
            "Iteration 310, loss = 0.53342508\n",
            "Iteration 311, loss = 0.53257875\n",
            "Iteration 312, loss = 0.53177593\n",
            "Iteration 313, loss = 0.53097117\n",
            "Iteration 314, loss = 0.53018492\n",
            "Iteration 315, loss = 0.52936632\n",
            "Iteration 316, loss = 0.52855277\n",
            "Iteration 317, loss = 0.52778396\n",
            "Iteration 318, loss = 0.52697223\n",
            "Iteration 319, loss = 0.52619183\n",
            "Iteration 320, loss = 0.52541713\n",
            "Iteration 321, loss = 0.52462833\n",
            "Iteration 322, loss = 0.52386001\n",
            "Iteration 323, loss = 0.52306059\n",
            "Iteration 324, loss = 0.52231742\n",
            "Iteration 325, loss = 0.52156822\n",
            "Iteration 326, loss = 0.52078610\n",
            "Iteration 327, loss = 0.52002556\n",
            "Iteration 328, loss = 0.51928240\n",
            "Iteration 329, loss = 0.51856447\n",
            "Iteration 330, loss = 0.51779692\n",
            "Iteration 331, loss = 0.51712040\n",
            "Iteration 332, loss = 0.51635937\n",
            "Iteration 333, loss = 0.51562672\n",
            "Iteration 334, loss = 0.51492369\n",
            "Iteration 335, loss = 0.51422300\n",
            "Iteration 336, loss = 0.51355184\n",
            "Iteration 337, loss = 0.51281686\n",
            "Iteration 338, loss = 0.51216393\n",
            "Iteration 339, loss = 0.51144909\n",
            "Iteration 340, loss = 0.51075577\n",
            "Iteration 341, loss = 0.51006902\n",
            "Iteration 342, loss = 0.50937719\n",
            "Iteration 343, loss = 0.50870029\n",
            "Iteration 344, loss = 0.50798743\n",
            "Iteration 345, loss = 0.50732062\n",
            "Iteration 346, loss = 0.50664136\n",
            "Iteration 347, loss = 0.50596144\n",
            "Iteration 348, loss = 0.50527281\n",
            "Iteration 349, loss = 0.50459240\n",
            "Iteration 350, loss = 0.50390686\n",
            "Iteration 351, loss = 0.50327986\n",
            "Iteration 352, loss = 0.50261261\n",
            "Iteration 353, loss = 0.50196310\n",
            "Iteration 354, loss = 0.50133082\n",
            "Iteration 355, loss = 0.50068890\n",
            "Iteration 356, loss = 0.50012676\n",
            "Iteration 357, loss = 0.49948985\n",
            "Iteration 358, loss = 0.49887559\n",
            "Iteration 359, loss = 0.49826736\n",
            "Iteration 360, loss = 0.49764745\n",
            "Iteration 361, loss = 0.49706679\n",
            "Iteration 362, loss = 0.49649669\n",
            "Iteration 363, loss = 0.49592029\n",
            "Iteration 364, loss = 0.49531788\n",
            "Iteration 365, loss = 0.49477425\n",
            "Iteration 366, loss = 0.49423418\n",
            "Iteration 367, loss = 0.49363399\n",
            "Iteration 368, loss = 0.49308978\n",
            "Iteration 369, loss = 0.49248024\n",
            "Iteration 370, loss = 0.49191555\n",
            "Iteration 371, loss = 0.49132381\n",
            "Iteration 372, loss = 0.49081459\n",
            "Iteration 373, loss = 0.49022917\n",
            "Iteration 374, loss = 0.48976447\n",
            "Iteration 375, loss = 0.48921859\n",
            "Iteration 376, loss = 0.48872838\n",
            "Iteration 377, loss = 0.48823432\n",
            "Iteration 378, loss = 0.48772199\n",
            "Iteration 379, loss = 0.48719518\n",
            "Iteration 380, loss = 0.48667381\n",
            "Iteration 381, loss = 0.48617317\n",
            "Iteration 382, loss = 0.48572536\n",
            "Iteration 383, loss = 0.48515184\n",
            "Iteration 384, loss = 0.48468383\n",
            "Iteration 385, loss = 0.48425992\n",
            "Iteration 386, loss = 0.48363588\n",
            "Iteration 387, loss = 0.48317546\n",
            "Iteration 388, loss = 0.48273213\n",
            "Iteration 389, loss = 0.48222065\n",
            "Iteration 390, loss = 0.48177039\n",
            "Iteration 391, loss = 0.48130486\n",
            "Iteration 392, loss = 0.48086078\n",
            "Iteration 393, loss = 0.48038894\n",
            "Iteration 394, loss = 0.47996188\n",
            "Iteration 395, loss = 0.47950809\n",
            "Iteration 396, loss = 0.47907884\n",
            "Iteration 397, loss = 0.47862875\n",
            "Iteration 398, loss = 0.47819389\n",
            "Iteration 399, loss = 0.47776757\n",
            "Iteration 400, loss = 0.47730695\n",
            "Iteration 401, loss = 0.47690521\n",
            "Iteration 402, loss = 0.47649618\n",
            "Iteration 403, loss = 0.47604665\n",
            "Iteration 404, loss = 0.47565498\n",
            "Iteration 405, loss = 0.47524533\n",
            "Iteration 406, loss = 0.47483152\n",
            "Iteration 407, loss = 0.47442333\n",
            "Iteration 408, loss = 0.47396718\n",
            "Iteration 409, loss = 0.47359957\n",
            "Iteration 410, loss = 0.47318179\n",
            "Iteration 411, loss = 0.47279577\n",
            "Iteration 412, loss = 0.47238688\n",
            "Iteration 413, loss = 0.47198588\n",
            "Iteration 414, loss = 0.47159663\n",
            "Iteration 415, loss = 0.47125723\n",
            "Iteration 416, loss = 0.47090205\n",
            "Iteration 417, loss = 0.47051215\n",
            "Iteration 418, loss = 0.47013749\n",
            "Iteration 419, loss = 0.46982214\n",
            "Iteration 420, loss = 0.46943317\n",
            "Iteration 421, loss = 0.46915742\n",
            "Iteration 422, loss = 0.46877993\n",
            "Iteration 423, loss = 0.46842045\n",
            "Iteration 424, loss = 0.46813501\n",
            "Iteration 425, loss = 0.46778668\n",
            "Iteration 426, loss = 0.46746553\n",
            "Iteration 427, loss = 0.46711111\n",
            "Iteration 428, loss = 0.46681741\n",
            "Iteration 429, loss = 0.46650701\n",
            "Iteration 430, loss = 0.46617212\n",
            "Iteration 431, loss = 0.46593226\n",
            "Iteration 432, loss = 0.46561199\n",
            "Iteration 433, loss = 0.46528326\n",
            "Iteration 434, loss = 0.46498124\n",
            "Iteration 435, loss = 0.46464234\n",
            "Iteration 436, loss = 0.46436694\n",
            "Iteration 437, loss = 0.46404180\n",
            "Iteration 438, loss = 0.46374140\n",
            "Iteration 439, loss = 0.46343441\n",
            "Iteration 440, loss = 0.46312376\n",
            "Iteration 441, loss = 0.46285417\n",
            "Iteration 442, loss = 0.46253715\n",
            "Iteration 443, loss = 0.46227342\n",
            "Iteration 444, loss = 0.46197638\n",
            "Iteration 445, loss = 0.46169538\n",
            "Iteration 446, loss = 0.46139866\n",
            "Iteration 447, loss = 0.46110595\n",
            "Iteration 448, loss = 0.46084025\n",
            "Iteration 449, loss = 0.46055906\n",
            "Iteration 450, loss = 0.46032902\n",
            "Iteration 451, loss = 0.46005385\n",
            "Iteration 452, loss = 0.45976772\n",
            "Iteration 453, loss = 0.45952393\n",
            "Iteration 454, loss = 0.45926323\n",
            "Iteration 455, loss = 0.45901391\n",
            "Iteration 456, loss = 0.45877589\n",
            "Iteration 457, loss = 0.45850199\n",
            "Iteration 458, loss = 0.45828648\n",
            "Iteration 459, loss = 0.45800346\n",
            "Iteration 460, loss = 0.45779345\n",
            "Iteration 461, loss = 0.45756576\n",
            "Iteration 462, loss = 0.45732539\n",
            "Iteration 463, loss = 0.45713158\n",
            "Iteration 464, loss = 0.45689090\n",
            "Iteration 465, loss = 0.45666745\n",
            "Iteration 466, loss = 0.45646799\n",
            "Iteration 467, loss = 0.45621980\n",
            "Iteration 468, loss = 0.45597690\n",
            "Iteration 469, loss = 0.45578657\n",
            "Iteration 470, loss = 0.45559033\n",
            "Iteration 471, loss = 0.45534708\n",
            "Iteration 472, loss = 0.45514219\n",
            "Iteration 473, loss = 0.45495692\n",
            "Iteration 474, loss = 0.45470797\n",
            "Iteration 475, loss = 0.45452677\n",
            "Iteration 476, loss = 0.45433950\n",
            "Iteration 477, loss = 0.45415479\n",
            "Iteration 478, loss = 0.45397135\n",
            "Iteration 479, loss = 0.45378143\n",
            "Iteration 480, loss = 0.45360286\n",
            "Iteration 481, loss = 0.45341302\n",
            "Iteration 482, loss = 0.45325219\n",
            "Iteration 483, loss = 0.45306001\n",
            "Iteration 484, loss = 0.45287077\n",
            "Iteration 485, loss = 0.45269859\n",
            "Iteration 486, loss = 0.45257487\n",
            "Iteration 487, loss = 0.45236193\n",
            "Iteration 488, loss = 0.45219860\n",
            "Iteration 489, loss = 0.45205172\n",
            "Iteration 490, loss = 0.45188648\n",
            "Iteration 491, loss = 0.45171026\n",
            "Iteration 492, loss = 0.45155963\n",
            "Iteration 493, loss = 0.45136484\n",
            "Iteration 494, loss = 0.45125023\n",
            "Iteration 495, loss = 0.45104584\n",
            "Iteration 496, loss = 0.45085971\n",
            "Iteration 497, loss = 0.45069608\n",
            "Iteration 498, loss = 0.45054806\n",
            "Iteration 499, loss = 0.45043188\n",
            "Iteration 500, loss = 0.45025394\n",
            "Iteration 501, loss = 0.45008432\n",
            "Iteration 502, loss = 0.44995561\n",
            "Iteration 503, loss = 0.44979461\n",
            "Iteration 504, loss = 0.44965049\n",
            "Iteration 505, loss = 0.44949196\n",
            "Iteration 506, loss = 0.44935867\n",
            "Iteration 507, loss = 0.44926580\n",
            "Iteration 508, loss = 0.44905472\n",
            "Iteration 509, loss = 0.44893160\n",
            "Iteration 510, loss = 0.44875804\n",
            "Iteration 511, loss = 0.44863332\n",
            "Iteration 512, loss = 0.44850995\n",
            "Iteration 513, loss = 0.44836548\n",
            "Iteration 514, loss = 0.44823910\n",
            "Iteration 515, loss = 0.44806962\n",
            "Iteration 516, loss = 0.44798075\n",
            "Iteration 517, loss = 0.44781948\n",
            "Iteration 518, loss = 0.44770226\n",
            "Iteration 519, loss = 0.44757468\n",
            "Iteration 520, loss = 0.44745474\n",
            "Iteration 521, loss = 0.44731706\n",
            "Iteration 522, loss = 0.44720000\n",
            "Iteration 523, loss = 0.44705229\n",
            "Iteration 524, loss = 0.44694678\n",
            "Iteration 525, loss = 0.44682149\n",
            "Iteration 526, loss = 0.44671374\n",
            "Iteration 527, loss = 0.44658683\n",
            "Iteration 528, loss = 0.44649285\n",
            "Iteration 529, loss = 0.44635217\n",
            "Iteration 530, loss = 0.44624071\n",
            "Iteration 531, loss = 0.44615952\n",
            "Iteration 532, loss = 0.44601053\n",
            "Iteration 533, loss = 0.44590829\n",
            "Iteration 534, loss = 0.44578016\n",
            "Iteration 535, loss = 0.44566454\n",
            "Iteration 536, loss = 0.44554856\n",
            "Iteration 537, loss = 0.44543801\n",
            "Iteration 538, loss = 0.44534616\n",
            "Iteration 539, loss = 0.44520914\n",
            "Iteration 540, loss = 0.44515087\n",
            "Iteration 541, loss = 0.44500716\n",
            "Iteration 542, loss = 0.44498061\n",
            "Iteration 543, loss = 0.44482896\n",
            "Iteration 544, loss = 0.44469568\n",
            "Iteration 545, loss = 0.44462757\n",
            "Iteration 546, loss = 0.44450930\n",
            "Iteration 547, loss = 0.44443526\n",
            "Iteration 548, loss = 0.44436838\n",
            "Iteration 549, loss = 0.44423255\n",
            "Iteration 550, loss = 0.44415191\n",
            "Iteration 551, loss = 0.44403918\n",
            "Iteration 552, loss = 0.44397275\n",
            "Iteration 553, loss = 0.44385956\n",
            "Iteration 554, loss = 0.44376255\n",
            "Iteration 555, loss = 0.44368523\n",
            "Iteration 556, loss = 0.44361670\n",
            "Iteration 557, loss = 0.44349189\n",
            "Iteration 558, loss = 0.44341814\n",
            "Iteration 559, loss = 0.44335954\n",
            "Iteration 560, loss = 0.44326535\n",
            "Iteration 561, loss = 0.44320709\n",
            "Iteration 562, loss = 0.44314291\n",
            "Iteration 563, loss = 0.44305888\n",
            "Iteration 564, loss = 0.44297744\n",
            "Iteration 565, loss = 0.44289644\n",
            "Iteration 566, loss = 0.44285050\n",
            "Iteration 567, loss = 0.44274360\n",
            "Iteration 568, loss = 0.44266708\n",
            "Iteration 569, loss = 0.44260818\n",
            "Iteration 570, loss = 0.44253516\n",
            "Iteration 571, loss = 0.44245504\n",
            "Iteration 572, loss = 0.44234569\n",
            "Iteration 573, loss = 0.44228291\n",
            "Iteration 574, loss = 0.44223853\n",
            "Iteration 575, loss = 0.44216701\n",
            "Iteration 576, loss = 0.44207025\n",
            "Iteration 577, loss = 0.44203152\n",
            "Iteration 578, loss = 0.44200681\n",
            "Iteration 579, loss = 0.44188192\n",
            "Iteration 580, loss = 0.44180441\n",
            "Iteration 581, loss = 0.44175205\n",
            "Iteration 582, loss = 0.44163597\n",
            "Iteration 583, loss = 0.44156666\n",
            "Iteration 584, loss = 0.44147454\n",
            "Iteration 585, loss = 0.44146921\n",
            "Iteration 586, loss = 0.44134494\n",
            "Iteration 587, loss = 0.44128080\n",
            "Iteration 588, loss = 0.44122648\n",
            "Iteration 589, loss = 0.44116975\n",
            "Iteration 590, loss = 0.44109559\n",
            "Iteration 591, loss = 0.44107078\n",
            "Iteration 592, loss = 0.44100623\n",
            "Iteration 593, loss = 0.44096313\n",
            "Iteration 594, loss = 0.44087890\n",
            "Iteration 595, loss = 0.44081691\n",
            "Iteration 596, loss = 0.44076884\n",
            "Iteration 597, loss = 0.44071473\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.71633136\n",
            "Iteration 2, loss = 0.71528775\n",
            "Iteration 3, loss = 0.71419178\n",
            "Iteration 4, loss = 0.71314489\n",
            "Iteration 5, loss = 0.71211780\n",
            "Iteration 6, loss = 0.71113406\n",
            "Iteration 7, loss = 0.71021722\n",
            "Iteration 8, loss = 0.70924399\n",
            "Iteration 9, loss = 0.70836311\n",
            "Iteration 10, loss = 0.70755755\n",
            "Iteration 11, loss = 0.70676380\n",
            "Iteration 12, loss = 0.70605033\n",
            "Iteration 13, loss = 0.70530609\n",
            "Iteration 14, loss = 0.70454170\n",
            "Iteration 15, loss = 0.70397148\n",
            "Iteration 16, loss = 0.70322229\n",
            "Iteration 17, loss = 0.70256029\n",
            "Iteration 18, loss = 0.70199314\n",
            "Iteration 19, loss = 0.70134630\n",
            "Iteration 20, loss = 0.70078807\n",
            "Iteration 21, loss = 0.70025982\n",
            "Iteration 22, loss = 0.69975972\n",
            "Iteration 23, loss = 0.69925133\n",
            "Iteration 24, loss = 0.69873691\n",
            "Iteration 25, loss = 0.69831045\n",
            "Iteration 26, loss = 0.69793250\n",
            "Iteration 27, loss = 0.69760155\n",
            "Iteration 28, loss = 0.69719977\n",
            "Iteration 29, loss = 0.69689041\n",
            "Iteration 30, loss = 0.69662441\n",
            "Iteration 31, loss = 0.69627490\n",
            "Iteration 32, loss = 0.69599476\n",
            "Iteration 33, loss = 0.69575347\n",
            "Iteration 34, loss = 0.69550030\n",
            "Iteration 35, loss = 0.69515684\n",
            "Iteration 36, loss = 0.69493753\n",
            "Iteration 37, loss = 0.69472983\n",
            "Iteration 38, loss = 0.69448669\n",
            "Iteration 39, loss = 0.69426437\n",
            "Iteration 40, loss = 0.69414664\n",
            "Iteration 41, loss = 0.69387871\n",
            "Iteration 42, loss = 0.69373335\n",
            "Iteration 43, loss = 0.69359589\n",
            "Iteration 44, loss = 0.69346052\n",
            "Iteration 45, loss = 0.69331586\n",
            "Iteration 46, loss = 0.69318124\n",
            "Iteration 47, loss = 0.69304356\n",
            "Iteration 48, loss = 0.69294328\n",
            "Iteration 49, loss = 0.69280718\n",
            "Iteration 50, loss = 0.69272831\n",
            "Iteration 51, loss = 0.69263892\n",
            "Iteration 52, loss = 0.69252819\n",
            "Iteration 53, loss = 0.69242447\n",
            "Iteration 54, loss = 0.69234401\n",
            "Iteration 55, loss = 0.69223858\n",
            "Iteration 56, loss = 0.69216639\n",
            "Iteration 57, loss = 0.69204671\n",
            "Iteration 58, loss = 0.69196518\n",
            "Iteration 59, loss = 0.69187712\n",
            "Iteration 60, loss = 0.69175841\n",
            "Iteration 61, loss = 0.69166982\n",
            "Iteration 62, loss = 0.69159146\n",
            "Iteration 63, loss = 0.69147905\n",
            "Iteration 64, loss = 0.69139350\n",
            "Iteration 65, loss = 0.69128192\n",
            "Iteration 66, loss = 0.69119478\n",
            "Iteration 67, loss = 0.69109322\n",
            "Iteration 68, loss = 0.69097681\n",
            "Iteration 69, loss = 0.69089310\n",
            "Iteration 70, loss = 0.69078930\n",
            "Iteration 71, loss = 0.69067238\n",
            "Iteration 72, loss = 0.69055435\n",
            "Iteration 73, loss = 0.69044265\n",
            "Iteration 74, loss = 0.69030235\n",
            "Iteration 75, loss = 0.69016101\n",
            "Iteration 76, loss = 0.69002140\n",
            "Iteration 77, loss = 0.68988008\n",
            "Iteration 78, loss = 0.68972691\n",
            "Iteration 79, loss = 0.68958202\n",
            "Iteration 80, loss = 0.68942268\n",
            "Iteration 81, loss = 0.68925900\n",
            "Iteration 82, loss = 0.68909306\n",
            "Iteration 83, loss = 0.68892988\n",
            "Iteration 84, loss = 0.68871749\n",
            "Iteration 85, loss = 0.68854392\n",
            "Iteration 86, loss = 0.68835526\n",
            "Iteration 87, loss = 0.68814279\n",
            "Iteration 88, loss = 0.68794149\n",
            "Iteration 89, loss = 0.68773358\n",
            "Iteration 90, loss = 0.68750996\n",
            "Iteration 91, loss = 0.68729146\n",
            "Iteration 92, loss = 0.68704906\n",
            "Iteration 93, loss = 0.68681048\n",
            "Iteration 94, loss = 0.68657140\n",
            "Iteration 95, loss = 0.68631405\n",
            "Iteration 96, loss = 0.68606718\n",
            "Iteration 97, loss = 0.68580825\n",
            "Iteration 98, loss = 0.68552507\n",
            "Iteration 99, loss = 0.68525871\n",
            "Iteration 100, loss = 0.68499014\n",
            "Iteration 101, loss = 0.68470620\n",
            "Iteration 102, loss = 0.68441432\n",
            "Iteration 103, loss = 0.68410934\n",
            "Iteration 104, loss = 0.68380194\n",
            "Iteration 105, loss = 0.68350177\n",
            "Iteration 106, loss = 0.68319522\n",
            "Iteration 107, loss = 0.68286295\n",
            "Iteration 108, loss = 0.68256262\n",
            "Iteration 109, loss = 0.68220649\n",
            "Iteration 110, loss = 0.68187526\n",
            "Iteration 111, loss = 0.68153324\n",
            "Iteration 112, loss = 0.68120161\n",
            "Iteration 113, loss = 0.68083881\n",
            "Iteration 114, loss = 0.68046237\n",
            "Iteration 115, loss = 0.68009404\n",
            "Iteration 116, loss = 0.67973240\n",
            "Iteration 117, loss = 0.67934765\n",
            "Iteration 118, loss = 0.67896688\n",
            "Iteration 119, loss = 0.67854655\n",
            "Iteration 120, loss = 0.67811642\n",
            "Iteration 121, loss = 0.67771489\n",
            "Iteration 122, loss = 0.67727693\n",
            "Iteration 123, loss = 0.67685491\n",
            "Iteration 124, loss = 0.67638842\n",
            "Iteration 125, loss = 0.67596597\n",
            "Iteration 126, loss = 0.67549547\n",
            "Iteration 127, loss = 0.67504295\n",
            "Iteration 128, loss = 0.67459019\n",
            "Iteration 129, loss = 0.67414388\n",
            "Iteration 130, loss = 0.67368343\n",
            "Iteration 131, loss = 0.67322079\n",
            "Iteration 132, loss = 0.67275195\n",
            "Iteration 133, loss = 0.67227554\n",
            "Iteration 134, loss = 0.67178765\n",
            "Iteration 135, loss = 0.67130698\n",
            "Iteration 136, loss = 0.67083181\n",
            "Iteration 137, loss = 0.67031651\n",
            "Iteration 138, loss = 0.66980837\n",
            "Iteration 139, loss = 0.66930138\n",
            "Iteration 140, loss = 0.66878808\n",
            "Iteration 141, loss = 0.66827664\n",
            "Iteration 142, loss = 0.66773279\n",
            "Iteration 143, loss = 0.66723074\n",
            "Iteration 144, loss = 0.66666117\n",
            "Iteration 145, loss = 0.66612378\n",
            "Iteration 146, loss = 0.66556875\n",
            "Iteration 147, loss = 0.66501779\n",
            "Iteration 148, loss = 0.66445844\n",
            "Iteration 149, loss = 0.66388369\n",
            "Iteration 150, loss = 0.66331636\n",
            "Iteration 151, loss = 0.66275422\n",
            "Iteration 152, loss = 0.66214231\n",
            "Iteration 153, loss = 0.66158599\n",
            "Iteration 154, loss = 0.66094791\n",
            "Iteration 155, loss = 0.66032007\n",
            "Iteration 156, loss = 0.65969056\n",
            "Iteration 157, loss = 0.65907984\n",
            "Iteration 158, loss = 0.65844217\n",
            "Iteration 159, loss = 0.65780772\n",
            "Iteration 160, loss = 0.65719784\n",
            "Iteration 161, loss = 0.65653080\n",
            "Iteration 162, loss = 0.65588857\n",
            "Iteration 163, loss = 0.65523519\n",
            "Iteration 164, loss = 0.65458787\n",
            "Iteration 165, loss = 0.65393829\n",
            "Iteration 166, loss = 0.65328668\n",
            "Iteration 167, loss = 0.65259678\n",
            "Iteration 168, loss = 0.65194355\n",
            "Iteration 169, loss = 0.65126333\n",
            "Iteration 170, loss = 0.65060257\n",
            "Iteration 171, loss = 0.64995121\n",
            "Iteration 172, loss = 0.64929550\n",
            "Iteration 173, loss = 0.64861803\n",
            "Iteration 174, loss = 0.64795741\n",
            "Iteration 175, loss = 0.64729388\n",
            "Iteration 176, loss = 0.64662529\n",
            "Iteration 177, loss = 0.64594369\n",
            "Iteration 178, loss = 0.64524659\n",
            "Iteration 179, loss = 0.64459911\n",
            "Iteration 180, loss = 0.64384159\n",
            "Iteration 181, loss = 0.64313480\n",
            "Iteration 182, loss = 0.64247000\n",
            "Iteration 183, loss = 0.64170854\n",
            "Iteration 184, loss = 0.64105324\n",
            "Iteration 185, loss = 0.64031865\n",
            "Iteration 186, loss = 0.63964741\n",
            "Iteration 187, loss = 0.63892900\n",
            "Iteration 188, loss = 0.63819862\n",
            "Iteration 189, loss = 0.63751062\n",
            "Iteration 190, loss = 0.63678135\n",
            "Iteration 191, loss = 0.63604555\n",
            "Iteration 192, loss = 0.63536390\n",
            "Iteration 193, loss = 0.63459359\n",
            "Iteration 194, loss = 0.63381152\n",
            "Iteration 195, loss = 0.63313251\n",
            "Iteration 196, loss = 0.63233162\n",
            "Iteration 197, loss = 0.63160837\n",
            "Iteration 198, loss = 0.63085286\n",
            "Iteration 199, loss = 0.63013228\n",
            "Iteration 200, loss = 0.62939414\n",
            "Iteration 201, loss = 0.62866990\n",
            "Iteration 202, loss = 0.62793631\n",
            "Iteration 203, loss = 0.62717257\n",
            "Iteration 204, loss = 0.62643312\n",
            "Iteration 205, loss = 0.62569982\n",
            "Iteration 206, loss = 0.62493611\n",
            "Iteration 207, loss = 0.62418676\n",
            "Iteration 208, loss = 0.62344263\n",
            "Iteration 209, loss = 0.62268532\n",
            "Iteration 210, loss = 0.62192014\n",
            "Iteration 211, loss = 0.62119356\n",
            "Iteration 212, loss = 0.62036058\n",
            "Iteration 213, loss = 0.61958806\n",
            "Iteration 214, loss = 0.61885376\n",
            "Iteration 215, loss = 0.61805645\n",
            "Iteration 216, loss = 0.61731239\n",
            "Iteration 217, loss = 0.61658323\n",
            "Iteration 218, loss = 0.61578761\n",
            "Iteration 219, loss = 0.61499781\n",
            "Iteration 220, loss = 0.61422576\n",
            "Iteration 221, loss = 0.61339797\n",
            "Iteration 222, loss = 0.61264553\n",
            "Iteration 223, loss = 0.61186196\n",
            "Iteration 224, loss = 0.61108093\n",
            "Iteration 225, loss = 0.61031372\n",
            "Iteration 226, loss = 0.60950810\n",
            "Iteration 227, loss = 0.60873205\n",
            "Iteration 228, loss = 0.60797215\n",
            "Iteration 229, loss = 0.60717515\n",
            "Iteration 230, loss = 0.60640835\n",
            "Iteration 231, loss = 0.60560854\n",
            "Iteration 232, loss = 0.60482829\n",
            "Iteration 233, loss = 0.60405786\n",
            "Iteration 234, loss = 0.60325715\n",
            "Iteration 235, loss = 0.60248226\n",
            "Iteration 236, loss = 0.60167152\n",
            "Iteration 237, loss = 0.60089327\n",
            "Iteration 238, loss = 0.60009861\n",
            "Iteration 239, loss = 0.59926609\n",
            "Iteration 240, loss = 0.59848449\n",
            "Iteration 241, loss = 0.59769187\n",
            "Iteration 242, loss = 0.59690809\n",
            "Iteration 243, loss = 0.59612068\n",
            "Iteration 244, loss = 0.59529492\n",
            "Iteration 245, loss = 0.59451174\n",
            "Iteration 246, loss = 0.59371983\n",
            "Iteration 247, loss = 0.59291990\n",
            "Iteration 248, loss = 0.59211952\n",
            "Iteration 249, loss = 0.59130886\n",
            "Iteration 250, loss = 0.59049495\n",
            "Iteration 251, loss = 0.58971627\n",
            "Iteration 252, loss = 0.58889009\n",
            "Iteration 253, loss = 0.58809927\n",
            "Iteration 254, loss = 0.58728734\n",
            "Iteration 255, loss = 0.58649265\n",
            "Iteration 256, loss = 0.58570927\n",
            "Iteration 257, loss = 0.58489239\n",
            "Iteration 258, loss = 0.58406329\n",
            "Iteration 259, loss = 0.58326845\n",
            "Iteration 260, loss = 0.58250926\n",
            "Iteration 261, loss = 0.58167646\n",
            "Iteration 262, loss = 0.58088627\n",
            "Iteration 263, loss = 0.58005769\n",
            "Iteration 264, loss = 0.57926810\n",
            "Iteration 265, loss = 0.57847261\n",
            "Iteration 266, loss = 0.57764057\n",
            "Iteration 267, loss = 0.57682092\n",
            "Iteration 268, loss = 0.57600295\n",
            "Iteration 269, loss = 0.57518508\n",
            "Iteration 270, loss = 0.57439292\n",
            "Iteration 271, loss = 0.57361468\n",
            "Iteration 272, loss = 0.57279304\n",
            "Iteration 273, loss = 0.57198883\n",
            "Iteration 274, loss = 0.57114496\n",
            "Iteration 275, loss = 0.57033053\n",
            "Iteration 276, loss = 0.56952384\n",
            "Iteration 277, loss = 0.56873076\n",
            "Iteration 278, loss = 0.56792756\n",
            "Iteration 279, loss = 0.56708959\n",
            "Iteration 280, loss = 0.56630250\n",
            "Iteration 281, loss = 0.56552705\n",
            "Iteration 282, loss = 0.56470669\n",
            "Iteration 283, loss = 0.56392165\n",
            "Iteration 284, loss = 0.56317109\n",
            "Iteration 285, loss = 0.56234330\n",
            "Iteration 286, loss = 0.56153595\n",
            "Iteration 287, loss = 0.56075940\n",
            "Iteration 288, loss = 0.55994760\n",
            "Iteration 289, loss = 0.55915025\n",
            "Iteration 290, loss = 0.55837923\n",
            "Iteration 291, loss = 0.55759812\n",
            "Iteration 292, loss = 0.55682628\n",
            "Iteration 293, loss = 0.55603401\n",
            "Iteration 294, loss = 0.55522287\n",
            "Iteration 295, loss = 0.55447065\n",
            "Iteration 296, loss = 0.55367247\n",
            "Iteration 297, loss = 0.55286837\n",
            "Iteration 298, loss = 0.55207349\n",
            "Iteration 299, loss = 0.55130299\n",
            "Iteration 300, loss = 0.55054075\n",
            "Iteration 301, loss = 0.54974716\n",
            "Iteration 302, loss = 0.54899018\n",
            "Iteration 303, loss = 0.54822726\n",
            "Iteration 304, loss = 0.54746446\n",
            "Iteration 305, loss = 0.54670165\n",
            "Iteration 306, loss = 0.54590226\n",
            "Iteration 307, loss = 0.54512075\n",
            "Iteration 308, loss = 0.54438332\n",
            "Iteration 309, loss = 0.54361303\n",
            "Iteration 310, loss = 0.54290301\n",
            "Iteration 311, loss = 0.54205892\n",
            "Iteration 312, loss = 0.54130779\n",
            "Iteration 313, loss = 0.54055643\n",
            "Iteration 314, loss = 0.53980005\n",
            "Iteration 315, loss = 0.53906278\n",
            "Iteration 316, loss = 0.53828078\n",
            "Iteration 317, loss = 0.53754235\n",
            "Iteration 318, loss = 0.53676727\n",
            "Iteration 319, loss = 0.53603849\n",
            "Iteration 320, loss = 0.53530980\n",
            "Iteration 321, loss = 0.53455605\n",
            "Iteration 322, loss = 0.53382336\n",
            "Iteration 323, loss = 0.53305699\n",
            "Iteration 324, loss = 0.53234296\n",
            "Iteration 325, loss = 0.53160631\n",
            "Iteration 326, loss = 0.53089053\n",
            "Iteration 327, loss = 0.53015871\n",
            "Iteration 328, loss = 0.52942892\n",
            "Iteration 329, loss = 0.52872744\n",
            "Iteration 330, loss = 0.52801346\n",
            "Iteration 331, loss = 0.52732996\n",
            "Iteration 332, loss = 0.52660115\n",
            "Iteration 333, loss = 0.52587450\n",
            "Iteration 334, loss = 0.52520880\n",
            "Iteration 335, loss = 0.52452608\n",
            "Iteration 336, loss = 0.52387224\n",
            "Iteration 337, loss = 0.52316462\n",
            "Iteration 338, loss = 0.52252249\n",
            "Iteration 339, loss = 0.52186677\n",
            "Iteration 340, loss = 0.52118559\n",
            "Iteration 341, loss = 0.52050924\n",
            "Iteration 342, loss = 0.51985435\n",
            "Iteration 343, loss = 0.51916222\n",
            "Iteration 344, loss = 0.51849038\n",
            "Iteration 345, loss = 0.51782926\n",
            "Iteration 346, loss = 0.51716505\n",
            "Iteration 347, loss = 0.51650440\n",
            "Iteration 348, loss = 0.51583733\n",
            "Iteration 349, loss = 0.51516920\n",
            "Iteration 350, loss = 0.51450316\n",
            "Iteration 351, loss = 0.51383130\n",
            "Iteration 352, loss = 0.51322866\n",
            "Iteration 353, loss = 0.51255839\n",
            "Iteration 354, loss = 0.51190817\n",
            "Iteration 355, loss = 0.51125441\n",
            "Iteration 356, loss = 0.51067883\n",
            "Iteration 357, loss = 0.51000983\n",
            "Iteration 358, loss = 0.50943855\n",
            "Iteration 359, loss = 0.50880130\n",
            "Iteration 360, loss = 0.50819094\n",
            "Iteration 361, loss = 0.50763475\n",
            "Iteration 362, loss = 0.50703285\n",
            "Iteration 363, loss = 0.50647313\n",
            "Iteration 364, loss = 0.50586880\n",
            "Iteration 365, loss = 0.50526974\n",
            "Iteration 366, loss = 0.50476764\n",
            "Iteration 367, loss = 0.50414963\n",
            "Iteration 368, loss = 0.50361917\n",
            "Iteration 369, loss = 0.50302184\n",
            "Iteration 370, loss = 0.50243790\n",
            "Iteration 371, loss = 0.50184365\n",
            "Iteration 372, loss = 0.50129923\n",
            "Iteration 373, loss = 0.50074037\n",
            "Iteration 374, loss = 0.50022201\n",
            "Iteration 375, loss = 0.49967888\n",
            "Iteration 376, loss = 0.49913895\n",
            "Iteration 377, loss = 0.49865096\n",
            "Iteration 378, loss = 0.49812637\n",
            "Iteration 379, loss = 0.49759843\n",
            "Iteration 380, loss = 0.49709097\n",
            "Iteration 381, loss = 0.49655305\n",
            "Iteration 382, loss = 0.49610234\n",
            "Iteration 383, loss = 0.49554468\n",
            "Iteration 384, loss = 0.49506522\n",
            "Iteration 385, loss = 0.49459361\n",
            "Iteration 386, loss = 0.49400921\n",
            "Iteration 387, loss = 0.49352346\n",
            "Iteration 388, loss = 0.49306795\n",
            "Iteration 389, loss = 0.49256351\n",
            "Iteration 390, loss = 0.49209632\n",
            "Iteration 391, loss = 0.49162688\n",
            "Iteration 392, loss = 0.49118188\n",
            "Iteration 393, loss = 0.49071797\n",
            "Iteration 394, loss = 0.49029972\n",
            "Iteration 395, loss = 0.48983547\n",
            "Iteration 396, loss = 0.48938063\n",
            "Iteration 397, loss = 0.48893337\n",
            "Iteration 398, loss = 0.48849405\n",
            "Iteration 399, loss = 0.48807136\n",
            "Iteration 400, loss = 0.48757894\n",
            "Iteration 401, loss = 0.48716476\n",
            "Iteration 402, loss = 0.48676267\n",
            "Iteration 403, loss = 0.48632226\n",
            "Iteration 404, loss = 0.48591494\n",
            "Iteration 405, loss = 0.48547774\n",
            "Iteration 406, loss = 0.48504868\n",
            "Iteration 407, loss = 0.48465573\n",
            "Iteration 408, loss = 0.48420485\n",
            "Iteration 409, loss = 0.48380208\n",
            "Iteration 410, loss = 0.48338270\n",
            "Iteration 411, loss = 0.48299883\n",
            "Iteration 412, loss = 0.48258642\n",
            "Iteration 413, loss = 0.48216826\n",
            "Iteration 414, loss = 0.48177867\n",
            "Iteration 415, loss = 0.48138904\n",
            "Iteration 416, loss = 0.48101973\n",
            "Iteration 417, loss = 0.48064014\n",
            "Iteration 418, loss = 0.48025631\n",
            "Iteration 419, loss = 0.47990770\n",
            "Iteration 420, loss = 0.47951912\n",
            "Iteration 421, loss = 0.47918727\n",
            "Iteration 422, loss = 0.47882331\n",
            "Iteration 423, loss = 0.47844276\n",
            "Iteration 424, loss = 0.47816362\n",
            "Iteration 425, loss = 0.47779515\n",
            "Iteration 426, loss = 0.47747850\n",
            "Iteration 427, loss = 0.47709275\n",
            "Iteration 428, loss = 0.47678348\n",
            "Iteration 429, loss = 0.47645873\n",
            "Iteration 430, loss = 0.47611499\n",
            "Iteration 431, loss = 0.47581990\n",
            "Iteration 432, loss = 0.47549676\n",
            "Iteration 433, loss = 0.47511954\n",
            "Iteration 434, loss = 0.47483297\n",
            "Iteration 435, loss = 0.47445976\n",
            "Iteration 436, loss = 0.47417237\n",
            "Iteration 437, loss = 0.47382982\n",
            "Iteration 438, loss = 0.47351780\n",
            "Iteration 439, loss = 0.47321667\n",
            "Iteration 440, loss = 0.47291057\n",
            "Iteration 441, loss = 0.47262484\n",
            "Iteration 442, loss = 0.47230453\n",
            "Iteration 443, loss = 0.47204079\n",
            "Iteration 444, loss = 0.47173672\n",
            "Iteration 445, loss = 0.47143407\n",
            "Iteration 446, loss = 0.47113556\n",
            "Iteration 447, loss = 0.47083699\n",
            "Iteration 448, loss = 0.47056828\n",
            "Iteration 449, loss = 0.47027108\n",
            "Iteration 450, loss = 0.47001636\n",
            "Iteration 451, loss = 0.46974672\n",
            "Iteration 452, loss = 0.46945722\n",
            "Iteration 453, loss = 0.46921487\n",
            "Iteration 454, loss = 0.46891222\n",
            "Iteration 455, loss = 0.46866698\n",
            "Iteration 456, loss = 0.46841673\n",
            "Iteration 457, loss = 0.46813298\n",
            "Iteration 458, loss = 0.46788903\n",
            "Iteration 459, loss = 0.46764152\n",
            "Iteration 460, loss = 0.46739850\n",
            "Iteration 461, loss = 0.46717052\n",
            "Iteration 462, loss = 0.46694813\n",
            "Iteration 463, loss = 0.46672734\n",
            "Iteration 464, loss = 0.46647666\n",
            "Iteration 465, loss = 0.46625727\n",
            "Iteration 466, loss = 0.46604479\n",
            "Iteration 467, loss = 0.46577893\n",
            "Iteration 468, loss = 0.46556108\n",
            "Iteration 469, loss = 0.46534098\n",
            "Iteration 470, loss = 0.46510257\n",
            "Iteration 471, loss = 0.46489501\n",
            "Iteration 472, loss = 0.46467103\n",
            "Iteration 473, loss = 0.46445950\n",
            "Iteration 474, loss = 0.46421803\n",
            "Iteration 475, loss = 0.46401984\n",
            "Iteration 476, loss = 0.46381737\n",
            "Iteration 477, loss = 0.46365487\n",
            "Iteration 478, loss = 0.46345502\n",
            "Iteration 479, loss = 0.46325067\n",
            "Iteration 480, loss = 0.46308200\n",
            "Iteration 481, loss = 0.46287820\n",
            "Iteration 482, loss = 0.46268050\n",
            "Iteration 483, loss = 0.46247484\n",
            "Iteration 484, loss = 0.46227990\n",
            "Iteration 485, loss = 0.46208394\n",
            "Iteration 486, loss = 0.46194668\n",
            "Iteration 487, loss = 0.46170880\n",
            "Iteration 488, loss = 0.46152479\n",
            "Iteration 489, loss = 0.46137970\n",
            "Iteration 490, loss = 0.46121717\n",
            "Iteration 491, loss = 0.46101897\n",
            "Iteration 492, loss = 0.46088123\n",
            "Iteration 493, loss = 0.46065120\n",
            "Iteration 494, loss = 0.46048800\n",
            "Iteration 495, loss = 0.46031775\n",
            "Iteration 496, loss = 0.46012878\n",
            "Iteration 497, loss = 0.45995408\n",
            "Iteration 498, loss = 0.45980737\n",
            "Iteration 499, loss = 0.45961728\n",
            "Iteration 500, loss = 0.45948001\n",
            "Iteration 501, loss = 0.45929270\n",
            "Iteration 502, loss = 0.45915126\n",
            "Iteration 503, loss = 0.45897374\n",
            "Iteration 504, loss = 0.45881093\n",
            "Iteration 505, loss = 0.45864846\n",
            "Iteration 506, loss = 0.45848080\n",
            "Iteration 507, loss = 0.45838070\n",
            "Iteration 508, loss = 0.45815096\n",
            "Iteration 509, loss = 0.45800076\n",
            "Iteration 510, loss = 0.45785637\n",
            "Iteration 511, loss = 0.45771078\n",
            "Iteration 512, loss = 0.45756627\n",
            "Iteration 513, loss = 0.45740755\n",
            "Iteration 514, loss = 0.45726637\n",
            "Iteration 515, loss = 0.45711580\n",
            "Iteration 516, loss = 0.45700767\n",
            "Iteration 517, loss = 0.45685322\n",
            "Iteration 518, loss = 0.45672684\n",
            "Iteration 519, loss = 0.45660193\n",
            "Iteration 520, loss = 0.45646487\n",
            "Iteration 521, loss = 0.45630378\n",
            "Iteration 522, loss = 0.45618275\n",
            "Iteration 523, loss = 0.45602202\n",
            "Iteration 524, loss = 0.45590024\n",
            "Iteration 525, loss = 0.45577331\n",
            "Iteration 526, loss = 0.45565637\n",
            "Iteration 527, loss = 0.45550181\n",
            "Iteration 528, loss = 0.45540062\n",
            "Iteration 529, loss = 0.45525959\n",
            "Iteration 530, loss = 0.45512692\n",
            "Iteration 531, loss = 0.45501789\n",
            "Iteration 532, loss = 0.45487125\n",
            "Iteration 533, loss = 0.45477694\n",
            "Iteration 534, loss = 0.45466121\n",
            "Iteration 535, loss = 0.45451370\n",
            "Iteration 536, loss = 0.45441162\n",
            "Iteration 537, loss = 0.45428487\n",
            "Iteration 538, loss = 0.45417521\n",
            "Iteration 539, loss = 0.45404587\n",
            "Iteration 540, loss = 0.45397100\n",
            "Iteration 541, loss = 0.45383235\n",
            "Iteration 542, loss = 0.45373536\n",
            "Iteration 543, loss = 0.45364540\n",
            "Iteration 544, loss = 0.45352063\n",
            "Iteration 545, loss = 0.45342211\n",
            "Iteration 546, loss = 0.45332890\n",
            "Iteration 547, loss = 0.45323344\n",
            "Iteration 548, loss = 0.45317227\n",
            "Iteration 549, loss = 0.45303923\n",
            "Iteration 550, loss = 0.45294252\n",
            "Iteration 551, loss = 0.45285051\n",
            "Iteration 552, loss = 0.45273061\n",
            "Iteration 553, loss = 0.45262125\n",
            "Iteration 554, loss = 0.45251896\n",
            "Iteration 555, loss = 0.45242639\n",
            "Iteration 556, loss = 0.45231413\n",
            "Iteration 557, loss = 0.45218852\n",
            "Iteration 558, loss = 0.45208754\n",
            "Iteration 559, loss = 0.45209326\n",
            "Iteration 560, loss = 0.45193998\n",
            "Iteration 561, loss = 0.45188278\n",
            "Iteration 562, loss = 0.45181457\n",
            "Iteration 563, loss = 0.45172815\n",
            "Iteration 564, loss = 0.45165635\n",
            "Iteration 565, loss = 0.45153244\n",
            "Iteration 566, loss = 0.45147881\n",
            "Iteration 567, loss = 0.45133078\n",
            "Iteration 568, loss = 0.45124037\n",
            "Iteration 569, loss = 0.45120907\n",
            "Iteration 570, loss = 0.45112126\n",
            "Iteration 571, loss = 0.45100890\n",
            "Iteration 572, loss = 0.45088687\n",
            "Iteration 573, loss = 0.45083308\n",
            "Iteration 574, loss = 0.45072425\n",
            "Iteration 575, loss = 0.45066016\n",
            "Iteration 576, loss = 0.45057150\n",
            "Iteration 577, loss = 0.45051938\n",
            "Iteration 578, loss = 0.45047422\n",
            "Iteration 579, loss = 0.45035172\n",
            "Iteration 580, loss = 0.45031427\n",
            "Iteration 581, loss = 0.45019667\n",
            "Iteration 582, loss = 0.45008112\n",
            "Iteration 583, loss = 0.45000472\n",
            "Iteration 584, loss = 0.44990347\n",
            "Iteration 585, loss = 0.44985472\n",
            "Iteration 586, loss = 0.44975475\n",
            "Iteration 587, loss = 0.44968479\n",
            "Iteration 588, loss = 0.44959782\n",
            "Iteration 589, loss = 0.44955308\n",
            "Iteration 590, loss = 0.44945990\n",
            "Iteration 591, loss = 0.44940679\n",
            "Iteration 592, loss = 0.44932283\n",
            "Iteration 593, loss = 0.44929648\n",
            "Iteration 594, loss = 0.44918464\n",
            "Iteration 595, loss = 0.44912055\n",
            "Iteration 596, loss = 0.44905571\n",
            "Iteration 597, loss = 0.44900607\n",
            "Iteration 598, loss = 0.44894658\n",
            "Iteration 599, loss = 0.44887474\n",
            "Iteration 600, loss = 0.44877923\n",
            "Iteration 601, loss = 0.44873112\n",
            "Iteration 602, loss = 0.44867866\n",
            "Iteration 603, loss = 0.44862836\n",
            "Iteration 604, loss = 0.44855650\n",
            "Iteration 605, loss = 0.44852031\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.71632995\n",
            "Iteration 2, loss = 0.71524002\n",
            "Iteration 3, loss = 0.71415568\n",
            "Iteration 4, loss = 0.71311245\n",
            "Iteration 5, loss = 0.71210480\n",
            "Iteration 6, loss = 0.71112511\n",
            "Iteration 7, loss = 0.71017822\n",
            "Iteration 8, loss = 0.70927331\n",
            "Iteration 9, loss = 0.70836170\n",
            "Iteration 10, loss = 0.70753141\n",
            "Iteration 11, loss = 0.70678646\n",
            "Iteration 12, loss = 0.70597518\n",
            "Iteration 13, loss = 0.70525100\n",
            "Iteration 14, loss = 0.70450237\n",
            "Iteration 15, loss = 0.70390566\n",
            "Iteration 16, loss = 0.70312369\n",
            "Iteration 17, loss = 0.70249786\n",
            "Iteration 18, loss = 0.70187694\n",
            "Iteration 19, loss = 0.70124141\n",
            "Iteration 20, loss = 0.70066767\n",
            "Iteration 21, loss = 0.70015383\n",
            "Iteration 22, loss = 0.69963126\n",
            "Iteration 23, loss = 0.69913047\n",
            "Iteration 24, loss = 0.69860612\n",
            "Iteration 25, loss = 0.69814583\n",
            "Iteration 26, loss = 0.69776128\n",
            "Iteration 27, loss = 0.69741295\n",
            "Iteration 28, loss = 0.69702152\n",
            "Iteration 29, loss = 0.69671964\n",
            "Iteration 30, loss = 0.69646848\n",
            "Iteration 31, loss = 0.69609527\n",
            "Iteration 32, loss = 0.69584541\n",
            "Iteration 33, loss = 0.69563453\n",
            "Iteration 34, loss = 0.69534208\n",
            "Iteration 35, loss = 0.69503091\n",
            "Iteration 36, loss = 0.69480588\n",
            "Iteration 37, loss = 0.69460877\n",
            "Iteration 38, loss = 0.69434950\n",
            "Iteration 39, loss = 0.69413315\n",
            "Iteration 40, loss = 0.69403737\n",
            "Iteration 41, loss = 0.69375334\n",
            "Iteration 42, loss = 0.69361518\n",
            "Iteration 43, loss = 0.69347858\n",
            "Iteration 44, loss = 0.69335749\n",
            "Iteration 45, loss = 0.69318982\n",
            "Iteration 46, loss = 0.69306989\n",
            "Iteration 47, loss = 0.69294896\n",
            "Iteration 48, loss = 0.69286616\n",
            "Iteration 49, loss = 0.69274156\n",
            "Iteration 50, loss = 0.69265768\n",
            "Iteration 51, loss = 0.69257600\n",
            "Iteration 52, loss = 0.69246401\n",
            "Iteration 53, loss = 0.69236768\n",
            "Iteration 54, loss = 0.69230165\n",
            "Iteration 55, loss = 0.69219987\n",
            "Iteration 56, loss = 0.69212196\n",
            "Iteration 57, loss = 0.69202133\n",
            "Iteration 58, loss = 0.69192745\n",
            "Iteration 59, loss = 0.69184840\n",
            "Iteration 60, loss = 0.69173863\n",
            "Iteration 61, loss = 0.69164206\n",
            "Iteration 62, loss = 0.69158069\n",
            "Iteration 63, loss = 0.69145127\n",
            "Iteration 64, loss = 0.69136378\n",
            "Iteration 65, loss = 0.69126586\n",
            "Iteration 66, loss = 0.69116729\n",
            "Iteration 67, loss = 0.69105874\n",
            "Iteration 68, loss = 0.69095538\n",
            "Iteration 69, loss = 0.69088990\n",
            "Iteration 70, loss = 0.69076623\n",
            "Iteration 71, loss = 0.69065318\n",
            "Iteration 72, loss = 0.69053266\n",
            "Iteration 73, loss = 0.69040887\n",
            "Iteration 74, loss = 0.69026848\n",
            "Iteration 75, loss = 0.69013069\n",
            "Iteration 76, loss = 0.68998284\n",
            "Iteration 77, loss = 0.68984145\n",
            "Iteration 78, loss = 0.68969517\n",
            "Iteration 79, loss = 0.68953725\n",
            "Iteration 80, loss = 0.68937219\n",
            "Iteration 81, loss = 0.68920305\n",
            "Iteration 82, loss = 0.68902714\n",
            "Iteration 83, loss = 0.68886782\n",
            "Iteration 84, loss = 0.68865694\n",
            "Iteration 85, loss = 0.68846880\n",
            "Iteration 86, loss = 0.68827498\n",
            "Iteration 87, loss = 0.68806884\n",
            "Iteration 88, loss = 0.68786987\n",
            "Iteration 89, loss = 0.68765509\n",
            "Iteration 90, loss = 0.68742781\n",
            "Iteration 91, loss = 0.68720074\n",
            "Iteration 92, loss = 0.68695434\n",
            "Iteration 93, loss = 0.68670704\n",
            "Iteration 94, loss = 0.68646466\n",
            "Iteration 95, loss = 0.68621725\n",
            "Iteration 96, loss = 0.68596051\n",
            "Iteration 97, loss = 0.68568719\n",
            "Iteration 98, loss = 0.68541617\n",
            "Iteration 99, loss = 0.68514068\n",
            "Iteration 100, loss = 0.68486441\n",
            "Iteration 101, loss = 0.68458015\n",
            "Iteration 102, loss = 0.68428237\n",
            "Iteration 103, loss = 0.68396320\n",
            "Iteration 104, loss = 0.68365273\n",
            "Iteration 105, loss = 0.68333084\n",
            "Iteration 106, loss = 0.68301612\n",
            "Iteration 107, loss = 0.68267473\n",
            "Iteration 108, loss = 0.68235025\n",
            "Iteration 109, loss = 0.68199849\n",
            "Iteration 110, loss = 0.68164672\n",
            "Iteration 111, loss = 0.68128386\n",
            "Iteration 112, loss = 0.68093566\n",
            "Iteration 113, loss = 0.68056106\n",
            "Iteration 114, loss = 0.68018493\n",
            "Iteration 115, loss = 0.67979428\n",
            "Iteration 116, loss = 0.67940966\n",
            "Iteration 117, loss = 0.67902171\n",
            "Iteration 118, loss = 0.67862058\n",
            "Iteration 119, loss = 0.67819379\n",
            "Iteration 120, loss = 0.67775981\n",
            "Iteration 121, loss = 0.67733631\n",
            "Iteration 122, loss = 0.67689827\n",
            "Iteration 123, loss = 0.67646763\n",
            "Iteration 124, loss = 0.67599426\n",
            "Iteration 125, loss = 0.67555420\n",
            "Iteration 126, loss = 0.67509391\n",
            "Iteration 127, loss = 0.67461927\n",
            "Iteration 128, loss = 0.67417937\n",
            "Iteration 129, loss = 0.67371292\n",
            "Iteration 130, loss = 0.67321599\n",
            "Iteration 131, loss = 0.67273363\n",
            "Iteration 132, loss = 0.67226346\n",
            "Iteration 133, loss = 0.67176339\n",
            "Iteration 134, loss = 0.67126456\n",
            "Iteration 135, loss = 0.67075835\n",
            "Iteration 136, loss = 0.67024865\n",
            "Iteration 137, loss = 0.66972760\n",
            "Iteration 138, loss = 0.66919285\n",
            "Iteration 139, loss = 0.66867324\n",
            "Iteration 140, loss = 0.66815425\n",
            "Iteration 141, loss = 0.66760870\n",
            "Iteration 142, loss = 0.66704607\n",
            "Iteration 143, loss = 0.66651545\n",
            "Iteration 144, loss = 0.66593206\n",
            "Iteration 145, loss = 0.66537153\n",
            "Iteration 146, loss = 0.66478747\n",
            "Iteration 147, loss = 0.66422362\n",
            "Iteration 148, loss = 0.66363992\n",
            "Iteration 149, loss = 0.66305089\n",
            "Iteration 150, loss = 0.66245588\n",
            "Iteration 151, loss = 0.66188358\n",
            "Iteration 152, loss = 0.66125607\n",
            "Iteration 153, loss = 0.66069688\n",
            "Iteration 154, loss = 0.66003826\n",
            "Iteration 155, loss = 0.65941119\n",
            "Iteration 156, loss = 0.65879155\n",
            "Iteration 157, loss = 0.65813469\n",
            "Iteration 158, loss = 0.65750141\n",
            "Iteration 159, loss = 0.65685683\n",
            "Iteration 160, loss = 0.65623202\n",
            "Iteration 161, loss = 0.65553325\n",
            "Iteration 162, loss = 0.65486992\n",
            "Iteration 163, loss = 0.65420194\n",
            "Iteration 164, loss = 0.65351123\n",
            "Iteration 165, loss = 0.65285660\n",
            "Iteration 166, loss = 0.65221611\n",
            "Iteration 167, loss = 0.65149910\n",
            "Iteration 168, loss = 0.65084310\n",
            "Iteration 169, loss = 0.65016749\n",
            "Iteration 170, loss = 0.64948587\n",
            "Iteration 171, loss = 0.64880827\n",
            "Iteration 172, loss = 0.64812660\n",
            "Iteration 173, loss = 0.64742853\n",
            "Iteration 174, loss = 0.64675817\n",
            "Iteration 175, loss = 0.64607958\n",
            "Iteration 176, loss = 0.64537077\n",
            "Iteration 177, loss = 0.64466151\n",
            "Iteration 178, loss = 0.64399148\n",
            "Iteration 179, loss = 0.64328520\n",
            "Iteration 180, loss = 0.64255494\n",
            "Iteration 181, loss = 0.64183407\n",
            "Iteration 182, loss = 0.64112631\n",
            "Iteration 183, loss = 0.64033987\n",
            "Iteration 184, loss = 0.63964847\n",
            "Iteration 185, loss = 0.63891228\n",
            "Iteration 186, loss = 0.63817483\n",
            "Iteration 187, loss = 0.63742089\n",
            "Iteration 188, loss = 0.63669276\n",
            "Iteration 189, loss = 0.63596051\n",
            "Iteration 190, loss = 0.63519377\n",
            "Iteration 191, loss = 0.63444430\n",
            "Iteration 192, loss = 0.63372915\n",
            "Iteration 193, loss = 0.63294606\n",
            "Iteration 194, loss = 0.63213787\n",
            "Iteration 195, loss = 0.63144975\n",
            "Iteration 196, loss = 0.63064480\n",
            "Iteration 197, loss = 0.62987718\n",
            "Iteration 198, loss = 0.62912589\n",
            "Iteration 199, loss = 0.62840937\n",
            "Iteration 200, loss = 0.62764678\n",
            "Iteration 201, loss = 0.62689117\n",
            "Iteration 202, loss = 0.62614192\n",
            "Iteration 203, loss = 0.62535908\n",
            "Iteration 204, loss = 0.62458127\n",
            "Iteration 205, loss = 0.62382446\n",
            "Iteration 206, loss = 0.62305525\n",
            "Iteration 207, loss = 0.62226003\n",
            "Iteration 208, loss = 0.62152198\n",
            "Iteration 209, loss = 0.62072878\n",
            "Iteration 210, loss = 0.61996170\n",
            "Iteration 211, loss = 0.61918901\n",
            "Iteration 212, loss = 0.61837010\n",
            "Iteration 213, loss = 0.61756965\n",
            "Iteration 214, loss = 0.61679750\n",
            "Iteration 215, loss = 0.61598205\n",
            "Iteration 216, loss = 0.61521555\n",
            "Iteration 217, loss = 0.61442008\n",
            "Iteration 218, loss = 0.61362237\n",
            "Iteration 219, loss = 0.61280929\n",
            "Iteration 220, loss = 0.61203598\n",
            "Iteration 221, loss = 0.61116857\n",
            "Iteration 222, loss = 0.61042164\n",
            "Iteration 223, loss = 0.60961170\n",
            "Iteration 224, loss = 0.60881309\n",
            "Iteration 225, loss = 0.60804315\n",
            "Iteration 226, loss = 0.60721389\n",
            "Iteration 227, loss = 0.60642266\n",
            "Iteration 228, loss = 0.60565066\n",
            "Iteration 229, loss = 0.60482412\n",
            "Iteration 230, loss = 0.60401413\n",
            "Iteration 231, loss = 0.60320835\n",
            "Iteration 232, loss = 0.60241561\n",
            "Iteration 233, loss = 0.60160822\n",
            "Iteration 234, loss = 0.60080053\n",
            "Iteration 235, loss = 0.59999912\n",
            "Iteration 236, loss = 0.59919089\n",
            "Iteration 237, loss = 0.59838383\n",
            "Iteration 238, loss = 0.59760116\n",
            "Iteration 239, loss = 0.59674996\n",
            "Iteration 240, loss = 0.59597959\n",
            "Iteration 241, loss = 0.59517990\n",
            "Iteration 242, loss = 0.59441217\n",
            "Iteration 243, loss = 0.59359677\n",
            "Iteration 244, loss = 0.59280001\n",
            "Iteration 245, loss = 0.59202988\n",
            "Iteration 246, loss = 0.59121482\n",
            "Iteration 247, loss = 0.59041417\n",
            "Iteration 248, loss = 0.58959796\n",
            "Iteration 249, loss = 0.58878780\n",
            "Iteration 250, loss = 0.58797465\n",
            "Iteration 251, loss = 0.58717221\n",
            "Iteration 252, loss = 0.58632627\n",
            "Iteration 253, loss = 0.58554397\n",
            "Iteration 254, loss = 0.58470259\n",
            "Iteration 255, loss = 0.58390917\n",
            "Iteration 256, loss = 0.58311860\n",
            "Iteration 257, loss = 0.58230330\n",
            "Iteration 258, loss = 0.58146484\n",
            "Iteration 259, loss = 0.58067537\n",
            "Iteration 260, loss = 0.57989827\n",
            "Iteration 261, loss = 0.57906004\n",
            "Iteration 262, loss = 0.57825698\n",
            "Iteration 263, loss = 0.57745967\n",
            "Iteration 264, loss = 0.57665966\n",
            "Iteration 265, loss = 0.57586169\n",
            "Iteration 266, loss = 0.57501876\n",
            "Iteration 267, loss = 0.57419665\n",
            "Iteration 268, loss = 0.57336623\n",
            "Iteration 269, loss = 0.57254890\n",
            "Iteration 270, loss = 0.57174942\n",
            "Iteration 271, loss = 0.57096336\n",
            "Iteration 272, loss = 0.57014120\n",
            "Iteration 273, loss = 0.56930886\n",
            "Iteration 274, loss = 0.56849685\n",
            "Iteration 275, loss = 0.56768595\n",
            "Iteration 276, loss = 0.56686885\n",
            "Iteration 277, loss = 0.56603844\n",
            "Iteration 278, loss = 0.56523605\n",
            "Iteration 279, loss = 0.56441726\n",
            "Iteration 280, loss = 0.56360986\n",
            "Iteration 281, loss = 0.56280202\n",
            "Iteration 282, loss = 0.56197961\n",
            "Iteration 283, loss = 0.56115382\n",
            "Iteration 284, loss = 0.56036155\n",
            "Iteration 285, loss = 0.55955414\n",
            "Iteration 286, loss = 0.55874041\n",
            "Iteration 287, loss = 0.55795123\n",
            "Iteration 288, loss = 0.55713258\n",
            "Iteration 289, loss = 0.55631751\n",
            "Iteration 290, loss = 0.55553370\n",
            "Iteration 291, loss = 0.55472051\n",
            "Iteration 292, loss = 0.55392821\n",
            "Iteration 293, loss = 0.55311176\n",
            "Iteration 294, loss = 0.55229820\n",
            "Iteration 295, loss = 0.55150505\n",
            "Iteration 296, loss = 0.55071646\n",
            "Iteration 297, loss = 0.54990320\n",
            "Iteration 298, loss = 0.54908802\n",
            "Iteration 299, loss = 0.54829342\n",
            "Iteration 300, loss = 0.54750627\n",
            "Iteration 301, loss = 0.54666725\n",
            "Iteration 302, loss = 0.54587278\n",
            "Iteration 303, loss = 0.54508658\n",
            "Iteration 304, loss = 0.54429435\n",
            "Iteration 305, loss = 0.54352275\n",
            "Iteration 306, loss = 0.54274792\n",
            "Iteration 307, loss = 0.54191843\n",
            "Iteration 308, loss = 0.54118572\n",
            "Iteration 309, loss = 0.54041284\n",
            "Iteration 310, loss = 0.53969043\n",
            "Iteration 311, loss = 0.53885564\n",
            "Iteration 312, loss = 0.53809599\n",
            "Iteration 313, loss = 0.53730503\n",
            "Iteration 314, loss = 0.53653824\n",
            "Iteration 315, loss = 0.53579008\n",
            "Iteration 316, loss = 0.53497679\n",
            "Iteration 317, loss = 0.53423949\n",
            "Iteration 318, loss = 0.53344721\n",
            "Iteration 319, loss = 0.53271019\n",
            "Iteration 320, loss = 0.53198889\n",
            "Iteration 321, loss = 0.53120587\n",
            "Iteration 322, loss = 0.53047448\n",
            "Iteration 323, loss = 0.52968660\n",
            "Iteration 324, loss = 0.52898285\n",
            "Iteration 325, loss = 0.52820986\n",
            "Iteration 326, loss = 0.52748708\n",
            "Iteration 327, loss = 0.52672323\n",
            "Iteration 328, loss = 0.52598306\n",
            "Iteration 329, loss = 0.52526628\n",
            "Iteration 330, loss = 0.52455016\n",
            "Iteration 331, loss = 0.52382961\n",
            "Iteration 332, loss = 0.52311805\n",
            "Iteration 333, loss = 0.52239607\n",
            "Iteration 334, loss = 0.52173535\n",
            "Iteration 335, loss = 0.52104309\n",
            "Iteration 336, loss = 0.52038924\n",
            "Iteration 337, loss = 0.51967162\n",
            "Iteration 338, loss = 0.51901635\n",
            "Iteration 339, loss = 0.51836841\n",
            "Iteration 340, loss = 0.51767946\n",
            "Iteration 341, loss = 0.51699489\n",
            "Iteration 342, loss = 0.51633339\n",
            "Iteration 343, loss = 0.51562437\n",
            "Iteration 344, loss = 0.51495493\n",
            "Iteration 345, loss = 0.51427313\n",
            "Iteration 346, loss = 0.51361229\n",
            "Iteration 347, loss = 0.51293555\n",
            "Iteration 348, loss = 0.51227301\n",
            "Iteration 349, loss = 0.51162915\n",
            "Iteration 350, loss = 0.51094097\n",
            "Iteration 351, loss = 0.51027718\n",
            "Iteration 352, loss = 0.50963838\n",
            "Iteration 353, loss = 0.50898545\n",
            "Iteration 354, loss = 0.50834715\n",
            "Iteration 355, loss = 0.50768879\n",
            "Iteration 356, loss = 0.50710132\n",
            "Iteration 357, loss = 0.50645700\n",
            "Iteration 358, loss = 0.50586171\n",
            "Iteration 359, loss = 0.50524281\n",
            "Iteration 360, loss = 0.50462847\n",
            "Iteration 361, loss = 0.50406303\n",
            "Iteration 362, loss = 0.50346461\n",
            "Iteration 363, loss = 0.50289195\n",
            "Iteration 364, loss = 0.50227614\n",
            "Iteration 365, loss = 0.50171617\n",
            "Iteration 366, loss = 0.50116493\n",
            "Iteration 367, loss = 0.50055286\n",
            "Iteration 368, loss = 0.49999953\n",
            "Iteration 369, loss = 0.49942640\n",
            "Iteration 370, loss = 0.49884881\n",
            "Iteration 371, loss = 0.49826699\n",
            "Iteration 372, loss = 0.49769562\n",
            "Iteration 373, loss = 0.49712611\n",
            "Iteration 374, loss = 0.49660897\n",
            "Iteration 375, loss = 0.49606544\n",
            "Iteration 376, loss = 0.49553872\n",
            "Iteration 377, loss = 0.49502711\n",
            "Iteration 378, loss = 0.49450904\n",
            "Iteration 379, loss = 0.49396003\n",
            "Iteration 380, loss = 0.49344354\n",
            "Iteration 381, loss = 0.49290986\n",
            "Iteration 382, loss = 0.49242750\n",
            "Iteration 383, loss = 0.49186771\n",
            "Iteration 384, loss = 0.49139580\n",
            "Iteration 385, loss = 0.49089227\n",
            "Iteration 386, loss = 0.49033956\n",
            "Iteration 387, loss = 0.48984856\n",
            "Iteration 388, loss = 0.48939876\n",
            "Iteration 389, loss = 0.48891053\n",
            "Iteration 390, loss = 0.48844328\n",
            "Iteration 391, loss = 0.48796885\n",
            "Iteration 392, loss = 0.48754488\n",
            "Iteration 393, loss = 0.48709333\n",
            "Iteration 394, loss = 0.48667846\n",
            "Iteration 395, loss = 0.48621764\n",
            "Iteration 396, loss = 0.48574931\n",
            "Iteration 397, loss = 0.48530253\n",
            "Iteration 398, loss = 0.48488174\n",
            "Iteration 399, loss = 0.48443330\n",
            "Iteration 400, loss = 0.48398408\n",
            "Iteration 401, loss = 0.48355291\n",
            "Iteration 402, loss = 0.48313550\n",
            "Iteration 403, loss = 0.48271407\n",
            "Iteration 404, loss = 0.48231148\n",
            "Iteration 405, loss = 0.48187348\n",
            "Iteration 406, loss = 0.48145066\n",
            "Iteration 407, loss = 0.48106198\n",
            "Iteration 408, loss = 0.48061165\n",
            "Iteration 409, loss = 0.48020265\n",
            "Iteration 410, loss = 0.47978107\n",
            "Iteration 411, loss = 0.47937292\n",
            "Iteration 412, loss = 0.47898257\n",
            "Iteration 413, loss = 0.47854951\n",
            "Iteration 414, loss = 0.47814713\n",
            "Iteration 415, loss = 0.47777409\n",
            "Iteration 416, loss = 0.47740703\n",
            "Iteration 417, loss = 0.47702537\n",
            "Iteration 418, loss = 0.47664661\n",
            "Iteration 419, loss = 0.47629241\n",
            "Iteration 420, loss = 0.47590645\n",
            "Iteration 421, loss = 0.47557349\n",
            "Iteration 422, loss = 0.47522770\n",
            "Iteration 423, loss = 0.47485166\n",
            "Iteration 424, loss = 0.47455089\n",
            "Iteration 425, loss = 0.47418826\n",
            "Iteration 426, loss = 0.47387531\n",
            "Iteration 427, loss = 0.47348010\n",
            "Iteration 428, loss = 0.47315565\n",
            "Iteration 429, loss = 0.47283903\n",
            "Iteration 430, loss = 0.47247962\n",
            "Iteration 431, loss = 0.47215451\n",
            "Iteration 432, loss = 0.47185122\n",
            "Iteration 433, loss = 0.47149574\n",
            "Iteration 434, loss = 0.47121013\n",
            "Iteration 435, loss = 0.47085850\n",
            "Iteration 436, loss = 0.47053270\n",
            "Iteration 437, loss = 0.47025209\n",
            "Iteration 438, loss = 0.46991838\n",
            "Iteration 439, loss = 0.46962167\n",
            "Iteration 440, loss = 0.46932562\n",
            "Iteration 441, loss = 0.46903084\n",
            "Iteration 442, loss = 0.46874150\n",
            "Iteration 443, loss = 0.46841618\n",
            "Iteration 444, loss = 0.46813187\n",
            "Iteration 445, loss = 0.46782125\n",
            "Iteration 446, loss = 0.46753087\n",
            "Iteration 447, loss = 0.46722197\n",
            "Iteration 448, loss = 0.46694368\n",
            "Iteration 449, loss = 0.46664235\n",
            "Iteration 450, loss = 0.46639219\n",
            "Iteration 451, loss = 0.46609468\n",
            "Iteration 452, loss = 0.46583017\n",
            "Iteration 453, loss = 0.46557322\n",
            "Iteration 454, loss = 0.46528594\n",
            "Iteration 455, loss = 0.46503365\n",
            "Iteration 456, loss = 0.46476979\n",
            "Iteration 457, loss = 0.46449878\n",
            "Iteration 458, loss = 0.46426626\n",
            "Iteration 459, loss = 0.46400679\n",
            "Iteration 460, loss = 0.46376517\n",
            "Iteration 461, loss = 0.46352740\n",
            "Iteration 462, loss = 0.46331504\n",
            "Iteration 463, loss = 0.46309510\n",
            "Iteration 464, loss = 0.46285887\n",
            "Iteration 465, loss = 0.46263604\n",
            "Iteration 466, loss = 0.46243340\n",
            "Iteration 467, loss = 0.46216564\n",
            "Iteration 468, loss = 0.46196007\n",
            "Iteration 469, loss = 0.46177314\n",
            "Iteration 470, loss = 0.46155634\n",
            "Iteration 471, loss = 0.46135686\n",
            "Iteration 472, loss = 0.46115179\n",
            "Iteration 473, loss = 0.46092476\n",
            "Iteration 474, loss = 0.46071662\n",
            "Iteration 475, loss = 0.46050209\n",
            "Iteration 476, loss = 0.46030853\n",
            "Iteration 477, loss = 0.46011599\n",
            "Iteration 478, loss = 0.45993572\n",
            "Iteration 479, loss = 0.45972927\n",
            "Iteration 480, loss = 0.45955105\n",
            "Iteration 481, loss = 0.45935079\n",
            "Iteration 482, loss = 0.45917890\n",
            "Iteration 483, loss = 0.45897325\n",
            "Iteration 484, loss = 0.45878299\n",
            "Iteration 485, loss = 0.45860696\n",
            "Iteration 486, loss = 0.45844864\n",
            "Iteration 487, loss = 0.45825289\n",
            "Iteration 488, loss = 0.45807696\n",
            "Iteration 489, loss = 0.45791768\n",
            "Iteration 490, loss = 0.45778178\n",
            "Iteration 491, loss = 0.45756422\n",
            "Iteration 492, loss = 0.45748115\n",
            "Iteration 493, loss = 0.45723084\n",
            "Iteration 494, loss = 0.45707400\n",
            "Iteration 495, loss = 0.45690844\n",
            "Iteration 496, loss = 0.45671658\n",
            "Iteration 497, loss = 0.45655515\n",
            "Iteration 498, loss = 0.45641832\n",
            "Iteration 499, loss = 0.45624153\n",
            "Iteration 500, loss = 0.45609314\n",
            "Iteration 501, loss = 0.45592281\n",
            "Iteration 502, loss = 0.45579429\n",
            "Iteration 503, loss = 0.45562760\n",
            "Iteration 504, loss = 0.45545566\n",
            "Iteration 505, loss = 0.45530066\n",
            "Iteration 506, loss = 0.45514791\n",
            "Iteration 507, loss = 0.45501073\n",
            "Iteration 508, loss = 0.45483075\n",
            "Iteration 509, loss = 0.45468553\n",
            "Iteration 510, loss = 0.45453967\n",
            "Iteration 511, loss = 0.45443170\n",
            "Iteration 512, loss = 0.45427487\n",
            "Iteration 513, loss = 0.45410491\n",
            "Iteration 514, loss = 0.45396508\n",
            "Iteration 515, loss = 0.45382853\n",
            "Iteration 516, loss = 0.45372855\n",
            "Iteration 517, loss = 0.45357845\n",
            "Iteration 518, loss = 0.45344794\n",
            "Iteration 519, loss = 0.45336421\n",
            "Iteration 520, loss = 0.45319238\n",
            "Iteration 521, loss = 0.45303063\n",
            "Iteration 522, loss = 0.45293042\n",
            "Iteration 523, loss = 0.45276689\n",
            "Iteration 524, loss = 0.45264244\n",
            "Iteration 525, loss = 0.45252238\n",
            "Iteration 526, loss = 0.45239875\n",
            "Iteration 527, loss = 0.45226504\n",
            "Iteration 528, loss = 0.45217385\n",
            "Iteration 529, loss = 0.45203429\n",
            "Iteration 530, loss = 0.45193547\n",
            "Iteration 531, loss = 0.45182254\n",
            "Iteration 532, loss = 0.45168686\n",
            "Iteration 533, loss = 0.45159752\n",
            "Iteration 534, loss = 0.45147835\n",
            "Iteration 535, loss = 0.45136985\n",
            "Iteration 536, loss = 0.45125392\n",
            "Iteration 537, loss = 0.45114109\n",
            "Iteration 538, loss = 0.45106208\n",
            "Iteration 539, loss = 0.45095788\n",
            "Iteration 540, loss = 0.45083852\n",
            "Iteration 541, loss = 0.45072076\n",
            "Iteration 542, loss = 0.45064589\n",
            "Iteration 543, loss = 0.45054674\n",
            "Iteration 544, loss = 0.45045003\n",
            "Iteration 545, loss = 0.45035258\n",
            "Iteration 546, loss = 0.45027567\n",
            "Iteration 547, loss = 0.45016770\n",
            "Iteration 548, loss = 0.45008688\n",
            "Iteration 549, loss = 0.44997981\n",
            "Iteration 550, loss = 0.44988370\n",
            "Iteration 551, loss = 0.44978735\n",
            "Iteration 552, loss = 0.44968076\n",
            "Iteration 553, loss = 0.44959533\n",
            "Iteration 554, loss = 0.44948689\n",
            "Iteration 555, loss = 0.44941511\n",
            "Iteration 556, loss = 0.44931815\n",
            "Iteration 557, loss = 0.44920575\n",
            "Iteration 558, loss = 0.44912824\n",
            "Iteration 559, loss = 0.44911652\n",
            "Iteration 560, loss = 0.44898882\n",
            "Iteration 561, loss = 0.44892419\n",
            "Iteration 562, loss = 0.44886869\n",
            "Iteration 563, loss = 0.44879343\n",
            "Iteration 564, loss = 0.44873545\n",
            "Iteration 565, loss = 0.44862976\n",
            "Iteration 566, loss = 0.44853501\n",
            "Iteration 567, loss = 0.44845308\n",
            "Iteration 568, loss = 0.44834764\n",
            "Iteration 569, loss = 0.44830725\n",
            "Iteration 570, loss = 0.44822312\n",
            "Iteration 571, loss = 0.44812738\n",
            "Iteration 572, loss = 0.44800208\n",
            "Iteration 573, loss = 0.44793350\n",
            "Iteration 574, loss = 0.44785283\n",
            "Iteration 575, loss = 0.44777981\n",
            "Iteration 576, loss = 0.44769352\n",
            "Iteration 577, loss = 0.44763868\n",
            "Iteration 578, loss = 0.44764255\n",
            "Iteration 579, loss = 0.44754345\n",
            "Iteration 580, loss = 0.44748445\n",
            "Iteration 581, loss = 0.44744996\n",
            "Iteration 582, loss = 0.44725419\n",
            "Iteration 583, loss = 0.44720771\n",
            "Iteration 584, loss = 0.44710832\n",
            "Iteration 585, loss = 0.44705974\n",
            "Iteration 586, loss = 0.44695809\n",
            "Iteration 587, loss = 0.44691470\n",
            "Iteration 588, loss = 0.44681909\n",
            "Iteration 589, loss = 0.44677243\n",
            "Iteration 590, loss = 0.44670408\n",
            "Iteration 591, loss = 0.44664816\n",
            "Iteration 592, loss = 0.44658051\n",
            "Iteration 593, loss = 0.44655038\n",
            "Iteration 594, loss = 0.44645754\n",
            "Iteration 595, loss = 0.44639781\n",
            "Iteration 596, loss = 0.44633517\n",
            "Iteration 597, loss = 0.44632427\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.71637018\n",
            "Iteration 2, loss = 0.71527963\n",
            "Iteration 3, loss = 0.71423532\n",
            "Iteration 4, loss = 0.71317064\n",
            "Iteration 5, loss = 0.71217122\n",
            "Iteration 6, loss = 0.71125678\n",
            "Iteration 7, loss = 0.71027745\n",
            "Iteration 8, loss = 0.70938656\n",
            "Iteration 9, loss = 0.70852417\n",
            "Iteration 10, loss = 0.70768044\n",
            "Iteration 11, loss = 0.70692153\n",
            "Iteration 12, loss = 0.70615139\n",
            "Iteration 13, loss = 0.70538379\n",
            "Iteration 14, loss = 0.70468847\n",
            "Iteration 15, loss = 0.70406962\n",
            "Iteration 16, loss = 0.70333634\n",
            "Iteration 17, loss = 0.70269349\n",
            "Iteration 18, loss = 0.70207275\n",
            "Iteration 19, loss = 0.70146129\n",
            "Iteration 20, loss = 0.70087716\n",
            "Iteration 21, loss = 0.70037522\n",
            "Iteration 22, loss = 0.69984543\n",
            "Iteration 23, loss = 0.69935764\n",
            "Iteration 24, loss = 0.69876107\n",
            "Iteration 25, loss = 0.69835964\n",
            "Iteration 26, loss = 0.69795187\n",
            "Iteration 27, loss = 0.69758565\n",
            "Iteration 28, loss = 0.69717217\n",
            "Iteration 29, loss = 0.69686838\n",
            "Iteration 30, loss = 0.69657929\n",
            "Iteration 31, loss = 0.69620966\n",
            "Iteration 32, loss = 0.69597116\n",
            "Iteration 33, loss = 0.69577260\n",
            "Iteration 34, loss = 0.69546915\n",
            "Iteration 35, loss = 0.69517208\n",
            "Iteration 36, loss = 0.69498038\n",
            "Iteration 37, loss = 0.69480807\n",
            "Iteration 38, loss = 0.69452300\n",
            "Iteration 39, loss = 0.69431424\n",
            "Iteration 40, loss = 0.69418619\n",
            "Iteration 41, loss = 0.69394541\n",
            "Iteration 42, loss = 0.69378263\n",
            "Iteration 43, loss = 0.69364666\n",
            "Iteration 44, loss = 0.69351513\n",
            "Iteration 45, loss = 0.69333315\n",
            "Iteration 46, loss = 0.69321562\n",
            "Iteration 47, loss = 0.69308543\n",
            "Iteration 48, loss = 0.69300099\n",
            "Iteration 49, loss = 0.69288600\n",
            "Iteration 50, loss = 0.69279185\n",
            "Iteration 51, loss = 0.69272793\n",
            "Iteration 52, loss = 0.69260466\n",
            "Iteration 53, loss = 0.69251755\n",
            "Iteration 54, loss = 0.69244763\n",
            "Iteration 55, loss = 0.69237194\n",
            "Iteration 56, loss = 0.69228756\n",
            "Iteration 57, loss = 0.69220214\n",
            "Iteration 58, loss = 0.69212560\n",
            "Iteration 59, loss = 0.69204354\n",
            "Iteration 60, loss = 0.69194905\n",
            "Iteration 61, loss = 0.69187006\n",
            "Iteration 62, loss = 0.69182557\n",
            "Iteration 63, loss = 0.69171303\n",
            "Iteration 64, loss = 0.69164348\n",
            "Iteration 65, loss = 0.69155878\n",
            "Iteration 66, loss = 0.69147780\n",
            "Iteration 67, loss = 0.69138907\n",
            "Iteration 68, loss = 0.69130727\n",
            "Iteration 69, loss = 0.69127737\n",
            "Iteration 70, loss = 0.69117181\n",
            "Iteration 71, loss = 0.69107815\n",
            "Iteration 72, loss = 0.69097751\n",
            "Iteration 73, loss = 0.69087288\n",
            "Iteration 74, loss = 0.69076713\n",
            "Iteration 75, loss = 0.69065304\n",
            "Iteration 76, loss = 0.69053746\n",
            "Iteration 77, loss = 0.69041973\n",
            "Iteration 78, loss = 0.69029634\n",
            "Iteration 79, loss = 0.69017544\n",
            "Iteration 80, loss = 0.69003844\n",
            "Iteration 81, loss = 0.68989982\n",
            "Iteration 82, loss = 0.68975242\n",
            "Iteration 83, loss = 0.68962932\n",
            "Iteration 84, loss = 0.68944745\n",
            "Iteration 85, loss = 0.68929344\n",
            "Iteration 86, loss = 0.68914031\n",
            "Iteration 87, loss = 0.68897766\n",
            "Iteration 88, loss = 0.68880141\n",
            "Iteration 89, loss = 0.68862875\n",
            "Iteration 90, loss = 0.68844279\n",
            "Iteration 91, loss = 0.68825002\n",
            "Iteration 92, loss = 0.68804578\n",
            "Iteration 93, loss = 0.68783761\n",
            "Iteration 94, loss = 0.68763533\n",
            "Iteration 95, loss = 0.68743588\n",
            "Iteration 96, loss = 0.68722869\n",
            "Iteration 97, loss = 0.68699670\n",
            "Iteration 98, loss = 0.68677299\n",
            "Iteration 99, loss = 0.68653975\n",
            "Iteration 100, loss = 0.68630192\n",
            "Iteration 101, loss = 0.68606821\n",
            "Iteration 102, loss = 0.68583180\n",
            "Iteration 103, loss = 0.68556501\n",
            "Iteration 104, loss = 0.68530494\n",
            "Iteration 105, loss = 0.68503667\n",
            "Iteration 106, loss = 0.68478109\n",
            "Iteration 107, loss = 0.68449947\n",
            "Iteration 108, loss = 0.68422244\n",
            "Iteration 109, loss = 0.68393672\n",
            "Iteration 110, loss = 0.68363245\n",
            "Iteration 111, loss = 0.68333218\n",
            "Iteration 112, loss = 0.68303062\n",
            "Iteration 113, loss = 0.68272246\n",
            "Iteration 114, loss = 0.68240332\n",
            "Iteration 115, loss = 0.68206966\n",
            "Iteration 116, loss = 0.68173596\n",
            "Iteration 117, loss = 0.68142499\n",
            "Iteration 118, loss = 0.68106223\n",
            "Iteration 119, loss = 0.68070807\n",
            "Iteration 120, loss = 0.68034018\n",
            "Iteration 121, loss = 0.67998268\n",
            "Iteration 122, loss = 0.67960070\n",
            "Iteration 123, loss = 0.67923462\n",
            "Iteration 124, loss = 0.67882768\n",
            "Iteration 125, loss = 0.67845605\n",
            "Iteration 126, loss = 0.67805489\n",
            "Iteration 127, loss = 0.67764850\n",
            "Iteration 128, loss = 0.67726339\n",
            "Iteration 129, loss = 0.67685542\n",
            "Iteration 130, loss = 0.67642819\n",
            "Iteration 131, loss = 0.67601155\n",
            "Iteration 132, loss = 0.67559263\n",
            "Iteration 133, loss = 0.67517232\n",
            "Iteration 134, loss = 0.67472812\n",
            "Iteration 135, loss = 0.67427901\n",
            "Iteration 136, loss = 0.67385223\n",
            "Iteration 137, loss = 0.67339134\n",
            "Iteration 138, loss = 0.67293258\n",
            "Iteration 139, loss = 0.67249026\n",
            "Iteration 140, loss = 0.67204541\n",
            "Iteration 141, loss = 0.67156513\n",
            "Iteration 142, loss = 0.67109848\n",
            "Iteration 143, loss = 0.67063485\n",
            "Iteration 144, loss = 0.67011881\n",
            "Iteration 145, loss = 0.66963227\n",
            "Iteration 146, loss = 0.66913867\n",
            "Iteration 147, loss = 0.66864750\n",
            "Iteration 148, loss = 0.66813878\n",
            "Iteration 149, loss = 0.66761804\n",
            "Iteration 150, loss = 0.66708961\n",
            "Iteration 151, loss = 0.66658672\n",
            "Iteration 152, loss = 0.66603486\n",
            "Iteration 153, loss = 0.66552628\n",
            "Iteration 154, loss = 0.66495137\n",
            "Iteration 155, loss = 0.66440954\n",
            "Iteration 156, loss = 0.66384548\n",
            "Iteration 157, loss = 0.66324952\n",
            "Iteration 158, loss = 0.66268781\n",
            "Iteration 159, loss = 0.66211879\n",
            "Iteration 160, loss = 0.66155479\n",
            "Iteration 161, loss = 0.66091470\n",
            "Iteration 162, loss = 0.66032069\n",
            "Iteration 163, loss = 0.65971705\n",
            "Iteration 164, loss = 0.65910552\n",
            "Iteration 165, loss = 0.65850788\n",
            "Iteration 166, loss = 0.65794451\n",
            "Iteration 167, loss = 0.65728877\n",
            "Iteration 168, loss = 0.65668862\n",
            "Iteration 169, loss = 0.65608707\n",
            "Iteration 170, loss = 0.65546189\n",
            "Iteration 171, loss = 0.65484309\n",
            "Iteration 172, loss = 0.65421741\n",
            "Iteration 173, loss = 0.65359098\n",
            "Iteration 174, loss = 0.65295380\n",
            "Iteration 175, loss = 0.65236020\n",
            "Iteration 176, loss = 0.65168888\n",
            "Iteration 177, loss = 0.65103647\n",
            "Iteration 178, loss = 0.65041735\n",
            "Iteration 179, loss = 0.64976730\n",
            "Iteration 180, loss = 0.64910235\n",
            "Iteration 181, loss = 0.64843440\n",
            "Iteration 182, loss = 0.64776772\n",
            "Iteration 183, loss = 0.64705352\n",
            "Iteration 184, loss = 0.64641268\n",
            "Iteration 185, loss = 0.64572128\n",
            "Iteration 186, loss = 0.64503942\n",
            "Iteration 187, loss = 0.64433481\n",
            "Iteration 188, loss = 0.64367421\n",
            "Iteration 189, loss = 0.64298068\n",
            "Iteration 190, loss = 0.64227143\n",
            "Iteration 191, loss = 0.64156125\n",
            "Iteration 192, loss = 0.64089609\n",
            "Iteration 193, loss = 0.64016256\n",
            "Iteration 194, loss = 0.63940721\n",
            "Iteration 195, loss = 0.63873837\n",
            "Iteration 196, loss = 0.63798958\n",
            "Iteration 197, loss = 0.63726745\n",
            "Iteration 198, loss = 0.63654198\n",
            "Iteration 199, loss = 0.63587550\n",
            "Iteration 200, loss = 0.63515199\n",
            "Iteration 201, loss = 0.63442621\n",
            "Iteration 202, loss = 0.63372060\n",
            "Iteration 203, loss = 0.63299638\n",
            "Iteration 204, loss = 0.63224933\n",
            "Iteration 205, loss = 0.63153808\n",
            "Iteration 206, loss = 0.63080511\n",
            "Iteration 207, loss = 0.63004198\n",
            "Iteration 208, loss = 0.62933033\n",
            "Iteration 209, loss = 0.62856899\n",
            "Iteration 210, loss = 0.62782794\n",
            "Iteration 211, loss = 0.62704750\n",
            "Iteration 212, loss = 0.62628769\n",
            "Iteration 213, loss = 0.62550971\n",
            "Iteration 214, loss = 0.62478274\n",
            "Iteration 215, loss = 0.62398505\n",
            "Iteration 216, loss = 0.62326248\n",
            "Iteration 217, loss = 0.62246888\n",
            "Iteration 218, loss = 0.62170208\n",
            "Iteration 219, loss = 0.62090793\n",
            "Iteration 220, loss = 0.62017845\n",
            "Iteration 221, loss = 0.61933007\n",
            "Iteration 222, loss = 0.61856949\n",
            "Iteration 223, loss = 0.61779652\n",
            "Iteration 224, loss = 0.61701874\n",
            "Iteration 225, loss = 0.61623975\n",
            "Iteration 226, loss = 0.61543416\n",
            "Iteration 227, loss = 0.61465796\n",
            "Iteration 228, loss = 0.61388172\n",
            "Iteration 229, loss = 0.61307167\n",
            "Iteration 230, loss = 0.61226881\n",
            "Iteration 231, loss = 0.61149092\n",
            "Iteration 232, loss = 0.61071169\n",
            "Iteration 233, loss = 0.60991301\n",
            "Iteration 234, loss = 0.60912769\n",
            "Iteration 235, loss = 0.60833805\n",
            "Iteration 236, loss = 0.60754851\n",
            "Iteration 237, loss = 0.60674704\n",
            "Iteration 238, loss = 0.60597916\n",
            "Iteration 239, loss = 0.60514649\n",
            "Iteration 240, loss = 0.60438156\n",
            "Iteration 241, loss = 0.60358252\n",
            "Iteration 242, loss = 0.60279957\n",
            "Iteration 243, loss = 0.60201109\n",
            "Iteration 244, loss = 0.60120184\n",
            "Iteration 245, loss = 0.60046523\n",
            "Iteration 246, loss = 0.59961679\n",
            "Iteration 247, loss = 0.59884055\n",
            "Iteration 248, loss = 0.59803065\n",
            "Iteration 249, loss = 0.59720770\n",
            "Iteration 250, loss = 0.59640181\n",
            "Iteration 251, loss = 0.59563127\n",
            "Iteration 252, loss = 0.59478826\n",
            "Iteration 253, loss = 0.59399508\n",
            "Iteration 254, loss = 0.59317319\n",
            "Iteration 255, loss = 0.59237344\n",
            "Iteration 256, loss = 0.59157478\n",
            "Iteration 257, loss = 0.59077115\n",
            "Iteration 258, loss = 0.58994725\n",
            "Iteration 259, loss = 0.58914872\n",
            "Iteration 260, loss = 0.58838300\n",
            "Iteration 261, loss = 0.58757169\n",
            "Iteration 262, loss = 0.58675203\n",
            "Iteration 263, loss = 0.58598185\n",
            "Iteration 264, loss = 0.58520145\n",
            "Iteration 265, loss = 0.58439756\n",
            "Iteration 266, loss = 0.58357763\n",
            "Iteration 267, loss = 0.58279127\n",
            "Iteration 268, loss = 0.58198678\n",
            "Iteration 269, loss = 0.58117229\n",
            "Iteration 270, loss = 0.58039964\n",
            "Iteration 271, loss = 0.57959406\n",
            "Iteration 272, loss = 0.57880837\n",
            "Iteration 273, loss = 0.57795691\n",
            "Iteration 274, loss = 0.57716712\n",
            "Iteration 275, loss = 0.57639379\n",
            "Iteration 276, loss = 0.57556445\n",
            "Iteration 277, loss = 0.57476758\n",
            "Iteration 278, loss = 0.57396262\n",
            "Iteration 279, loss = 0.57317430\n",
            "Iteration 280, loss = 0.57235306\n",
            "Iteration 281, loss = 0.57154834\n",
            "Iteration 282, loss = 0.57073151\n",
            "Iteration 283, loss = 0.56993450\n",
            "Iteration 284, loss = 0.56916475\n",
            "Iteration 285, loss = 0.56837720\n",
            "Iteration 286, loss = 0.56757891\n",
            "Iteration 287, loss = 0.56679932\n",
            "Iteration 288, loss = 0.56601479\n",
            "Iteration 289, loss = 0.56521188\n",
            "Iteration 290, loss = 0.56443446\n",
            "Iteration 291, loss = 0.56363967\n",
            "Iteration 292, loss = 0.56284908\n",
            "Iteration 293, loss = 0.56205724\n",
            "Iteration 294, loss = 0.56124634\n",
            "Iteration 295, loss = 0.56046174\n",
            "Iteration 296, loss = 0.55969659\n",
            "Iteration 297, loss = 0.55890728\n",
            "Iteration 298, loss = 0.55810432\n",
            "Iteration 299, loss = 0.55733078\n",
            "Iteration 300, loss = 0.55655589\n",
            "Iteration 301, loss = 0.55572878\n",
            "Iteration 302, loss = 0.55493731\n",
            "Iteration 303, loss = 0.55414732\n",
            "Iteration 304, loss = 0.55338820\n",
            "Iteration 305, loss = 0.55262656\n",
            "Iteration 306, loss = 0.55181585\n",
            "Iteration 307, loss = 0.55103319\n",
            "Iteration 308, loss = 0.55029465\n",
            "Iteration 309, loss = 0.54954447\n",
            "Iteration 310, loss = 0.54876814\n",
            "Iteration 311, loss = 0.54797605\n",
            "Iteration 312, loss = 0.54721890\n",
            "Iteration 313, loss = 0.54647269\n",
            "Iteration 314, loss = 0.54569505\n",
            "Iteration 315, loss = 0.54501283\n",
            "Iteration 316, loss = 0.54416916\n",
            "Iteration 317, loss = 0.54344069\n",
            "Iteration 318, loss = 0.54267117\n",
            "Iteration 319, loss = 0.54193935\n",
            "Iteration 320, loss = 0.54120941\n",
            "Iteration 321, loss = 0.54041381\n",
            "Iteration 322, loss = 0.53967606\n",
            "Iteration 323, loss = 0.53893717\n",
            "Iteration 324, loss = 0.53819373\n",
            "Iteration 325, loss = 0.53743268\n",
            "Iteration 326, loss = 0.53668820\n",
            "Iteration 327, loss = 0.53594654\n",
            "Iteration 328, loss = 0.53517908\n",
            "Iteration 329, loss = 0.53447369\n",
            "Iteration 330, loss = 0.53373599\n",
            "Iteration 331, loss = 0.53300942\n",
            "Iteration 332, loss = 0.53229214\n",
            "Iteration 333, loss = 0.53156465\n",
            "Iteration 334, loss = 0.53088017\n",
            "Iteration 335, loss = 0.53018346\n",
            "Iteration 336, loss = 0.52948757\n",
            "Iteration 337, loss = 0.52877937\n",
            "Iteration 338, loss = 0.52809577\n",
            "Iteration 339, loss = 0.52741567\n",
            "Iteration 340, loss = 0.52669892\n",
            "Iteration 341, loss = 0.52602358\n",
            "Iteration 342, loss = 0.52531133\n",
            "Iteration 343, loss = 0.52461071\n",
            "Iteration 344, loss = 0.52390330\n",
            "Iteration 345, loss = 0.52321415\n",
            "Iteration 346, loss = 0.52252479\n",
            "Iteration 347, loss = 0.52182446\n",
            "Iteration 348, loss = 0.52112013\n",
            "Iteration 349, loss = 0.52044371\n",
            "Iteration 350, loss = 0.51975326\n",
            "Iteration 351, loss = 0.51904144\n",
            "Iteration 352, loss = 0.51838995\n",
            "Iteration 353, loss = 0.51768048\n",
            "Iteration 354, loss = 0.51702210\n",
            "Iteration 355, loss = 0.51633651\n",
            "Iteration 356, loss = 0.51570782\n",
            "Iteration 357, loss = 0.51502867\n",
            "Iteration 358, loss = 0.51439498\n",
            "Iteration 359, loss = 0.51373175\n",
            "Iteration 360, loss = 0.51309672\n",
            "Iteration 361, loss = 0.51250694\n",
            "Iteration 362, loss = 0.51188597\n",
            "Iteration 363, loss = 0.51126870\n",
            "Iteration 364, loss = 0.51063549\n",
            "Iteration 365, loss = 0.51004699\n",
            "Iteration 366, loss = 0.50942518\n",
            "Iteration 367, loss = 0.50880180\n",
            "Iteration 368, loss = 0.50821252\n",
            "Iteration 369, loss = 0.50758428\n",
            "Iteration 370, loss = 0.50696092\n",
            "Iteration 371, loss = 0.50634376\n",
            "Iteration 372, loss = 0.50574817\n",
            "Iteration 373, loss = 0.50510939\n",
            "Iteration 374, loss = 0.50461165\n",
            "Iteration 375, loss = 0.50396655\n",
            "Iteration 376, loss = 0.50341276\n",
            "Iteration 377, loss = 0.50285468\n",
            "Iteration 378, loss = 0.50231402\n",
            "Iteration 379, loss = 0.50171422\n",
            "Iteration 380, loss = 0.50117520\n",
            "Iteration 381, loss = 0.50060990\n",
            "Iteration 382, loss = 0.50008401\n",
            "Iteration 383, loss = 0.49950991\n",
            "Iteration 384, loss = 0.49896862\n",
            "Iteration 385, loss = 0.49846533\n",
            "Iteration 386, loss = 0.49784537\n",
            "Iteration 387, loss = 0.49730081\n",
            "Iteration 388, loss = 0.49679889\n",
            "Iteration 389, loss = 0.49626299\n",
            "Iteration 390, loss = 0.49574878\n",
            "Iteration 391, loss = 0.49523910\n",
            "Iteration 392, loss = 0.49477783\n",
            "Iteration 393, loss = 0.49428036\n",
            "Iteration 394, loss = 0.49380799\n",
            "Iteration 395, loss = 0.49331085\n",
            "Iteration 396, loss = 0.49283484\n",
            "Iteration 397, loss = 0.49235958\n",
            "Iteration 398, loss = 0.49191592\n",
            "Iteration 399, loss = 0.49142534\n",
            "Iteration 400, loss = 0.49093057\n",
            "Iteration 401, loss = 0.49046991\n",
            "Iteration 402, loss = 0.49002900\n",
            "Iteration 403, loss = 0.48955986\n",
            "Iteration 404, loss = 0.48913670\n",
            "Iteration 405, loss = 0.48867528\n",
            "Iteration 406, loss = 0.48821874\n",
            "Iteration 407, loss = 0.48778442\n",
            "Iteration 408, loss = 0.48730434\n",
            "Iteration 409, loss = 0.48690231\n",
            "Iteration 410, loss = 0.48642340\n",
            "Iteration 411, loss = 0.48599581\n",
            "Iteration 412, loss = 0.48555865\n",
            "Iteration 413, loss = 0.48511006\n",
            "Iteration 414, loss = 0.48468484\n",
            "Iteration 415, loss = 0.48427131\n",
            "Iteration 416, loss = 0.48388095\n",
            "Iteration 417, loss = 0.48345863\n",
            "Iteration 418, loss = 0.48303253\n",
            "Iteration 419, loss = 0.48265972\n",
            "Iteration 420, loss = 0.48223428\n",
            "Iteration 421, loss = 0.48185365\n",
            "Iteration 422, loss = 0.48150231\n",
            "Iteration 423, loss = 0.48108867\n",
            "Iteration 424, loss = 0.48076936\n",
            "Iteration 425, loss = 0.48035725\n",
            "Iteration 426, loss = 0.48000784\n",
            "Iteration 427, loss = 0.47961584\n",
            "Iteration 428, loss = 0.47926782\n",
            "Iteration 429, loss = 0.47891778\n",
            "Iteration 430, loss = 0.47851323\n",
            "Iteration 431, loss = 0.47821718\n",
            "Iteration 432, loss = 0.47787359\n",
            "Iteration 433, loss = 0.47749611\n",
            "Iteration 434, loss = 0.47716036\n",
            "Iteration 435, loss = 0.47677639\n",
            "Iteration 436, loss = 0.47644124\n",
            "Iteration 437, loss = 0.47610275\n",
            "Iteration 438, loss = 0.47577389\n",
            "Iteration 439, loss = 0.47544768\n",
            "Iteration 440, loss = 0.47513752\n",
            "Iteration 441, loss = 0.47481443\n",
            "Iteration 442, loss = 0.47449605\n",
            "Iteration 443, loss = 0.47418372\n",
            "Iteration 444, loss = 0.47387970\n",
            "Iteration 445, loss = 0.47355069\n",
            "Iteration 446, loss = 0.47322440\n",
            "Iteration 447, loss = 0.47292056\n",
            "Iteration 448, loss = 0.47262010\n",
            "Iteration 449, loss = 0.47229801\n",
            "Iteration 450, loss = 0.47201151\n",
            "Iteration 451, loss = 0.47172010\n",
            "Iteration 452, loss = 0.47144318\n",
            "Iteration 453, loss = 0.47115017\n",
            "Iteration 454, loss = 0.47085952\n",
            "Iteration 455, loss = 0.47059594\n",
            "Iteration 456, loss = 0.47031546\n",
            "Iteration 457, loss = 0.47003326\n",
            "Iteration 458, loss = 0.46977195\n",
            "Iteration 459, loss = 0.46951333\n",
            "Iteration 460, loss = 0.46924640\n",
            "Iteration 461, loss = 0.46901358\n",
            "Iteration 462, loss = 0.46877255\n",
            "Iteration 463, loss = 0.46852404\n",
            "Iteration 464, loss = 0.46827300\n",
            "Iteration 465, loss = 0.46803031\n",
            "Iteration 466, loss = 0.46779763\n",
            "Iteration 467, loss = 0.46753623\n",
            "Iteration 468, loss = 0.46729199\n",
            "Iteration 469, loss = 0.46707845\n",
            "Iteration 470, loss = 0.46684106\n",
            "Iteration 471, loss = 0.46661724\n",
            "Iteration 472, loss = 0.46639099\n",
            "Iteration 473, loss = 0.46614725\n",
            "Iteration 474, loss = 0.46590741\n",
            "Iteration 475, loss = 0.46569541\n",
            "Iteration 476, loss = 0.46548231\n",
            "Iteration 477, loss = 0.46528230\n",
            "Iteration 478, loss = 0.46507834\n",
            "Iteration 479, loss = 0.46484493\n",
            "Iteration 480, loss = 0.46464896\n",
            "Iteration 481, loss = 0.46443225\n",
            "Iteration 482, loss = 0.46423258\n",
            "Iteration 483, loss = 0.46402216\n",
            "Iteration 484, loss = 0.46380008\n",
            "Iteration 485, loss = 0.46360026\n",
            "Iteration 486, loss = 0.46345768\n",
            "Iteration 487, loss = 0.46322502\n",
            "Iteration 488, loss = 0.46303014\n",
            "Iteration 489, loss = 0.46285048\n",
            "Iteration 490, loss = 0.46266738\n",
            "Iteration 491, loss = 0.46246285\n",
            "Iteration 492, loss = 0.46231692\n",
            "Iteration 493, loss = 0.46207652\n",
            "Iteration 494, loss = 0.46188103\n",
            "Iteration 495, loss = 0.46172809\n",
            "Iteration 496, loss = 0.46151756\n",
            "Iteration 497, loss = 0.46134005\n",
            "Iteration 498, loss = 0.46119853\n",
            "Iteration 499, loss = 0.46103342\n",
            "Iteration 500, loss = 0.46082626\n",
            "Iteration 501, loss = 0.46065588\n",
            "Iteration 502, loss = 0.46050421\n",
            "Iteration 503, loss = 0.46032667\n",
            "Iteration 504, loss = 0.46013407\n",
            "Iteration 505, loss = 0.45997980\n",
            "Iteration 506, loss = 0.45982104\n",
            "Iteration 507, loss = 0.45965487\n",
            "Iteration 508, loss = 0.45948719\n",
            "Iteration 509, loss = 0.45934466\n",
            "Iteration 510, loss = 0.45915393\n",
            "Iteration 511, loss = 0.45902606\n",
            "Iteration 512, loss = 0.45887592\n",
            "Iteration 513, loss = 0.45870516\n",
            "Iteration 514, loss = 0.45853836\n",
            "Iteration 515, loss = 0.45838507\n",
            "Iteration 516, loss = 0.45826340\n",
            "Iteration 517, loss = 0.45811238\n",
            "Iteration 518, loss = 0.45797138\n",
            "Iteration 519, loss = 0.45788550\n",
            "Iteration 520, loss = 0.45769447\n",
            "Iteration 521, loss = 0.45751845\n",
            "Iteration 522, loss = 0.45741359\n",
            "Iteration 523, loss = 0.45722226\n",
            "Iteration 524, loss = 0.45709109\n",
            "Iteration 525, loss = 0.45695723\n",
            "Iteration 526, loss = 0.45682852\n",
            "Iteration 527, loss = 0.45669125\n",
            "Iteration 528, loss = 0.45657988\n",
            "Iteration 529, loss = 0.45642798\n",
            "Iteration 530, loss = 0.45631421\n",
            "Iteration 531, loss = 0.45621145\n",
            "Iteration 532, loss = 0.45604706\n",
            "Iteration 533, loss = 0.45597719\n",
            "Iteration 534, loss = 0.45581872\n",
            "Iteration 535, loss = 0.45569770\n",
            "Iteration 536, loss = 0.45558796\n",
            "Iteration 537, loss = 0.45544944\n",
            "Iteration 538, loss = 0.45536643\n",
            "Iteration 539, loss = 0.45523287\n",
            "Iteration 540, loss = 0.45512203\n",
            "Iteration 541, loss = 0.45500715\n",
            "Iteration 542, loss = 0.45489991\n",
            "Iteration 543, loss = 0.45479486\n",
            "Iteration 544, loss = 0.45468528\n",
            "Iteration 545, loss = 0.45458391\n",
            "Iteration 546, loss = 0.45450474\n",
            "Iteration 547, loss = 0.45439259\n",
            "Iteration 548, loss = 0.45429990\n",
            "Iteration 549, loss = 0.45420497\n",
            "Iteration 550, loss = 0.45409226\n",
            "Iteration 551, loss = 0.45400478\n",
            "Iteration 552, loss = 0.45388374\n",
            "Iteration 553, loss = 0.45378523\n",
            "Iteration 554, loss = 0.45367004\n",
            "Iteration 555, loss = 0.45358161\n",
            "Iteration 556, loss = 0.45348106\n",
            "Iteration 557, loss = 0.45336168\n",
            "Iteration 558, loss = 0.45329837\n",
            "Iteration 559, loss = 0.45325499\n",
            "Iteration 560, loss = 0.45312531\n",
            "Iteration 561, loss = 0.45306680\n",
            "Iteration 562, loss = 0.45300276\n",
            "Iteration 563, loss = 0.45290879\n",
            "Iteration 564, loss = 0.45285732\n",
            "Iteration 565, loss = 0.45275439\n",
            "Iteration 566, loss = 0.45264111\n",
            "Iteration 567, loss = 0.45256315\n",
            "Iteration 568, loss = 0.45246060\n",
            "Iteration 569, loss = 0.45239402\n",
            "Iteration 570, loss = 0.45230937\n",
            "Iteration 571, loss = 0.45218608\n",
            "Iteration 572, loss = 0.45209323\n",
            "Iteration 573, loss = 0.45201574\n",
            "Iteration 574, loss = 0.45193071\n",
            "Iteration 575, loss = 0.45184230\n",
            "Iteration 576, loss = 0.45173550\n",
            "Iteration 577, loss = 0.45167862\n",
            "Iteration 578, loss = 0.45171984\n",
            "Iteration 579, loss = 0.45157201\n",
            "Iteration 580, loss = 0.45150466\n",
            "Iteration 581, loss = 0.45144760\n",
            "Iteration 582, loss = 0.45129646\n",
            "Iteration 583, loss = 0.45124265\n",
            "Iteration 584, loss = 0.45113991\n",
            "Iteration 585, loss = 0.45107550\n",
            "Iteration 586, loss = 0.45099295\n",
            "Iteration 587, loss = 0.45095242\n",
            "Iteration 588, loss = 0.45084974\n",
            "Iteration 589, loss = 0.45079107\n",
            "Iteration 590, loss = 0.45074820\n",
            "Iteration 591, loss = 0.45067715\n",
            "Iteration 592, loss = 0.45061619\n",
            "Iteration 593, loss = 0.45057526\n",
            "Iteration 594, loss = 0.45048656\n",
            "Iteration 595, loss = 0.45040975\n",
            "Iteration 596, loss = 0.45036328\n",
            "Iteration 597, loss = 0.45031893\n",
            "Iteration 598, loss = 0.45023845\n",
            "Iteration 599, loss = 0.45016659\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.71711807\n",
            "Iteration 2, loss = 0.71586397\n",
            "Iteration 3, loss = 0.71493458\n",
            "Iteration 4, loss = 0.71377472\n",
            "Iteration 5, loss = 0.71275038\n",
            "Iteration 6, loss = 0.71184623\n",
            "Iteration 7, loss = 0.71078662\n",
            "Iteration 8, loss = 0.70982546\n",
            "Iteration 9, loss = 0.70897576\n",
            "Iteration 10, loss = 0.70801571\n",
            "Iteration 11, loss = 0.70718221\n",
            "Iteration 12, loss = 0.70635551\n",
            "Iteration 13, loss = 0.70551483\n",
            "Iteration 14, loss = 0.70485136\n",
            "Iteration 15, loss = 0.70400059\n",
            "Iteration 16, loss = 0.70337484\n",
            "Iteration 17, loss = 0.70260910\n",
            "Iteration 18, loss = 0.70200277\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.71712569\n",
            "Iteration 2, loss = 0.71593409\n",
            "Iteration 3, loss = 0.71498259\n",
            "Iteration 4, loss = 0.71391058\n",
            "Iteration 5, loss = 0.71285265\n",
            "Iteration 6, loss = 0.71200107\n",
            "Iteration 7, loss = 0.71099403\n",
            "Iteration 8, loss = 0.71004458\n",
            "Iteration 9, loss = 0.70919876\n",
            "Iteration 10, loss = 0.70826713\n",
            "Iteration 11, loss = 0.70746579\n",
            "Iteration 12, loss = 0.70660958\n",
            "Iteration 13, loss = 0.70579791\n",
            "Iteration 14, loss = 0.70511576\n",
            "Iteration 15, loss = 0.70435527\n",
            "Iteration 16, loss = 0.70366970\n",
            "Iteration 17, loss = 0.70291417\n",
            "Iteration 18, loss = 0.70228020\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.71708868\n",
            "Iteration 2, loss = 0.71585281\n",
            "Iteration 3, loss = 0.71482846\n",
            "Iteration 4, loss = 0.71373929\n",
            "Iteration 5, loss = 0.71264244\n",
            "Iteration 6, loss = 0.71179257\n",
            "Iteration 7, loss = 0.71070568\n",
            "Iteration 8, loss = 0.70972092\n",
            "Iteration 9, loss = 0.70882161\n",
            "Iteration 10, loss = 0.70793441\n",
            "Iteration 11, loss = 0.70714118\n",
            "Iteration 12, loss = 0.70631186\n",
            "Iteration 13, loss = 0.70547731\n",
            "Iteration 14, loss = 0.70481863\n",
            "Iteration 15, loss = 0.70406213\n",
            "Iteration 16, loss = 0.70342998\n",
            "Iteration 17, loss = 0.70265306\n",
            "Iteration 18, loss = 0.70208012\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.71709503\n",
            "Iteration 2, loss = 0.71588915\n",
            "Iteration 3, loss = 0.71491711\n",
            "Iteration 4, loss = 0.71383716\n",
            "Iteration 5, loss = 0.71278470\n",
            "Iteration 6, loss = 0.71190597\n",
            "Iteration 7, loss = 0.71086813\n",
            "Iteration 8, loss = 0.70990736\n",
            "Iteration 9, loss = 0.70900638\n",
            "Iteration 10, loss = 0.70812529\n",
            "Iteration 11, loss = 0.70733261\n",
            "Iteration 12, loss = 0.70642457\n",
            "Iteration 13, loss = 0.70559984\n",
            "Iteration 14, loss = 0.70493451\n",
            "Iteration 15, loss = 0.70420525\n",
            "Iteration 16, loss = 0.70349177\n",
            "Iteration 17, loss = 0.70279350\n",
            "Iteration 18, loss = 0.70214295\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.71703974\n",
            "Iteration 2, loss = 0.71588542\n",
            "Iteration 3, loss = 0.71477858\n",
            "Iteration 4, loss = 0.71367209\n",
            "Iteration 5, loss = 0.71256737\n",
            "Iteration 6, loss = 0.71174364\n",
            "Iteration 7, loss = 0.71066989\n",
            "Iteration 8, loss = 0.70971830\n",
            "Iteration 9, loss = 0.70882839\n",
            "Iteration 10, loss = 0.70801967\n",
            "Iteration 11, loss = 0.70721694\n",
            "Iteration 12, loss = 0.70637381\n",
            "Iteration 13, loss = 0.70558334\n",
            "Iteration 14, loss = 0.70493271\n",
            "Iteration 15, loss = 0.70424014\n",
            "Iteration 16, loss = 0.70351352\n",
            "Iteration 17, loss = 0.70287053\n",
            "Iteration 18, loss = 0.70226272\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.71637661\n",
            "Iteration 2, loss = 0.71525936\n",
            "Iteration 3, loss = 0.71415897\n",
            "Iteration 4, loss = 0.71313144\n",
            "Iteration 5, loss = 0.71199846\n",
            "Iteration 6, loss = 0.71113812\n",
            "Iteration 7, loss = 0.71014127\n",
            "Iteration 8, loss = 0.70918612\n",
            "Iteration 9, loss = 0.70834956\n",
            "Iteration 10, loss = 0.70749947\n",
            "Iteration 11, loss = 0.70672606\n",
            "Iteration 12, loss = 0.70590015\n",
            "Iteration 13, loss = 0.70517982\n",
            "Iteration 14, loss = 0.70442197\n",
            "Iteration 15, loss = 0.70376690\n",
            "Iteration 16, loss = 0.70308132\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.71633851\n",
            "Iteration 2, loss = 0.71527056\n",
            "Iteration 3, loss = 0.71423059\n",
            "Iteration 4, loss = 0.71312904\n",
            "Iteration 5, loss = 0.71213935\n",
            "Iteration 6, loss = 0.71118687\n",
            "Iteration 7, loss = 0.71024315\n",
            "Iteration 8, loss = 0.70924614\n",
            "Iteration 9, loss = 0.70841515\n",
            "Iteration 10, loss = 0.70756432\n",
            "Iteration 11, loss = 0.70674513\n",
            "Iteration 12, loss = 0.70601579\n",
            "Iteration 13, loss = 0.70524084\n",
            "Iteration 14, loss = 0.70447708\n",
            "Iteration 15, loss = 0.70384692\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.71633136\n",
            "Iteration 2, loss = 0.71528775\n",
            "Iteration 3, loss = 0.71419178\n",
            "Iteration 4, loss = 0.71314489\n",
            "Iteration 5, loss = 0.71211780\n",
            "Iteration 6, loss = 0.71113406\n",
            "Iteration 7, loss = 0.71021722\n",
            "Iteration 8, loss = 0.70924399\n",
            "Iteration 9, loss = 0.70836311\n",
            "Iteration 10, loss = 0.70755755\n",
            "Iteration 11, loss = 0.70676380\n",
            "Iteration 12, loss = 0.70605033\n",
            "Iteration 13, loss = 0.70530609\n",
            "Iteration 14, loss = 0.70454170\n",
            "Iteration 15, loss = 0.70397148\n",
            "Iteration 16, loss = 0.70322229\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.71632995\n",
            "Iteration 2, loss = 0.71524002\n",
            "Iteration 3, loss = 0.71415568\n",
            "Iteration 4, loss = 0.71311245\n",
            "Iteration 5, loss = 0.71210480\n",
            "Iteration 6, loss = 0.71112511\n",
            "Iteration 7, loss = 0.71017822\n",
            "Iteration 8, loss = 0.70927331\n",
            "Iteration 9, loss = 0.70836170\n",
            "Iteration 10, loss = 0.70753141\n",
            "Iteration 11, loss = 0.70678646\n",
            "Iteration 12, loss = 0.70597518\n",
            "Iteration 13, loss = 0.70525100\n",
            "Iteration 14, loss = 0.70450237\n",
            "Iteration 15, loss = 0.70390566\n",
            "Iteration 16, loss = 0.70312369\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.71637018\n",
            "Iteration 2, loss = 0.71527963\n",
            "Iteration 3, loss = 0.71423532\n",
            "Iteration 4, loss = 0.71317064\n",
            "Iteration 5, loss = 0.71217122\n",
            "Iteration 6, loss = 0.71125678\n",
            "Iteration 7, loss = 0.71027745\n",
            "Iteration 8, loss = 0.70938656\n",
            "Iteration 9, loss = 0.70852417\n",
            "Iteration 10, loss = 0.70768044\n",
            "Iteration 11, loss = 0.70692153\n",
            "Iteration 12, loss = 0.70615139\n",
            "Iteration 13, loss = 0.70538379\n",
            "Iteration 14, loss = 0.70468847\n",
            "Iteration 15, loss = 0.70406962\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.81059396\n",
            "Iteration 2, loss = 0.80947781\n",
            "Iteration 3, loss = 0.80772490\n",
            "Iteration 4, loss = 0.80559245\n",
            "Iteration 5, loss = 0.80320885\n",
            "Iteration 6, loss = 0.80073822\n",
            "Iteration 7, loss = 0.79831709\n",
            "Iteration 8, loss = 0.79566829\n",
            "Iteration 9, loss = 0.79314901\n",
            "Iteration 10, loss = 0.79052377\n",
            "Iteration 11, loss = 0.78798018\n",
            "Iteration 12, loss = 0.78545524\n",
            "Iteration 13, loss = 0.78303482\n",
            "Iteration 14, loss = 0.78061982\n",
            "Iteration 15, loss = 0.77815844\n",
            "Iteration 16, loss = 0.77580966\n",
            "Iteration 17, loss = 0.77367297\n",
            "Iteration 18, loss = 0.77137954\n",
            "Iteration 19, loss = 0.76915508\n",
            "Iteration 20, loss = 0.76697622\n",
            "Iteration 21, loss = 0.76479181\n",
            "Iteration 22, loss = 0.76276593\n",
            "Iteration 23, loss = 0.76074046\n",
            "Iteration 24, loss = 0.75882433\n",
            "Iteration 25, loss = 0.75680439\n",
            "Iteration 26, loss = 0.75495299\n",
            "Iteration 27, loss = 0.75304353\n",
            "Iteration 28, loss = 0.75133965\n",
            "Iteration 29, loss = 0.74941787\n",
            "Iteration 30, loss = 0.74775859\n",
            "Iteration 31, loss = 0.74598920\n",
            "Iteration 32, loss = 0.74431791\n",
            "Iteration 33, loss = 0.74272727\n",
            "Iteration 34, loss = 0.74108175\n",
            "Iteration 35, loss = 0.73950682\n",
            "Iteration 36, loss = 0.73793804\n",
            "Iteration 37, loss = 0.73640962\n",
            "Iteration 38, loss = 0.73496498\n",
            "Iteration 39, loss = 0.73355344\n",
            "Iteration 40, loss = 0.73206265\n",
            "Iteration 41, loss = 0.73063771\n",
            "Iteration 42, loss = 0.72932165\n",
            "Iteration 43, loss = 0.72786582\n",
            "Iteration 44, loss = 0.72668935\n",
            "Iteration 45, loss = 0.72534010\n",
            "Iteration 46, loss = 0.72416583\n",
            "Iteration 47, loss = 0.72290570\n",
            "Iteration 48, loss = 0.72169007\n",
            "Iteration 49, loss = 0.72057663\n",
            "Iteration 50, loss = 0.71945553\n",
            "Iteration 51, loss = 0.71832054\n",
            "Iteration 52, loss = 0.71726250\n",
            "Iteration 53, loss = 0.71621480\n",
            "Iteration 54, loss = 0.71514747\n",
            "Iteration 55, loss = 0.71420619\n",
            "Iteration 56, loss = 0.71323766\n",
            "Iteration 57, loss = 0.71231156\n",
            "Iteration 58, loss = 0.71141527\n",
            "Iteration 59, loss = 0.71041918\n",
            "Iteration 60, loss = 0.70962512\n",
            "Iteration 61, loss = 0.70874544\n",
            "Iteration 62, loss = 0.70794062\n",
            "Iteration 63, loss = 0.70705091\n",
            "Iteration 64, loss = 0.70630716\n",
            "Iteration 65, loss = 0.70551470\n",
            "Iteration 66, loss = 0.70477311\n",
            "Iteration 67, loss = 0.70401869\n",
            "Iteration 68, loss = 0.70328557\n",
            "Iteration 69, loss = 0.70256086\n",
            "Iteration 70, loss = 0.70192119\n",
            "Iteration 71, loss = 0.70126641\n",
            "Iteration 72, loss = 0.70056272\n",
            "Iteration 73, loss = 0.69996793\n",
            "Iteration 74, loss = 0.69933505\n",
            "Iteration 75, loss = 0.69873728\n",
            "Iteration 76, loss = 0.69812743\n",
            "Iteration 77, loss = 0.69754379\n",
            "Iteration 78, loss = 0.69700411\n",
            "Iteration 79, loss = 0.69642284\n",
            "Iteration 80, loss = 0.69591109\n",
            "Iteration 81, loss = 0.69534092\n",
            "Iteration 82, loss = 0.69482910\n",
            "Iteration 83, loss = 0.69431794\n",
            "Iteration 84, loss = 0.69383298\n",
            "Iteration 85, loss = 0.69334945\n",
            "Iteration 86, loss = 0.69283728\n",
            "Iteration 87, loss = 0.69240772\n",
            "Iteration 88, loss = 0.69194827\n",
            "Iteration 89, loss = 0.69149210\n",
            "Iteration 90, loss = 0.69105950\n",
            "Iteration 91, loss = 0.69061818\n",
            "Iteration 92, loss = 0.69017087\n",
            "Iteration 93, loss = 0.68982034\n",
            "Iteration 94, loss = 0.68940824\n",
            "Iteration 95, loss = 0.68898536\n",
            "Iteration 96, loss = 0.68857253\n",
            "Iteration 97, loss = 0.68821509\n",
            "Iteration 98, loss = 0.68781055\n",
            "Iteration 99, loss = 0.68746762\n",
            "Iteration 100, loss = 0.68712164\n",
            "Iteration 101, loss = 0.68672461\n",
            "Iteration 102, loss = 0.68634595\n",
            "Iteration 103, loss = 0.68598454\n",
            "Iteration 104, loss = 0.68566814\n",
            "Iteration 105, loss = 0.68529516\n",
            "Iteration 106, loss = 0.68495195\n",
            "Iteration 107, loss = 0.68463611\n",
            "Iteration 108, loss = 0.68426193\n",
            "Iteration 109, loss = 0.68395278\n",
            "Iteration 110, loss = 0.68361494\n",
            "Iteration 111, loss = 0.68329764\n",
            "Iteration 112, loss = 0.68297143\n",
            "Iteration 113, loss = 0.68264024\n",
            "Iteration 114, loss = 0.68233426\n",
            "Iteration 115, loss = 0.68200655\n",
            "Iteration 116, loss = 0.68171389\n",
            "Iteration 117, loss = 0.68139209\n",
            "Iteration 118, loss = 0.68109237\n",
            "Iteration 119, loss = 0.68077061\n",
            "Iteration 120, loss = 0.68047026\n",
            "Iteration 121, loss = 0.68017066\n",
            "Iteration 122, loss = 0.67986731\n",
            "Iteration 123, loss = 0.67956891\n",
            "Iteration 124, loss = 0.67927297\n",
            "Iteration 125, loss = 0.67900593\n",
            "Iteration 126, loss = 0.67867023\n",
            "Iteration 127, loss = 0.67837558\n",
            "Iteration 128, loss = 0.67809149\n",
            "Iteration 129, loss = 0.67780275\n",
            "Iteration 130, loss = 0.67749527\n",
            "Iteration 131, loss = 0.67721986\n",
            "Iteration 132, loss = 0.67691570\n",
            "Iteration 133, loss = 0.67661248\n",
            "Iteration 134, loss = 0.67633363\n",
            "Iteration 135, loss = 0.67603716\n",
            "Iteration 136, loss = 0.67574882\n",
            "Iteration 137, loss = 0.67544021\n",
            "Iteration 138, loss = 0.67514979\n",
            "Iteration 139, loss = 0.67485153\n",
            "Iteration 140, loss = 0.67458581\n",
            "Iteration 141, loss = 0.67426964\n",
            "Iteration 142, loss = 0.67396245\n",
            "Iteration 143, loss = 0.67367986\n",
            "Iteration 144, loss = 0.67338164\n",
            "Iteration 145, loss = 0.67307127\n",
            "Iteration 146, loss = 0.67280023\n",
            "Iteration 147, loss = 0.67247641\n",
            "Iteration 148, loss = 0.67216745\n",
            "Iteration 149, loss = 0.67185824\n",
            "Iteration 150, loss = 0.67156342\n",
            "Iteration 151, loss = 0.67125271\n",
            "Iteration 152, loss = 0.67095216\n",
            "Iteration 153, loss = 0.67063168\n",
            "Iteration 154, loss = 0.67031587\n",
            "Iteration 155, loss = 0.67000412\n",
            "Iteration 156, loss = 0.66969848\n",
            "Iteration 157, loss = 0.66937672\n",
            "Iteration 158, loss = 0.66904960\n",
            "Iteration 159, loss = 0.66872528\n",
            "Iteration 160, loss = 0.66840592\n",
            "Iteration 161, loss = 0.66807552\n",
            "Iteration 162, loss = 0.66775977\n",
            "Iteration 163, loss = 0.66741724\n",
            "Iteration 164, loss = 0.66708146\n",
            "Iteration 165, loss = 0.66676370\n",
            "Iteration 166, loss = 0.66644075\n",
            "Iteration 167, loss = 0.66610752\n",
            "Iteration 168, loss = 0.66577046\n",
            "Iteration 169, loss = 0.66543408\n",
            "Iteration 170, loss = 0.66512061\n",
            "Iteration 171, loss = 0.66480762\n",
            "Iteration 172, loss = 0.66448486\n",
            "Iteration 173, loss = 0.66418769\n",
            "Iteration 174, loss = 0.66385439\n",
            "Iteration 175, loss = 0.66355917\n",
            "Iteration 176, loss = 0.66324964\n",
            "Iteration 177, loss = 0.66295336\n",
            "Iteration 178, loss = 0.66265022\n",
            "Iteration 179, loss = 0.66234364\n",
            "Iteration 180, loss = 0.66205382\n",
            "Iteration 181, loss = 0.66175723\n",
            "Iteration 182, loss = 0.66144865\n",
            "Iteration 183, loss = 0.66113742\n",
            "Iteration 184, loss = 0.66084933\n",
            "Iteration 185, loss = 0.66053439\n",
            "Iteration 186, loss = 0.66023432\n",
            "Iteration 187, loss = 0.65990254\n",
            "Iteration 188, loss = 0.65960478\n",
            "Iteration 189, loss = 0.65930537\n",
            "Iteration 190, loss = 0.65900663\n",
            "Iteration 191, loss = 0.65869727\n",
            "Iteration 192, loss = 0.65841615\n",
            "Iteration 193, loss = 0.65809593\n",
            "Iteration 194, loss = 0.65780307\n",
            "Iteration 195, loss = 0.65750764\n",
            "Iteration 196, loss = 0.65720868\n",
            "Iteration 197, loss = 0.65689272\n",
            "Iteration 198, loss = 0.65658584\n",
            "Iteration 199, loss = 0.65629207\n",
            "Iteration 200, loss = 0.65597635\n",
            "Iteration 201, loss = 0.65567106\n",
            "Iteration 202, loss = 0.65537064\n",
            "Iteration 203, loss = 0.65504950\n",
            "Iteration 204, loss = 0.65474583\n",
            "Iteration 205, loss = 0.65441790\n",
            "Iteration 206, loss = 0.65410490\n",
            "Iteration 207, loss = 0.65379478\n",
            "Iteration 208, loss = 0.65348850\n",
            "Iteration 209, loss = 0.65317982\n",
            "Iteration 210, loss = 0.65285541\n",
            "Iteration 211, loss = 0.65252238\n",
            "Iteration 212, loss = 0.65219647\n",
            "Iteration 213, loss = 0.65187779\n",
            "Iteration 214, loss = 0.65155218\n",
            "Iteration 215, loss = 0.65122684\n",
            "Iteration 216, loss = 0.65090710\n",
            "Iteration 217, loss = 0.65055566\n",
            "Iteration 218, loss = 0.65021428\n",
            "Iteration 219, loss = 0.64990996\n",
            "Iteration 220, loss = 0.64955212\n",
            "Iteration 221, loss = 0.64921266\n",
            "Iteration 222, loss = 0.64886987\n",
            "Iteration 223, loss = 0.64853322\n",
            "Iteration 224, loss = 0.64819082\n",
            "Iteration 225, loss = 0.64784320\n",
            "Iteration 226, loss = 0.64749197\n",
            "Iteration 227, loss = 0.64712873\n",
            "Iteration 228, loss = 0.64677404\n",
            "Iteration 229, loss = 0.64641405\n",
            "Iteration 230, loss = 0.64607861\n",
            "Iteration 231, loss = 0.64570352\n",
            "Iteration 232, loss = 0.64535019\n",
            "Iteration 233, loss = 0.64496808\n",
            "Iteration 234, loss = 0.64461339\n",
            "Iteration 235, loss = 0.64422982\n",
            "Iteration 236, loss = 0.64384983\n",
            "Iteration 237, loss = 0.64348667\n",
            "Iteration 238, loss = 0.64310001\n",
            "Iteration 239, loss = 0.64271693\n",
            "Iteration 240, loss = 0.64230695\n",
            "Iteration 241, loss = 0.64192167\n",
            "Iteration 242, loss = 0.64151198\n",
            "Iteration 243, loss = 0.64111513\n",
            "Iteration 244, loss = 0.64070241\n",
            "Iteration 245, loss = 0.64028073\n",
            "Iteration 246, loss = 0.63986397\n",
            "Iteration 247, loss = 0.63944628\n",
            "Iteration 248, loss = 0.63898903\n",
            "Iteration 249, loss = 0.63858379\n",
            "Iteration 250, loss = 0.63811070\n",
            "Iteration 251, loss = 0.63765526\n",
            "Iteration 252, loss = 0.63718647\n",
            "Iteration 253, loss = 0.63671348\n",
            "Iteration 254, loss = 0.63621961\n",
            "Iteration 255, loss = 0.63572778\n",
            "Iteration 256, loss = 0.63522628\n",
            "Iteration 257, loss = 0.63472775\n",
            "Iteration 258, loss = 0.63419666\n",
            "Iteration 259, loss = 0.63367278\n",
            "Iteration 260, loss = 0.63313342\n",
            "Iteration 261, loss = 0.63258849\n",
            "Iteration 262, loss = 0.63205979\n",
            "Iteration 263, loss = 0.63149927\n",
            "Iteration 264, loss = 0.63093387\n",
            "Iteration 265, loss = 0.63036351\n",
            "Iteration 266, loss = 0.62981816\n",
            "Iteration 267, loss = 0.62923631\n",
            "Iteration 268, loss = 0.62867086\n",
            "Iteration 269, loss = 0.62812831\n",
            "Iteration 270, loss = 0.62761187\n",
            "Iteration 271, loss = 0.62714314\n",
            "Iteration 272, loss = 0.62665846\n",
            "Iteration 273, loss = 0.62618707\n",
            "Iteration 274, loss = 0.62572836\n",
            "Iteration 275, loss = 0.62526519\n",
            "Iteration 276, loss = 0.62480022\n",
            "Iteration 277, loss = 0.62432947\n",
            "Iteration 278, loss = 0.62388643\n",
            "Iteration 279, loss = 0.62342207\n",
            "Iteration 280, loss = 0.62295955\n",
            "Iteration 281, loss = 0.62248407\n",
            "Iteration 282, loss = 0.62202655\n",
            "Iteration 283, loss = 0.62153923\n",
            "Iteration 284, loss = 0.62103447\n",
            "Iteration 285, loss = 0.62054210\n",
            "Iteration 286, loss = 0.62006550\n",
            "Iteration 287, loss = 0.61954691\n",
            "Iteration 288, loss = 0.61903387\n",
            "Iteration 289, loss = 0.61854360\n",
            "Iteration 290, loss = 0.61803600\n",
            "Iteration 291, loss = 0.61751082\n",
            "Iteration 292, loss = 0.61699394\n",
            "Iteration 293, loss = 0.61648514\n",
            "Iteration 294, loss = 0.61596649\n",
            "Iteration 295, loss = 0.61538896\n",
            "Iteration 296, loss = 0.61487253\n",
            "Iteration 297, loss = 0.61432024\n",
            "Iteration 298, loss = 0.61377266\n",
            "Iteration 299, loss = 0.61323040\n",
            "Iteration 300, loss = 0.61266289\n",
            "Iteration 301, loss = 0.61212582\n",
            "Iteration 302, loss = 0.61156817\n",
            "Iteration 303, loss = 0.61098849\n",
            "Iteration 304, loss = 0.61045004\n",
            "Iteration 305, loss = 0.60989648\n",
            "Iteration 306, loss = 0.60933405\n",
            "Iteration 307, loss = 0.60876267\n",
            "Iteration 308, loss = 0.60819469\n",
            "Iteration 309, loss = 0.60764563\n",
            "Iteration 310, loss = 0.60706105\n",
            "Iteration 311, loss = 0.60650790\n",
            "Iteration 312, loss = 0.60596443\n",
            "Iteration 313, loss = 0.60537359\n",
            "Iteration 314, loss = 0.60479552\n",
            "Iteration 315, loss = 0.60424583\n",
            "Iteration 316, loss = 0.60366519\n",
            "Iteration 317, loss = 0.60309475\n",
            "Iteration 318, loss = 0.60253974\n",
            "Iteration 319, loss = 0.60195609\n",
            "Iteration 320, loss = 0.60138160\n",
            "Iteration 321, loss = 0.60077830\n",
            "Iteration 322, loss = 0.60021451\n",
            "Iteration 323, loss = 0.59961276\n",
            "Iteration 324, loss = 0.59901928\n",
            "Iteration 325, loss = 0.59843353\n",
            "Iteration 326, loss = 0.59783897\n",
            "Iteration 327, loss = 0.59725841\n",
            "Iteration 328, loss = 0.59667762\n",
            "Iteration 329, loss = 0.59607661\n",
            "Iteration 330, loss = 0.59551559\n",
            "Iteration 331, loss = 0.59494173\n",
            "Iteration 332, loss = 0.59435928\n",
            "Iteration 333, loss = 0.59378261\n",
            "Iteration 334, loss = 0.59325945\n",
            "Iteration 335, loss = 0.59262347\n",
            "Iteration 336, loss = 0.59205568\n",
            "Iteration 337, loss = 0.59148250\n",
            "Iteration 338, loss = 0.59090395\n",
            "Iteration 339, loss = 0.59033704\n",
            "Iteration 340, loss = 0.58973496\n",
            "Iteration 341, loss = 0.58918083\n",
            "Iteration 342, loss = 0.58859930\n",
            "Iteration 343, loss = 0.58801930\n",
            "Iteration 344, loss = 0.58743556\n",
            "Iteration 345, loss = 0.58683363\n",
            "Iteration 346, loss = 0.58627005\n",
            "Iteration 347, loss = 0.58564770\n",
            "Iteration 348, loss = 0.58507804\n",
            "Iteration 349, loss = 0.58449178\n",
            "Iteration 350, loss = 0.58387017\n",
            "Iteration 351, loss = 0.58327895\n",
            "Iteration 352, loss = 0.58267058\n",
            "Iteration 353, loss = 0.58207295\n",
            "Iteration 354, loss = 0.58148985\n",
            "Iteration 355, loss = 0.58088353\n",
            "Iteration 356, loss = 0.58027730\n",
            "Iteration 357, loss = 0.57966514\n",
            "Iteration 358, loss = 0.57907434\n",
            "Iteration 359, loss = 0.57846724\n",
            "Iteration 360, loss = 0.57787481\n",
            "Iteration 361, loss = 0.57726619\n",
            "Iteration 362, loss = 0.57670232\n",
            "Iteration 363, loss = 0.57609725\n",
            "Iteration 364, loss = 0.57550342\n",
            "Iteration 365, loss = 0.57490089\n",
            "Iteration 366, loss = 0.57432488\n",
            "Iteration 367, loss = 0.57371537\n",
            "Iteration 368, loss = 0.57313668\n",
            "Iteration 369, loss = 0.57254734\n",
            "Iteration 370, loss = 0.57198505\n",
            "Iteration 371, loss = 0.57135712\n",
            "Iteration 372, loss = 0.57077577\n",
            "Iteration 373, loss = 0.57019773\n",
            "Iteration 374, loss = 0.56960418\n",
            "Iteration 375, loss = 0.56900718\n",
            "Iteration 376, loss = 0.56843427\n",
            "Iteration 377, loss = 0.56784206\n",
            "Iteration 378, loss = 0.56724014\n",
            "Iteration 379, loss = 0.56668322\n",
            "Iteration 380, loss = 0.56607164\n",
            "Iteration 381, loss = 0.56546830\n",
            "Iteration 382, loss = 0.56489633\n",
            "Iteration 383, loss = 0.56432391\n",
            "Iteration 384, loss = 0.56374122\n",
            "Iteration 385, loss = 0.56314306\n",
            "Iteration 386, loss = 0.56259415\n",
            "Iteration 387, loss = 0.56200897\n",
            "Iteration 388, loss = 0.56142824\n",
            "Iteration 389, loss = 0.56087130\n",
            "Iteration 390, loss = 0.56029739\n",
            "Iteration 391, loss = 0.55973972\n",
            "Iteration 392, loss = 0.55916147\n",
            "Iteration 393, loss = 0.55861784\n",
            "Iteration 394, loss = 0.55802648\n",
            "Iteration 395, loss = 0.55747745\n",
            "Iteration 396, loss = 0.55692130\n",
            "Iteration 397, loss = 0.55635100\n",
            "Iteration 398, loss = 0.55581457\n",
            "Iteration 399, loss = 0.55526367\n",
            "Iteration 400, loss = 0.55472520\n",
            "Iteration 401, loss = 0.55419258\n",
            "Iteration 402, loss = 0.55365884\n",
            "Iteration 403, loss = 0.55311832\n",
            "Iteration 404, loss = 0.55258684\n",
            "Iteration 405, loss = 0.55206596\n",
            "Iteration 406, loss = 0.55156736\n",
            "Iteration 407, loss = 0.55104235\n",
            "Iteration 408, loss = 0.55052947\n",
            "Iteration 409, loss = 0.55000935\n",
            "Iteration 410, loss = 0.54952058\n",
            "Iteration 411, loss = 0.54902029\n",
            "Iteration 412, loss = 0.54850746\n",
            "Iteration 413, loss = 0.54802595\n",
            "Iteration 414, loss = 0.54752786\n",
            "Iteration 415, loss = 0.54703961\n",
            "Iteration 416, loss = 0.54654923\n",
            "Iteration 417, loss = 0.54603577\n",
            "Iteration 418, loss = 0.54553689\n",
            "Iteration 419, loss = 0.54506004\n",
            "Iteration 420, loss = 0.54459713\n",
            "Iteration 421, loss = 0.54406497\n",
            "Iteration 422, loss = 0.54357600\n",
            "Iteration 423, loss = 0.54312561\n",
            "Iteration 424, loss = 0.54263669\n",
            "Iteration 425, loss = 0.54214686\n",
            "Iteration 426, loss = 0.54166955\n",
            "Iteration 427, loss = 0.54119188\n",
            "Iteration 428, loss = 0.54072265\n",
            "Iteration 429, loss = 0.54026983\n",
            "Iteration 430, loss = 0.53977325\n",
            "Iteration 431, loss = 0.53932611\n",
            "Iteration 432, loss = 0.53887102\n",
            "Iteration 433, loss = 0.53840376\n",
            "Iteration 434, loss = 0.53796097\n",
            "Iteration 435, loss = 0.53748159\n",
            "Iteration 436, loss = 0.53703123\n",
            "Iteration 437, loss = 0.53658075\n",
            "Iteration 438, loss = 0.53614042\n",
            "Iteration 439, loss = 0.53569694\n",
            "Iteration 440, loss = 0.53523950\n",
            "Iteration 441, loss = 0.53481672\n",
            "Iteration 442, loss = 0.53435794\n",
            "Iteration 443, loss = 0.53393306\n",
            "Iteration 444, loss = 0.53350805\n",
            "Iteration 445, loss = 0.53306313\n",
            "Iteration 446, loss = 0.53261971\n",
            "Iteration 447, loss = 0.53221110\n",
            "Iteration 448, loss = 0.53176440\n",
            "Iteration 449, loss = 0.53133343\n",
            "Iteration 450, loss = 0.53094709\n",
            "Iteration 451, loss = 0.53048612\n",
            "Iteration 452, loss = 0.53008339\n",
            "Iteration 453, loss = 0.52966966\n",
            "Iteration 454, loss = 0.52927867\n",
            "Iteration 455, loss = 0.52885008\n",
            "Iteration 456, loss = 0.52843526\n",
            "Iteration 457, loss = 0.52802994\n",
            "Iteration 458, loss = 0.52761134\n",
            "Iteration 459, loss = 0.52721770\n",
            "Iteration 460, loss = 0.52681052\n",
            "Iteration 461, loss = 0.52638304\n",
            "Iteration 462, loss = 0.52600237\n",
            "Iteration 463, loss = 0.52556211\n",
            "Iteration 464, loss = 0.52517241\n",
            "Iteration 465, loss = 0.52476478\n",
            "Iteration 466, loss = 0.52435140\n",
            "Iteration 467, loss = 0.52397307\n",
            "Iteration 468, loss = 0.52357507\n",
            "Iteration 469, loss = 0.52315038\n",
            "Iteration 470, loss = 0.52277085\n",
            "Iteration 471, loss = 0.52237426\n",
            "Iteration 472, loss = 0.52198123\n",
            "Iteration 473, loss = 0.52160292\n",
            "Iteration 474, loss = 0.52118280\n",
            "Iteration 475, loss = 0.52078186\n",
            "Iteration 476, loss = 0.52040086\n",
            "Iteration 477, loss = 0.52000949\n",
            "Iteration 478, loss = 0.51959773\n",
            "Iteration 479, loss = 0.51921002\n",
            "Iteration 480, loss = 0.51882291\n",
            "Iteration 481, loss = 0.51842885\n",
            "Iteration 482, loss = 0.51800849\n",
            "Iteration 483, loss = 0.51764141\n",
            "Iteration 484, loss = 0.51722780\n",
            "Iteration 485, loss = 0.51685193\n",
            "Iteration 486, loss = 0.51646288\n",
            "Iteration 487, loss = 0.51606446\n",
            "Iteration 488, loss = 0.51567903\n",
            "Iteration 489, loss = 0.51532384\n",
            "Iteration 490, loss = 0.51492511\n",
            "Iteration 491, loss = 0.51455895\n",
            "Iteration 492, loss = 0.51415981\n",
            "Iteration 493, loss = 0.51381369\n",
            "Iteration 494, loss = 0.51343305\n",
            "Iteration 495, loss = 0.51306913\n",
            "Iteration 496, loss = 0.51270476\n",
            "Iteration 497, loss = 0.51234250\n",
            "Iteration 498, loss = 0.51197136\n",
            "Iteration 499, loss = 0.51160400\n",
            "Iteration 500, loss = 0.51127200\n",
            "Iteration 501, loss = 0.51089761\n",
            "Iteration 502, loss = 0.51054631\n",
            "Iteration 503, loss = 0.51023094\n",
            "Iteration 504, loss = 0.50985023\n",
            "Iteration 505, loss = 0.50953318\n",
            "Iteration 506, loss = 0.50918067\n",
            "Iteration 507, loss = 0.50883327\n",
            "Iteration 508, loss = 0.50850449\n",
            "Iteration 509, loss = 0.50817185\n",
            "Iteration 510, loss = 0.50783965\n",
            "Iteration 511, loss = 0.50748678\n",
            "Iteration 512, loss = 0.50714973\n",
            "Iteration 513, loss = 0.50681824\n",
            "Iteration 514, loss = 0.50650988\n",
            "Iteration 515, loss = 0.50616897\n",
            "Iteration 516, loss = 0.50582891\n",
            "Iteration 517, loss = 0.50550905\n",
            "Iteration 518, loss = 0.50520326\n",
            "Iteration 519, loss = 0.50486100\n",
            "Iteration 520, loss = 0.50456466\n",
            "Iteration 521, loss = 0.50422936\n",
            "Iteration 522, loss = 0.50392506\n",
            "Iteration 523, loss = 0.50360677\n",
            "Iteration 524, loss = 0.50332133\n",
            "Iteration 525, loss = 0.50304103\n",
            "Iteration 526, loss = 0.50273369\n",
            "Iteration 527, loss = 0.50243917\n",
            "Iteration 528, loss = 0.50215350\n",
            "Iteration 529, loss = 0.50187229\n",
            "Iteration 530, loss = 0.50158246\n",
            "Iteration 531, loss = 0.50129363\n",
            "Iteration 532, loss = 0.50100645\n",
            "Iteration 533, loss = 0.50074758\n",
            "Iteration 534, loss = 0.50046335\n",
            "Iteration 535, loss = 0.50018317\n",
            "Iteration 536, loss = 0.49993018\n",
            "Iteration 537, loss = 0.49965395\n",
            "Iteration 538, loss = 0.49940750\n",
            "Iteration 539, loss = 0.49911756\n",
            "Iteration 540, loss = 0.49887202\n",
            "Iteration 541, loss = 0.49862800\n",
            "Iteration 542, loss = 0.49836900\n",
            "Iteration 543, loss = 0.49810096\n",
            "Iteration 544, loss = 0.49786172\n",
            "Iteration 545, loss = 0.49760697\n",
            "Iteration 546, loss = 0.49736720\n",
            "Iteration 547, loss = 0.49712759\n",
            "Iteration 548, loss = 0.49689197\n",
            "Iteration 549, loss = 0.49665518\n",
            "Iteration 550, loss = 0.49642210\n",
            "Iteration 551, loss = 0.49619318\n",
            "Iteration 552, loss = 0.49595861\n",
            "Iteration 553, loss = 0.49573226\n",
            "Iteration 554, loss = 0.49550629\n",
            "Iteration 555, loss = 0.49527971\n",
            "Iteration 556, loss = 0.49505968\n",
            "Iteration 557, loss = 0.49484034\n",
            "Iteration 558, loss = 0.49464588\n",
            "Iteration 559, loss = 0.49438437\n",
            "Iteration 560, loss = 0.49418394\n",
            "Iteration 561, loss = 0.49396448\n",
            "Iteration 562, loss = 0.49375965\n",
            "Iteration 563, loss = 0.49354909\n",
            "Iteration 564, loss = 0.49335044\n",
            "Iteration 565, loss = 0.49312393\n",
            "Iteration 566, loss = 0.49293303\n",
            "Iteration 567, loss = 0.49272174\n",
            "Iteration 568, loss = 0.49252643\n",
            "Iteration 569, loss = 0.49231129\n",
            "Iteration 570, loss = 0.49209725\n",
            "Iteration 571, loss = 0.49190959\n",
            "Iteration 572, loss = 0.49169315\n",
            "Iteration 573, loss = 0.49149424\n",
            "Iteration 574, loss = 0.49128146\n",
            "Iteration 575, loss = 0.49108373\n",
            "Iteration 576, loss = 0.49087245\n",
            "Iteration 577, loss = 0.49066949\n",
            "Iteration 578, loss = 0.49045311\n",
            "Iteration 579, loss = 0.49025568\n",
            "Iteration 580, loss = 0.49006631\n",
            "Iteration 581, loss = 0.48985928\n",
            "Iteration 582, loss = 0.48965575\n",
            "Iteration 583, loss = 0.48946654\n",
            "Iteration 584, loss = 0.48926922\n",
            "Iteration 585, loss = 0.48905396\n",
            "Iteration 586, loss = 0.48886113\n",
            "Iteration 587, loss = 0.48867456\n",
            "Iteration 588, loss = 0.48846755\n",
            "Iteration 589, loss = 0.48826911\n",
            "Iteration 590, loss = 0.48807883\n",
            "Iteration 591, loss = 0.48789282\n",
            "Iteration 592, loss = 0.48769149\n",
            "Iteration 593, loss = 0.48749946\n",
            "Iteration 594, loss = 0.48732391\n",
            "Iteration 595, loss = 0.48713995\n",
            "Iteration 596, loss = 0.48696153\n",
            "Iteration 597, loss = 0.48679024\n",
            "Iteration 598, loss = 0.48660179\n",
            "Iteration 599, loss = 0.48644268\n",
            "Iteration 600, loss = 0.48626930\n",
            "Iteration 601, loss = 0.48610423\n",
            "Iteration 602, loss = 0.48592328\n",
            "Iteration 603, loss = 0.48576350\n",
            "Iteration 604, loss = 0.48561425\n",
            "Iteration 605, loss = 0.48543925\n",
            "Iteration 606, loss = 0.48525687\n",
            "Iteration 607, loss = 0.48510970\n",
            "Iteration 608, loss = 0.48496286\n",
            "Iteration 609, loss = 0.48479446\n",
            "Iteration 610, loss = 0.48461577\n",
            "Iteration 611, loss = 0.48446939\n",
            "Iteration 612, loss = 0.48431782\n",
            "Iteration 613, loss = 0.48415616\n",
            "Iteration 614, loss = 0.48399162\n",
            "Iteration 615, loss = 0.48384329\n",
            "Iteration 616, loss = 0.48369915\n",
            "Iteration 617, loss = 0.48356131\n",
            "Iteration 618, loss = 0.48340273\n",
            "Iteration 619, loss = 0.48322989\n",
            "Iteration 620, loss = 0.48308708\n",
            "Iteration 621, loss = 0.48294533\n",
            "Iteration 622, loss = 0.48282787\n",
            "Iteration 623, loss = 0.48264909\n",
            "Iteration 624, loss = 0.48250776\n",
            "Iteration 625, loss = 0.48236940\n",
            "Iteration 626, loss = 0.48223237\n",
            "Iteration 627, loss = 0.48209433\n",
            "Iteration 628, loss = 0.48195370\n",
            "Iteration 629, loss = 0.48181047\n",
            "Iteration 630, loss = 0.48169250\n",
            "Iteration 631, loss = 0.48154210\n",
            "Iteration 632, loss = 0.48139148\n",
            "Iteration 633, loss = 0.48124171\n",
            "Iteration 634, loss = 0.48111635\n",
            "Iteration 635, loss = 0.48097042\n",
            "Iteration 636, loss = 0.48081594\n",
            "Iteration 637, loss = 0.48066418\n",
            "Iteration 638, loss = 0.48053846\n",
            "Iteration 639, loss = 0.48039858\n",
            "Iteration 640, loss = 0.48023093\n",
            "Iteration 641, loss = 0.48009323\n",
            "Iteration 642, loss = 0.47992189\n",
            "Iteration 643, loss = 0.47979718\n",
            "Iteration 644, loss = 0.47961141\n",
            "Iteration 645, loss = 0.47944844\n",
            "Iteration 646, loss = 0.47928148\n",
            "Iteration 647, loss = 0.47914535\n",
            "Iteration 648, loss = 0.47897962\n",
            "Iteration 649, loss = 0.47881806\n",
            "Iteration 650, loss = 0.47864459\n",
            "Iteration 651, loss = 0.47849400\n",
            "Iteration 652, loss = 0.47835885\n",
            "Iteration 653, loss = 0.47819256\n",
            "Iteration 654, loss = 0.47804943\n",
            "Iteration 655, loss = 0.47790200\n",
            "Iteration 656, loss = 0.47775748\n",
            "Iteration 657, loss = 0.47760442\n",
            "Iteration 658, loss = 0.47745613\n",
            "Iteration 659, loss = 0.47729839\n",
            "Iteration 660, loss = 0.47717598\n",
            "Iteration 661, loss = 0.47702871\n",
            "Iteration 662, loss = 0.47687853\n",
            "Iteration 663, loss = 0.47675267\n",
            "Iteration 664, loss = 0.47659670\n",
            "Iteration 665, loss = 0.47645629\n",
            "Iteration 666, loss = 0.47633543\n",
            "Iteration 667, loss = 0.47620751\n",
            "Iteration 668, loss = 0.47606093\n",
            "Iteration 669, loss = 0.47593101\n",
            "Iteration 670, loss = 0.47580660\n",
            "Iteration 671, loss = 0.47570032\n",
            "Iteration 672, loss = 0.47556302\n",
            "Iteration 673, loss = 0.47545561\n",
            "Iteration 674, loss = 0.47531895\n",
            "Iteration 675, loss = 0.47520507\n",
            "Iteration 676, loss = 0.47506834\n",
            "Iteration 677, loss = 0.47494130\n",
            "Iteration 678, loss = 0.47480636\n",
            "Iteration 679, loss = 0.47467506\n",
            "Iteration 680, loss = 0.47453525\n",
            "Iteration 681, loss = 0.47440548\n",
            "Iteration 682, loss = 0.47427383\n",
            "Iteration 683, loss = 0.47416572\n",
            "Iteration 684, loss = 0.47400508\n",
            "Iteration 685, loss = 0.47389143\n",
            "Iteration 686, loss = 0.47376668\n",
            "Iteration 687, loss = 0.47363944\n",
            "Iteration 688, loss = 0.47350409\n",
            "Iteration 689, loss = 0.47339327\n",
            "Iteration 690, loss = 0.47328050\n",
            "Iteration 691, loss = 0.47315515\n",
            "Iteration 692, loss = 0.47303440\n",
            "Iteration 693, loss = 0.47293134\n",
            "Iteration 694, loss = 0.47281517\n",
            "Iteration 695, loss = 0.47270113\n",
            "Iteration 696, loss = 0.47259780\n",
            "Iteration 697, loss = 0.47248056\n",
            "Iteration 698, loss = 0.47239366\n",
            "Iteration 699, loss = 0.47228990\n",
            "Iteration 700, loss = 0.47218718\n",
            "Iteration 701, loss = 0.47209523\n",
            "Iteration 702, loss = 0.47200041\n",
            "Iteration 703, loss = 0.47190125\n",
            "Iteration 704, loss = 0.47180769\n",
            "Iteration 705, loss = 0.47170593\n",
            "Iteration 706, loss = 0.47161434\n",
            "Iteration 707, loss = 0.47150477\n",
            "Iteration 708, loss = 0.47143425\n",
            "Iteration 709, loss = 0.47132517\n",
            "Iteration 710, loss = 0.47123815\n",
            "Iteration 711, loss = 0.47114573\n",
            "Iteration 712, loss = 0.47102559\n",
            "Iteration 713, loss = 0.47092450\n",
            "Iteration 714, loss = 0.47081160\n",
            "Iteration 715, loss = 0.47069771\n",
            "Iteration 716, loss = 0.47059704\n",
            "Iteration 717, loss = 0.47048530\n",
            "Iteration 718, loss = 0.47037101\n",
            "Iteration 719, loss = 0.47027655\n",
            "Iteration 720, loss = 0.47017063\n",
            "Iteration 721, loss = 0.47006355\n",
            "Iteration 722, loss = 0.46996091\n",
            "Iteration 723, loss = 0.46986193\n",
            "Iteration 724, loss = 0.46976351\n",
            "Iteration 725, loss = 0.46964993\n",
            "Iteration 726, loss = 0.46955218\n",
            "Iteration 727, loss = 0.46949191\n",
            "Iteration 728, loss = 0.46935047\n",
            "Iteration 729, loss = 0.46927019\n",
            "Iteration 730, loss = 0.46917984\n",
            "Iteration 731, loss = 0.46909378\n",
            "Iteration 732, loss = 0.46902184\n",
            "Iteration 733, loss = 0.46892383\n",
            "Iteration 734, loss = 0.46883810\n",
            "Iteration 735, loss = 0.46874936\n",
            "Iteration 736, loss = 0.46868269\n",
            "Iteration 737, loss = 0.46858138\n",
            "Iteration 738, loss = 0.46850104\n",
            "Iteration 739, loss = 0.46842280\n",
            "Iteration 740, loss = 0.46834177\n",
            "Iteration 741, loss = 0.46825852\n",
            "Iteration 742, loss = 0.46818473\n",
            "Iteration 743, loss = 0.46810228\n",
            "Iteration 744, loss = 0.46802210\n",
            "Iteration 745, loss = 0.46795476\n",
            "Iteration 746, loss = 0.46787213\n",
            "Iteration 747, loss = 0.46780187\n",
            "Iteration 748, loss = 0.46774099\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Melhores hiperparâmetros encontrados através do Grid Search:\n",
            "{'activation': 'relu', 'max_iter': 1000, 'solver': 'sgd', 'tol': 0.0001}\n",
            "Melhor pontuação (acurácia) encontrada através do Grid Search: 0.7893333333333332\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ajuste do modelo MLP aos dados de treinamento\n",
        "rnaf_best_grid = MLPClassifier(**best_params_gridf)\n",
        "rnaf_best_grid.fit(X_train_oversampled, y_train_oversampled)\n",
        "\n",
        "# Predições nos dados de teste usando o modelo com melhores hiperparâmetros encontrados pelo Grid Search\n",
        "pred_gridf = rnaf_best_grid.predict(X_test)"
      ],
      "metadata": {
        "id": "vEcFmkydz3p7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cálculo e impressão da acurácia nos dados de teste\n",
        "accuracy_grid = accuracy_score(y_test, pred_gridf)\n",
        "print(\"Acurácia do modelo MLP com melhores hiperparâmetros pelo Grid Search:\", accuracy_grid)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d895fb9f-522e-4d2d-88ab-0edaaac66508",
        "id": "VboCDJLxz3p8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acurácia do modelo MLP com melhores hiperparâmetros pelo Grid Search: 0.7798507462686567\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Matriz de confusão para o modelo com melhores hiperparâmetros pelo Grid Search\n",
        "print(\"Matriz de Confusão - Grid Search\")\n",
        "cm_rnaf_grid = confusion_matrix(y_test, pred_gridf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9a5d7a3-5a99-4a1e-8aaf-486ee19e1650",
        "id": "nqaTsmqqz3p8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matriz de Confusão - Grid Search\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtém a média das acurácias (10 folds) referente ao conjunto treino\n",
        "rnaf_grid = g_results.loc[g_search.best_index_,'mean_test_score']"
      ],
      "metadata": {
        "id": "O8MKVtOSz3p8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RandomSearch"
      ],
      "metadata": {
        "id": "1dVRd67fz3p8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Random Search\n",
        "param_dist = {\n",
        "    'activation': ['relu', 'tanh', 'logistic'],\n",
        "    'solver': ['sgd', 'adam'],\n",
        "    'max_iter': [1000, 2000],\n",
        "    'tol': [0.0001, 0.001],\n",
        "}\n",
        "\n",
        "r_search = RandomizedSearchCV(estimator=rnaf, param_distributions=param_dist, cv=10)\n",
        "r_search.fit(X_train_oversampled, y_train_oversampled)\n",
        "\n",
        "best_params_random = r_search.best_params_\n",
        "best_score_random = r_search.best_score_\n",
        "\n",
        "print(\"Melhores hiperparâmetros encontrados através do Random Search:\")\n",
        "print(best_params_random)\n",
        "print(\"Melhor pontuação (acurácia) encontrada através do Random Search:\", best_score_random)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff1758c3-295f-4831-a2c3-35e894a552f1",
        "id": "lnYPeGHqz3p8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mA saída de streaming foi truncada nas últimas 5000 linhas.\u001b[0m\n",
            "Iteration 137, loss = 0.68918127\n",
            "Iteration 138, loss = 0.68901808\n",
            "Iteration 139, loss = 0.68886153\n",
            "Iteration 140, loss = 0.68871136\n",
            "Iteration 141, loss = 0.68856991\n",
            "Iteration 142, loss = 0.68840118\n",
            "Iteration 143, loss = 0.68828110\n",
            "Iteration 144, loss = 0.68810656\n",
            "Iteration 145, loss = 0.68797346\n",
            "Iteration 146, loss = 0.68781834\n",
            "Iteration 147, loss = 0.68766971\n",
            "Iteration 148, loss = 0.68753823\n",
            "Iteration 149, loss = 0.68739415\n",
            "Iteration 150, loss = 0.68724666\n",
            "Iteration 151, loss = 0.68709454\n",
            "Iteration 152, loss = 0.68696186\n",
            "Iteration 153, loss = 0.68683596\n",
            "Iteration 154, loss = 0.68666633\n",
            "Iteration 155, loss = 0.68653518\n",
            "Iteration 156, loss = 0.68637973\n",
            "Iteration 157, loss = 0.68624639\n",
            "Iteration 158, loss = 0.68609754\n",
            "Iteration 159, loss = 0.68595364\n",
            "Iteration 160, loss = 0.68582855\n",
            "Iteration 161, loss = 0.68565229\n",
            "Iteration 162, loss = 0.68551819\n",
            "Iteration 163, loss = 0.68537397\n",
            "Iteration 164, loss = 0.68522941\n",
            "Iteration 165, loss = 0.68509242\n",
            "Iteration 166, loss = 0.68497066\n",
            "Iteration 167, loss = 0.68481752\n",
            "Iteration 168, loss = 0.68468282\n",
            "Iteration 169, loss = 0.68453898\n",
            "Iteration 170, loss = 0.68439706\n",
            "Iteration 171, loss = 0.68426777\n",
            "Iteration 172, loss = 0.68412713\n",
            "Iteration 173, loss = 0.68398673\n",
            "Iteration 174, loss = 0.68385321\n",
            "Iteration 175, loss = 0.68372941\n",
            "Iteration 176, loss = 0.68358435\n",
            "Iteration 177, loss = 0.68345773\n",
            "Iteration 178, loss = 0.68332383\n",
            "Iteration 179, loss = 0.68319934\n",
            "Iteration 180, loss = 0.68305462\n",
            "Iteration 181, loss = 0.68292029\n",
            "Iteration 182, loss = 0.68278723\n",
            "Iteration 183, loss = 0.68264152\n",
            "Iteration 184, loss = 0.68250519\n",
            "Iteration 185, loss = 0.68236963\n",
            "Iteration 186, loss = 0.68222690\n",
            "Iteration 187, loss = 0.68208339\n",
            "Iteration 188, loss = 0.68194805\n",
            "Iteration 189, loss = 0.68178955\n",
            "Iteration 190, loss = 0.68164732\n",
            "Iteration 191, loss = 0.68149456\n",
            "Iteration 192, loss = 0.68135798\n",
            "Iteration 193, loss = 0.68119295\n",
            "Iteration 194, loss = 0.68104322\n",
            "Iteration 195, loss = 0.68088974\n",
            "Iteration 196, loss = 0.68074411\n",
            "Iteration 197, loss = 0.68059219\n",
            "Iteration 198, loss = 0.68044179\n",
            "Iteration 199, loss = 0.68029880\n",
            "Iteration 200, loss = 0.68014495\n",
            "Iteration 201, loss = 0.67998072\n",
            "Iteration 202, loss = 0.67982370\n",
            "Iteration 203, loss = 0.67966386\n",
            "Iteration 204, loss = 0.67950313\n",
            "Iteration 205, loss = 0.67934621\n",
            "Iteration 206, loss = 0.67918324\n",
            "Iteration 207, loss = 0.67901316\n",
            "Iteration 208, loss = 0.67884674\n",
            "Iteration 209, loss = 0.67868414\n",
            "Iteration 210, loss = 0.67850655\n",
            "Iteration 211, loss = 0.67833313\n",
            "Iteration 212, loss = 0.67815457\n",
            "Iteration 213, loss = 0.67797602\n",
            "Iteration 214, loss = 0.67780580\n",
            "Iteration 215, loss = 0.67762607\n",
            "Iteration 216, loss = 0.67745168\n",
            "Iteration 217, loss = 0.67725956\n",
            "Iteration 218, loss = 0.67706877\n",
            "Iteration 219, loss = 0.67688132\n",
            "Iteration 220, loss = 0.67669421\n",
            "Iteration 221, loss = 0.67648521\n",
            "Iteration 222, loss = 0.67629515\n",
            "Iteration 223, loss = 0.67609539\n",
            "Iteration 224, loss = 0.67589939\n",
            "Iteration 225, loss = 0.67570391\n",
            "Iteration 226, loss = 0.67549338\n",
            "Iteration 227, loss = 0.67528953\n",
            "Iteration 228, loss = 0.67508520\n",
            "Iteration 229, loss = 0.67486890\n",
            "Iteration 230, loss = 0.67465019\n",
            "Iteration 231, loss = 0.67444820\n",
            "Iteration 232, loss = 0.67423011\n",
            "Iteration 233, loss = 0.67400229\n",
            "Iteration 234, loss = 0.67378726\n",
            "Iteration 235, loss = 0.67356677\n",
            "Iteration 236, loss = 0.67333685\n",
            "Iteration 237, loss = 0.67311779\n",
            "Iteration 238, loss = 0.67289454\n",
            "Iteration 239, loss = 0.67266628\n",
            "Iteration 240, loss = 0.67244165\n",
            "Iteration 241, loss = 0.67221172\n",
            "Iteration 242, loss = 0.67198017\n",
            "Iteration 243, loss = 0.67174849\n",
            "Iteration 244, loss = 0.67150946\n",
            "Iteration 245, loss = 0.67127992\n",
            "Iteration 246, loss = 0.67102972\n",
            "Iteration 247, loss = 0.67078889\n",
            "Iteration 248, loss = 0.67054283\n",
            "Iteration 249, loss = 0.67028465\n",
            "Iteration 250, loss = 0.67003487\n",
            "Iteration 251, loss = 0.66978302\n",
            "Iteration 252, loss = 0.66952265\n",
            "Iteration 253, loss = 0.66925615\n",
            "Iteration 254, loss = 0.66898978\n",
            "Iteration 255, loss = 0.66872837\n",
            "Iteration 256, loss = 0.66846475\n",
            "Iteration 257, loss = 0.66819649\n",
            "Iteration 258, loss = 0.66791527\n",
            "Iteration 259, loss = 0.66764557\n",
            "Iteration 260, loss = 0.66736919\n",
            "Iteration 261, loss = 0.66710109\n",
            "Iteration 262, loss = 0.66681789\n",
            "Iteration 263, loss = 0.66655082\n",
            "Iteration 264, loss = 0.66626828\n",
            "Iteration 265, loss = 0.66599139\n",
            "Iteration 266, loss = 0.66569436\n",
            "Iteration 267, loss = 0.66541489\n",
            "Iteration 268, loss = 0.66512158\n",
            "Iteration 269, loss = 0.66482484\n",
            "Iteration 270, loss = 0.66454291\n",
            "Iteration 271, loss = 0.66424817\n",
            "Iteration 272, loss = 0.66395754\n",
            "Iteration 273, loss = 0.66364668\n",
            "Iteration 274, loss = 0.66335300\n",
            "Iteration 275, loss = 0.66306585\n",
            "Iteration 276, loss = 0.66275448\n",
            "Iteration 277, loss = 0.66245162\n",
            "Iteration 278, loss = 0.66214651\n",
            "Iteration 279, loss = 0.66184087\n",
            "Iteration 280, loss = 0.66152367\n",
            "Iteration 281, loss = 0.66120980\n",
            "Iteration 282, loss = 0.66089608\n",
            "Iteration 283, loss = 0.66057302\n",
            "Iteration 284, loss = 0.66025665\n",
            "Iteration 285, loss = 0.65994342\n",
            "Iteration 286, loss = 0.65961479\n",
            "Iteration 287, loss = 0.65929370\n",
            "Iteration 288, loss = 0.65897444\n",
            "Iteration 289, loss = 0.65864418\n",
            "Iteration 290, loss = 0.65831017\n",
            "Iteration 291, loss = 0.65797584\n",
            "Iteration 292, loss = 0.65762624\n",
            "Iteration 293, loss = 0.65728270\n",
            "Iteration 294, loss = 0.65692364\n",
            "Iteration 295, loss = 0.65658525\n",
            "Iteration 296, loss = 0.65622627\n",
            "Iteration 297, loss = 0.65586919\n",
            "Iteration 298, loss = 0.65551418\n",
            "Iteration 299, loss = 0.65514972\n",
            "Iteration 300, loss = 0.65477930\n",
            "Iteration 301, loss = 0.65440972\n",
            "Iteration 302, loss = 0.65403493\n",
            "Iteration 303, loss = 0.65366785\n",
            "Iteration 304, loss = 0.65329199\n",
            "Iteration 305, loss = 0.65292330\n",
            "Iteration 306, loss = 0.65253432\n",
            "Iteration 307, loss = 0.65214697\n",
            "Iteration 308, loss = 0.65178044\n",
            "Iteration 309, loss = 0.65140944\n",
            "Iteration 310, loss = 0.65102135\n",
            "Iteration 311, loss = 0.65063538\n",
            "Iteration 312, loss = 0.65026453\n",
            "Iteration 313, loss = 0.64988040\n",
            "Iteration 314, loss = 0.64948135\n",
            "Iteration 315, loss = 0.64912053\n",
            "Iteration 316, loss = 0.64870645\n",
            "Iteration 317, loss = 0.64830791\n",
            "Iteration 318, loss = 0.64791513\n",
            "Iteration 319, loss = 0.64750699\n",
            "Iteration 320, loss = 0.64710648\n",
            "Iteration 321, loss = 0.64668020\n",
            "Iteration 322, loss = 0.64626779\n",
            "Iteration 323, loss = 0.64586549\n",
            "Iteration 324, loss = 0.64542906\n",
            "Iteration 325, loss = 0.64501439\n",
            "Iteration 326, loss = 0.64459316\n",
            "Iteration 327, loss = 0.64417334\n",
            "Iteration 328, loss = 0.64374804\n",
            "Iteration 329, loss = 0.64332838\n",
            "Iteration 330, loss = 0.64289836\n",
            "Iteration 331, loss = 0.64247714\n",
            "Iteration 332, loss = 0.64204862\n",
            "Iteration 333, loss = 0.64159915\n",
            "Iteration 334, loss = 0.64116892\n",
            "Iteration 335, loss = 0.64071852\n",
            "Iteration 336, loss = 0.64029251\n",
            "Iteration 337, loss = 0.63982660\n",
            "Iteration 338, loss = 0.63939022\n",
            "Iteration 339, loss = 0.63893202\n",
            "Iteration 340, loss = 0.63847821\n",
            "Iteration 341, loss = 0.63802344\n",
            "Iteration 342, loss = 0.63755360\n",
            "Iteration 343, loss = 0.63709027\n",
            "Iteration 344, loss = 0.63662781\n",
            "Iteration 345, loss = 0.63616090\n",
            "Iteration 346, loss = 0.63567824\n",
            "Iteration 347, loss = 0.63520736\n",
            "Iteration 348, loss = 0.63472357\n",
            "Iteration 349, loss = 0.63423733\n",
            "Iteration 350, loss = 0.63375438\n",
            "Iteration 351, loss = 0.63326152\n",
            "Iteration 352, loss = 0.63278035\n",
            "Iteration 353, loss = 0.63229853\n",
            "Iteration 354, loss = 0.63180026\n",
            "Iteration 355, loss = 0.63130993\n",
            "Iteration 356, loss = 0.63084648\n",
            "Iteration 357, loss = 0.63034968\n",
            "Iteration 358, loss = 0.62984913\n",
            "Iteration 359, loss = 0.62935897\n",
            "Iteration 360, loss = 0.62886825\n",
            "Iteration 361, loss = 0.62837757\n",
            "Iteration 362, loss = 0.62787604\n",
            "Iteration 363, loss = 0.62736103\n",
            "Iteration 364, loss = 0.62684564\n",
            "Iteration 365, loss = 0.62634399\n",
            "Iteration 366, loss = 0.62581546\n",
            "Iteration 367, loss = 0.62529413\n",
            "Iteration 368, loss = 0.62477252\n",
            "Iteration 369, loss = 0.62422843\n",
            "Iteration 370, loss = 0.62369870\n",
            "Iteration 371, loss = 0.62317030\n",
            "Iteration 372, loss = 0.62262323\n",
            "Iteration 373, loss = 0.62209007\n",
            "Iteration 374, loss = 0.62157457\n",
            "Iteration 375, loss = 0.62103465\n",
            "Iteration 376, loss = 0.62051762\n",
            "Iteration 377, loss = 0.61996711\n",
            "Iteration 378, loss = 0.61944203\n",
            "Iteration 379, loss = 0.61889928\n",
            "Iteration 380, loss = 0.61835907\n",
            "Iteration 381, loss = 0.61780830\n",
            "Iteration 382, loss = 0.61726217\n",
            "Iteration 383, loss = 0.61669859\n",
            "Iteration 384, loss = 0.61612881\n",
            "Iteration 385, loss = 0.61556728\n",
            "Iteration 386, loss = 0.61497593\n",
            "Iteration 387, loss = 0.61441529\n",
            "Iteration 388, loss = 0.61383735\n",
            "Iteration 389, loss = 0.61329236\n",
            "Iteration 390, loss = 0.61271595\n",
            "Iteration 391, loss = 0.61213260\n",
            "Iteration 392, loss = 0.61157317\n",
            "Iteration 393, loss = 0.61098867\n",
            "Iteration 394, loss = 0.61041942\n",
            "Iteration 395, loss = 0.60981898\n",
            "Iteration 396, loss = 0.60924311\n",
            "Iteration 397, loss = 0.60866515\n",
            "Iteration 398, loss = 0.60808615\n",
            "Iteration 399, loss = 0.60749147\n",
            "Iteration 400, loss = 0.60690726\n",
            "Iteration 401, loss = 0.60632058\n",
            "Iteration 402, loss = 0.60572455\n",
            "Iteration 403, loss = 0.60513032\n",
            "Iteration 404, loss = 0.60455506\n",
            "Iteration 405, loss = 0.60394669\n",
            "Iteration 406, loss = 0.60334429\n",
            "Iteration 407, loss = 0.60274424\n",
            "Iteration 408, loss = 0.60212447\n",
            "Iteration 409, loss = 0.60151832\n",
            "Iteration 410, loss = 0.60089253\n",
            "Iteration 411, loss = 0.60026518\n",
            "Iteration 412, loss = 0.59964119\n",
            "Iteration 413, loss = 0.59902948\n",
            "Iteration 414, loss = 0.59842576\n",
            "Iteration 415, loss = 0.59780264\n",
            "Iteration 416, loss = 0.59720837\n",
            "Iteration 417, loss = 0.59656758\n",
            "Iteration 418, loss = 0.59596142\n",
            "Iteration 419, loss = 0.59535164\n",
            "Iteration 420, loss = 0.59471837\n",
            "Iteration 421, loss = 0.59413232\n",
            "Iteration 422, loss = 0.59351267\n",
            "Iteration 423, loss = 0.59289953\n",
            "Iteration 424, loss = 0.59229867\n",
            "Iteration 425, loss = 0.59168042\n",
            "Iteration 426, loss = 0.59106490\n",
            "Iteration 427, loss = 0.59042591\n",
            "Iteration 428, loss = 0.58981058\n",
            "Iteration 429, loss = 0.58919539\n",
            "Iteration 430, loss = 0.58856537\n",
            "Iteration 431, loss = 0.58794165\n",
            "Iteration 432, loss = 0.58733731\n",
            "Iteration 433, loss = 0.58667062\n",
            "Iteration 434, loss = 0.58604514\n",
            "Iteration 435, loss = 0.58541421\n",
            "Iteration 436, loss = 0.58479113\n",
            "Iteration 437, loss = 0.58416102\n",
            "Iteration 438, loss = 0.58354179\n",
            "Iteration 439, loss = 0.58292520\n",
            "Iteration 440, loss = 0.58230662\n",
            "Iteration 441, loss = 0.58168926\n",
            "Iteration 442, loss = 0.58106996\n",
            "Iteration 443, loss = 0.58046225\n",
            "Iteration 444, loss = 0.57980507\n",
            "Iteration 445, loss = 0.57917797\n",
            "Iteration 446, loss = 0.57853959\n",
            "Iteration 447, loss = 0.57791399\n",
            "Iteration 448, loss = 0.57730707\n",
            "Iteration 449, loss = 0.57665650\n",
            "Iteration 450, loss = 0.57603960\n",
            "Iteration 451, loss = 0.57541877\n",
            "Iteration 452, loss = 0.57480086\n",
            "Iteration 453, loss = 0.57417253\n",
            "Iteration 454, loss = 0.57353819\n",
            "Iteration 455, loss = 0.57291300\n",
            "Iteration 456, loss = 0.57228124\n",
            "Iteration 457, loss = 0.57165692\n",
            "Iteration 458, loss = 0.57103922\n",
            "Iteration 459, loss = 0.57042633\n",
            "Iteration 460, loss = 0.56981227\n",
            "Iteration 461, loss = 0.56920136\n",
            "Iteration 462, loss = 0.56861828\n",
            "Iteration 463, loss = 0.56801502\n",
            "Iteration 464, loss = 0.56740802\n",
            "Iteration 465, loss = 0.56682154\n",
            "Iteration 466, loss = 0.56624285\n",
            "Iteration 467, loss = 0.56561077\n",
            "Iteration 468, loss = 0.56502849\n",
            "Iteration 469, loss = 0.56443902\n",
            "Iteration 470, loss = 0.56383027\n",
            "Iteration 471, loss = 0.56325064\n",
            "Iteration 472, loss = 0.56266983\n",
            "Iteration 473, loss = 0.56205379\n",
            "Iteration 474, loss = 0.56147236\n",
            "Iteration 475, loss = 0.56088406\n",
            "Iteration 476, loss = 0.56030252\n",
            "Iteration 477, loss = 0.55973629\n",
            "Iteration 478, loss = 0.55915502\n",
            "Iteration 479, loss = 0.55855916\n",
            "Iteration 480, loss = 0.55798319\n",
            "Iteration 481, loss = 0.55741648\n",
            "Iteration 482, loss = 0.55685243\n",
            "Iteration 483, loss = 0.55626626\n",
            "Iteration 484, loss = 0.55567953\n",
            "Iteration 485, loss = 0.55511620\n",
            "Iteration 486, loss = 0.55456208\n",
            "Iteration 487, loss = 0.55398052\n",
            "Iteration 488, loss = 0.55342513\n",
            "Iteration 489, loss = 0.55287316\n",
            "Iteration 490, loss = 0.55232757\n",
            "Iteration 491, loss = 0.55177106\n",
            "Iteration 492, loss = 0.55121989\n",
            "Iteration 493, loss = 0.55065557\n",
            "Iteration 494, loss = 0.55008935\n",
            "Iteration 495, loss = 0.54954231\n",
            "Iteration 496, loss = 0.54897226\n",
            "Iteration 497, loss = 0.54841932\n",
            "Iteration 498, loss = 0.54791684\n",
            "Iteration 499, loss = 0.54736950\n",
            "Iteration 500, loss = 0.54684552\n",
            "Iteration 501, loss = 0.54631415\n",
            "Iteration 502, loss = 0.54579776\n",
            "Iteration 503, loss = 0.54524942\n",
            "Iteration 504, loss = 0.54473394\n",
            "Iteration 505, loss = 0.54419564\n",
            "Iteration 506, loss = 0.54366595\n",
            "Iteration 507, loss = 0.54313953\n",
            "Iteration 508, loss = 0.54261306\n",
            "Iteration 509, loss = 0.54213165\n",
            "Iteration 510, loss = 0.54158690\n",
            "Iteration 511, loss = 0.54109449\n",
            "Iteration 512, loss = 0.54058148\n",
            "Iteration 513, loss = 0.54005620\n",
            "Iteration 514, loss = 0.53953643\n",
            "Iteration 515, loss = 0.53905313\n",
            "Iteration 516, loss = 0.53856962\n",
            "Iteration 517, loss = 0.53805990\n",
            "Iteration 518, loss = 0.53756941\n",
            "Iteration 519, loss = 0.53708866\n",
            "Iteration 520, loss = 0.53660042\n",
            "Iteration 521, loss = 0.53609750\n",
            "Iteration 522, loss = 0.53558275\n",
            "Iteration 523, loss = 0.53511038\n",
            "Iteration 524, loss = 0.53463209\n",
            "Iteration 525, loss = 0.53416018\n",
            "Iteration 526, loss = 0.53368407\n",
            "Iteration 527, loss = 0.53321096\n",
            "Iteration 528, loss = 0.53274915\n",
            "Iteration 529, loss = 0.53226605\n",
            "Iteration 530, loss = 0.53180530\n",
            "Iteration 531, loss = 0.53136233\n",
            "Iteration 532, loss = 0.53086875\n",
            "Iteration 533, loss = 0.53042434\n",
            "Iteration 534, loss = 0.52996088\n",
            "Iteration 535, loss = 0.52951422\n",
            "Iteration 536, loss = 0.52905688\n",
            "Iteration 537, loss = 0.52858760\n",
            "Iteration 538, loss = 0.52815983\n",
            "Iteration 539, loss = 0.52768695\n",
            "Iteration 540, loss = 0.52725240\n",
            "Iteration 541, loss = 0.52683299\n",
            "Iteration 542, loss = 0.52637772\n",
            "Iteration 543, loss = 0.52594930\n",
            "Iteration 544, loss = 0.52551709\n",
            "Iteration 545, loss = 0.52508524\n",
            "Iteration 546, loss = 0.52468429\n",
            "Iteration 547, loss = 0.52425832\n",
            "Iteration 548, loss = 0.52384168\n",
            "Iteration 549, loss = 0.52344755\n",
            "Iteration 550, loss = 0.52303674\n",
            "Iteration 551, loss = 0.52260343\n",
            "Iteration 552, loss = 0.52219780\n",
            "Iteration 553, loss = 0.52179192\n",
            "Iteration 554, loss = 0.52137938\n",
            "Iteration 555, loss = 0.52097078\n",
            "Iteration 556, loss = 0.52057017\n",
            "Iteration 557, loss = 0.52015109\n",
            "Iteration 558, loss = 0.51977356\n",
            "Iteration 559, loss = 0.51937993\n",
            "Iteration 560, loss = 0.51899027\n",
            "Iteration 561, loss = 0.51861254\n",
            "Iteration 562, loss = 0.51824214\n",
            "Iteration 563, loss = 0.51788033\n",
            "Iteration 564, loss = 0.51751389\n",
            "Iteration 565, loss = 0.51711410\n",
            "Iteration 566, loss = 0.51674404\n",
            "Iteration 567, loss = 0.51637045\n",
            "Iteration 568, loss = 0.51599154\n",
            "Iteration 569, loss = 0.51563033\n",
            "Iteration 570, loss = 0.51525737\n",
            "Iteration 571, loss = 0.51486218\n",
            "Iteration 572, loss = 0.51446954\n",
            "Iteration 573, loss = 0.51411142\n",
            "Iteration 574, loss = 0.51374860\n",
            "Iteration 575, loss = 0.51336696\n",
            "Iteration 576, loss = 0.51298839\n",
            "Iteration 577, loss = 0.51264221\n",
            "Iteration 578, loss = 0.51231661\n",
            "Iteration 579, loss = 0.51191903\n",
            "Iteration 580, loss = 0.51156119\n",
            "Iteration 581, loss = 0.51121758\n",
            "Iteration 582, loss = 0.51084603\n",
            "Iteration 583, loss = 0.51051986\n",
            "Iteration 584, loss = 0.51013341\n",
            "Iteration 585, loss = 0.50979832\n",
            "Iteration 586, loss = 0.50945388\n",
            "Iteration 587, loss = 0.50911163\n",
            "Iteration 588, loss = 0.50877838\n",
            "Iteration 589, loss = 0.50844083\n",
            "Iteration 590, loss = 0.50813734\n",
            "Iteration 591, loss = 0.50781676\n",
            "Iteration 592, loss = 0.50747348\n",
            "Iteration 593, loss = 0.50715644\n",
            "Iteration 594, loss = 0.50684565\n",
            "Iteration 595, loss = 0.50649664\n",
            "Iteration 596, loss = 0.50620403\n",
            "Iteration 597, loss = 0.50588713\n",
            "Iteration 598, loss = 0.50556831\n",
            "Iteration 599, loss = 0.50524638\n",
            "Iteration 600, loss = 0.50492264\n",
            "Iteration 601, loss = 0.50463363\n",
            "Iteration 602, loss = 0.50431507\n",
            "Iteration 603, loss = 0.50401353\n",
            "Iteration 604, loss = 0.50369178\n",
            "Iteration 605, loss = 0.50341224\n",
            "Iteration 606, loss = 0.50310107\n",
            "Iteration 607, loss = 0.50282588\n",
            "Iteration 608, loss = 0.50254369\n",
            "Iteration 609, loss = 0.50222920\n",
            "Iteration 610, loss = 0.50194092\n",
            "Iteration 611, loss = 0.50165090\n",
            "Iteration 612, loss = 0.50137374\n",
            "Iteration 613, loss = 0.50109587\n",
            "Iteration 614, loss = 0.50080094\n",
            "Iteration 615, loss = 0.50051667\n",
            "Iteration 616, loss = 0.50024150\n",
            "Iteration 617, loss = 0.49996802\n",
            "Iteration 618, loss = 0.49969177\n",
            "Iteration 619, loss = 0.49940605\n",
            "Iteration 620, loss = 0.49915659\n",
            "Iteration 621, loss = 0.49887076\n",
            "Iteration 622, loss = 0.49859398\n",
            "Iteration 623, loss = 0.49833288\n",
            "Iteration 624, loss = 0.49803779\n",
            "Iteration 625, loss = 0.49776624\n",
            "Iteration 626, loss = 0.49749303\n",
            "Iteration 627, loss = 0.49722196\n",
            "Iteration 628, loss = 0.49693786\n",
            "Iteration 629, loss = 0.49667550\n",
            "Iteration 630, loss = 0.49642866\n",
            "Iteration 631, loss = 0.49614773\n",
            "Iteration 632, loss = 0.49588517\n",
            "Iteration 633, loss = 0.49564189\n",
            "Iteration 634, loss = 0.49536607\n",
            "Iteration 635, loss = 0.49513897\n",
            "Iteration 636, loss = 0.49489761\n",
            "Iteration 637, loss = 0.49464025\n",
            "Iteration 638, loss = 0.49440341\n",
            "Iteration 639, loss = 0.49416581\n",
            "Iteration 640, loss = 0.49395024\n",
            "Iteration 641, loss = 0.49369480\n",
            "Iteration 642, loss = 0.49344902\n",
            "Iteration 643, loss = 0.49322674\n",
            "Iteration 644, loss = 0.49298841\n",
            "Iteration 645, loss = 0.49275288\n",
            "Iteration 646, loss = 0.49251554\n",
            "Iteration 647, loss = 0.49228218\n",
            "Iteration 648, loss = 0.49205852\n",
            "Iteration 649, loss = 0.49184145\n",
            "Iteration 650, loss = 0.49161782\n",
            "Iteration 651, loss = 0.49139027\n",
            "Iteration 652, loss = 0.49118906\n",
            "Iteration 653, loss = 0.49096560\n",
            "Iteration 654, loss = 0.49072281\n",
            "Iteration 655, loss = 0.49053485\n",
            "Iteration 656, loss = 0.49032752\n",
            "Iteration 657, loss = 0.49010965\n",
            "Iteration 658, loss = 0.48990088\n",
            "Iteration 659, loss = 0.48971147\n",
            "Iteration 660, loss = 0.48949131\n",
            "Iteration 661, loss = 0.48928969\n",
            "Iteration 662, loss = 0.48909764\n",
            "Iteration 663, loss = 0.48886736\n",
            "Iteration 664, loss = 0.48866302\n",
            "Iteration 665, loss = 0.48847216\n",
            "Iteration 666, loss = 0.48825900\n",
            "Iteration 667, loss = 0.48805556\n",
            "Iteration 668, loss = 0.48785037\n",
            "Iteration 669, loss = 0.48765807\n",
            "Iteration 670, loss = 0.48747562\n",
            "Iteration 671, loss = 0.48728080\n",
            "Iteration 672, loss = 0.48707992\n",
            "Iteration 673, loss = 0.48691858\n",
            "Iteration 674, loss = 0.48671718\n",
            "Iteration 675, loss = 0.48653239\n",
            "Iteration 676, loss = 0.48633875\n",
            "Iteration 677, loss = 0.48614911\n",
            "Iteration 678, loss = 0.48596569\n",
            "Iteration 679, loss = 0.48576898\n",
            "Iteration 680, loss = 0.48558530\n",
            "Iteration 681, loss = 0.48540075\n",
            "Iteration 682, loss = 0.48523513\n",
            "Iteration 683, loss = 0.48503016\n",
            "Iteration 684, loss = 0.48485816\n",
            "Iteration 685, loss = 0.48468403\n",
            "Iteration 686, loss = 0.48451104\n",
            "Iteration 687, loss = 0.48433720\n",
            "Iteration 688, loss = 0.48416290\n",
            "Iteration 689, loss = 0.48398507\n",
            "Iteration 690, loss = 0.48386032\n",
            "Iteration 691, loss = 0.48367104\n",
            "Iteration 692, loss = 0.48351161\n",
            "Iteration 693, loss = 0.48335461\n",
            "Iteration 694, loss = 0.48320983\n",
            "Iteration 695, loss = 0.48303227\n",
            "Iteration 696, loss = 0.48289565\n",
            "Iteration 697, loss = 0.48272655\n",
            "Iteration 698, loss = 0.48255764\n",
            "Iteration 699, loss = 0.48239716\n",
            "Iteration 700, loss = 0.48225192\n",
            "Iteration 701, loss = 0.48210146\n",
            "Iteration 702, loss = 0.48192635\n",
            "Iteration 703, loss = 0.48176788\n",
            "Iteration 704, loss = 0.48163352\n",
            "Iteration 705, loss = 0.48145950\n",
            "Iteration 706, loss = 0.48131928\n",
            "Iteration 707, loss = 0.48116630\n",
            "Iteration 708, loss = 0.48101666\n",
            "Iteration 709, loss = 0.48086870\n",
            "Iteration 710, loss = 0.48073063\n",
            "Iteration 711, loss = 0.48060220\n",
            "Iteration 712, loss = 0.48046641\n",
            "Iteration 713, loss = 0.48028977\n",
            "Iteration 714, loss = 0.48015979\n",
            "Iteration 715, loss = 0.48001747\n",
            "Iteration 716, loss = 0.47986119\n",
            "Iteration 717, loss = 0.47972177\n",
            "Iteration 718, loss = 0.47956514\n",
            "Iteration 719, loss = 0.47943725\n",
            "Iteration 720, loss = 0.47929366\n",
            "Iteration 721, loss = 0.47916612\n",
            "Iteration 722, loss = 0.47902337\n",
            "Iteration 723, loss = 0.47888356\n",
            "Iteration 724, loss = 0.47875719\n",
            "Iteration 725, loss = 0.47863354\n",
            "Iteration 726, loss = 0.47847942\n",
            "Iteration 727, loss = 0.47835529\n",
            "Iteration 728, loss = 0.47820305\n",
            "Iteration 729, loss = 0.47807991\n",
            "Iteration 730, loss = 0.47795092\n",
            "Iteration 731, loss = 0.47782348\n",
            "Iteration 732, loss = 0.47768869\n",
            "Iteration 733, loss = 0.47755257\n",
            "Iteration 734, loss = 0.47741809\n",
            "Iteration 735, loss = 0.47726117\n",
            "Iteration 736, loss = 0.47715101\n",
            "Iteration 737, loss = 0.47703027\n",
            "Iteration 738, loss = 0.47688647\n",
            "Iteration 739, loss = 0.47677491\n",
            "Iteration 740, loss = 0.47664593\n",
            "Iteration 741, loss = 0.47652652\n",
            "Iteration 742, loss = 0.47640136\n",
            "Iteration 743, loss = 0.47628152\n",
            "Iteration 744, loss = 0.47615781\n",
            "Iteration 745, loss = 0.47602995\n",
            "Iteration 746, loss = 0.47592591\n",
            "Iteration 747, loss = 0.47584413\n",
            "Iteration 748, loss = 0.47566991\n",
            "Iteration 749, loss = 0.47558410\n",
            "Iteration 750, loss = 0.47545349\n",
            "Iteration 751, loss = 0.47539205\n",
            "Iteration 752, loss = 0.47523548\n",
            "Iteration 753, loss = 0.47513386\n",
            "Iteration 754, loss = 0.47501907\n",
            "Iteration 755, loss = 0.47490612\n",
            "Iteration 756, loss = 0.47478718\n",
            "Iteration 757, loss = 0.47467875\n",
            "Iteration 758, loss = 0.47456007\n",
            "Iteration 759, loss = 0.47443725\n",
            "Iteration 760, loss = 0.47435994\n",
            "Iteration 761, loss = 0.47424423\n",
            "Iteration 762, loss = 0.47410851\n",
            "Iteration 763, loss = 0.47399676\n",
            "Iteration 764, loss = 0.47389330\n",
            "Iteration 765, loss = 0.47379939\n",
            "Iteration 766, loss = 0.47369005\n",
            "Iteration 767, loss = 0.47360314\n",
            "Iteration 768, loss = 0.47347313\n",
            "Iteration 769, loss = 0.47338302\n",
            "Iteration 770, loss = 0.47329210\n",
            "Iteration 771, loss = 0.47316947\n",
            "Iteration 772, loss = 0.47309398\n",
            "Iteration 773, loss = 0.47297320\n",
            "Iteration 774, loss = 0.47288582\n",
            "Iteration 775, loss = 0.47278793\n",
            "Iteration 776, loss = 0.47271588\n",
            "Iteration 777, loss = 0.47259064\n",
            "Iteration 778, loss = 0.47248288\n",
            "Iteration 779, loss = 0.47241079\n",
            "Iteration 780, loss = 0.47228580\n",
            "Iteration 781, loss = 0.47220436\n",
            "Iteration 782, loss = 0.47210513\n",
            "Iteration 783, loss = 0.47200434\n",
            "Iteration 784, loss = 0.47190323\n",
            "Iteration 785, loss = 0.47182154\n",
            "Iteration 786, loss = 0.47171549\n",
            "Iteration 787, loss = 0.47164121\n",
            "Iteration 788, loss = 0.47150884\n",
            "Iteration 789, loss = 0.47143549\n",
            "Iteration 790, loss = 0.47132839\n",
            "Iteration 791, loss = 0.47125631\n",
            "Iteration 792, loss = 0.47116134\n",
            "Iteration 793, loss = 0.47107364\n",
            "Iteration 794, loss = 0.47099223\n",
            "Iteration 795, loss = 0.47089574\n",
            "Iteration 796, loss = 0.47080300\n",
            "Iteration 797, loss = 0.47071571\n",
            "Iteration 798, loss = 0.47061738\n",
            "Iteration 799, loss = 0.47053498\n",
            "Iteration 800, loss = 0.47044916\n",
            "Iteration 801, loss = 0.47037180\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.71711807\n",
            "Iteration 2, loss = 0.71586397\n",
            "Iteration 3, loss = 0.71493458\n",
            "Iteration 4, loss = 0.71377472\n",
            "Iteration 5, loss = 0.71275038\n",
            "Iteration 6, loss = 0.71184623\n",
            "Iteration 7, loss = 0.71078662\n",
            "Iteration 8, loss = 0.70982546\n",
            "Iteration 9, loss = 0.70897576\n",
            "Iteration 10, loss = 0.70801571\n",
            "Iteration 11, loss = 0.70718221\n",
            "Iteration 12, loss = 0.70635551\n",
            "Iteration 13, loss = 0.70551483\n",
            "Iteration 14, loss = 0.70485136\n",
            "Iteration 15, loss = 0.70400059\n",
            "Iteration 16, loss = 0.70337484\n",
            "Iteration 17, loss = 0.70260910\n",
            "Iteration 18, loss = 0.70200277\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.71712569\n",
            "Iteration 2, loss = 0.71593409\n",
            "Iteration 3, loss = 0.71498259\n",
            "Iteration 4, loss = 0.71391058\n",
            "Iteration 5, loss = 0.71285265\n",
            "Iteration 6, loss = 0.71200107\n",
            "Iteration 7, loss = 0.71099403\n",
            "Iteration 8, loss = 0.71004458\n",
            "Iteration 9, loss = 0.70919876\n",
            "Iteration 10, loss = 0.70826713\n",
            "Iteration 11, loss = 0.70746579\n",
            "Iteration 12, loss = 0.70660958\n",
            "Iteration 13, loss = 0.70579791\n",
            "Iteration 14, loss = 0.70511576\n",
            "Iteration 15, loss = 0.70435527\n",
            "Iteration 16, loss = 0.70366970\n",
            "Iteration 17, loss = 0.70291417\n",
            "Iteration 18, loss = 0.70228020\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.71708868\n",
            "Iteration 2, loss = 0.71585281\n",
            "Iteration 3, loss = 0.71482846\n",
            "Iteration 4, loss = 0.71373929\n",
            "Iteration 5, loss = 0.71264244\n",
            "Iteration 6, loss = 0.71179257\n",
            "Iteration 7, loss = 0.71070568\n",
            "Iteration 8, loss = 0.70972092\n",
            "Iteration 9, loss = 0.70882161\n",
            "Iteration 10, loss = 0.70793441\n",
            "Iteration 11, loss = 0.70714118\n",
            "Iteration 12, loss = 0.70631186\n",
            "Iteration 13, loss = 0.70547731\n",
            "Iteration 14, loss = 0.70481863\n",
            "Iteration 15, loss = 0.70406213\n",
            "Iteration 16, loss = 0.70342998\n",
            "Iteration 17, loss = 0.70265306\n",
            "Iteration 18, loss = 0.70208012\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.71709503\n",
            "Iteration 2, loss = 0.71588915\n",
            "Iteration 3, loss = 0.71491711\n",
            "Iteration 4, loss = 0.71383716\n",
            "Iteration 5, loss = 0.71278470\n",
            "Iteration 6, loss = 0.71190597\n",
            "Iteration 7, loss = 0.71086813\n",
            "Iteration 8, loss = 0.70990736\n",
            "Iteration 9, loss = 0.70900638\n",
            "Iteration 10, loss = 0.70812529\n",
            "Iteration 11, loss = 0.70733261\n",
            "Iteration 12, loss = 0.70642457\n",
            "Iteration 13, loss = 0.70559984\n",
            "Iteration 14, loss = 0.70493451\n",
            "Iteration 15, loss = 0.70420525\n",
            "Iteration 16, loss = 0.70349177\n",
            "Iteration 17, loss = 0.70279350\n",
            "Iteration 18, loss = 0.70214295\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.71703974\n",
            "Iteration 2, loss = 0.71588542\n",
            "Iteration 3, loss = 0.71477858\n",
            "Iteration 4, loss = 0.71367209\n",
            "Iteration 5, loss = 0.71256737\n",
            "Iteration 6, loss = 0.71174364\n",
            "Iteration 7, loss = 0.71066989\n",
            "Iteration 8, loss = 0.70971830\n",
            "Iteration 9, loss = 0.70882839\n",
            "Iteration 10, loss = 0.70801967\n",
            "Iteration 11, loss = 0.70721694\n",
            "Iteration 12, loss = 0.70637381\n",
            "Iteration 13, loss = 0.70558334\n",
            "Iteration 14, loss = 0.70493271\n",
            "Iteration 15, loss = 0.70424014\n",
            "Iteration 16, loss = 0.70351352\n",
            "Iteration 17, loss = 0.70287053\n",
            "Iteration 18, loss = 0.70226272\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.71637661\n",
            "Iteration 2, loss = 0.71525936\n",
            "Iteration 3, loss = 0.71415897\n",
            "Iteration 4, loss = 0.71313144\n",
            "Iteration 5, loss = 0.71199846\n",
            "Iteration 6, loss = 0.71113812\n",
            "Iteration 7, loss = 0.71014127\n",
            "Iteration 8, loss = 0.70918612\n",
            "Iteration 9, loss = 0.70834956\n",
            "Iteration 10, loss = 0.70749947\n",
            "Iteration 11, loss = 0.70672606\n",
            "Iteration 12, loss = 0.70590015\n",
            "Iteration 13, loss = 0.70517982\n",
            "Iteration 14, loss = 0.70442197\n",
            "Iteration 15, loss = 0.70376690\n",
            "Iteration 16, loss = 0.70308132\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.71633851\n",
            "Iteration 2, loss = 0.71527056\n",
            "Iteration 3, loss = 0.71423059\n",
            "Iteration 4, loss = 0.71312904\n",
            "Iteration 5, loss = 0.71213935\n",
            "Iteration 6, loss = 0.71118687\n",
            "Iteration 7, loss = 0.71024315\n",
            "Iteration 8, loss = 0.70924614\n",
            "Iteration 9, loss = 0.70841515\n",
            "Iteration 10, loss = 0.70756432\n",
            "Iteration 11, loss = 0.70674513\n",
            "Iteration 12, loss = 0.70601579\n",
            "Iteration 13, loss = 0.70524084\n",
            "Iteration 14, loss = 0.70447708\n",
            "Iteration 15, loss = 0.70384692\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.71633136\n",
            "Iteration 2, loss = 0.71528775\n",
            "Iteration 3, loss = 0.71419178\n",
            "Iteration 4, loss = 0.71314489\n",
            "Iteration 5, loss = 0.71211780\n",
            "Iteration 6, loss = 0.71113406\n",
            "Iteration 7, loss = 0.71021722\n",
            "Iteration 8, loss = 0.70924399\n",
            "Iteration 9, loss = 0.70836311\n",
            "Iteration 10, loss = 0.70755755\n",
            "Iteration 11, loss = 0.70676380\n",
            "Iteration 12, loss = 0.70605033\n",
            "Iteration 13, loss = 0.70530609\n",
            "Iteration 14, loss = 0.70454170\n",
            "Iteration 15, loss = 0.70397148\n",
            "Iteration 16, loss = 0.70322229\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.71632995\n",
            "Iteration 2, loss = 0.71524002\n",
            "Iteration 3, loss = 0.71415568\n",
            "Iteration 4, loss = 0.71311245\n",
            "Iteration 5, loss = 0.71210480\n",
            "Iteration 6, loss = 0.71112511\n",
            "Iteration 7, loss = 0.71017822\n",
            "Iteration 8, loss = 0.70927331\n",
            "Iteration 9, loss = 0.70836170\n",
            "Iteration 10, loss = 0.70753141\n",
            "Iteration 11, loss = 0.70678646\n",
            "Iteration 12, loss = 0.70597518\n",
            "Iteration 13, loss = 0.70525100\n",
            "Iteration 14, loss = 0.70450237\n",
            "Iteration 15, loss = 0.70390566\n",
            "Iteration 16, loss = 0.70312369\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.71637018\n",
            "Iteration 2, loss = 0.71527963\n",
            "Iteration 3, loss = 0.71423532\n",
            "Iteration 4, loss = 0.71317064\n",
            "Iteration 5, loss = 0.71217122\n",
            "Iteration 6, loss = 0.71125678\n",
            "Iteration 7, loss = 0.71027745\n",
            "Iteration 8, loss = 0.70938656\n",
            "Iteration 9, loss = 0.70852417\n",
            "Iteration 10, loss = 0.70768044\n",
            "Iteration 11, loss = 0.70692153\n",
            "Iteration 12, loss = 0.70615139\n",
            "Iteration 13, loss = 0.70538379\n",
            "Iteration 14, loss = 0.70468847\n",
            "Iteration 15, loss = 0.70406962\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.71738768\n",
            "Iteration 2, loss = 0.71704572\n",
            "Iteration 3, loss = 0.71660725\n",
            "Iteration 4, loss = 0.71596549\n",
            "Iteration 5, loss = 0.71531625\n",
            "Iteration 6, loss = 0.71469947\n",
            "Iteration 7, loss = 0.71396903\n",
            "Iteration 8, loss = 0.71328030\n",
            "Iteration 9, loss = 0.71263146\n",
            "Iteration 10, loss = 0.71191347\n",
            "Iteration 11, loss = 0.71127098\n",
            "Iteration 12, loss = 0.71063321\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.71742312\n",
            "Iteration 2, loss = 0.71707214\n",
            "Iteration 3, loss = 0.71660740\n",
            "Iteration 4, loss = 0.71599519\n",
            "Iteration 5, loss = 0.71532133\n",
            "Iteration 6, loss = 0.71472424\n",
            "Iteration 7, loss = 0.71401903\n",
            "Iteration 8, loss = 0.71333392\n",
            "Iteration 9, loss = 0.71269140\n",
            "Iteration 10, loss = 0.71199907\n",
            "Iteration 11, loss = 0.71138144\n",
            "Iteration 12, loss = 0.71073497\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.71738163\n",
            "Iteration 2, loss = 0.71704389\n",
            "Iteration 3, loss = 0.71658616\n",
            "Iteration 4, loss = 0.71600688\n",
            "Iteration 5, loss = 0.71534361\n",
            "Iteration 6, loss = 0.71475785\n",
            "Iteration 7, loss = 0.71400749\n",
            "Iteration 8, loss = 0.71330133\n",
            "Iteration 9, loss = 0.71261237\n",
            "Iteration 10, loss = 0.71192925\n",
            "Iteration 11, loss = 0.71130602\n",
            "Iteration 12, loss = 0.71066536\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.71736906\n",
            "Iteration 2, loss = 0.71701681\n",
            "Iteration 3, loss = 0.71654884\n",
            "Iteration 4, loss = 0.71593957\n",
            "Iteration 5, loss = 0.71526324\n",
            "Iteration 6, loss = 0.71464509\n",
            "Iteration 7, loss = 0.71391241\n",
            "Iteration 8, loss = 0.71321203\n",
            "Iteration 9, loss = 0.71252839\n",
            "Iteration 10, loss = 0.71185601\n",
            "Iteration 11, loss = 0.71123230\n",
            "Iteration 12, loss = 0.71054099\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.71738768\n",
            "Iteration 2, loss = 0.71707108\n",
            "Iteration 3, loss = 0.71658168\n",
            "Iteration 4, loss = 0.71598456\n",
            "Iteration 5, loss = 0.71529691\n",
            "Iteration 6, loss = 0.71470892\n",
            "Iteration 7, loss = 0.71394187\n",
            "Iteration 8, loss = 0.71323387\n",
            "Iteration 9, loss = 0.71255801\n",
            "Iteration 10, loss = 0.71193806\n",
            "Iteration 11, loss = 0.71130664\n",
            "Iteration 12, loss = 0.71065473\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.71672616\n",
            "Iteration 2, loss = 0.71640724\n",
            "Iteration 3, loss = 0.71590039\n",
            "Iteration 4, loss = 0.71531332\n",
            "Iteration 5, loss = 0.71459453\n",
            "Iteration 6, loss = 0.71397023\n",
            "Iteration 7, loss = 0.71323868\n",
            "Iteration 8, loss = 0.71251784\n",
            "Iteration 9, loss = 0.71187160\n",
            "Iteration 10, loss = 0.71122248\n",
            "Iteration 11, loss = 0.71060939\n",
            "Iteration 12, loss = 0.70996545\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.71670285\n",
            "Iteration 2, loss = 0.71636823\n",
            "Iteration 3, loss = 0.71586783\n",
            "Iteration 4, loss = 0.71524593\n",
            "Iteration 5, loss = 0.71461309\n",
            "Iteration 6, loss = 0.71396170\n",
            "Iteration 7, loss = 0.71329015\n",
            "Iteration 8, loss = 0.71257576\n",
            "Iteration 9, loss = 0.71195264\n",
            "Iteration 10, loss = 0.71132275\n",
            "Iteration 11, loss = 0.71070727\n",
            "Iteration 12, loss = 0.71014340\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.71671597\n",
            "Iteration 2, loss = 0.71639104\n",
            "Iteration 3, loss = 0.71587420\n",
            "Iteration 4, loss = 0.71527653\n",
            "Iteration 5, loss = 0.71463182\n",
            "Iteration 6, loss = 0.71397179\n",
            "Iteration 7, loss = 0.71332206\n",
            "Iteration 8, loss = 0.71261582\n",
            "Iteration 9, loss = 0.71196239\n",
            "Iteration 10, loss = 0.71135800\n",
            "Iteration 11, loss = 0.71075131\n",
            "Iteration 12, loss = 0.71019124\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.71670937\n",
            "Iteration 2, loss = 0.71637796\n",
            "Iteration 3, loss = 0.71585965\n",
            "Iteration 4, loss = 0.71525212\n",
            "Iteration 5, loss = 0.71459822\n",
            "Iteration 6, loss = 0.71392303\n",
            "Iteration 7, loss = 0.71324573\n",
            "Iteration 8, loss = 0.71256875\n",
            "Iteration 9, loss = 0.71188349\n",
            "Iteration 10, loss = 0.71125511\n",
            "Iteration 11, loss = 0.71066969\n",
            "Iteration 12, loss = 0.71004382\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.71675191\n",
            "Iteration 2, loss = 0.71641196\n",
            "Iteration 3, loss = 0.71589572\n",
            "Iteration 4, loss = 0.71526442\n",
            "Iteration 5, loss = 0.71459604\n",
            "Iteration 6, loss = 0.71394406\n",
            "Iteration 7, loss = 0.71322893\n",
            "Iteration 8, loss = 0.71255076\n",
            "Iteration 9, loss = 0.71188671\n",
            "Iteration 10, loss = 0.71124279\n",
            "Iteration 11, loss = 0.71064679\n",
            "Iteration 12, loss = 0.71004469\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.89102520\n",
            "Iteration 2, loss = 0.88585986\n",
            "Iteration 3, loss = 0.88123689\n",
            "Iteration 4, loss = 0.87628337\n",
            "Iteration 5, loss = 0.87153747\n",
            "Iteration 6, loss = 0.86704128\n",
            "Iteration 7, loss = 0.86233568\n",
            "Iteration 8, loss = 0.85781008\n",
            "Iteration 9, loss = 0.85343785\n",
            "Iteration 10, loss = 0.84891277\n",
            "Iteration 11, loss = 0.84461761\n",
            "Iteration 12, loss = 0.84036848\n",
            "Iteration 13, loss = 0.83606680\n",
            "Iteration 14, loss = 0.83205728\n",
            "Iteration 15, loss = 0.82773636\n",
            "Iteration 16, loss = 0.82377788\n",
            "Iteration 17, loss = 0.81956828\n",
            "Iteration 18, loss = 0.81557601\n",
            "Iteration 19, loss = 0.81161315\n",
            "Iteration 20, loss = 0.80764891\n",
            "Iteration 21, loss = 0.80360178\n",
            "Iteration 22, loss = 0.79963197\n",
            "Iteration 23, loss = 0.79591690\n",
            "Iteration 24, loss = 0.79180876\n",
            "Iteration 25, loss = 0.78814090\n",
            "Iteration 26, loss = 0.78417899\n",
            "Iteration 27, loss = 0.78040455\n",
            "Iteration 28, loss = 0.77636193\n",
            "Iteration 29, loss = 0.77245950\n",
            "Iteration 30, loss = 0.76864595\n",
            "Iteration 31, loss = 0.76460822\n",
            "Iteration 32, loss = 0.76067549\n",
            "Iteration 33, loss = 0.75710456\n",
            "Iteration 34, loss = 0.75326593\n",
            "Iteration 35, loss = 0.74971538\n",
            "Iteration 36, loss = 0.74600771\n",
            "Iteration 37, loss = 0.74255701\n",
            "Iteration 38, loss = 0.73927111\n",
            "Iteration 39, loss = 0.73587841\n",
            "Iteration 40, loss = 0.73262166\n",
            "Iteration 41, loss = 0.72952079\n",
            "Iteration 42, loss = 0.72642009\n",
            "Iteration 43, loss = 0.72331168\n",
            "Iteration 44, loss = 0.72046501\n",
            "Iteration 45, loss = 0.71764067\n",
            "Iteration 46, loss = 0.71496879\n",
            "Iteration 47, loss = 0.71226003\n",
            "Iteration 48, loss = 0.70965080\n",
            "Iteration 49, loss = 0.70723204\n",
            "Iteration 50, loss = 0.70477509\n",
            "Iteration 51, loss = 0.70249042\n",
            "Iteration 52, loss = 0.70033615\n",
            "Iteration 53, loss = 0.69811505\n",
            "Iteration 54, loss = 0.69594047\n",
            "Iteration 55, loss = 0.69402518\n",
            "Iteration 56, loss = 0.69193776\n",
            "Iteration 57, loss = 0.69000612\n",
            "Iteration 58, loss = 0.68830698\n",
            "Iteration 59, loss = 0.68650194\n",
            "Iteration 60, loss = 0.68471826\n",
            "Iteration 61, loss = 0.68300752\n",
            "Iteration 62, loss = 0.68143949\n",
            "Iteration 63, loss = 0.67967962\n",
            "Iteration 64, loss = 0.67802401\n",
            "Iteration 65, loss = 0.67646078\n",
            "Iteration 66, loss = 0.67484303\n",
            "Iteration 67, loss = 0.67319922\n",
            "Iteration 68, loss = 0.67151506\n",
            "Iteration 69, loss = 0.66994136\n",
            "Iteration 70, loss = 0.66830716\n",
            "Iteration 71, loss = 0.66679343\n",
            "Iteration 72, loss = 0.66512411\n",
            "Iteration 73, loss = 0.66340461\n",
            "Iteration 74, loss = 0.66179302\n",
            "Iteration 75, loss = 0.66001844\n",
            "Iteration 76, loss = 0.65830475\n",
            "Iteration 77, loss = 0.65656774\n",
            "Iteration 78, loss = 0.65476132\n",
            "Iteration 79, loss = 0.65306689\n",
            "Iteration 80, loss = 0.65135256\n",
            "Iteration 81, loss = 0.64958836\n",
            "Iteration 82, loss = 0.64778164\n",
            "Iteration 83, loss = 0.64602316\n",
            "Iteration 84, loss = 0.64428746\n",
            "Iteration 85, loss = 0.64240701\n",
            "Iteration 86, loss = 0.64080861\n",
            "Iteration 87, loss = 0.63895139\n",
            "Iteration 88, loss = 0.63721033\n",
            "Iteration 89, loss = 0.63545612\n",
            "Iteration 90, loss = 0.63379372\n",
            "Iteration 91, loss = 0.63200506\n",
            "Iteration 92, loss = 0.63032008\n",
            "Iteration 93, loss = 0.62861912\n",
            "Iteration 94, loss = 0.62703006\n",
            "Iteration 95, loss = 0.62533584\n",
            "Iteration 96, loss = 0.62361349\n",
            "Iteration 97, loss = 0.62198590\n",
            "Iteration 98, loss = 0.62030720\n",
            "Iteration 99, loss = 0.61861645\n",
            "Iteration 100, loss = 0.61700690\n",
            "Iteration 101, loss = 0.61528327\n",
            "Iteration 102, loss = 0.61371250\n",
            "Iteration 103, loss = 0.61197306\n",
            "Iteration 104, loss = 0.61043217\n",
            "Iteration 105, loss = 0.60872868\n",
            "Iteration 106, loss = 0.60714053\n",
            "Iteration 107, loss = 0.60547416\n",
            "Iteration 108, loss = 0.60375861\n",
            "Iteration 109, loss = 0.60220559\n",
            "Iteration 110, loss = 0.60052257\n",
            "Iteration 111, loss = 0.59881816\n",
            "Iteration 112, loss = 0.59721006\n",
            "Iteration 113, loss = 0.59544059\n",
            "Iteration 114, loss = 0.59375187\n",
            "Iteration 115, loss = 0.59212234\n",
            "Iteration 116, loss = 0.59045278\n",
            "Iteration 117, loss = 0.58868266\n",
            "Iteration 118, loss = 0.58703047\n",
            "Iteration 119, loss = 0.58542560\n",
            "Iteration 120, loss = 0.58371150\n",
            "Iteration 121, loss = 0.58206128\n",
            "Iteration 122, loss = 0.58047875\n",
            "Iteration 123, loss = 0.57884641\n",
            "Iteration 124, loss = 0.57719915\n",
            "Iteration 125, loss = 0.57558651\n",
            "Iteration 126, loss = 0.57400852\n",
            "Iteration 127, loss = 0.57236158\n",
            "Iteration 128, loss = 0.57067163\n",
            "Iteration 129, loss = 0.56902601\n",
            "Iteration 130, loss = 0.56737924\n",
            "Iteration 131, loss = 0.56577523\n",
            "Iteration 132, loss = 0.56420141\n",
            "Iteration 133, loss = 0.56250550\n",
            "Iteration 134, loss = 0.56095333\n",
            "Iteration 135, loss = 0.55927564\n",
            "Iteration 136, loss = 0.55767946\n",
            "Iteration 137, loss = 0.55606199\n",
            "Iteration 138, loss = 0.55435344\n",
            "Iteration 139, loss = 0.55279545\n",
            "Iteration 140, loss = 0.55100923\n",
            "Iteration 141, loss = 0.54934694\n",
            "Iteration 142, loss = 0.54746794\n",
            "Iteration 143, loss = 0.54576960\n",
            "Iteration 144, loss = 0.54400655\n",
            "Iteration 145, loss = 0.54207925\n",
            "Iteration 146, loss = 0.54031970\n",
            "Iteration 147, loss = 0.53838579\n",
            "Iteration 148, loss = 0.53646389\n",
            "Iteration 149, loss = 0.53461760\n",
            "Iteration 150, loss = 0.53262543\n",
            "Iteration 151, loss = 0.53069639\n",
            "Iteration 152, loss = 0.52871463\n",
            "Iteration 153, loss = 0.52669846\n",
            "Iteration 154, loss = 0.52468013\n",
            "Iteration 155, loss = 0.52270157\n",
            "Iteration 156, loss = 0.52073035\n",
            "Iteration 157, loss = 0.51880749\n",
            "Iteration 158, loss = 0.51689397\n",
            "Iteration 159, loss = 0.51490479\n",
            "Iteration 160, loss = 0.51311304\n",
            "Iteration 161, loss = 0.51129279\n",
            "Iteration 162, loss = 0.50959457\n",
            "Iteration 163, loss = 0.50796252\n",
            "Iteration 164, loss = 0.50619604\n",
            "Iteration 165, loss = 0.50469476\n",
            "Iteration 166, loss = 0.50316331\n",
            "Iteration 167, loss = 0.50164495\n",
            "Iteration 168, loss = 0.50032832\n",
            "Iteration 169, loss = 0.49890195\n",
            "Iteration 170, loss = 0.49748794\n",
            "Iteration 171, loss = 0.49621710\n",
            "Iteration 172, loss = 0.49488132\n",
            "Iteration 173, loss = 0.49367013\n",
            "Iteration 174, loss = 0.49276895\n",
            "Iteration 175, loss = 0.49135614\n",
            "Iteration 176, loss = 0.49025450\n",
            "Iteration 177, loss = 0.48924491\n",
            "Iteration 178, loss = 0.48825745\n",
            "Iteration 179, loss = 0.48717781\n",
            "Iteration 180, loss = 0.48624057\n",
            "Iteration 181, loss = 0.48532458\n",
            "Iteration 182, loss = 0.48437275\n",
            "Iteration 183, loss = 0.48345110\n",
            "Iteration 184, loss = 0.48270822\n",
            "Iteration 185, loss = 0.48186028\n",
            "Iteration 186, loss = 0.48089155\n",
            "Iteration 187, loss = 0.48009096\n",
            "Iteration 188, loss = 0.47940900\n",
            "Iteration 189, loss = 0.47849880\n",
            "Iteration 190, loss = 0.47774660\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.89213036\n",
            "Iteration 2, loss = 0.88705691\n",
            "Iteration 3, loss = 0.88238085\n",
            "Iteration 4, loss = 0.87756212\n",
            "Iteration 5, loss = 0.87275868\n",
            "Iteration 6, loss = 0.86835549\n",
            "Iteration 7, loss = 0.86374343\n",
            "Iteration 8, loss = 0.85923618\n",
            "Iteration 9, loss = 0.85490322\n",
            "Iteration 10, loss = 0.85045080\n",
            "Iteration 11, loss = 0.84622777\n",
            "Iteration 12, loss = 0.84195490\n",
            "Iteration 13, loss = 0.83771133\n",
            "Iteration 14, loss = 0.83370135\n",
            "Iteration 15, loss = 0.82957448\n",
            "Iteration 16, loss = 0.82555904\n",
            "Iteration 17, loss = 0.82141078\n",
            "Iteration 18, loss = 0.81742906\n",
            "Iteration 19, loss = 0.81354101\n",
            "Iteration 20, loss = 0.80955953\n",
            "Iteration 21, loss = 0.80558300\n",
            "Iteration 22, loss = 0.80160515\n",
            "Iteration 23, loss = 0.79791952\n",
            "Iteration 24, loss = 0.79390299\n",
            "Iteration 25, loss = 0.79022152\n",
            "Iteration 26, loss = 0.78636035\n",
            "Iteration 27, loss = 0.78268935\n",
            "Iteration 28, loss = 0.77871710\n",
            "Iteration 29, loss = 0.77502949\n",
            "Iteration 30, loss = 0.77131891\n",
            "Iteration 31, loss = 0.76748298\n",
            "Iteration 32, loss = 0.76372368\n",
            "Iteration 33, loss = 0.76014070\n",
            "Iteration 34, loss = 0.75649793\n",
            "Iteration 35, loss = 0.75293771\n",
            "Iteration 36, loss = 0.74938677\n",
            "Iteration 37, loss = 0.74603249\n",
            "Iteration 38, loss = 0.74274880\n",
            "Iteration 39, loss = 0.73944137\n",
            "Iteration 40, loss = 0.73629698\n",
            "Iteration 41, loss = 0.73324854\n",
            "Iteration 42, loss = 0.73028085\n",
            "Iteration 43, loss = 0.72742226\n",
            "Iteration 44, loss = 0.72460354\n",
            "Iteration 45, loss = 0.72181875\n",
            "Iteration 46, loss = 0.71930007\n",
            "Iteration 47, loss = 0.71664848\n",
            "Iteration 48, loss = 0.71418150\n",
            "Iteration 49, loss = 0.71184945\n",
            "Iteration 50, loss = 0.70948650\n",
            "Iteration 51, loss = 0.70744504\n",
            "Iteration 52, loss = 0.70530476\n",
            "Iteration 53, loss = 0.70335981\n",
            "Iteration 54, loss = 0.70138387\n",
            "Iteration 55, loss = 0.69949565\n",
            "Iteration 56, loss = 0.69771104\n",
            "Iteration 57, loss = 0.69598645\n",
            "Iteration 58, loss = 0.69452265\n",
            "Iteration 59, loss = 0.69294917\n",
            "Iteration 60, loss = 0.69140708\n",
            "Iteration 61, loss = 0.68990219\n",
            "Iteration 62, loss = 0.68859935\n",
            "Iteration 63, loss = 0.68715645\n",
            "Iteration 64, loss = 0.68574269\n",
            "Iteration 65, loss = 0.68447795\n",
            "Iteration 66, loss = 0.68315411\n",
            "Iteration 67, loss = 0.68183448\n",
            "Iteration 68, loss = 0.68052036\n",
            "Iteration 69, loss = 0.67927016\n",
            "Iteration 70, loss = 0.67800590\n",
            "Iteration 71, loss = 0.67681070\n",
            "Iteration 72, loss = 0.67549834\n",
            "Iteration 73, loss = 0.67416257\n",
            "Iteration 74, loss = 0.67288282\n",
            "Iteration 75, loss = 0.67147233\n",
            "Iteration 76, loss = 0.67006935\n",
            "Iteration 77, loss = 0.66874208\n",
            "Iteration 78, loss = 0.66721310\n",
            "Iteration 79, loss = 0.66582384\n",
            "Iteration 80, loss = 0.66447745\n",
            "Iteration 81, loss = 0.66291129\n",
            "Iteration 82, loss = 0.66135839\n",
            "Iteration 83, loss = 0.65981448\n",
            "Iteration 84, loss = 0.65823584\n",
            "Iteration 85, loss = 0.65660664\n",
            "Iteration 86, loss = 0.65512927\n",
            "Iteration 87, loss = 0.65343447\n",
            "Iteration 88, loss = 0.65186964\n",
            "Iteration 89, loss = 0.65024655\n",
            "Iteration 90, loss = 0.64872628\n",
            "Iteration 91, loss = 0.64705348\n",
            "Iteration 92, loss = 0.64546456\n",
            "Iteration 93, loss = 0.64381657\n",
            "Iteration 94, loss = 0.64229699\n",
            "Iteration 95, loss = 0.64066518\n",
            "Iteration 96, loss = 0.63909229\n",
            "Iteration 97, loss = 0.63743648\n",
            "Iteration 98, loss = 0.63591004\n",
            "Iteration 99, loss = 0.63431341\n",
            "Iteration 100, loss = 0.63279284\n",
            "Iteration 101, loss = 0.63119454\n",
            "Iteration 102, loss = 0.62971402\n",
            "Iteration 103, loss = 0.62809920\n",
            "Iteration 104, loss = 0.62663793\n",
            "Iteration 105, loss = 0.62511026\n",
            "Iteration 106, loss = 0.62366015\n",
            "Iteration 107, loss = 0.62208697\n",
            "Iteration 108, loss = 0.62050661\n",
            "Iteration 109, loss = 0.61903594\n",
            "Iteration 110, loss = 0.61749175\n",
            "Iteration 111, loss = 0.61597519\n",
            "Iteration 112, loss = 0.61444794\n",
            "Iteration 113, loss = 0.61287103\n",
            "Iteration 114, loss = 0.61133890\n",
            "Iteration 115, loss = 0.60983144\n",
            "Iteration 116, loss = 0.60822525\n",
            "Iteration 117, loss = 0.60660997\n",
            "Iteration 118, loss = 0.60505804\n",
            "Iteration 119, loss = 0.60356740\n",
            "Iteration 120, loss = 0.60197532\n",
            "Iteration 121, loss = 0.60043972\n",
            "Iteration 122, loss = 0.59892662\n",
            "Iteration 123, loss = 0.59739691\n",
            "Iteration 124, loss = 0.59584412\n",
            "Iteration 125, loss = 0.59432230\n",
            "Iteration 126, loss = 0.59288344\n",
            "Iteration 127, loss = 0.59130218\n",
            "Iteration 128, loss = 0.58976303\n",
            "Iteration 129, loss = 0.58817266\n",
            "Iteration 130, loss = 0.58660190\n",
            "Iteration 131, loss = 0.58514350\n",
            "Iteration 132, loss = 0.58360341\n",
            "Iteration 133, loss = 0.58205606\n",
            "Iteration 134, loss = 0.58060190\n",
            "Iteration 135, loss = 0.57903723\n",
            "Iteration 136, loss = 0.57748095\n",
            "Iteration 137, loss = 0.57595476\n",
            "Iteration 138, loss = 0.57433760\n",
            "Iteration 139, loss = 0.57284967\n",
            "Iteration 140, loss = 0.57122142\n",
            "Iteration 141, loss = 0.56971185\n",
            "Iteration 142, loss = 0.56802218\n",
            "Iteration 143, loss = 0.56649152\n",
            "Iteration 144, loss = 0.56490126\n",
            "Iteration 145, loss = 0.56309020\n",
            "Iteration 146, loss = 0.56157711\n",
            "Iteration 147, loss = 0.55979366\n",
            "Iteration 148, loss = 0.55803863\n",
            "Iteration 149, loss = 0.55634948\n",
            "Iteration 150, loss = 0.55449756\n",
            "Iteration 151, loss = 0.55282182\n",
            "Iteration 152, loss = 0.55091553\n",
            "Iteration 153, loss = 0.54904685\n",
            "Iteration 154, loss = 0.54717471\n",
            "Iteration 155, loss = 0.54521902\n",
            "Iteration 156, loss = 0.54333187\n",
            "Iteration 157, loss = 0.54134755\n",
            "Iteration 158, loss = 0.53954353\n",
            "Iteration 159, loss = 0.53748549\n",
            "Iteration 160, loss = 0.53565168\n",
            "Iteration 161, loss = 0.53372970\n",
            "Iteration 162, loss = 0.53196547\n",
            "Iteration 163, loss = 0.53030471\n",
            "Iteration 164, loss = 0.52844050\n",
            "Iteration 165, loss = 0.52684015\n",
            "Iteration 166, loss = 0.52524480\n",
            "Iteration 167, loss = 0.52363403\n",
            "Iteration 168, loss = 0.52218611\n",
            "Iteration 169, loss = 0.52069795\n",
            "Iteration 170, loss = 0.51918927\n",
            "Iteration 171, loss = 0.51780183\n",
            "Iteration 172, loss = 0.51651102\n",
            "Iteration 173, loss = 0.51513798\n",
            "Iteration 174, loss = 0.51405313\n",
            "Iteration 175, loss = 0.51260261\n",
            "Iteration 176, loss = 0.51136711\n",
            "Iteration 177, loss = 0.51020170\n",
            "Iteration 178, loss = 0.50923989\n",
            "Iteration 179, loss = 0.50790485\n",
            "Iteration 180, loss = 0.50687860\n",
            "Iteration 181, loss = 0.50581577\n",
            "Iteration 182, loss = 0.50476965\n",
            "Iteration 183, loss = 0.50375592\n",
            "Iteration 184, loss = 0.50281732\n",
            "Iteration 185, loss = 0.50188816\n",
            "Iteration 186, loss = 0.50079704\n",
            "Iteration 187, loss = 0.49992536\n",
            "Iteration 188, loss = 0.49901622\n",
            "Iteration 189, loss = 0.49808161\n",
            "Iteration 190, loss = 0.49721089\n",
            "Iteration 191, loss = 0.49630188\n",
            "Iteration 192, loss = 0.49548094\n",
            "Iteration 193, loss = 0.49468250\n",
            "Iteration 194, loss = 0.49381879\n",
            "Iteration 195, loss = 0.49305114\n",
            "Iteration 196, loss = 0.49218619\n",
            "Iteration 197, loss = 0.49140552\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.89019142\n",
            "Iteration 2, loss = 0.88511226\n",
            "Iteration 3, loss = 0.88039824\n",
            "Iteration 4, loss = 0.87561940\n",
            "Iteration 5, loss = 0.87082217\n",
            "Iteration 6, loss = 0.86646709\n",
            "Iteration 7, loss = 0.86174838\n",
            "Iteration 8, loss = 0.85721719\n",
            "Iteration 9, loss = 0.85281092\n",
            "Iteration 10, loss = 0.84843727\n",
            "Iteration 11, loss = 0.84424716\n",
            "Iteration 12, loss = 0.84001548\n",
            "Iteration 13, loss = 0.83571858\n",
            "Iteration 14, loss = 0.83174132\n",
            "Iteration 15, loss = 0.82757196\n",
            "Iteration 16, loss = 0.82360701\n",
            "Iteration 17, loss = 0.81934148\n",
            "Iteration 18, loss = 0.81539814\n",
            "Iteration 19, loss = 0.81145828\n",
            "Iteration 20, loss = 0.80743784\n",
            "Iteration 21, loss = 0.80341311\n",
            "Iteration 22, loss = 0.79938904\n",
            "Iteration 23, loss = 0.79558855\n",
            "Iteration 24, loss = 0.79143718\n",
            "Iteration 25, loss = 0.78762841\n",
            "Iteration 26, loss = 0.78364622\n",
            "Iteration 27, loss = 0.77987581\n",
            "Iteration 28, loss = 0.77578896\n",
            "Iteration 29, loss = 0.77191700\n",
            "Iteration 30, loss = 0.76819969\n",
            "Iteration 31, loss = 0.76417201\n",
            "Iteration 32, loss = 0.76033937\n",
            "Iteration 33, loss = 0.75656218\n",
            "Iteration 34, loss = 0.75288249\n",
            "Iteration 35, loss = 0.74916848\n",
            "Iteration 36, loss = 0.74555793\n",
            "Iteration 37, loss = 0.74204776\n",
            "Iteration 38, loss = 0.73872753\n",
            "Iteration 39, loss = 0.73530092\n",
            "Iteration 40, loss = 0.73211244\n",
            "Iteration 41, loss = 0.72907875\n",
            "Iteration 42, loss = 0.72604973\n",
            "Iteration 43, loss = 0.72309847\n",
            "Iteration 44, loss = 0.72021627\n",
            "Iteration 45, loss = 0.71739768\n",
            "Iteration 46, loss = 0.71484158\n",
            "Iteration 47, loss = 0.71209168\n",
            "Iteration 48, loss = 0.70959995\n",
            "Iteration 49, loss = 0.70719829\n",
            "Iteration 50, loss = 0.70482327\n",
            "Iteration 51, loss = 0.70265418\n",
            "Iteration 52, loss = 0.70045226\n",
            "Iteration 53, loss = 0.69829647\n",
            "Iteration 54, loss = 0.69628524\n",
            "Iteration 55, loss = 0.69431942\n",
            "Iteration 56, loss = 0.69238792\n",
            "Iteration 57, loss = 0.69044105\n",
            "Iteration 58, loss = 0.68881703\n",
            "Iteration 59, loss = 0.68701075\n",
            "Iteration 60, loss = 0.68535003\n",
            "Iteration 61, loss = 0.68363078\n",
            "Iteration 62, loss = 0.68207675\n",
            "Iteration 63, loss = 0.68044197\n",
            "Iteration 64, loss = 0.67878890\n",
            "Iteration 65, loss = 0.67720904\n",
            "Iteration 66, loss = 0.67558881\n",
            "Iteration 67, loss = 0.67396658\n",
            "Iteration 68, loss = 0.67228237\n",
            "Iteration 69, loss = 0.67068321\n",
            "Iteration 70, loss = 0.66901469\n",
            "Iteration 71, loss = 0.66741690\n",
            "Iteration 72, loss = 0.66573734\n",
            "Iteration 73, loss = 0.66397173\n",
            "Iteration 74, loss = 0.66230016\n",
            "Iteration 75, loss = 0.66052988\n",
            "Iteration 76, loss = 0.65867432\n",
            "Iteration 77, loss = 0.65694974\n",
            "Iteration 78, loss = 0.65510697\n",
            "Iteration 79, loss = 0.65334531\n",
            "Iteration 80, loss = 0.65164317\n",
            "Iteration 81, loss = 0.64983827\n",
            "Iteration 82, loss = 0.64798912\n",
            "Iteration 83, loss = 0.64623791\n",
            "Iteration 84, loss = 0.64441004\n",
            "Iteration 85, loss = 0.64259073\n",
            "Iteration 86, loss = 0.64096132\n",
            "Iteration 87, loss = 0.63913228\n",
            "Iteration 88, loss = 0.63739301\n",
            "Iteration 89, loss = 0.63568053\n",
            "Iteration 90, loss = 0.63399589\n",
            "Iteration 91, loss = 0.63227755\n",
            "Iteration 92, loss = 0.63057372\n",
            "Iteration 93, loss = 0.62886390\n",
            "Iteration 94, loss = 0.62720661\n",
            "Iteration 95, loss = 0.62559922\n",
            "Iteration 96, loss = 0.62401108\n",
            "Iteration 97, loss = 0.62224847\n",
            "Iteration 98, loss = 0.62069335\n",
            "Iteration 99, loss = 0.61913106\n",
            "Iteration 100, loss = 0.61760712\n",
            "Iteration 101, loss = 0.61599757\n",
            "Iteration 102, loss = 0.61449334\n",
            "Iteration 103, loss = 0.61288521\n",
            "Iteration 104, loss = 0.61140510\n",
            "Iteration 105, loss = 0.60982118\n",
            "Iteration 106, loss = 0.60834426\n",
            "Iteration 107, loss = 0.60681692\n",
            "Iteration 108, loss = 0.60526590\n",
            "Iteration 109, loss = 0.60378506\n",
            "Iteration 110, loss = 0.60224887\n",
            "Iteration 111, loss = 0.60072984\n",
            "Iteration 112, loss = 0.59920566\n",
            "Iteration 113, loss = 0.59766201\n",
            "Iteration 114, loss = 0.59610915\n",
            "Iteration 115, loss = 0.59454431\n",
            "Iteration 116, loss = 0.59300153\n",
            "Iteration 117, loss = 0.59145322\n",
            "Iteration 118, loss = 0.58990015\n",
            "Iteration 119, loss = 0.58842263\n",
            "Iteration 120, loss = 0.58686800\n",
            "Iteration 121, loss = 0.58536777\n",
            "Iteration 122, loss = 0.58382898\n",
            "Iteration 123, loss = 0.58237311\n",
            "Iteration 124, loss = 0.58075791\n",
            "Iteration 125, loss = 0.57927187\n",
            "Iteration 126, loss = 0.57776304\n",
            "Iteration 127, loss = 0.57627581\n",
            "Iteration 128, loss = 0.57472439\n",
            "Iteration 129, loss = 0.57314853\n",
            "Iteration 130, loss = 0.57164771\n",
            "Iteration 131, loss = 0.57017829\n",
            "Iteration 132, loss = 0.56873113\n",
            "Iteration 133, loss = 0.56729369\n",
            "Iteration 134, loss = 0.56588971\n",
            "Iteration 135, loss = 0.56441024\n",
            "Iteration 136, loss = 0.56295235\n",
            "Iteration 137, loss = 0.56149164\n",
            "Iteration 138, loss = 0.55996289\n",
            "Iteration 139, loss = 0.55852642\n",
            "Iteration 140, loss = 0.55705371\n",
            "Iteration 141, loss = 0.55559400\n",
            "Iteration 142, loss = 0.55407253\n",
            "Iteration 143, loss = 0.55265906\n",
            "Iteration 144, loss = 0.55115218\n",
            "Iteration 145, loss = 0.54953475\n",
            "Iteration 146, loss = 0.54808181\n",
            "Iteration 147, loss = 0.54654392\n",
            "Iteration 148, loss = 0.54496606\n",
            "Iteration 149, loss = 0.54342262\n",
            "Iteration 150, loss = 0.54186132\n",
            "Iteration 151, loss = 0.54038440\n",
            "Iteration 152, loss = 0.53879837\n",
            "Iteration 153, loss = 0.53706756\n",
            "Iteration 154, loss = 0.53549764\n",
            "Iteration 155, loss = 0.53378910\n",
            "Iteration 156, loss = 0.53218308\n",
            "Iteration 157, loss = 0.53044681\n",
            "Iteration 158, loss = 0.52879679\n",
            "Iteration 159, loss = 0.52690221\n",
            "Iteration 160, loss = 0.52519859\n",
            "Iteration 161, loss = 0.52338378\n",
            "Iteration 162, loss = 0.52165683\n",
            "Iteration 163, loss = 0.51992138\n",
            "Iteration 164, loss = 0.51806016\n",
            "Iteration 165, loss = 0.51641198\n",
            "Iteration 166, loss = 0.51472786\n",
            "Iteration 167, loss = 0.51296436\n",
            "Iteration 168, loss = 0.51140374\n",
            "Iteration 169, loss = 0.50972245\n",
            "Iteration 170, loss = 0.50814531\n",
            "Iteration 171, loss = 0.50659951\n",
            "Iteration 172, loss = 0.50506125\n",
            "Iteration 173, loss = 0.50357942\n",
            "Iteration 174, loss = 0.50233998\n",
            "Iteration 175, loss = 0.50086048\n",
            "Iteration 176, loss = 0.49955115\n",
            "Iteration 177, loss = 0.49831528\n",
            "Iteration 178, loss = 0.49718717\n",
            "Iteration 179, loss = 0.49589475\n",
            "Iteration 180, loss = 0.49483406\n",
            "Iteration 181, loss = 0.49375130\n",
            "Iteration 182, loss = 0.49265397\n",
            "Iteration 183, loss = 0.49159462\n",
            "Iteration 184, loss = 0.49063462\n",
            "Iteration 185, loss = 0.48960728\n",
            "Iteration 186, loss = 0.48857433\n",
            "Iteration 187, loss = 0.48768141\n",
            "Iteration 188, loss = 0.48680313\n",
            "Iteration 189, loss = 0.48590473\n",
            "Iteration 190, loss = 0.48502632\n",
            "Iteration 191, loss = 0.48415984\n",
            "Iteration 192, loss = 0.48333182\n",
            "Iteration 193, loss = 0.48249130\n",
            "Iteration 194, loss = 0.48161167\n",
            "Iteration 195, loss = 0.48087577\n",
            "Iteration 196, loss = 0.47995732\n",
            "Iteration 197, loss = 0.47926810\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.88938252\n",
            "Iteration 2, loss = 0.88431037\n",
            "Iteration 3, loss = 0.87964867\n",
            "Iteration 4, loss = 0.87487011\n",
            "Iteration 5, loss = 0.87013497\n",
            "Iteration 6, loss = 0.86573575\n",
            "Iteration 7, loss = 0.86111407\n",
            "Iteration 8, loss = 0.85666038\n",
            "Iteration 9, loss = 0.85228304\n",
            "Iteration 10, loss = 0.84797957\n",
            "Iteration 11, loss = 0.84382261\n",
            "Iteration 12, loss = 0.83948249\n",
            "Iteration 13, loss = 0.83524143\n",
            "Iteration 14, loss = 0.83125444\n",
            "Iteration 15, loss = 0.82715405\n",
            "Iteration 16, loss = 0.82301804\n",
            "Iteration 17, loss = 0.81891320\n",
            "Iteration 18, loss = 0.81479931\n",
            "Iteration 19, loss = 0.81079967\n",
            "Iteration 20, loss = 0.80667733\n",
            "Iteration 21, loss = 0.80255305\n",
            "Iteration 22, loss = 0.79862371\n",
            "Iteration 23, loss = 0.79459919\n",
            "Iteration 24, loss = 0.79043779\n",
            "Iteration 25, loss = 0.78658988\n",
            "Iteration 26, loss = 0.78253545\n",
            "Iteration 27, loss = 0.77878724\n",
            "Iteration 28, loss = 0.77460761\n",
            "Iteration 29, loss = 0.77067513\n",
            "Iteration 30, loss = 0.76679698\n",
            "Iteration 31, loss = 0.76267795\n",
            "Iteration 32, loss = 0.75886877\n",
            "Iteration 33, loss = 0.75493640\n",
            "Iteration 34, loss = 0.75113559\n",
            "Iteration 35, loss = 0.74728277\n",
            "Iteration 36, loss = 0.74365077\n",
            "Iteration 37, loss = 0.74005821\n",
            "Iteration 38, loss = 0.73664444\n",
            "Iteration 39, loss = 0.73303601\n",
            "Iteration 40, loss = 0.72973701\n",
            "Iteration 41, loss = 0.72651141\n",
            "Iteration 42, loss = 0.72333690\n",
            "Iteration 43, loss = 0.72024230\n",
            "Iteration 44, loss = 0.71740940\n",
            "Iteration 45, loss = 0.71437346\n",
            "Iteration 46, loss = 0.71166979\n",
            "Iteration 47, loss = 0.70888898\n",
            "Iteration 48, loss = 0.70635509\n",
            "Iteration 49, loss = 0.70374529\n",
            "Iteration 50, loss = 0.70135918\n",
            "Iteration 51, loss = 0.69919058\n",
            "Iteration 52, loss = 0.69686895\n",
            "Iteration 53, loss = 0.69460485\n",
            "Iteration 54, loss = 0.69265086\n",
            "Iteration 55, loss = 0.69058593\n",
            "Iteration 56, loss = 0.68865776\n",
            "Iteration 57, loss = 0.68673993\n",
            "Iteration 58, loss = 0.68498073\n",
            "Iteration 59, loss = 0.68319763\n",
            "Iteration 60, loss = 0.68146881\n",
            "Iteration 61, loss = 0.67964701\n",
            "Iteration 62, loss = 0.67803600\n",
            "Iteration 63, loss = 0.67628475\n",
            "Iteration 64, loss = 0.67457618\n",
            "Iteration 65, loss = 0.67298158\n",
            "Iteration 66, loss = 0.67125097\n",
            "Iteration 67, loss = 0.66963612\n",
            "Iteration 68, loss = 0.66791882\n",
            "Iteration 69, loss = 0.66631027\n",
            "Iteration 70, loss = 0.66459925\n",
            "Iteration 71, loss = 0.66293697\n",
            "Iteration 72, loss = 0.66126279\n",
            "Iteration 73, loss = 0.65953603\n",
            "Iteration 74, loss = 0.65779023\n",
            "Iteration 75, loss = 0.65606590\n",
            "Iteration 76, loss = 0.65425436\n",
            "Iteration 77, loss = 0.65261894\n",
            "Iteration 78, loss = 0.65088775\n",
            "Iteration 79, loss = 0.64914883\n",
            "Iteration 80, loss = 0.64748831\n",
            "Iteration 81, loss = 0.64571937\n",
            "Iteration 82, loss = 0.64391291\n",
            "Iteration 83, loss = 0.64224081\n",
            "Iteration 84, loss = 0.64053326\n",
            "Iteration 85, loss = 0.63872309\n",
            "Iteration 86, loss = 0.63709321\n",
            "Iteration 87, loss = 0.63528623\n",
            "Iteration 88, loss = 0.63362332\n",
            "Iteration 89, loss = 0.63188637\n",
            "Iteration 90, loss = 0.63025918\n",
            "Iteration 91, loss = 0.62855344\n",
            "Iteration 92, loss = 0.62677768\n",
            "Iteration 93, loss = 0.62516992\n",
            "Iteration 94, loss = 0.62344263\n",
            "Iteration 95, loss = 0.62176117\n",
            "Iteration 96, loss = 0.62018358\n",
            "Iteration 97, loss = 0.61842417\n",
            "Iteration 98, loss = 0.61675606\n",
            "Iteration 99, loss = 0.61517200\n",
            "Iteration 100, loss = 0.61352907\n",
            "Iteration 101, loss = 0.61187919\n",
            "Iteration 102, loss = 0.61026513\n",
            "Iteration 103, loss = 0.60863137\n",
            "Iteration 104, loss = 0.60701375\n",
            "Iteration 105, loss = 0.60538876\n",
            "Iteration 106, loss = 0.60383478\n",
            "Iteration 107, loss = 0.60223177\n",
            "Iteration 108, loss = 0.60058241\n",
            "Iteration 109, loss = 0.59901041\n",
            "Iteration 110, loss = 0.59734115\n",
            "Iteration 111, loss = 0.59582043\n",
            "Iteration 112, loss = 0.59414029\n",
            "Iteration 113, loss = 0.59257599\n",
            "Iteration 114, loss = 0.59088531\n",
            "Iteration 115, loss = 0.58929248\n",
            "Iteration 116, loss = 0.58762226\n",
            "Iteration 117, loss = 0.58597118\n",
            "Iteration 118, loss = 0.58433644\n",
            "Iteration 119, loss = 0.58279170\n",
            "Iteration 120, loss = 0.58110941\n",
            "Iteration 121, loss = 0.57950273\n",
            "Iteration 122, loss = 0.57789451\n",
            "Iteration 123, loss = 0.57634351\n",
            "Iteration 124, loss = 0.57465856\n",
            "Iteration 125, loss = 0.57310993\n",
            "Iteration 126, loss = 0.57153971\n",
            "Iteration 127, loss = 0.56992191\n",
            "Iteration 128, loss = 0.56836111\n",
            "Iteration 129, loss = 0.56673997\n",
            "Iteration 130, loss = 0.56511534\n",
            "Iteration 131, loss = 0.56356691\n",
            "Iteration 132, loss = 0.56197251\n",
            "Iteration 133, loss = 0.56035424\n",
            "Iteration 134, loss = 0.55881652\n",
            "Iteration 135, loss = 0.55713458\n",
            "Iteration 136, loss = 0.55548953\n",
            "Iteration 137, loss = 0.55392630\n",
            "Iteration 138, loss = 0.55221412\n",
            "Iteration 139, loss = 0.55060432\n",
            "Iteration 140, loss = 0.54884944\n",
            "Iteration 141, loss = 0.54719558\n",
            "Iteration 142, loss = 0.54541373\n",
            "Iteration 143, loss = 0.54368175\n",
            "Iteration 144, loss = 0.54196578\n",
            "Iteration 145, loss = 0.54006373\n",
            "Iteration 146, loss = 0.53832321\n",
            "Iteration 147, loss = 0.53652681\n",
            "Iteration 148, loss = 0.53454549\n",
            "Iteration 149, loss = 0.53262824\n",
            "Iteration 150, loss = 0.53066846\n",
            "Iteration 151, loss = 0.52877404\n",
            "Iteration 152, loss = 0.52680743\n",
            "Iteration 153, loss = 0.52491363\n",
            "Iteration 154, loss = 0.52285555\n",
            "Iteration 155, loss = 0.52097899\n",
            "Iteration 156, loss = 0.51900630\n",
            "Iteration 157, loss = 0.51709148\n",
            "Iteration 158, loss = 0.51523949\n",
            "Iteration 159, loss = 0.51327498\n",
            "Iteration 160, loss = 0.51148410\n",
            "Iteration 161, loss = 0.50984019\n",
            "Iteration 162, loss = 0.50813032\n",
            "Iteration 163, loss = 0.50655480\n",
            "Iteration 164, loss = 0.50495786\n",
            "Iteration 165, loss = 0.50347626\n",
            "Iteration 166, loss = 0.50211292\n",
            "Iteration 167, loss = 0.50061814\n",
            "Iteration 168, loss = 0.49942028\n",
            "Iteration 169, loss = 0.49809261\n",
            "Iteration 170, loss = 0.49665198\n",
            "Iteration 171, loss = 0.49541164\n",
            "Iteration 172, loss = 0.49421449\n",
            "Iteration 173, loss = 0.49302784\n",
            "Iteration 174, loss = 0.49192066\n",
            "Iteration 175, loss = 0.49066447\n",
            "Iteration 176, loss = 0.48959447\n",
            "Iteration 177, loss = 0.48848447\n",
            "Iteration 178, loss = 0.48752573\n",
            "Iteration 179, loss = 0.48632420\n",
            "Iteration 180, loss = 0.48539149\n",
            "Iteration 181, loss = 0.48438906\n",
            "Iteration 182, loss = 0.48332134\n",
            "Iteration 183, loss = 0.48242304\n",
            "Iteration 184, loss = 0.48146765\n",
            "Iteration 185, loss = 0.48064041\n",
            "Iteration 186, loss = 0.47969736\n",
            "Iteration 187, loss = 0.47879915\n",
            "Iteration 188, loss = 0.47787708\n",
            "Iteration 189, loss = 0.47705166\n",
            "Iteration 190, loss = 0.47613845\n",
            "Iteration 191, loss = 0.47531493\n",
            "Iteration 192, loss = 0.47455246\n",
            "Iteration 193, loss = 0.47371768\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.89044401\n",
            "Iteration 2, loss = 0.88549830\n",
            "Iteration 3, loss = 0.88062185\n",
            "Iteration 4, loss = 0.87578655\n",
            "Iteration 5, loss = 0.87097375\n",
            "Iteration 6, loss = 0.86663539\n",
            "Iteration 7, loss = 0.86191637\n",
            "Iteration 8, loss = 0.85741578\n",
            "Iteration 9, loss = 0.85302636\n",
            "Iteration 10, loss = 0.84880712\n",
            "Iteration 11, loss = 0.84457911\n",
            "Iteration 12, loss = 0.84031016\n",
            "Iteration 13, loss = 0.83610525\n",
            "Iteration 14, loss = 0.83213597\n",
            "Iteration 15, loss = 0.82809289\n",
            "Iteration 16, loss = 0.82395334\n",
            "Iteration 17, loss = 0.81995976\n",
            "Iteration 18, loss = 0.81596661\n",
            "Iteration 19, loss = 0.81197089\n",
            "Iteration 20, loss = 0.80792314\n",
            "Iteration 21, loss = 0.80391769\n",
            "Iteration 22, loss = 0.79998441\n",
            "Iteration 23, loss = 0.79590002\n",
            "Iteration 24, loss = 0.79186874\n",
            "Iteration 25, loss = 0.78795933\n",
            "Iteration 26, loss = 0.78390082\n",
            "Iteration 27, loss = 0.78017957\n",
            "Iteration 28, loss = 0.77608669\n",
            "Iteration 29, loss = 0.77225120\n",
            "Iteration 30, loss = 0.76848067\n",
            "Iteration 31, loss = 0.76443101\n",
            "Iteration 32, loss = 0.76080153\n",
            "Iteration 33, loss = 0.75705149\n",
            "Iteration 34, loss = 0.75326208\n",
            "Iteration 35, loss = 0.74946112\n",
            "Iteration 36, loss = 0.74598778\n",
            "Iteration 37, loss = 0.74260446\n",
            "Iteration 38, loss = 0.73919165\n",
            "Iteration 39, loss = 0.73570764\n",
            "Iteration 40, loss = 0.73255016\n",
            "Iteration 41, loss = 0.72950818\n",
            "Iteration 42, loss = 0.72628303\n",
            "Iteration 43, loss = 0.72342354\n",
            "Iteration 44, loss = 0.72068173\n",
            "Iteration 45, loss = 0.71776125\n",
            "Iteration 46, loss = 0.71509881\n",
            "Iteration 47, loss = 0.71258442\n",
            "Iteration 48, loss = 0.71006564\n",
            "Iteration 49, loss = 0.70760989\n",
            "Iteration 50, loss = 0.70541952\n",
            "Iteration 51, loss = 0.70315450\n",
            "Iteration 52, loss = 0.70095628\n",
            "Iteration 53, loss = 0.69890554\n",
            "Iteration 54, loss = 0.69700394\n",
            "Iteration 55, loss = 0.69506160\n",
            "Iteration 56, loss = 0.69332865\n",
            "Iteration 57, loss = 0.69150818\n",
            "Iteration 58, loss = 0.68975731\n",
            "Iteration 59, loss = 0.68815924\n",
            "Iteration 60, loss = 0.68653704\n",
            "Iteration 61, loss = 0.68490053\n",
            "Iteration 62, loss = 0.68340055\n",
            "Iteration 63, loss = 0.68190186\n",
            "Iteration 64, loss = 0.68039501\n",
            "Iteration 65, loss = 0.67899110\n",
            "Iteration 66, loss = 0.67756719\n",
            "Iteration 67, loss = 0.67616421\n",
            "Iteration 68, loss = 0.67466630\n",
            "Iteration 69, loss = 0.67330016\n",
            "Iteration 70, loss = 0.67183781\n",
            "Iteration 71, loss = 0.67041018\n",
            "Iteration 72, loss = 0.66896066\n",
            "Iteration 73, loss = 0.66741968\n",
            "Iteration 74, loss = 0.66591813\n",
            "Iteration 75, loss = 0.66442936\n",
            "Iteration 76, loss = 0.66276738\n",
            "Iteration 77, loss = 0.66131581\n",
            "Iteration 78, loss = 0.65978495\n",
            "Iteration 79, loss = 0.65816631\n",
            "Iteration 80, loss = 0.65672855\n",
            "Iteration 81, loss = 0.65514155\n",
            "Iteration 82, loss = 0.65351412\n",
            "Iteration 83, loss = 0.65198488\n",
            "Iteration 84, loss = 0.65038357\n",
            "Iteration 85, loss = 0.64877095\n",
            "Iteration 86, loss = 0.64721097\n",
            "Iteration 87, loss = 0.64549580\n",
            "Iteration 88, loss = 0.64389271\n",
            "Iteration 89, loss = 0.64234713\n",
            "Iteration 90, loss = 0.64077682\n",
            "Iteration 91, loss = 0.63916957\n",
            "Iteration 92, loss = 0.63757837\n",
            "Iteration 93, loss = 0.63604077\n",
            "Iteration 94, loss = 0.63443799\n",
            "Iteration 95, loss = 0.63293663\n",
            "Iteration 96, loss = 0.63152617\n",
            "Iteration 97, loss = 0.62980929\n",
            "Iteration 98, loss = 0.62828505\n",
            "Iteration 99, loss = 0.62679626\n",
            "Iteration 100, loss = 0.62521688\n",
            "Iteration 101, loss = 0.62374306\n",
            "Iteration 102, loss = 0.62222279\n",
            "Iteration 103, loss = 0.62074761\n",
            "Iteration 104, loss = 0.61920827\n",
            "Iteration 105, loss = 0.61774243\n",
            "Iteration 106, loss = 0.61631308\n",
            "Iteration 107, loss = 0.61483809\n",
            "Iteration 108, loss = 0.61337499\n",
            "Iteration 109, loss = 0.61188155\n",
            "Iteration 110, loss = 0.61036647\n",
            "Iteration 111, loss = 0.60890174\n",
            "Iteration 112, loss = 0.60733891\n",
            "Iteration 113, loss = 0.60594738\n",
            "Iteration 114, loss = 0.60437089\n",
            "Iteration 115, loss = 0.60277643\n",
            "Iteration 116, loss = 0.60124083\n",
            "Iteration 117, loss = 0.59975073\n",
            "Iteration 118, loss = 0.59819236\n",
            "Iteration 119, loss = 0.59667549\n",
            "Iteration 120, loss = 0.59509592\n",
            "Iteration 121, loss = 0.59361521\n",
            "Iteration 122, loss = 0.59200450\n",
            "Iteration 123, loss = 0.59053517\n",
            "Iteration 124, loss = 0.58894651\n",
            "Iteration 125, loss = 0.58742495\n",
            "Iteration 126, loss = 0.58589760\n",
            "Iteration 127, loss = 0.58436044\n",
            "Iteration 128, loss = 0.58284348\n",
            "Iteration 129, loss = 0.58130859\n",
            "Iteration 130, loss = 0.57976238\n",
            "Iteration 131, loss = 0.57831289\n",
            "Iteration 132, loss = 0.57674229\n",
            "Iteration 133, loss = 0.57519516\n",
            "Iteration 134, loss = 0.57372493\n",
            "Iteration 135, loss = 0.57208599\n",
            "Iteration 136, loss = 0.57049551\n",
            "Iteration 137, loss = 0.56901183\n",
            "Iteration 138, loss = 0.56730928\n",
            "Iteration 139, loss = 0.56586414\n",
            "Iteration 140, loss = 0.56418168\n",
            "Iteration 141, loss = 0.56255405\n",
            "Iteration 142, loss = 0.56086467\n",
            "Iteration 143, loss = 0.55914781\n",
            "Iteration 144, loss = 0.55746010\n",
            "Iteration 145, loss = 0.55569869\n",
            "Iteration 146, loss = 0.55397322\n",
            "Iteration 147, loss = 0.55221400\n",
            "Iteration 148, loss = 0.55039481\n",
            "Iteration 149, loss = 0.54854713\n",
            "Iteration 150, loss = 0.54662582\n",
            "Iteration 151, loss = 0.54494479\n",
            "Iteration 152, loss = 0.54283548\n",
            "Iteration 153, loss = 0.54098025\n",
            "Iteration 154, loss = 0.53891048\n",
            "Iteration 155, loss = 0.53694173\n",
            "Iteration 156, loss = 0.53483836\n",
            "Iteration 157, loss = 0.53274860\n",
            "Iteration 158, loss = 0.53084901\n",
            "Iteration 159, loss = 0.52874015\n",
            "Iteration 160, loss = 0.52674594\n",
            "Iteration 161, loss = 0.52484929\n",
            "Iteration 162, loss = 0.52292276\n",
            "Iteration 163, loss = 0.52117582\n",
            "Iteration 164, loss = 0.51935414\n",
            "Iteration 165, loss = 0.51771797\n",
            "Iteration 166, loss = 0.51609575\n",
            "Iteration 167, loss = 0.51447867\n",
            "Iteration 168, loss = 0.51293698\n",
            "Iteration 169, loss = 0.51145122\n",
            "Iteration 170, loss = 0.50987351\n",
            "Iteration 171, loss = 0.50850621\n",
            "Iteration 172, loss = 0.50710577\n",
            "Iteration 173, loss = 0.50572840\n",
            "Iteration 174, loss = 0.50446542\n",
            "Iteration 175, loss = 0.50318276\n",
            "Iteration 176, loss = 0.50191896\n",
            "Iteration 177, loss = 0.50079787\n",
            "Iteration 178, loss = 0.49969103\n",
            "Iteration 179, loss = 0.49847567\n",
            "Iteration 180, loss = 0.49742824\n",
            "Iteration 181, loss = 0.49637837\n",
            "Iteration 182, loss = 0.49518373\n",
            "Iteration 183, loss = 0.49425445\n",
            "Iteration 184, loss = 0.49315850\n",
            "Iteration 185, loss = 0.49222672\n",
            "Iteration 186, loss = 0.49123082\n",
            "Iteration 187, loss = 0.49024142\n",
            "Iteration 188, loss = 0.48932014\n",
            "Iteration 189, loss = 0.48833774\n",
            "Iteration 190, loss = 0.48743452\n",
            "Iteration 191, loss = 0.48654896\n",
            "Iteration 192, loss = 0.48574905\n",
            "Iteration 193, loss = 0.48484986\n",
            "Iteration 194, loss = 0.48403013\n",
            "Iteration 195, loss = 0.48325417\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.88782679\n",
            "Iteration 2, loss = 0.88287836\n",
            "Iteration 3, loss = 0.87793901\n",
            "Iteration 4, loss = 0.87319033\n",
            "Iteration 5, loss = 0.86827053\n",
            "Iteration 6, loss = 0.86382360\n",
            "Iteration 7, loss = 0.85916782\n",
            "Iteration 8, loss = 0.85463834\n",
            "Iteration 9, loss = 0.85029566\n",
            "Iteration 10, loss = 0.84597518\n",
            "Iteration 11, loss = 0.84178035\n",
            "Iteration 12, loss = 0.83751310\n",
            "Iteration 13, loss = 0.83342793\n",
            "Iteration 14, loss = 0.82925235\n",
            "Iteration 15, loss = 0.82529937\n",
            "Iteration 16, loss = 0.82123102\n",
            "Iteration 17, loss = 0.81733524\n",
            "Iteration 18, loss = 0.81335672\n",
            "Iteration 19, loss = 0.80939569\n",
            "Iteration 20, loss = 0.80546339\n",
            "Iteration 21, loss = 0.80159763\n",
            "Iteration 22, loss = 0.79767916\n",
            "Iteration 23, loss = 0.79372374\n",
            "Iteration 24, loss = 0.78973354\n",
            "Iteration 25, loss = 0.78594696\n",
            "Iteration 26, loss = 0.78198097\n",
            "Iteration 27, loss = 0.77836946\n",
            "Iteration 28, loss = 0.77435116\n",
            "Iteration 29, loss = 0.77063117\n",
            "Iteration 30, loss = 0.76694729\n",
            "Iteration 31, loss = 0.76291419\n",
            "Iteration 32, loss = 0.75929418\n",
            "Iteration 33, loss = 0.75560181\n",
            "Iteration 34, loss = 0.75201587\n",
            "Iteration 35, loss = 0.74813881\n",
            "Iteration 36, loss = 0.74461536\n",
            "Iteration 37, loss = 0.74126403\n",
            "Iteration 38, loss = 0.73794600\n",
            "Iteration 39, loss = 0.73441059\n",
            "Iteration 40, loss = 0.73135540\n",
            "Iteration 41, loss = 0.72817460\n",
            "Iteration 42, loss = 0.72508618\n",
            "Iteration 43, loss = 0.72225149\n",
            "Iteration 44, loss = 0.71954608\n",
            "Iteration 45, loss = 0.71659143\n",
            "Iteration 46, loss = 0.71389608\n",
            "Iteration 47, loss = 0.71122469\n",
            "Iteration 48, loss = 0.70883813\n",
            "Iteration 49, loss = 0.70646286\n",
            "Iteration 50, loss = 0.70412559\n",
            "Iteration 51, loss = 0.70196350\n",
            "Iteration 52, loss = 0.69971907\n",
            "Iteration 53, loss = 0.69776203\n",
            "Iteration 54, loss = 0.69598659\n",
            "Iteration 55, loss = 0.69397031\n",
            "Iteration 56, loss = 0.69217862\n",
            "Iteration 57, loss = 0.69046584\n",
            "Iteration 58, loss = 0.68875559\n",
            "Iteration 59, loss = 0.68710252\n",
            "Iteration 60, loss = 0.68550457\n",
            "Iteration 61, loss = 0.68383140\n",
            "Iteration 62, loss = 0.68236718\n",
            "Iteration 63, loss = 0.68081816\n",
            "Iteration 64, loss = 0.67935439\n",
            "Iteration 65, loss = 0.67788119\n",
            "Iteration 66, loss = 0.67640945\n",
            "Iteration 67, loss = 0.67494187\n",
            "Iteration 68, loss = 0.67347398\n",
            "Iteration 69, loss = 0.67201897\n",
            "Iteration 70, loss = 0.67054467\n",
            "Iteration 71, loss = 0.66903605\n",
            "Iteration 72, loss = 0.66750356\n",
            "Iteration 73, loss = 0.66596368\n",
            "Iteration 74, loss = 0.66439777\n",
            "Iteration 75, loss = 0.66274407\n",
            "Iteration 76, loss = 0.66109634\n",
            "Iteration 77, loss = 0.65947461\n",
            "Iteration 78, loss = 0.65794548\n",
            "Iteration 79, loss = 0.65617761\n",
            "Iteration 80, loss = 0.65461281\n",
            "Iteration 81, loss = 0.65296548\n",
            "Iteration 82, loss = 0.65120877\n",
            "Iteration 83, loss = 0.64959715\n",
            "Iteration 84, loss = 0.64788857\n",
            "Iteration 85, loss = 0.64623696\n",
            "Iteration 86, loss = 0.64464376\n",
            "Iteration 87, loss = 0.64295944\n",
            "Iteration 88, loss = 0.64144263\n",
            "Iteration 89, loss = 0.63986014\n",
            "Iteration 90, loss = 0.63833751\n",
            "Iteration 91, loss = 0.63674157\n",
            "Iteration 92, loss = 0.63515123\n",
            "Iteration 93, loss = 0.63363411\n",
            "Iteration 94, loss = 0.63210022\n",
            "Iteration 95, loss = 0.63063809\n",
            "Iteration 96, loss = 0.62915758\n",
            "Iteration 97, loss = 0.62762665\n",
            "Iteration 98, loss = 0.62620659\n",
            "Iteration 99, loss = 0.62472151\n",
            "Iteration 100, loss = 0.62325691\n",
            "Iteration 101, loss = 0.62185198\n",
            "Iteration 102, loss = 0.62037054\n",
            "Iteration 103, loss = 0.61896870\n",
            "Iteration 104, loss = 0.61751670\n",
            "Iteration 105, loss = 0.61612567\n",
            "Iteration 106, loss = 0.61480507\n",
            "Iteration 107, loss = 0.61338310\n",
            "Iteration 108, loss = 0.61197466\n",
            "Iteration 109, loss = 0.61058174\n",
            "Iteration 110, loss = 0.60919899\n",
            "Iteration 111, loss = 0.60783881\n",
            "Iteration 112, loss = 0.60641726\n",
            "Iteration 113, loss = 0.60504287\n",
            "Iteration 114, loss = 0.60358786\n",
            "Iteration 115, loss = 0.60220081\n",
            "Iteration 116, loss = 0.60075500\n",
            "Iteration 117, loss = 0.59938323\n",
            "Iteration 118, loss = 0.59797996\n",
            "Iteration 119, loss = 0.59660573\n",
            "Iteration 120, loss = 0.59518540\n",
            "Iteration 121, loss = 0.59381130\n",
            "Iteration 122, loss = 0.59238602\n",
            "Iteration 123, loss = 0.59112847\n",
            "Iteration 124, loss = 0.58965544\n",
            "Iteration 125, loss = 0.58834009\n",
            "Iteration 126, loss = 0.58697078\n",
            "Iteration 127, loss = 0.58564538\n",
            "Iteration 128, loss = 0.58428211\n",
            "Iteration 129, loss = 0.58293802\n",
            "Iteration 130, loss = 0.58165326\n",
            "Iteration 131, loss = 0.58031886\n",
            "Iteration 132, loss = 0.57902634\n",
            "Iteration 133, loss = 0.57773923\n",
            "Iteration 134, loss = 0.57646086\n",
            "Iteration 135, loss = 0.57513630\n",
            "Iteration 136, loss = 0.57378334\n",
            "Iteration 137, loss = 0.57258142\n",
            "Iteration 138, loss = 0.57123147\n",
            "Iteration 139, loss = 0.57007281\n",
            "Iteration 140, loss = 0.56873171\n",
            "Iteration 141, loss = 0.56744400\n",
            "Iteration 142, loss = 0.56613196\n",
            "Iteration 143, loss = 0.56481444\n",
            "Iteration 144, loss = 0.56351445\n",
            "Iteration 145, loss = 0.56228020\n",
            "Iteration 146, loss = 0.56094542\n",
            "Iteration 147, loss = 0.55962810\n",
            "Iteration 148, loss = 0.55836080\n",
            "Iteration 149, loss = 0.55700883\n",
            "Iteration 150, loss = 0.55577635\n",
            "Iteration 151, loss = 0.55453828\n",
            "Iteration 152, loss = 0.55330971\n",
            "Iteration 153, loss = 0.55201174\n",
            "Iteration 154, loss = 0.55073142\n",
            "Iteration 155, loss = 0.54944848\n",
            "Iteration 156, loss = 0.54822858\n",
            "Iteration 157, loss = 0.54685943\n",
            "Iteration 158, loss = 0.54560989\n",
            "Iteration 159, loss = 0.54429570\n",
            "Iteration 160, loss = 0.54301585\n",
            "Iteration 161, loss = 0.54172692\n",
            "Iteration 162, loss = 0.54030857\n",
            "Iteration 163, loss = 0.53890363\n",
            "Iteration 164, loss = 0.53759036\n",
            "Iteration 165, loss = 0.53624277\n",
            "Iteration 166, loss = 0.53486085\n",
            "Iteration 167, loss = 0.53336609\n",
            "Iteration 168, loss = 0.53200996\n",
            "Iteration 169, loss = 0.53054310\n",
            "Iteration 170, loss = 0.52905769\n",
            "Iteration 171, loss = 0.52763761\n",
            "Iteration 172, loss = 0.52616145\n",
            "Iteration 173, loss = 0.52468102\n",
            "Iteration 174, loss = 0.52326755\n",
            "Iteration 175, loss = 0.52167672\n",
            "Iteration 176, loss = 0.52019807\n",
            "Iteration 177, loss = 0.51880900\n",
            "Iteration 178, loss = 0.51748262\n",
            "Iteration 179, loss = 0.51606436\n",
            "Iteration 180, loss = 0.51467900\n",
            "Iteration 181, loss = 0.51344124\n",
            "Iteration 182, loss = 0.51207430\n",
            "Iteration 183, loss = 0.51084442\n",
            "Iteration 184, loss = 0.50972127\n",
            "Iteration 185, loss = 0.50847921\n",
            "Iteration 186, loss = 0.50727707\n",
            "Iteration 187, loss = 0.50611675\n",
            "Iteration 188, loss = 0.50504308\n",
            "Iteration 189, loss = 0.50394714\n",
            "Iteration 190, loss = 0.50290989\n",
            "Iteration 191, loss = 0.50197630\n",
            "Iteration 192, loss = 0.50101596\n",
            "Iteration 193, loss = 0.50012299\n",
            "Iteration 194, loss = 0.49914147\n",
            "Iteration 195, loss = 0.49836892\n",
            "Iteration 196, loss = 0.49745810\n",
            "Iteration 197, loss = 0.49669079\n",
            "Iteration 198, loss = 0.49588848\n",
            "Iteration 199, loss = 0.49512683\n",
            "Iteration 200, loss = 0.49441849\n",
            "Iteration 201, loss = 0.49369183\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.88685347\n",
            "Iteration 2, loss = 0.88192398\n",
            "Iteration 3, loss = 0.87710042\n",
            "Iteration 4, loss = 0.87220553\n",
            "Iteration 5, loss = 0.86756813\n",
            "Iteration 6, loss = 0.86301782\n",
            "Iteration 7, loss = 0.85851412\n",
            "Iteration 8, loss = 0.85394955\n",
            "Iteration 9, loss = 0.84965897\n",
            "Iteration 10, loss = 0.84538636\n",
            "Iteration 11, loss = 0.84113482\n",
            "Iteration 12, loss = 0.83704395\n",
            "Iteration 13, loss = 0.83288747\n",
            "Iteration 14, loss = 0.82868703\n",
            "Iteration 15, loss = 0.82476276\n",
            "Iteration 16, loss = 0.82063651\n",
            "Iteration 17, loss = 0.81664188\n",
            "Iteration 18, loss = 0.81267061\n",
            "Iteration 19, loss = 0.80852437\n",
            "Iteration 20, loss = 0.80458792\n",
            "Iteration 21, loss = 0.80064384\n",
            "Iteration 22, loss = 0.79667661\n",
            "Iteration 23, loss = 0.79275309\n",
            "Iteration 24, loss = 0.78873654\n",
            "Iteration 25, loss = 0.78489962\n",
            "Iteration 26, loss = 0.78110525\n",
            "Iteration 27, loss = 0.77728030\n",
            "Iteration 28, loss = 0.77336369\n",
            "Iteration 29, loss = 0.76940555\n",
            "Iteration 30, loss = 0.76575283\n",
            "Iteration 31, loss = 0.76165324\n",
            "Iteration 32, loss = 0.75794126\n",
            "Iteration 33, loss = 0.75412191\n",
            "Iteration 34, loss = 0.75051165\n",
            "Iteration 35, loss = 0.74654101\n",
            "Iteration 36, loss = 0.74298387\n",
            "Iteration 37, loss = 0.73956316\n",
            "Iteration 38, loss = 0.73597045\n",
            "Iteration 39, loss = 0.73253898\n",
            "Iteration 40, loss = 0.72938085\n",
            "Iteration 41, loss = 0.72591777\n",
            "Iteration 42, loss = 0.72281873\n",
            "Iteration 43, loss = 0.71991379\n",
            "Iteration 44, loss = 0.71713271\n",
            "Iteration 45, loss = 0.71428983\n",
            "Iteration 46, loss = 0.71145589\n",
            "Iteration 47, loss = 0.70884786\n",
            "Iteration 48, loss = 0.70640160\n",
            "Iteration 49, loss = 0.70394736\n",
            "Iteration 50, loss = 0.70163819\n",
            "Iteration 51, loss = 0.69957236\n",
            "Iteration 52, loss = 0.69727490\n",
            "Iteration 53, loss = 0.69509044\n",
            "Iteration 54, loss = 0.69319100\n",
            "Iteration 55, loss = 0.69111086\n",
            "Iteration 56, loss = 0.68925961\n",
            "Iteration 57, loss = 0.68731277\n",
            "Iteration 58, loss = 0.68556899\n",
            "Iteration 59, loss = 0.68377480\n",
            "Iteration 60, loss = 0.68200010\n",
            "Iteration 61, loss = 0.68023701\n",
            "Iteration 62, loss = 0.67859301\n",
            "Iteration 63, loss = 0.67684714\n",
            "Iteration 64, loss = 0.67519784\n",
            "Iteration 65, loss = 0.67352733\n",
            "Iteration 66, loss = 0.67188818\n",
            "Iteration 67, loss = 0.67027888\n",
            "Iteration 68, loss = 0.66863031\n",
            "Iteration 69, loss = 0.66695417\n",
            "Iteration 70, loss = 0.66537091\n",
            "Iteration 71, loss = 0.66366080\n",
            "Iteration 72, loss = 0.66205590\n",
            "Iteration 73, loss = 0.66026935\n",
            "Iteration 74, loss = 0.65862398\n",
            "Iteration 75, loss = 0.65685064\n",
            "Iteration 76, loss = 0.65503192\n",
            "Iteration 77, loss = 0.65337306\n",
            "Iteration 78, loss = 0.65166741\n",
            "Iteration 79, loss = 0.64984717\n",
            "Iteration 80, loss = 0.64817177\n",
            "Iteration 81, loss = 0.64645593\n",
            "Iteration 82, loss = 0.64464749\n",
            "Iteration 83, loss = 0.64297678\n",
            "Iteration 84, loss = 0.64120964\n",
            "Iteration 85, loss = 0.63955726\n",
            "Iteration 86, loss = 0.63784006\n",
            "Iteration 87, loss = 0.63610908\n",
            "Iteration 88, loss = 0.63457267\n",
            "Iteration 89, loss = 0.63285966\n",
            "Iteration 90, loss = 0.63123070\n",
            "Iteration 91, loss = 0.62964743\n",
            "Iteration 92, loss = 0.62796665\n",
            "Iteration 93, loss = 0.62642862\n",
            "Iteration 94, loss = 0.62477487\n",
            "Iteration 95, loss = 0.62324248\n",
            "Iteration 96, loss = 0.62167026\n",
            "Iteration 97, loss = 0.62002925\n",
            "Iteration 98, loss = 0.61843274\n",
            "Iteration 99, loss = 0.61685037\n",
            "Iteration 100, loss = 0.61530209\n",
            "Iteration 101, loss = 0.61370245\n",
            "Iteration 102, loss = 0.61209344\n",
            "Iteration 103, loss = 0.61052418\n",
            "Iteration 104, loss = 0.60892561\n",
            "Iteration 105, loss = 0.60731989\n",
            "Iteration 106, loss = 0.60583073\n",
            "Iteration 107, loss = 0.60423132\n",
            "Iteration 108, loss = 0.60264774\n",
            "Iteration 109, loss = 0.60105454\n",
            "Iteration 110, loss = 0.59951677\n",
            "Iteration 111, loss = 0.59792929\n",
            "Iteration 112, loss = 0.59631752\n",
            "Iteration 113, loss = 0.59474837\n",
            "Iteration 114, loss = 0.59308419\n",
            "Iteration 115, loss = 0.59151893\n",
            "Iteration 116, loss = 0.58982006\n",
            "Iteration 117, loss = 0.58836279\n",
            "Iteration 118, loss = 0.58673855\n",
            "Iteration 119, loss = 0.58514184\n",
            "Iteration 120, loss = 0.58350665\n",
            "Iteration 121, loss = 0.58194967\n",
            "Iteration 122, loss = 0.58032300\n",
            "Iteration 123, loss = 0.57872825\n",
            "Iteration 124, loss = 0.57707119\n",
            "Iteration 125, loss = 0.57543125\n",
            "Iteration 126, loss = 0.57383169\n",
            "Iteration 127, loss = 0.57221487\n",
            "Iteration 128, loss = 0.57058457\n",
            "Iteration 129, loss = 0.56898802\n",
            "Iteration 130, loss = 0.56742641\n",
            "Iteration 131, loss = 0.56580495\n",
            "Iteration 132, loss = 0.56422606\n",
            "Iteration 133, loss = 0.56267518\n",
            "Iteration 134, loss = 0.56096076\n",
            "Iteration 135, loss = 0.55933055\n",
            "Iteration 136, loss = 0.55765281\n",
            "Iteration 137, loss = 0.55602139\n",
            "Iteration 138, loss = 0.55425404\n",
            "Iteration 139, loss = 0.55254667\n",
            "Iteration 140, loss = 0.55085003\n",
            "Iteration 141, loss = 0.54906334\n",
            "Iteration 142, loss = 0.54719605\n",
            "Iteration 143, loss = 0.54539698\n",
            "Iteration 144, loss = 0.54351731\n",
            "Iteration 145, loss = 0.54160643\n",
            "Iteration 146, loss = 0.53978334\n",
            "Iteration 147, loss = 0.53779629\n",
            "Iteration 148, loss = 0.53595327\n",
            "Iteration 149, loss = 0.53392255\n",
            "Iteration 150, loss = 0.53203263\n",
            "Iteration 151, loss = 0.53022417\n",
            "Iteration 152, loss = 0.52813595\n",
            "Iteration 153, loss = 0.52630376\n",
            "Iteration 154, loss = 0.52447242\n",
            "Iteration 155, loss = 0.52262972\n",
            "Iteration 156, loss = 0.52074471\n",
            "Iteration 157, loss = 0.51897452\n",
            "Iteration 158, loss = 0.51727065\n",
            "Iteration 159, loss = 0.51553705\n",
            "Iteration 160, loss = 0.51390108\n",
            "Iteration 161, loss = 0.51221661\n",
            "Iteration 162, loss = 0.51057212\n",
            "Iteration 163, loss = 0.50894075\n",
            "Iteration 164, loss = 0.50742441\n",
            "Iteration 165, loss = 0.50595716\n",
            "Iteration 166, loss = 0.50448849\n",
            "Iteration 167, loss = 0.50311981\n",
            "Iteration 168, loss = 0.50183835\n",
            "Iteration 169, loss = 0.50057339\n",
            "Iteration 170, loss = 0.49918794\n",
            "Iteration 171, loss = 0.49793456\n",
            "Iteration 172, loss = 0.49672791\n",
            "Iteration 173, loss = 0.49558727\n",
            "Iteration 174, loss = 0.49438980\n",
            "Iteration 175, loss = 0.49312867\n",
            "Iteration 176, loss = 0.49197769\n",
            "Iteration 177, loss = 0.49088450\n",
            "Iteration 178, loss = 0.48980921\n",
            "Iteration 179, loss = 0.48868692\n",
            "Iteration 180, loss = 0.48773696\n",
            "Iteration 181, loss = 0.48660787\n",
            "Iteration 182, loss = 0.48557732\n",
            "Iteration 183, loss = 0.48467126\n",
            "Iteration 184, loss = 0.48362649\n",
            "Iteration 185, loss = 0.48261785\n",
            "Iteration 186, loss = 0.48177756\n",
            "Iteration 187, loss = 0.48083638\n",
            "Iteration 188, loss = 0.47992524\n",
            "Iteration 189, loss = 0.47900655\n",
            "Iteration 190, loss = 0.47814183\n",
            "Iteration 191, loss = 0.47730737\n",
            "Iteration 192, loss = 0.47654153\n",
            "Iteration 193, loss = 0.47580043\n",
            "Iteration 194, loss = 0.47497709\n",
            "Iteration 195, loss = 0.47430384\n",
            "Iteration 196, loss = 0.47355545\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.88797693\n",
            "Iteration 2, loss = 0.88309699\n",
            "Iteration 3, loss = 0.87818018\n",
            "Iteration 4, loss = 0.87336745\n",
            "Iteration 5, loss = 0.86866274\n",
            "Iteration 6, loss = 0.86406822\n",
            "Iteration 7, loss = 0.85957481\n",
            "Iteration 8, loss = 0.85502675\n",
            "Iteration 9, loss = 0.85065177\n",
            "Iteration 10, loss = 0.84641603\n",
            "Iteration 11, loss = 0.84219865\n",
            "Iteration 12, loss = 0.83811891\n",
            "Iteration 13, loss = 0.83400782\n",
            "Iteration 14, loss = 0.82980811\n",
            "Iteration 15, loss = 0.82601106\n",
            "Iteration 16, loss = 0.82182318\n",
            "Iteration 17, loss = 0.81780628\n",
            "Iteration 18, loss = 0.81391935\n",
            "Iteration 19, loss = 0.80984691\n",
            "Iteration 20, loss = 0.80592559\n",
            "Iteration 21, loss = 0.80204486\n",
            "Iteration 22, loss = 0.79815114\n",
            "Iteration 23, loss = 0.79418698\n",
            "Iteration 24, loss = 0.79019544\n",
            "Iteration 25, loss = 0.78632144\n",
            "Iteration 26, loss = 0.78250759\n",
            "Iteration 27, loss = 0.77879861\n",
            "Iteration 28, loss = 0.77481820\n",
            "Iteration 29, loss = 0.77110045\n",
            "Iteration 30, loss = 0.76744099\n",
            "Iteration 31, loss = 0.76352544\n",
            "Iteration 32, loss = 0.75979654\n",
            "Iteration 33, loss = 0.75617196\n",
            "Iteration 34, loss = 0.75250027\n",
            "Iteration 35, loss = 0.74856699\n",
            "Iteration 36, loss = 0.74508466\n",
            "Iteration 37, loss = 0.74158192\n",
            "Iteration 38, loss = 0.73803463\n",
            "Iteration 39, loss = 0.73460996\n",
            "Iteration 40, loss = 0.73151940\n",
            "Iteration 41, loss = 0.72803185\n",
            "Iteration 42, loss = 0.72503679\n",
            "Iteration 43, loss = 0.72211372\n",
            "Iteration 44, loss = 0.71930074\n",
            "Iteration 45, loss = 0.71651467\n",
            "Iteration 46, loss = 0.71381316\n",
            "Iteration 47, loss = 0.71120007\n",
            "Iteration 48, loss = 0.70880789\n",
            "Iteration 49, loss = 0.70633541\n",
            "Iteration 50, loss = 0.70422273\n",
            "Iteration 51, loss = 0.70209064\n",
            "Iteration 52, loss = 0.69999421\n",
            "Iteration 53, loss = 0.69795786\n",
            "Iteration 54, loss = 0.69610573\n",
            "Iteration 55, loss = 0.69418380\n",
            "Iteration 56, loss = 0.69248121\n",
            "Iteration 57, loss = 0.69061528\n",
            "Iteration 58, loss = 0.68896537\n",
            "Iteration 59, loss = 0.68731674\n",
            "Iteration 60, loss = 0.68559620\n",
            "Iteration 61, loss = 0.68403343\n",
            "Iteration 62, loss = 0.68247671\n",
            "Iteration 63, loss = 0.68087282\n",
            "Iteration 64, loss = 0.67940650\n",
            "Iteration 65, loss = 0.67783212\n",
            "Iteration 66, loss = 0.67639875\n",
            "Iteration 67, loss = 0.67492994\n",
            "Iteration 68, loss = 0.67344936\n",
            "Iteration 69, loss = 0.67196450\n",
            "Iteration 70, loss = 0.67045872\n",
            "Iteration 71, loss = 0.66890513\n",
            "Iteration 72, loss = 0.66738815\n",
            "Iteration 73, loss = 0.66579703\n",
            "Iteration 74, loss = 0.66418860\n",
            "Iteration 75, loss = 0.66257280\n",
            "Iteration 76, loss = 0.66083333\n",
            "Iteration 77, loss = 0.65925228\n",
            "Iteration 78, loss = 0.65761825\n",
            "Iteration 79, loss = 0.65589452\n",
            "Iteration 80, loss = 0.65427994\n",
            "Iteration 81, loss = 0.65254365\n",
            "Iteration 82, loss = 0.65081617\n",
            "Iteration 83, loss = 0.64921212\n",
            "Iteration 84, loss = 0.64752603\n",
            "Iteration 85, loss = 0.64590507\n",
            "Iteration 86, loss = 0.64420137\n",
            "Iteration 87, loss = 0.64250674\n",
            "Iteration 88, loss = 0.64093103\n",
            "Iteration 89, loss = 0.63932707\n",
            "Iteration 90, loss = 0.63766340\n",
            "Iteration 91, loss = 0.63609178\n",
            "Iteration 92, loss = 0.63449443\n",
            "Iteration 93, loss = 0.63289624\n",
            "Iteration 94, loss = 0.63127400\n",
            "Iteration 95, loss = 0.62975196\n",
            "Iteration 96, loss = 0.62815383\n",
            "Iteration 97, loss = 0.62658286\n",
            "Iteration 98, loss = 0.62497504\n",
            "Iteration 99, loss = 0.62343279\n",
            "Iteration 100, loss = 0.62194191\n",
            "Iteration 101, loss = 0.62038356\n",
            "Iteration 102, loss = 0.61880756\n",
            "Iteration 103, loss = 0.61729149\n",
            "Iteration 104, loss = 0.61572689\n",
            "Iteration 105, loss = 0.61420371\n",
            "Iteration 106, loss = 0.61275126\n",
            "Iteration 107, loss = 0.61117529\n",
            "Iteration 108, loss = 0.60972782\n",
            "Iteration 109, loss = 0.60814439\n",
            "Iteration 110, loss = 0.60664847\n",
            "Iteration 111, loss = 0.60512624\n",
            "Iteration 112, loss = 0.60364148\n",
            "Iteration 113, loss = 0.60209833\n",
            "Iteration 114, loss = 0.60045952\n",
            "Iteration 115, loss = 0.59902775\n",
            "Iteration 116, loss = 0.59752243\n",
            "Iteration 117, loss = 0.59604204\n",
            "Iteration 118, loss = 0.59452276\n",
            "Iteration 119, loss = 0.59301413\n",
            "Iteration 120, loss = 0.59142552\n",
            "Iteration 121, loss = 0.58997976\n",
            "Iteration 122, loss = 0.58841020\n",
            "Iteration 123, loss = 0.58688429\n",
            "Iteration 124, loss = 0.58529529\n",
            "Iteration 125, loss = 0.58375703\n",
            "Iteration 126, loss = 0.58220425\n",
            "Iteration 127, loss = 0.58069860\n",
            "Iteration 128, loss = 0.57922657\n",
            "Iteration 129, loss = 0.57775320\n",
            "Iteration 130, loss = 0.57629352\n",
            "Iteration 131, loss = 0.57482083\n",
            "Iteration 132, loss = 0.57343858\n",
            "Iteration 133, loss = 0.57201929\n",
            "Iteration 134, loss = 0.57051243\n",
            "Iteration 135, loss = 0.56909553\n",
            "Iteration 136, loss = 0.56762208\n",
            "Iteration 137, loss = 0.56618294\n",
            "Iteration 138, loss = 0.56468040\n",
            "Iteration 139, loss = 0.56318972\n",
            "Iteration 140, loss = 0.56169593\n",
            "Iteration 141, loss = 0.56017146\n",
            "Iteration 142, loss = 0.55858272\n",
            "Iteration 143, loss = 0.55705675\n",
            "Iteration 144, loss = 0.55549874\n",
            "Iteration 145, loss = 0.55389167\n",
            "Iteration 146, loss = 0.55234796\n",
            "Iteration 147, loss = 0.55079000\n",
            "Iteration 148, loss = 0.54915151\n",
            "Iteration 149, loss = 0.54746446\n",
            "Iteration 150, loss = 0.54582522\n",
            "Iteration 151, loss = 0.54418577\n",
            "Iteration 152, loss = 0.54237279\n",
            "Iteration 153, loss = 0.54071234\n",
            "Iteration 154, loss = 0.53900880\n",
            "Iteration 155, loss = 0.53717042\n",
            "Iteration 156, loss = 0.53537317\n",
            "Iteration 157, loss = 0.53365375\n",
            "Iteration 158, loss = 0.53189634\n",
            "Iteration 159, loss = 0.53015492\n",
            "Iteration 160, loss = 0.52848047\n",
            "Iteration 161, loss = 0.52669192\n",
            "Iteration 162, loss = 0.52499022\n",
            "Iteration 163, loss = 0.52335327\n",
            "Iteration 164, loss = 0.52182060\n",
            "Iteration 165, loss = 0.52029897\n",
            "Iteration 166, loss = 0.51883490\n",
            "Iteration 167, loss = 0.51732806\n",
            "Iteration 168, loss = 0.51597275\n",
            "Iteration 169, loss = 0.51465527\n",
            "Iteration 170, loss = 0.51320634\n",
            "Iteration 171, loss = 0.51193486\n",
            "Iteration 172, loss = 0.51070117\n",
            "Iteration 173, loss = 0.50945870\n",
            "Iteration 174, loss = 0.50822907\n",
            "Iteration 175, loss = 0.50700004\n",
            "Iteration 176, loss = 0.50581118\n",
            "Iteration 177, loss = 0.50471600\n",
            "Iteration 178, loss = 0.50365059\n",
            "Iteration 179, loss = 0.50256650\n",
            "Iteration 180, loss = 0.50148473\n",
            "Iteration 181, loss = 0.50035046\n",
            "Iteration 182, loss = 0.49933608\n",
            "Iteration 183, loss = 0.49834196\n",
            "Iteration 184, loss = 0.49731184\n",
            "Iteration 185, loss = 0.49629052\n",
            "Iteration 186, loss = 0.49544525\n",
            "Iteration 187, loss = 0.49441996\n",
            "Iteration 188, loss = 0.49352894\n",
            "Iteration 189, loss = 0.49260301\n",
            "Iteration 190, loss = 0.49171986\n",
            "Iteration 191, loss = 0.49081791\n",
            "Iteration 192, loss = 0.48996984\n",
            "Iteration 193, loss = 0.48919918\n",
            "Iteration 194, loss = 0.48834648\n",
            "Iteration 195, loss = 0.48763868\n",
            "Iteration 196, loss = 0.48687337\n",
            "Iteration 197, loss = 0.48613167\n",
            "Iteration 198, loss = 0.48537297\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.88692127\n",
            "Iteration 2, loss = 0.88200891\n",
            "Iteration 3, loss = 0.87713824\n",
            "Iteration 4, loss = 0.87236076\n",
            "Iteration 5, loss = 0.86771842\n",
            "Iteration 6, loss = 0.86314027\n",
            "Iteration 7, loss = 0.85865068\n",
            "Iteration 8, loss = 0.85423349\n",
            "Iteration 9, loss = 0.84985369\n",
            "Iteration 10, loss = 0.84560277\n",
            "Iteration 11, loss = 0.84152202\n",
            "Iteration 12, loss = 0.83731970\n",
            "Iteration 13, loss = 0.83327838\n",
            "Iteration 14, loss = 0.82916344\n",
            "Iteration 15, loss = 0.82534640\n",
            "Iteration 16, loss = 0.82112474\n",
            "Iteration 17, loss = 0.81719326\n",
            "Iteration 18, loss = 0.81320329\n",
            "Iteration 19, loss = 0.80916668\n",
            "Iteration 20, loss = 0.80520667\n",
            "Iteration 21, loss = 0.80135445\n",
            "Iteration 22, loss = 0.79738497\n",
            "Iteration 23, loss = 0.79342720\n",
            "Iteration 24, loss = 0.78937291\n",
            "Iteration 25, loss = 0.78540928\n",
            "Iteration 26, loss = 0.78154129\n",
            "Iteration 27, loss = 0.77775435\n",
            "Iteration 28, loss = 0.77376603\n",
            "Iteration 29, loss = 0.77003600\n",
            "Iteration 30, loss = 0.76634837\n",
            "Iteration 31, loss = 0.76234262\n",
            "Iteration 32, loss = 0.75865611\n",
            "Iteration 33, loss = 0.75508473\n",
            "Iteration 34, loss = 0.75126962\n",
            "Iteration 35, loss = 0.74736361\n",
            "Iteration 36, loss = 0.74379155\n",
            "Iteration 37, loss = 0.74029365\n",
            "Iteration 38, loss = 0.73662237\n",
            "Iteration 39, loss = 0.73316064\n",
            "Iteration 40, loss = 0.73009245\n",
            "Iteration 41, loss = 0.72649130\n",
            "Iteration 42, loss = 0.72347066\n",
            "Iteration 43, loss = 0.72046949\n",
            "Iteration 44, loss = 0.71765200\n",
            "Iteration 45, loss = 0.71472895\n",
            "Iteration 46, loss = 0.71203435\n",
            "Iteration 47, loss = 0.70942779\n",
            "Iteration 48, loss = 0.70706297\n",
            "Iteration 49, loss = 0.70458356\n",
            "Iteration 50, loss = 0.70238749\n",
            "Iteration 51, loss = 0.70022597\n",
            "Iteration 52, loss = 0.69805682\n",
            "Iteration 53, loss = 0.69595093\n",
            "Iteration 54, loss = 0.69411962\n",
            "Iteration 55, loss = 0.69214002\n",
            "Iteration 56, loss = 0.69031574\n",
            "Iteration 57, loss = 0.68844321\n",
            "Iteration 58, loss = 0.68661010\n",
            "Iteration 59, loss = 0.68490977\n",
            "Iteration 60, loss = 0.68311469\n",
            "Iteration 61, loss = 0.68136876\n",
            "Iteration 62, loss = 0.67974476\n",
            "Iteration 63, loss = 0.67794451\n",
            "Iteration 64, loss = 0.67634546\n",
            "Iteration 65, loss = 0.67469614\n",
            "Iteration 66, loss = 0.67305416\n",
            "Iteration 67, loss = 0.67146877\n",
            "Iteration 68, loss = 0.66981650\n",
            "Iteration 69, loss = 0.66815066\n",
            "Iteration 70, loss = 0.66649946\n",
            "Iteration 71, loss = 0.66484057\n",
            "Iteration 72, loss = 0.66313246\n",
            "Iteration 73, loss = 0.66136341\n",
            "Iteration 74, loss = 0.65962130\n",
            "Iteration 75, loss = 0.65787690\n",
            "Iteration 76, loss = 0.65599277\n",
            "Iteration 77, loss = 0.65424371\n",
            "Iteration 78, loss = 0.65256248\n",
            "Iteration 79, loss = 0.65066458\n",
            "Iteration 80, loss = 0.64895049\n",
            "Iteration 81, loss = 0.64714663\n",
            "Iteration 82, loss = 0.64531178\n",
            "Iteration 83, loss = 0.64369013\n",
            "Iteration 84, loss = 0.64195718\n",
            "Iteration 85, loss = 0.64024865\n",
            "Iteration 86, loss = 0.63849453\n",
            "Iteration 87, loss = 0.63681685\n",
            "Iteration 88, loss = 0.63517021\n",
            "Iteration 89, loss = 0.63355254\n",
            "Iteration 90, loss = 0.63184981\n",
            "Iteration 91, loss = 0.63022353\n",
            "Iteration 92, loss = 0.62855571\n",
            "Iteration 93, loss = 0.62690609\n",
            "Iteration 94, loss = 0.62524418\n",
            "Iteration 95, loss = 0.62362148\n",
            "Iteration 96, loss = 0.62197490\n",
            "Iteration 97, loss = 0.62032678\n",
            "Iteration 98, loss = 0.61872965\n",
            "Iteration 99, loss = 0.61708972\n",
            "Iteration 100, loss = 0.61553763\n",
            "Iteration 101, loss = 0.61393824\n",
            "Iteration 102, loss = 0.61231484\n",
            "Iteration 103, loss = 0.61074633\n",
            "Iteration 104, loss = 0.60911767\n",
            "Iteration 105, loss = 0.60748529\n",
            "Iteration 106, loss = 0.60598430\n",
            "Iteration 107, loss = 0.60435892\n",
            "Iteration 108, loss = 0.60284945\n",
            "Iteration 109, loss = 0.60120920\n",
            "Iteration 110, loss = 0.59962945\n",
            "Iteration 111, loss = 0.59805913\n",
            "Iteration 112, loss = 0.59653614\n",
            "Iteration 113, loss = 0.59493725\n",
            "Iteration 114, loss = 0.59323849\n",
            "Iteration 115, loss = 0.59175677\n",
            "Iteration 116, loss = 0.59018289\n",
            "Iteration 117, loss = 0.58863681\n",
            "Iteration 118, loss = 0.58706168\n",
            "Iteration 119, loss = 0.58554963\n",
            "Iteration 120, loss = 0.58392037\n",
            "Iteration 121, loss = 0.58244802\n",
            "Iteration 122, loss = 0.58088828\n",
            "Iteration 123, loss = 0.57941010\n",
            "Iteration 124, loss = 0.57786901\n",
            "Iteration 125, loss = 0.57631021\n",
            "Iteration 126, loss = 0.57484336\n",
            "Iteration 127, loss = 0.57331865\n",
            "Iteration 128, loss = 0.57191654\n",
            "Iteration 129, loss = 0.57047568\n",
            "Iteration 130, loss = 0.56895454\n",
            "Iteration 131, loss = 0.56749401\n",
            "Iteration 132, loss = 0.56614360\n",
            "Iteration 133, loss = 0.56471664\n",
            "Iteration 134, loss = 0.56323012\n",
            "Iteration 135, loss = 0.56183308\n",
            "Iteration 136, loss = 0.56034945\n",
            "Iteration 137, loss = 0.55894591\n",
            "Iteration 138, loss = 0.55749273\n",
            "Iteration 139, loss = 0.55609792\n",
            "Iteration 140, loss = 0.55462274\n",
            "Iteration 141, loss = 0.55321061\n",
            "Iteration 142, loss = 0.55168761\n",
            "Iteration 143, loss = 0.55026860\n",
            "Iteration 144, loss = 0.54882434\n",
            "Iteration 145, loss = 0.54728173\n",
            "Iteration 146, loss = 0.54586132\n",
            "Iteration 147, loss = 0.54441472\n",
            "Iteration 148, loss = 0.54287726\n",
            "Iteration 149, loss = 0.54130681\n",
            "Iteration 150, loss = 0.53978519\n",
            "Iteration 151, loss = 0.53832669\n",
            "Iteration 152, loss = 0.53664852\n",
            "Iteration 153, loss = 0.53516663\n",
            "Iteration 154, loss = 0.53361356\n",
            "Iteration 155, loss = 0.53193075\n",
            "Iteration 156, loss = 0.53030015\n",
            "Iteration 157, loss = 0.52862696\n",
            "Iteration 158, loss = 0.52694990\n",
            "Iteration 159, loss = 0.52535623\n",
            "Iteration 160, loss = 0.52371096\n",
            "Iteration 161, loss = 0.52202507\n",
            "Iteration 162, loss = 0.52029150\n",
            "Iteration 163, loss = 0.51873261\n",
            "Iteration 164, loss = 0.51713821\n",
            "Iteration 165, loss = 0.51562132\n",
            "Iteration 166, loss = 0.51415803\n",
            "Iteration 167, loss = 0.51260354\n",
            "Iteration 168, loss = 0.51125928\n",
            "Iteration 169, loss = 0.51000293\n",
            "Iteration 170, loss = 0.50858551\n",
            "Iteration 171, loss = 0.50730682\n",
            "Iteration 172, loss = 0.50610750\n",
            "Iteration 173, loss = 0.50493836\n",
            "Iteration 174, loss = 0.50367471\n",
            "Iteration 175, loss = 0.50252360\n",
            "Iteration 176, loss = 0.50131124\n",
            "Iteration 177, loss = 0.50027178\n",
            "Iteration 178, loss = 0.49938764\n",
            "Iteration 179, loss = 0.49833076\n",
            "Iteration 180, loss = 0.49726889\n",
            "Iteration 181, loss = 0.49622126\n",
            "Iteration 182, loss = 0.49521481\n",
            "Iteration 183, loss = 0.49429908\n",
            "Iteration 184, loss = 0.49332058\n",
            "Iteration 185, loss = 0.49240557\n",
            "Iteration 186, loss = 0.49158636\n",
            "Iteration 187, loss = 0.49064782\n",
            "Iteration 188, loss = 0.48984393\n",
            "Iteration 189, loss = 0.48900655\n",
            "Iteration 190, loss = 0.48816906\n",
            "Iteration 191, loss = 0.48730487\n",
            "Iteration 192, loss = 0.48655916\n",
            "Iteration 193, loss = 0.48585355\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.88908498\n",
            "Iteration 2, loss = 0.88416135\n",
            "Iteration 3, loss = 0.87936016\n",
            "Iteration 4, loss = 0.87455039\n",
            "Iteration 5, loss = 0.86996757\n",
            "Iteration 6, loss = 0.86552542\n",
            "Iteration 7, loss = 0.86102176\n",
            "Iteration 8, loss = 0.85666521\n",
            "Iteration 9, loss = 0.85242926\n",
            "Iteration 10, loss = 0.84821095\n",
            "Iteration 11, loss = 0.84414361\n",
            "Iteration 12, loss = 0.84006199\n",
            "Iteration 13, loss = 0.83597713\n",
            "Iteration 14, loss = 0.83200346\n",
            "Iteration 15, loss = 0.82818749\n",
            "Iteration 16, loss = 0.82409702\n",
            "Iteration 17, loss = 0.82015717\n",
            "Iteration 18, loss = 0.81621125\n",
            "Iteration 19, loss = 0.81223727\n",
            "Iteration 20, loss = 0.80826903\n",
            "Iteration 21, loss = 0.80444908\n",
            "Iteration 22, loss = 0.80046208\n",
            "Iteration 23, loss = 0.79653625\n",
            "Iteration 24, loss = 0.79235558\n",
            "Iteration 25, loss = 0.78850472\n",
            "Iteration 26, loss = 0.78457688\n",
            "Iteration 27, loss = 0.78074819\n",
            "Iteration 28, loss = 0.77669408\n",
            "Iteration 29, loss = 0.77293867\n",
            "Iteration 30, loss = 0.76915777\n",
            "Iteration 31, loss = 0.76512629\n",
            "Iteration 32, loss = 0.76145131\n",
            "Iteration 33, loss = 0.75789543\n",
            "Iteration 34, loss = 0.75404033\n",
            "Iteration 35, loss = 0.75015881\n",
            "Iteration 36, loss = 0.74668495\n",
            "Iteration 37, loss = 0.74330125\n",
            "Iteration 38, loss = 0.73954062\n",
            "Iteration 39, loss = 0.73611410\n",
            "Iteration 40, loss = 0.73299411\n",
            "Iteration 41, loss = 0.72955791\n",
            "Iteration 42, loss = 0.72644308\n",
            "Iteration 43, loss = 0.72348075\n",
            "Iteration 44, loss = 0.72063082\n",
            "Iteration 45, loss = 0.71768595\n",
            "Iteration 46, loss = 0.71502962\n",
            "Iteration 47, loss = 0.71241751\n",
            "Iteration 48, loss = 0.71009717\n",
            "Iteration 49, loss = 0.70769835\n",
            "Iteration 50, loss = 0.70549371\n",
            "Iteration 51, loss = 0.70346707\n",
            "Iteration 52, loss = 0.70130767\n",
            "Iteration 53, loss = 0.69933582\n",
            "Iteration 54, loss = 0.69755111\n",
            "Iteration 55, loss = 0.69577642\n",
            "Iteration 56, loss = 0.69401999\n",
            "Iteration 57, loss = 0.69229328\n",
            "Iteration 58, loss = 0.69071076\n",
            "Iteration 59, loss = 0.68913348\n",
            "Iteration 60, loss = 0.68754863\n",
            "Iteration 61, loss = 0.68602078\n",
            "Iteration 62, loss = 0.68463510\n",
            "Iteration 63, loss = 0.68308954\n",
            "Iteration 64, loss = 0.68177555\n",
            "Iteration 65, loss = 0.68040024\n",
            "Iteration 66, loss = 0.67907812\n",
            "Iteration 67, loss = 0.67777370\n",
            "Iteration 68, loss = 0.67647220\n",
            "Iteration 69, loss = 0.67513972\n",
            "Iteration 70, loss = 0.67382897\n",
            "Iteration 71, loss = 0.67243132\n",
            "Iteration 72, loss = 0.67103087\n",
            "Iteration 73, loss = 0.66957560\n",
            "Iteration 74, loss = 0.66812389\n",
            "Iteration 75, loss = 0.66665559\n",
            "Iteration 76, loss = 0.66509479\n",
            "Iteration 77, loss = 0.66364460\n",
            "Iteration 78, loss = 0.66219629\n",
            "Iteration 79, loss = 0.66059480\n",
            "Iteration 80, loss = 0.65909731\n",
            "Iteration 81, loss = 0.65756553\n",
            "Iteration 82, loss = 0.65593824\n",
            "Iteration 83, loss = 0.65448434\n",
            "Iteration 84, loss = 0.65290844\n",
            "Iteration 85, loss = 0.65136421\n",
            "Iteration 86, loss = 0.64976589\n",
            "Iteration 87, loss = 0.64825161\n",
            "Iteration 88, loss = 0.64671958\n",
            "Iteration 89, loss = 0.64523017\n",
            "Iteration 90, loss = 0.64367331\n",
            "Iteration 91, loss = 0.64218992\n",
            "Iteration 92, loss = 0.64061410\n",
            "Iteration 93, loss = 0.63907897\n",
            "Iteration 94, loss = 0.63753289\n",
            "Iteration 95, loss = 0.63603263\n",
            "Iteration 96, loss = 0.63450914\n",
            "Iteration 97, loss = 0.63293314\n",
            "Iteration 98, loss = 0.63143558\n",
            "Iteration 99, loss = 0.62985767\n",
            "Iteration 100, loss = 0.62835150\n",
            "Iteration 101, loss = 0.62683906\n",
            "Iteration 102, loss = 0.62534236\n",
            "Iteration 103, loss = 0.62392229\n",
            "Iteration 104, loss = 0.62235659\n",
            "Iteration 105, loss = 0.62085165\n",
            "Iteration 106, loss = 0.61947135\n",
            "Iteration 107, loss = 0.61792634\n",
            "Iteration 108, loss = 0.61650303\n",
            "Iteration 109, loss = 0.61497125\n",
            "Iteration 110, loss = 0.61347811\n",
            "Iteration 111, loss = 0.61199907\n",
            "Iteration 112, loss = 0.61047474\n",
            "Iteration 113, loss = 0.60898426\n",
            "Iteration 114, loss = 0.60740656\n",
            "Iteration 115, loss = 0.60591384\n",
            "Iteration 116, loss = 0.60435442\n",
            "Iteration 117, loss = 0.60288523\n",
            "Iteration 118, loss = 0.60132350\n",
            "Iteration 119, loss = 0.59983997\n",
            "Iteration 120, loss = 0.59824419\n",
            "Iteration 121, loss = 0.59672782\n",
            "Iteration 122, loss = 0.59516809\n",
            "Iteration 123, loss = 0.59366611\n",
            "Iteration 124, loss = 0.59208948\n",
            "Iteration 125, loss = 0.59054263\n",
            "Iteration 126, loss = 0.58904087\n",
            "Iteration 127, loss = 0.58745921\n",
            "Iteration 128, loss = 0.58597235\n",
            "Iteration 129, loss = 0.58449612\n",
            "Iteration 130, loss = 0.58293013\n",
            "Iteration 131, loss = 0.58139279\n",
            "Iteration 132, loss = 0.57989249\n",
            "Iteration 133, loss = 0.57846414\n",
            "Iteration 134, loss = 0.57682262\n",
            "Iteration 135, loss = 0.57529478\n",
            "Iteration 136, loss = 0.57376999\n",
            "Iteration 137, loss = 0.57221559\n",
            "Iteration 138, loss = 0.57070256\n",
            "Iteration 139, loss = 0.56919712\n",
            "Iteration 140, loss = 0.56771102\n",
            "Iteration 141, loss = 0.56614778\n",
            "Iteration 142, loss = 0.56457286\n",
            "Iteration 143, loss = 0.56304801\n",
            "Iteration 144, loss = 0.56144239\n",
            "Iteration 145, loss = 0.55988157\n",
            "Iteration 146, loss = 0.55836618\n",
            "Iteration 147, loss = 0.55684789\n",
            "Iteration 148, loss = 0.55529139\n",
            "Iteration 149, loss = 0.55366041\n",
            "Iteration 150, loss = 0.55215366\n",
            "Iteration 151, loss = 0.55059410\n",
            "Iteration 152, loss = 0.54895181\n",
            "Iteration 153, loss = 0.54742135\n",
            "Iteration 154, loss = 0.54591917\n",
            "Iteration 155, loss = 0.54421825\n",
            "Iteration 156, loss = 0.54260045\n",
            "Iteration 157, loss = 0.54091905\n",
            "Iteration 158, loss = 0.53923011\n",
            "Iteration 159, loss = 0.53755063\n",
            "Iteration 160, loss = 0.53586097\n",
            "Iteration 161, loss = 0.53408514\n",
            "Iteration 162, loss = 0.53218874\n",
            "Iteration 163, loss = 0.53031854\n",
            "Iteration 164, loss = 0.52856260\n",
            "Iteration 165, loss = 0.52677176\n",
            "Iteration 166, loss = 0.52503580\n",
            "Iteration 167, loss = 0.52311080\n",
            "Iteration 168, loss = 0.52144699\n",
            "Iteration 169, loss = 0.51996950\n",
            "Iteration 170, loss = 0.51822344\n",
            "Iteration 171, loss = 0.51657536\n",
            "Iteration 172, loss = 0.51512576\n",
            "Iteration 173, loss = 0.51367024\n",
            "Iteration 174, loss = 0.51212774\n",
            "Iteration 175, loss = 0.51069132\n",
            "Iteration 176, loss = 0.50923983\n",
            "Iteration 177, loss = 0.50783424\n",
            "Iteration 178, loss = 0.50670465\n",
            "Iteration 179, loss = 0.50543246\n",
            "Iteration 180, loss = 0.50417594\n",
            "Iteration 181, loss = 0.50298896\n",
            "Iteration 182, loss = 0.50179454\n",
            "Iteration 183, loss = 0.50071493\n",
            "Iteration 184, loss = 0.49962514\n",
            "Iteration 185, loss = 0.49852040\n",
            "Iteration 186, loss = 0.49757273\n",
            "Iteration 187, loss = 0.49652220\n",
            "Iteration 188, loss = 0.49559512\n",
            "Iteration 189, loss = 0.49463601\n",
            "Iteration 190, loss = 0.49365870\n",
            "Iteration 191, loss = 0.49275679\n",
            "Iteration 192, loss = 0.49194045\n",
            "Iteration 193, loss = 0.49108846\n",
            "Iteration 194, loss = 0.49024389\n",
            "Iteration 195, loss = 0.48947663\n",
            "Iteration 196, loss = 0.48872565\n",
            "Iteration 197, loss = 0.48797110\n",
            "Iteration 198, loss = 0.48719199\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.81075010\n",
            "Iteration 2, loss = 0.80786590\n",
            "Iteration 3, loss = 0.80534330\n",
            "Iteration 4, loss = 0.80251941\n",
            "Iteration 5, loss = 0.79982032\n",
            "Iteration 6, loss = 0.79717700\n",
            "Iteration 7, loss = 0.79432491\n",
            "Iteration 8, loss = 0.79151275\n",
            "Iteration 9, loss = 0.78882073\n",
            "Iteration 10, loss = 0.78586009\n",
            "Iteration 11, loss = 0.78304272\n",
            "Iteration 12, loss = 0.78022938\n",
            "Iteration 13, loss = 0.77729575\n",
            "Iteration 14, loss = 0.77459891\n",
            "Iteration 15, loss = 0.77160489\n",
            "Iteration 16, loss = 0.76885663\n",
            "Iteration 17, loss = 0.76587935\n",
            "Iteration 18, loss = 0.76309189\n",
            "Iteration 19, loss = 0.76022225\n",
            "Iteration 20, loss = 0.75732067\n",
            "Iteration 21, loss = 0.75429435\n",
            "Iteration 22, loss = 0.75132544\n",
            "Iteration 23, loss = 0.74855460\n",
            "Iteration 24, loss = 0.74534771\n",
            "Iteration 25, loss = 0.74251917\n",
            "Iteration 26, loss = 0.73944778\n",
            "Iteration 27, loss = 0.73652338\n",
            "Iteration 28, loss = 0.73334766\n",
            "Iteration 29, loss = 0.73021832\n",
            "Iteration 30, loss = 0.72713172\n",
            "Iteration 31, loss = 0.72375439\n",
            "Iteration 32, loss = 0.72041687\n",
            "Iteration 33, loss = 0.71736391\n",
            "Iteration 34, loss = 0.71409966\n",
            "Iteration 35, loss = 0.71105796\n",
            "Iteration 36, loss = 0.70781353\n",
            "Iteration 37, loss = 0.70475602\n",
            "Iteration 38, loss = 0.70180972\n",
            "Iteration 39, loss = 0.69870449\n",
            "Iteration 40, loss = 0.69564779\n",
            "Iteration 41, loss = 0.69271317\n",
            "Iteration 42, loss = 0.68962777\n",
            "Iteration 43, loss = 0.68654612\n",
            "Iteration 44, loss = 0.68360184\n",
            "Iteration 45, loss = 0.68069602\n",
            "Iteration 46, loss = 0.67787332\n",
            "Iteration 47, loss = 0.67499752\n",
            "Iteration 48, loss = 0.67217733\n",
            "Iteration 49, loss = 0.66937727\n",
            "Iteration 50, loss = 0.66654853\n",
            "Iteration 51, loss = 0.66375606\n",
            "Iteration 52, loss = 0.66102184\n",
            "Iteration 53, loss = 0.65822033\n",
            "Iteration 54, loss = 0.65536726\n",
            "Iteration 55, loss = 0.65265520\n",
            "Iteration 56, loss = 0.64977496\n",
            "Iteration 57, loss = 0.64700450\n",
            "Iteration 58, loss = 0.64435537\n",
            "Iteration 59, loss = 0.64161846\n",
            "Iteration 60, loss = 0.63885215\n",
            "Iteration 61, loss = 0.63623196\n",
            "Iteration 62, loss = 0.63363164\n",
            "Iteration 63, loss = 0.63091253\n",
            "Iteration 64, loss = 0.62821682\n",
            "Iteration 65, loss = 0.62564105\n",
            "Iteration 66, loss = 0.62298080\n",
            "Iteration 67, loss = 0.62035065\n",
            "Iteration 68, loss = 0.61762023\n",
            "Iteration 69, loss = 0.61512312\n",
            "Iteration 70, loss = 0.61254172\n",
            "Iteration 71, loss = 0.61013139\n",
            "Iteration 72, loss = 0.60756914\n",
            "Iteration 73, loss = 0.60495022\n",
            "Iteration 74, loss = 0.60251516\n",
            "Iteration 75, loss = 0.60002201\n",
            "Iteration 76, loss = 0.59751737\n",
            "Iteration 77, loss = 0.59496036\n",
            "Iteration 78, loss = 0.59234282\n",
            "Iteration 79, loss = 0.58991080\n",
            "Iteration 80, loss = 0.58742885\n",
            "Iteration 81, loss = 0.58499854\n",
            "Iteration 82, loss = 0.58254275\n",
            "Iteration 83, loss = 0.58033427\n",
            "Iteration 84, loss = 0.57819442\n",
            "Iteration 85, loss = 0.57583616\n",
            "Iteration 86, loss = 0.57381451\n",
            "Iteration 87, loss = 0.57159442\n",
            "Iteration 88, loss = 0.56953084\n",
            "Iteration 89, loss = 0.56743957\n",
            "Iteration 90, loss = 0.56550749\n",
            "Iteration 91, loss = 0.56339626\n",
            "Iteration 92, loss = 0.56144062\n",
            "Iteration 93, loss = 0.55950070\n",
            "Iteration 94, loss = 0.55768601\n",
            "Iteration 95, loss = 0.55587922\n",
            "Iteration 96, loss = 0.55395832\n",
            "Iteration 97, loss = 0.55233156\n",
            "Iteration 98, loss = 0.55052270\n",
            "Iteration 99, loss = 0.54877207\n",
            "Iteration 100, loss = 0.54700678\n",
            "Iteration 101, loss = 0.54510805\n",
            "Iteration 102, loss = 0.54333411\n",
            "Iteration 103, loss = 0.54151436\n",
            "Iteration 104, loss = 0.53977420\n",
            "Iteration 105, loss = 0.53802490\n",
            "Iteration 106, loss = 0.53636972\n",
            "Iteration 107, loss = 0.53481320\n",
            "Iteration 108, loss = 0.53315209\n",
            "Iteration 109, loss = 0.53167853\n",
            "Iteration 110, loss = 0.53011805\n",
            "Iteration 111, loss = 0.52862140\n",
            "Iteration 112, loss = 0.52712264\n",
            "Iteration 113, loss = 0.52560955\n",
            "Iteration 114, loss = 0.52416505\n",
            "Iteration 115, loss = 0.52272346\n",
            "Iteration 116, loss = 0.52129169\n",
            "Iteration 117, loss = 0.51987194\n",
            "Iteration 118, loss = 0.51850629\n",
            "Iteration 119, loss = 0.51717949\n",
            "Iteration 120, loss = 0.51578127\n",
            "Iteration 121, loss = 0.51445311\n",
            "Iteration 122, loss = 0.51317509\n",
            "Iteration 123, loss = 0.51190988\n",
            "Iteration 124, loss = 0.51060881\n",
            "Iteration 125, loss = 0.50935230\n",
            "Iteration 126, loss = 0.50817671\n",
            "Iteration 127, loss = 0.50692246\n",
            "Iteration 128, loss = 0.50577318\n",
            "Iteration 129, loss = 0.50463987\n",
            "Iteration 130, loss = 0.50353940\n",
            "Iteration 131, loss = 0.50251581\n",
            "Iteration 132, loss = 0.50142359\n",
            "Iteration 133, loss = 0.50029200\n",
            "Iteration 134, loss = 0.49923203\n",
            "Iteration 135, loss = 0.49823651\n",
            "Iteration 136, loss = 0.49718836\n",
            "Iteration 137, loss = 0.49615881\n",
            "Iteration 138, loss = 0.49509361\n",
            "Iteration 139, loss = 0.49424844\n",
            "Iteration 140, loss = 0.49324683\n",
            "Iteration 141, loss = 0.49231331\n",
            "Iteration 142, loss = 0.49125945\n",
            "Iteration 143, loss = 0.49036589\n",
            "Iteration 144, loss = 0.48948330\n",
            "Iteration 145, loss = 0.48843619\n",
            "Iteration 146, loss = 0.48757727\n",
            "Iteration 147, loss = 0.48670052\n",
            "Iteration 148, loss = 0.48579212\n",
            "Iteration 149, loss = 0.48500518\n",
            "Iteration 150, loss = 0.48412260\n",
            "Iteration 151, loss = 0.48328194\n",
            "Iteration 152, loss = 0.48253577\n",
            "Iteration 153, loss = 0.48169279\n",
            "Iteration 154, loss = 0.48098386\n",
            "Iteration 155, loss = 0.48023917\n",
            "Iteration 156, loss = 0.47951461\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.81075049\n",
            "Iteration 2, loss = 0.80788012\n",
            "Iteration 3, loss = 0.80528539\n",
            "Iteration 4, loss = 0.80250421\n",
            "Iteration 5, loss = 0.79973283\n",
            "Iteration 6, loss = 0.79710681\n",
            "Iteration 7, loss = 0.79427950\n",
            "Iteration 8, loss = 0.79142484\n",
            "Iteration 9, loss = 0.78870518\n",
            "Iteration 10, loss = 0.78573910\n",
            "Iteration 11, loss = 0.78293656\n",
            "Iteration 12, loss = 0.78003868\n",
            "Iteration 13, loss = 0.77712295\n",
            "Iteration 14, loss = 0.77437043\n",
            "Iteration 15, loss = 0.77146820\n",
            "Iteration 16, loss = 0.76862446\n",
            "Iteration 17, loss = 0.76563419\n",
            "Iteration 18, loss = 0.76281734\n",
            "Iteration 19, loss = 0.75993093\n",
            "Iteration 20, loss = 0.75696192\n",
            "Iteration 21, loss = 0.75394581\n",
            "Iteration 22, loss = 0.75092739\n",
            "Iteration 23, loss = 0.74812801\n",
            "Iteration 24, loss = 0.74495096\n",
            "Iteration 25, loss = 0.74203558\n",
            "Iteration 26, loss = 0.73895776\n",
            "Iteration 27, loss = 0.73595331\n",
            "Iteration 28, loss = 0.73268468\n",
            "Iteration 29, loss = 0.72961093\n",
            "Iteration 30, loss = 0.72649463\n",
            "Iteration 31, loss = 0.72322430\n",
            "Iteration 32, loss = 0.71994232\n",
            "Iteration 33, loss = 0.71684623\n",
            "Iteration 34, loss = 0.71369381\n",
            "Iteration 35, loss = 0.71061248\n",
            "Iteration 36, loss = 0.70746455\n",
            "Iteration 37, loss = 0.70447063\n",
            "Iteration 38, loss = 0.70149738\n",
            "Iteration 39, loss = 0.69846505\n",
            "Iteration 40, loss = 0.69552205\n",
            "Iteration 41, loss = 0.69271161\n",
            "Iteration 42, loss = 0.68984164\n",
            "Iteration 43, loss = 0.68705389\n",
            "Iteration 44, loss = 0.68424049\n",
            "Iteration 45, loss = 0.68150009\n",
            "Iteration 46, loss = 0.67889952\n",
            "Iteration 47, loss = 0.67618794\n",
            "Iteration 48, loss = 0.67358413\n",
            "Iteration 49, loss = 0.67101971\n",
            "Iteration 50, loss = 0.66840710\n",
            "Iteration 51, loss = 0.66598871\n",
            "Iteration 52, loss = 0.66342857\n",
            "Iteration 53, loss = 0.66097429\n",
            "Iteration 54, loss = 0.65847794\n",
            "Iteration 55, loss = 0.65592491\n",
            "Iteration 56, loss = 0.65349783\n",
            "Iteration 57, loss = 0.65105694\n",
            "Iteration 58, loss = 0.64874050\n",
            "Iteration 59, loss = 0.64634003\n",
            "Iteration 60, loss = 0.64384945\n",
            "Iteration 61, loss = 0.64150375\n",
            "Iteration 62, loss = 0.63914437\n",
            "Iteration 63, loss = 0.63676498\n",
            "Iteration 64, loss = 0.63431754\n",
            "Iteration 65, loss = 0.63201919\n",
            "Iteration 66, loss = 0.62963540\n",
            "Iteration 67, loss = 0.62730287\n",
            "Iteration 68, loss = 0.62488943\n",
            "Iteration 69, loss = 0.62267392\n",
            "Iteration 70, loss = 0.62041549\n",
            "Iteration 71, loss = 0.61820612\n",
            "Iteration 72, loss = 0.61598221\n",
            "Iteration 73, loss = 0.61365261\n",
            "Iteration 74, loss = 0.61146024\n",
            "Iteration 75, loss = 0.60923600\n",
            "Iteration 76, loss = 0.60696957\n",
            "Iteration 77, loss = 0.60485166\n",
            "Iteration 78, loss = 0.60255830\n",
            "Iteration 79, loss = 0.60041973\n",
            "Iteration 80, loss = 0.59836855\n",
            "Iteration 81, loss = 0.59608822\n",
            "Iteration 82, loss = 0.59384882\n",
            "Iteration 83, loss = 0.59179749\n",
            "Iteration 84, loss = 0.58969697\n",
            "Iteration 85, loss = 0.58759314\n",
            "Iteration 86, loss = 0.58572873\n",
            "Iteration 87, loss = 0.58375689\n",
            "Iteration 88, loss = 0.58201011\n",
            "Iteration 89, loss = 0.58015807\n",
            "Iteration 90, loss = 0.57838758\n",
            "Iteration 91, loss = 0.57650518\n",
            "Iteration 92, loss = 0.57480915\n",
            "Iteration 93, loss = 0.57305084\n",
            "Iteration 94, loss = 0.57143302\n",
            "Iteration 95, loss = 0.56986880\n",
            "Iteration 96, loss = 0.56814888\n",
            "Iteration 97, loss = 0.56664297\n",
            "Iteration 98, loss = 0.56505499\n",
            "Iteration 99, loss = 0.56346239\n",
            "Iteration 100, loss = 0.56193272\n",
            "Iteration 101, loss = 0.56031864\n",
            "Iteration 102, loss = 0.55881473\n",
            "Iteration 103, loss = 0.55716693\n",
            "Iteration 104, loss = 0.55561426\n",
            "Iteration 105, loss = 0.55398395\n",
            "Iteration 106, loss = 0.55241401\n",
            "Iteration 107, loss = 0.55081285\n",
            "Iteration 108, loss = 0.54921960\n",
            "Iteration 109, loss = 0.54764687\n",
            "Iteration 110, loss = 0.54588975\n",
            "Iteration 111, loss = 0.54433535\n",
            "Iteration 112, loss = 0.54283403\n",
            "Iteration 113, loss = 0.54140243\n",
            "Iteration 114, loss = 0.54003012\n",
            "Iteration 115, loss = 0.53869984\n",
            "Iteration 116, loss = 0.53725400\n",
            "Iteration 117, loss = 0.53591453\n",
            "Iteration 118, loss = 0.53463036\n",
            "Iteration 119, loss = 0.53340024\n",
            "Iteration 120, loss = 0.53212504\n",
            "Iteration 121, loss = 0.53088944\n",
            "Iteration 122, loss = 0.52969457\n",
            "Iteration 123, loss = 0.52856658\n",
            "Iteration 124, loss = 0.52738964\n",
            "Iteration 125, loss = 0.52623412\n",
            "Iteration 126, loss = 0.52515099\n",
            "Iteration 127, loss = 0.52403730\n",
            "Iteration 128, loss = 0.52294570\n",
            "Iteration 129, loss = 0.52183846\n",
            "Iteration 130, loss = 0.52073785\n",
            "Iteration 131, loss = 0.51980177\n",
            "Iteration 132, loss = 0.51876180\n",
            "Iteration 133, loss = 0.51777714\n",
            "Iteration 134, loss = 0.51682641\n",
            "Iteration 135, loss = 0.51589614\n",
            "Iteration 136, loss = 0.51493580\n",
            "Iteration 137, loss = 0.51403565\n",
            "Iteration 138, loss = 0.51310999\n",
            "Iteration 139, loss = 0.51225112\n",
            "Iteration 140, loss = 0.51137785\n",
            "Iteration 141, loss = 0.51054902\n",
            "Iteration 142, loss = 0.50967900\n",
            "Iteration 143, loss = 0.50884314\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.81045049\n",
            "Iteration 2, loss = 0.80759751\n",
            "Iteration 3, loss = 0.80494773\n",
            "Iteration 4, loss = 0.80218697\n",
            "Iteration 5, loss = 0.79941028\n",
            "Iteration 6, loss = 0.79681833\n",
            "Iteration 7, loss = 0.79395732\n",
            "Iteration 8, loss = 0.79107258\n",
            "Iteration 9, loss = 0.78834439\n",
            "Iteration 10, loss = 0.78547194\n",
            "Iteration 11, loss = 0.78270253\n",
            "Iteration 12, loss = 0.77986962\n",
            "Iteration 13, loss = 0.77696868\n",
            "Iteration 14, loss = 0.77426780\n",
            "Iteration 15, loss = 0.77139543\n",
            "Iteration 16, loss = 0.76864173\n",
            "Iteration 17, loss = 0.76564945\n",
            "Iteration 18, loss = 0.76291301\n",
            "Iteration 19, loss = 0.76008553\n",
            "Iteration 20, loss = 0.75715624\n",
            "Iteration 21, loss = 0.75419128\n",
            "Iteration 22, loss = 0.75122219\n",
            "Iteration 23, loss = 0.74843152\n",
            "Iteration 24, loss = 0.74524424\n",
            "Iteration 25, loss = 0.74232698\n",
            "Iteration 26, loss = 0.73926421\n",
            "Iteration 27, loss = 0.73630541\n",
            "Iteration 28, loss = 0.73307808\n",
            "Iteration 29, loss = 0.72999344\n",
            "Iteration 30, loss = 0.72696815\n",
            "Iteration 31, loss = 0.72362935\n",
            "Iteration 32, loss = 0.72042052\n",
            "Iteration 33, loss = 0.71723833\n",
            "Iteration 34, loss = 0.71413080\n",
            "Iteration 35, loss = 0.71100193\n",
            "Iteration 36, loss = 0.70790601\n",
            "Iteration 37, loss = 0.70484069\n",
            "Iteration 38, loss = 0.70190676\n",
            "Iteration 39, loss = 0.69883288\n",
            "Iteration 40, loss = 0.69584873\n",
            "Iteration 41, loss = 0.69302202\n",
            "Iteration 42, loss = 0.69007517\n",
            "Iteration 43, loss = 0.68710460\n",
            "Iteration 44, loss = 0.68411306\n",
            "Iteration 45, loss = 0.68121300\n",
            "Iteration 46, loss = 0.67845031\n",
            "Iteration 47, loss = 0.67547374\n",
            "Iteration 48, loss = 0.67270250\n",
            "Iteration 49, loss = 0.66991314\n",
            "Iteration 50, loss = 0.66709364\n",
            "Iteration 51, loss = 0.66440158\n",
            "Iteration 52, loss = 0.66162141\n",
            "Iteration 53, loss = 0.65880085\n",
            "Iteration 54, loss = 0.65615038\n",
            "Iteration 55, loss = 0.65338644\n",
            "Iteration 56, loss = 0.65071704\n",
            "Iteration 57, loss = 0.64792663\n",
            "Iteration 58, loss = 0.64534299\n",
            "Iteration 59, loss = 0.64268541\n",
            "Iteration 60, loss = 0.64004814\n",
            "Iteration 61, loss = 0.63740186\n",
            "Iteration 62, loss = 0.63479160\n",
            "Iteration 63, loss = 0.63221972\n",
            "Iteration 64, loss = 0.62959035\n",
            "Iteration 65, loss = 0.62702371\n",
            "Iteration 66, loss = 0.62444074\n",
            "Iteration 67, loss = 0.62191550\n",
            "Iteration 68, loss = 0.61925930\n",
            "Iteration 69, loss = 0.61682204\n",
            "Iteration 70, loss = 0.61431604\n",
            "Iteration 71, loss = 0.61194131\n",
            "Iteration 72, loss = 0.60951001\n",
            "Iteration 73, loss = 0.60699519\n",
            "Iteration 74, loss = 0.60460161\n",
            "Iteration 75, loss = 0.60227103\n",
            "Iteration 76, loss = 0.59974937\n",
            "Iteration 77, loss = 0.59743628\n",
            "Iteration 78, loss = 0.59506687\n",
            "Iteration 79, loss = 0.59275283\n",
            "Iteration 80, loss = 0.59051430\n",
            "Iteration 81, loss = 0.58812438\n",
            "Iteration 82, loss = 0.58578344\n",
            "Iteration 83, loss = 0.58373908\n",
            "Iteration 84, loss = 0.58164155\n",
            "Iteration 85, loss = 0.57957875\n",
            "Iteration 86, loss = 0.57766624\n",
            "Iteration 87, loss = 0.57559906\n",
            "Iteration 88, loss = 0.57362557\n",
            "Iteration 89, loss = 0.57175273\n",
            "Iteration 90, loss = 0.56993241\n",
            "Iteration 91, loss = 0.56803668\n",
            "Iteration 92, loss = 0.56628434\n",
            "Iteration 93, loss = 0.56455332\n",
            "Iteration 94, loss = 0.56286772\n",
            "Iteration 95, loss = 0.56129175\n",
            "Iteration 96, loss = 0.55964389\n",
            "Iteration 97, loss = 0.55804518\n",
            "Iteration 98, loss = 0.55653610\n",
            "Iteration 99, loss = 0.55507650\n",
            "Iteration 100, loss = 0.55364945\n",
            "Iteration 101, loss = 0.55221786\n",
            "Iteration 102, loss = 0.55085114\n",
            "Iteration 103, loss = 0.54942015\n",
            "Iteration 104, loss = 0.54811101\n",
            "Iteration 105, loss = 0.54664301\n",
            "Iteration 106, loss = 0.54535252\n",
            "Iteration 107, loss = 0.54394594\n",
            "Iteration 108, loss = 0.54257440\n",
            "Iteration 109, loss = 0.54129799\n",
            "Iteration 110, loss = 0.53990705\n",
            "Iteration 111, loss = 0.53856019\n",
            "Iteration 112, loss = 0.53717749\n",
            "Iteration 113, loss = 0.53582650\n",
            "Iteration 114, loss = 0.53447077\n",
            "Iteration 115, loss = 0.53298299\n",
            "Iteration 116, loss = 0.53157922\n",
            "Iteration 117, loss = 0.53016906\n",
            "Iteration 118, loss = 0.52871942\n",
            "Iteration 119, loss = 0.52733709\n",
            "Iteration 120, loss = 0.52591673\n",
            "Iteration 121, loss = 0.52456495\n",
            "Iteration 122, loss = 0.52319745\n",
            "Iteration 123, loss = 0.52194632\n",
            "Iteration 124, loss = 0.52052000\n",
            "Iteration 125, loss = 0.51927510\n",
            "Iteration 126, loss = 0.51793793\n",
            "Iteration 127, loss = 0.51667848\n",
            "Iteration 128, loss = 0.51534890\n",
            "Iteration 129, loss = 0.51398280\n",
            "Iteration 130, loss = 0.51271152\n",
            "Iteration 131, loss = 0.51151308\n",
            "Iteration 132, loss = 0.51024188\n",
            "Iteration 133, loss = 0.50900416\n",
            "Iteration 134, loss = 0.50793176\n",
            "Iteration 135, loss = 0.50660285\n",
            "Iteration 136, loss = 0.50531106\n",
            "Iteration 137, loss = 0.50411373\n",
            "Iteration 138, loss = 0.50281939\n",
            "Iteration 139, loss = 0.50160781\n",
            "Iteration 140, loss = 0.50034124\n",
            "Iteration 141, loss = 0.49920639\n",
            "Iteration 142, loss = 0.49798530\n",
            "Iteration 143, loss = 0.49695080\n",
            "Iteration 144, loss = 0.49597543\n",
            "Iteration 145, loss = 0.49489080\n",
            "Iteration 146, loss = 0.49389380\n",
            "Iteration 147, loss = 0.49292135\n",
            "Iteration 148, loss = 0.49186810\n",
            "Iteration 149, loss = 0.49082566\n",
            "Iteration 150, loss = 0.48973858\n",
            "Iteration 151, loss = 0.48878899\n",
            "Iteration 152, loss = 0.48778558\n",
            "Iteration 153, loss = 0.48672784\n",
            "Iteration 154, loss = 0.48579360\n",
            "Iteration 155, loss = 0.48487289\n",
            "Iteration 156, loss = 0.48394122\n",
            "Iteration 157, loss = 0.48298441\n",
            "Iteration 158, loss = 0.48215391\n",
            "Iteration 159, loss = 0.48112124\n",
            "Iteration 160, loss = 0.48010634\n",
            "Iteration 161, loss = 0.47912521\n",
            "Iteration 162, loss = 0.47813413\n",
            "Iteration 163, loss = 0.47710812\n",
            "Iteration 164, loss = 0.47598020\n",
            "Iteration 165, loss = 0.47495881\n",
            "Iteration 166, loss = 0.47382571\n",
            "Iteration 167, loss = 0.47285781\n",
            "Iteration 168, loss = 0.47182331\n",
            "Iteration 169, loss = 0.47082567\n",
            "Iteration 170, loss = 0.46980815\n",
            "Iteration 171, loss = 0.46881749\n",
            "Iteration 172, loss = 0.46780114\n",
            "Iteration 173, loss = 0.46687668\n",
            "Iteration 174, loss = 0.46604594\n",
            "Iteration 175, loss = 0.46506804\n",
            "Iteration 176, loss = 0.46418768\n",
            "Iteration 177, loss = 0.46339768\n",
            "Iteration 178, loss = 0.46261607\n",
            "Iteration 179, loss = 0.46181626\n",
            "Iteration 180, loss = 0.46102836\n",
            "Iteration 181, loss = 0.46038219\n",
            "Iteration 182, loss = 0.45964094\n",
            "Iteration 183, loss = 0.45891409\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.81043999\n",
            "Iteration 2, loss = 0.80761115\n",
            "Iteration 3, loss = 0.80500504\n",
            "Iteration 4, loss = 0.80224892\n",
            "Iteration 5, loss = 0.79950782\n",
            "Iteration 6, loss = 0.79688493\n",
            "Iteration 7, loss = 0.79403788\n",
            "Iteration 8, loss = 0.79121594\n",
            "Iteration 9, loss = 0.78846846\n",
            "Iteration 10, loss = 0.78562162\n",
            "Iteration 11, loss = 0.78288691\n",
            "Iteration 12, loss = 0.77997965\n",
            "Iteration 13, loss = 0.77712108\n",
            "Iteration 14, loss = 0.77443965\n",
            "Iteration 15, loss = 0.77158627\n",
            "Iteration 16, loss = 0.76876926\n",
            "Iteration 17, loss = 0.76586184\n",
            "Iteration 18, loss = 0.76301623\n",
            "Iteration 19, loss = 0.76010958\n",
            "Iteration 20, loss = 0.75711729\n",
            "Iteration 21, loss = 0.75409245\n",
            "Iteration 22, loss = 0.75121067\n",
            "Iteration 23, loss = 0.74825916\n",
            "Iteration 24, loss = 0.74511445\n",
            "Iteration 25, loss = 0.74219297\n",
            "Iteration 26, loss = 0.73912076\n",
            "Iteration 27, loss = 0.73624364\n",
            "Iteration 28, loss = 0.73298270\n",
            "Iteration 29, loss = 0.72991561\n",
            "Iteration 30, loss = 0.72682523\n",
            "Iteration 31, loss = 0.72342630\n",
            "Iteration 32, loss = 0.72022588\n",
            "Iteration 33, loss = 0.71692994\n",
            "Iteration 34, loss = 0.71369348\n",
            "Iteration 35, loss = 0.71044094\n",
            "Iteration 36, loss = 0.70720802\n",
            "Iteration 37, loss = 0.70399026\n",
            "Iteration 38, loss = 0.70091021\n",
            "Iteration 39, loss = 0.69756094\n",
            "Iteration 40, loss = 0.69435898\n",
            "Iteration 41, loss = 0.69123596\n",
            "Iteration 42, loss = 0.68806607\n",
            "Iteration 43, loss = 0.68484258\n",
            "Iteration 44, loss = 0.68177668\n",
            "Iteration 45, loss = 0.67860153\n",
            "Iteration 46, loss = 0.67565375\n",
            "Iteration 47, loss = 0.67253286\n",
            "Iteration 48, loss = 0.66967082\n",
            "Iteration 49, loss = 0.66668336\n",
            "Iteration 50, loss = 0.66376608\n",
            "Iteration 51, loss = 0.66100854\n",
            "Iteration 52, loss = 0.65808916\n",
            "Iteration 53, loss = 0.65511230\n",
            "Iteration 54, loss = 0.65235459\n",
            "Iteration 55, loss = 0.64946551\n",
            "Iteration 56, loss = 0.64671900\n",
            "Iteration 57, loss = 0.64385642\n",
            "Iteration 58, loss = 0.64113825\n",
            "Iteration 59, loss = 0.63847105\n",
            "Iteration 60, loss = 0.63575551\n",
            "Iteration 61, loss = 0.63295863\n",
            "Iteration 62, loss = 0.63026371\n",
            "Iteration 63, loss = 0.62754463\n",
            "Iteration 64, loss = 0.62486110\n",
            "Iteration 65, loss = 0.62222916\n",
            "Iteration 66, loss = 0.61958732\n",
            "Iteration 67, loss = 0.61697135\n",
            "Iteration 68, loss = 0.61414382\n",
            "Iteration 69, loss = 0.61154851\n",
            "Iteration 70, loss = 0.60885971\n",
            "Iteration 71, loss = 0.60616030\n",
            "Iteration 72, loss = 0.60356296\n",
            "Iteration 73, loss = 0.60086582\n",
            "Iteration 74, loss = 0.59817483\n",
            "Iteration 75, loss = 0.59568584\n",
            "Iteration 76, loss = 0.59296275\n",
            "Iteration 77, loss = 0.59061714\n",
            "Iteration 78, loss = 0.58822918\n",
            "Iteration 79, loss = 0.58595708\n",
            "Iteration 80, loss = 0.58384568\n",
            "Iteration 81, loss = 0.58154662\n",
            "Iteration 82, loss = 0.57956302\n",
            "Iteration 83, loss = 0.57767929\n",
            "Iteration 84, loss = 0.57569167\n",
            "Iteration 85, loss = 0.57371262\n",
            "Iteration 86, loss = 0.57196397\n",
            "Iteration 87, loss = 0.57001333\n",
            "Iteration 88, loss = 0.56824404\n",
            "Iteration 89, loss = 0.56645226\n",
            "Iteration 90, loss = 0.56482457\n",
            "Iteration 91, loss = 0.56316642\n",
            "Iteration 92, loss = 0.56138548\n",
            "Iteration 93, loss = 0.55988564\n",
            "Iteration 94, loss = 0.55822895\n",
            "Iteration 95, loss = 0.55667927\n",
            "Iteration 96, loss = 0.55518435\n",
            "Iteration 97, loss = 0.55372598\n",
            "Iteration 98, loss = 0.55217571\n",
            "Iteration 99, loss = 0.55077162\n",
            "Iteration 100, loss = 0.54932473\n",
            "Iteration 101, loss = 0.54785791\n",
            "Iteration 102, loss = 0.54640667\n",
            "Iteration 103, loss = 0.54493538\n",
            "Iteration 104, loss = 0.54352770\n",
            "Iteration 105, loss = 0.54197638\n",
            "Iteration 106, loss = 0.54059678\n",
            "Iteration 107, loss = 0.53909924\n",
            "Iteration 108, loss = 0.53756915\n",
            "Iteration 109, loss = 0.53604367\n",
            "Iteration 110, loss = 0.53446576\n",
            "Iteration 111, loss = 0.53300399\n",
            "Iteration 112, loss = 0.53147064\n",
            "Iteration 113, loss = 0.52989485\n",
            "Iteration 114, loss = 0.52828237\n",
            "Iteration 115, loss = 0.52658737\n",
            "Iteration 116, loss = 0.52490022\n",
            "Iteration 117, loss = 0.52326273\n",
            "Iteration 118, loss = 0.52159644\n",
            "Iteration 119, loss = 0.52000337\n",
            "Iteration 120, loss = 0.51841697\n",
            "Iteration 121, loss = 0.51684466\n",
            "Iteration 122, loss = 0.51531331\n",
            "Iteration 123, loss = 0.51383586\n",
            "Iteration 124, loss = 0.51240099\n",
            "Iteration 125, loss = 0.51098243\n",
            "Iteration 126, loss = 0.50960196\n",
            "Iteration 127, loss = 0.50832561\n",
            "Iteration 128, loss = 0.50704327\n",
            "Iteration 129, loss = 0.50576919\n",
            "Iteration 130, loss = 0.50454111\n",
            "Iteration 131, loss = 0.50341003\n",
            "Iteration 132, loss = 0.50220135\n",
            "Iteration 133, loss = 0.50102206\n",
            "Iteration 134, loss = 0.49987023\n",
            "Iteration 135, loss = 0.49870880\n",
            "Iteration 136, loss = 0.49759792\n",
            "Iteration 137, loss = 0.49650589\n",
            "Iteration 138, loss = 0.49543745\n",
            "Iteration 139, loss = 0.49439857\n",
            "Iteration 140, loss = 0.49335425\n",
            "Iteration 141, loss = 0.49241028\n",
            "Iteration 142, loss = 0.49137558\n",
            "Iteration 143, loss = 0.49043568\n",
            "Iteration 144, loss = 0.48952967\n",
            "Iteration 145, loss = 0.48849534\n",
            "Iteration 146, loss = 0.48754458\n",
            "Iteration 147, loss = 0.48659578\n",
            "Iteration 148, loss = 0.48563669\n",
            "Iteration 149, loss = 0.48472282\n",
            "Iteration 150, loss = 0.48380689\n",
            "Iteration 151, loss = 0.48294870\n",
            "Iteration 152, loss = 0.48211477\n",
            "Iteration 153, loss = 0.48125155\n",
            "Iteration 154, loss = 0.48041134\n",
            "Iteration 155, loss = 0.47957877\n",
            "Iteration 156, loss = 0.47869658\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.81064941\n",
            "Iteration 2, loss = 0.80788987\n",
            "Iteration 3, loss = 0.80516204\n",
            "Iteration 4, loss = 0.80237150\n",
            "Iteration 5, loss = 0.79955499\n",
            "Iteration 6, loss = 0.79698677\n",
            "Iteration 7, loss = 0.79410225\n",
            "Iteration 8, loss = 0.79123448\n",
            "Iteration 9, loss = 0.78845179\n",
            "Iteration 10, loss = 0.78565136\n",
            "Iteration 11, loss = 0.78289335\n",
            "Iteration 12, loss = 0.78001391\n",
            "Iteration 13, loss = 0.77716398\n",
            "Iteration 14, loss = 0.77449113\n",
            "Iteration 15, loss = 0.77167722\n",
            "Iteration 16, loss = 0.76881172\n",
            "Iteration 17, loss = 0.76601409\n",
            "Iteration 18, loss = 0.76326679\n",
            "Iteration 19, loss = 0.76041252\n",
            "Iteration 20, loss = 0.75745405\n",
            "Iteration 21, loss = 0.75451675\n",
            "Iteration 22, loss = 0.75163429\n",
            "Iteration 23, loss = 0.74860057\n",
            "Iteration 24, loss = 0.74554938\n",
            "Iteration 25, loss = 0.74256914\n",
            "Iteration 26, loss = 0.73943676\n",
            "Iteration 27, loss = 0.73655856\n",
            "Iteration 28, loss = 0.73332143\n",
            "Iteration 29, loss = 0.73027828\n",
            "Iteration 30, loss = 0.72719173\n",
            "Iteration 31, loss = 0.72380792\n",
            "Iteration 32, loss = 0.72072105\n",
            "Iteration 33, loss = 0.71753394\n",
            "Iteration 34, loss = 0.71433300\n",
            "Iteration 35, loss = 0.71111645\n",
            "Iteration 36, loss = 0.70807370\n",
            "Iteration 37, loss = 0.70509765\n",
            "Iteration 38, loss = 0.70205901\n",
            "Iteration 39, loss = 0.69890605\n",
            "Iteration 40, loss = 0.69591367\n",
            "Iteration 41, loss = 0.69301885\n",
            "Iteration 42, loss = 0.68994150\n",
            "Iteration 43, loss = 0.68703890\n",
            "Iteration 44, loss = 0.68418992\n",
            "Iteration 45, loss = 0.68122333\n",
            "Iteration 46, loss = 0.67836242\n",
            "Iteration 47, loss = 0.67562929\n",
            "Iteration 48, loss = 0.67285607\n",
            "Iteration 49, loss = 0.67008546\n",
            "Iteration 50, loss = 0.66749772\n",
            "Iteration 51, loss = 0.66475480\n",
            "Iteration 52, loss = 0.66205372\n",
            "Iteration 53, loss = 0.65942474\n",
            "Iteration 54, loss = 0.65686767\n",
            "Iteration 55, loss = 0.65424812\n",
            "Iteration 56, loss = 0.65172783\n",
            "Iteration 57, loss = 0.64904404\n",
            "Iteration 58, loss = 0.64634116\n",
            "Iteration 59, loss = 0.64386042\n",
            "Iteration 60, loss = 0.64126334\n",
            "Iteration 61, loss = 0.63860803\n",
            "Iteration 62, loss = 0.63602830\n",
            "Iteration 63, loss = 0.63352152\n",
            "Iteration 64, loss = 0.63105472\n",
            "Iteration 65, loss = 0.62852970\n",
            "Iteration 66, loss = 0.62614098\n",
            "Iteration 67, loss = 0.62370042\n",
            "Iteration 68, loss = 0.62111359\n",
            "Iteration 69, loss = 0.61879011\n",
            "Iteration 70, loss = 0.61638729\n",
            "Iteration 71, loss = 0.61398744\n",
            "Iteration 72, loss = 0.61162547\n",
            "Iteration 73, loss = 0.60904521\n",
            "Iteration 74, loss = 0.60652192\n",
            "Iteration 75, loss = 0.60422707\n",
            "Iteration 76, loss = 0.60150223\n",
            "Iteration 77, loss = 0.59920214\n",
            "Iteration 78, loss = 0.59686238\n",
            "Iteration 79, loss = 0.59437269\n",
            "Iteration 80, loss = 0.59217937\n",
            "Iteration 81, loss = 0.58985232\n",
            "Iteration 82, loss = 0.58753203\n",
            "Iteration 83, loss = 0.58537696\n",
            "Iteration 84, loss = 0.58320213\n",
            "Iteration 85, loss = 0.58112784\n",
            "Iteration 86, loss = 0.57914279\n",
            "Iteration 87, loss = 0.57704143\n",
            "Iteration 88, loss = 0.57513552\n",
            "Iteration 89, loss = 0.57325440\n",
            "Iteration 90, loss = 0.57140691\n",
            "Iteration 91, loss = 0.56955727\n",
            "Iteration 92, loss = 0.56768314\n",
            "Iteration 93, loss = 0.56598117\n",
            "Iteration 94, loss = 0.56414703\n",
            "Iteration 95, loss = 0.56248941\n",
            "Iteration 96, loss = 0.56076652\n",
            "Iteration 97, loss = 0.55914807\n",
            "Iteration 98, loss = 0.55736904\n",
            "Iteration 99, loss = 0.55570726\n",
            "Iteration 100, loss = 0.55397744\n",
            "Iteration 101, loss = 0.55235759\n",
            "Iteration 102, loss = 0.55064844\n",
            "Iteration 103, loss = 0.54877942\n",
            "Iteration 104, loss = 0.54698520\n",
            "Iteration 105, loss = 0.54506615\n",
            "Iteration 106, loss = 0.54334786\n",
            "Iteration 107, loss = 0.54152205\n",
            "Iteration 108, loss = 0.53982166\n",
            "Iteration 109, loss = 0.53825280\n",
            "Iteration 110, loss = 0.53669082\n",
            "Iteration 111, loss = 0.53513856\n",
            "Iteration 112, loss = 0.53361153\n",
            "Iteration 113, loss = 0.53223695\n",
            "Iteration 114, loss = 0.53066821\n",
            "Iteration 115, loss = 0.52915678\n",
            "Iteration 116, loss = 0.52767134\n",
            "Iteration 117, loss = 0.52637526\n",
            "Iteration 118, loss = 0.52494261\n",
            "Iteration 119, loss = 0.52363264\n",
            "Iteration 120, loss = 0.52223048\n",
            "Iteration 121, loss = 0.52090576\n",
            "Iteration 122, loss = 0.51959262\n",
            "Iteration 123, loss = 0.51832005\n",
            "Iteration 124, loss = 0.51710055\n",
            "Iteration 125, loss = 0.51589473\n",
            "Iteration 126, loss = 0.51464236\n",
            "Iteration 127, loss = 0.51349377\n",
            "Iteration 128, loss = 0.51233712\n",
            "Iteration 129, loss = 0.51111709\n",
            "Iteration 130, loss = 0.50989027\n",
            "Iteration 131, loss = 0.50877781\n",
            "Iteration 132, loss = 0.50759295\n",
            "Iteration 133, loss = 0.50643624\n",
            "Iteration 134, loss = 0.50544467\n",
            "Iteration 135, loss = 0.50433453\n",
            "Iteration 136, loss = 0.50325417\n",
            "Iteration 137, loss = 0.50227635\n",
            "Iteration 138, loss = 0.50121177\n",
            "Iteration 139, loss = 0.50024393\n",
            "Iteration 140, loss = 0.49929398\n",
            "Iteration 141, loss = 0.49831700\n",
            "Iteration 142, loss = 0.49734723\n",
            "Iteration 143, loss = 0.49643039\n",
            "Iteration 144, loss = 0.49551007\n",
            "Iteration 145, loss = 0.49456318\n",
            "Iteration 146, loss = 0.49362966\n",
            "Iteration 147, loss = 0.49276442\n",
            "Iteration 148, loss = 0.49190332\n",
            "Iteration 149, loss = 0.49107691\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.80959506\n",
            "Iteration 2, loss = 0.80689625\n",
            "Iteration 3, loss = 0.80418858\n",
            "Iteration 4, loss = 0.80150503\n",
            "Iteration 5, loss = 0.79873496\n",
            "Iteration 6, loss = 0.79622832\n",
            "Iteration 7, loss = 0.79350165\n",
            "Iteration 8, loss = 0.79072122\n",
            "Iteration 9, loss = 0.78810338\n",
            "Iteration 10, loss = 0.78537854\n",
            "Iteration 11, loss = 0.78271630\n",
            "Iteration 12, loss = 0.77996137\n",
            "Iteration 13, loss = 0.77728998\n",
            "Iteration 14, loss = 0.77453817\n",
            "Iteration 15, loss = 0.77186855\n",
            "Iteration 16, loss = 0.76914498\n",
            "Iteration 17, loss = 0.76648264\n",
            "Iteration 18, loss = 0.76379836\n",
            "Iteration 19, loss = 0.76101783\n",
            "Iteration 20, loss = 0.75819235\n",
            "Iteration 21, loss = 0.75542810\n",
            "Iteration 22, loss = 0.75260576\n",
            "Iteration 23, loss = 0.74970971\n",
            "Iteration 24, loss = 0.74672654\n",
            "Iteration 25, loss = 0.74388702\n",
            "Iteration 26, loss = 0.74085983\n",
            "Iteration 27, loss = 0.73805769\n",
            "Iteration 28, loss = 0.73493264\n",
            "Iteration 29, loss = 0.73199406\n",
            "Iteration 30, loss = 0.72900028\n",
            "Iteration 31, loss = 0.72561450\n",
            "Iteration 32, loss = 0.72253434\n",
            "Iteration 33, loss = 0.71938606\n",
            "Iteration 34, loss = 0.71629972\n",
            "Iteration 35, loss = 0.71298714\n",
            "Iteration 36, loss = 0.70990375\n",
            "Iteration 37, loss = 0.70694389\n",
            "Iteration 38, loss = 0.70399284\n",
            "Iteration 39, loss = 0.70077324\n",
            "Iteration 40, loss = 0.69787313\n",
            "Iteration 41, loss = 0.69485459\n",
            "Iteration 42, loss = 0.69192485\n",
            "Iteration 43, loss = 0.68906076\n",
            "Iteration 44, loss = 0.68628367\n",
            "Iteration 45, loss = 0.68332237\n",
            "Iteration 46, loss = 0.68049939\n",
            "Iteration 47, loss = 0.67770065\n",
            "Iteration 48, loss = 0.67510783\n",
            "Iteration 49, loss = 0.67242597\n",
            "Iteration 50, loss = 0.66983241\n",
            "Iteration 51, loss = 0.66725878\n",
            "Iteration 52, loss = 0.66457910\n",
            "Iteration 53, loss = 0.66207271\n",
            "Iteration 54, loss = 0.65968375\n",
            "Iteration 55, loss = 0.65704258\n",
            "Iteration 56, loss = 0.65452992\n",
            "Iteration 57, loss = 0.65198027\n",
            "Iteration 58, loss = 0.64936119\n",
            "Iteration 59, loss = 0.64692620\n",
            "Iteration 60, loss = 0.64443094\n",
            "Iteration 61, loss = 0.64183657\n",
            "Iteration 62, loss = 0.63937246\n",
            "Iteration 63, loss = 0.63687264\n",
            "Iteration 64, loss = 0.63447054\n",
            "Iteration 65, loss = 0.63206626\n",
            "Iteration 66, loss = 0.62965278\n",
            "Iteration 67, loss = 0.62730347\n",
            "Iteration 68, loss = 0.62485994\n",
            "Iteration 69, loss = 0.62262187\n",
            "Iteration 70, loss = 0.62027090\n",
            "Iteration 71, loss = 0.61797179\n",
            "Iteration 72, loss = 0.61569491\n",
            "Iteration 73, loss = 0.61340164\n",
            "Iteration 74, loss = 0.61112127\n",
            "Iteration 75, loss = 0.60881279\n",
            "Iteration 76, loss = 0.60648161\n",
            "Iteration 77, loss = 0.60416585\n",
            "Iteration 78, loss = 0.60200534\n",
            "Iteration 79, loss = 0.59948905\n",
            "Iteration 80, loss = 0.59723111\n",
            "Iteration 81, loss = 0.59488043\n",
            "Iteration 82, loss = 0.59253022\n",
            "Iteration 83, loss = 0.59040930\n",
            "Iteration 84, loss = 0.58829494\n",
            "Iteration 85, loss = 0.58633059\n",
            "Iteration 86, loss = 0.58442959\n",
            "Iteration 87, loss = 0.58246762\n",
            "Iteration 88, loss = 0.58066906\n",
            "Iteration 89, loss = 0.57892931\n",
            "Iteration 90, loss = 0.57719115\n",
            "Iteration 91, loss = 0.57542300\n",
            "Iteration 92, loss = 0.57360311\n",
            "Iteration 93, loss = 0.57196162\n",
            "Iteration 94, loss = 0.57026328\n",
            "Iteration 95, loss = 0.56877259\n",
            "Iteration 96, loss = 0.56721106\n",
            "Iteration 97, loss = 0.56573956\n",
            "Iteration 98, loss = 0.56427344\n",
            "Iteration 99, loss = 0.56286867\n",
            "Iteration 100, loss = 0.56150109\n",
            "Iteration 101, loss = 0.56016356\n",
            "Iteration 102, loss = 0.55873765\n",
            "Iteration 103, loss = 0.55743173\n",
            "Iteration 104, loss = 0.55608134\n",
            "Iteration 105, loss = 0.55474469\n",
            "Iteration 106, loss = 0.55347254\n",
            "Iteration 107, loss = 0.55212370\n",
            "Iteration 108, loss = 0.55078320\n",
            "Iteration 109, loss = 0.54955188\n",
            "Iteration 110, loss = 0.54830923\n",
            "Iteration 111, loss = 0.54702190\n",
            "Iteration 112, loss = 0.54574718\n",
            "Iteration 113, loss = 0.54447933\n",
            "Iteration 114, loss = 0.54320102\n",
            "Iteration 115, loss = 0.54193051\n",
            "Iteration 116, loss = 0.54072438\n",
            "Iteration 117, loss = 0.53958905\n",
            "Iteration 118, loss = 0.53839488\n",
            "Iteration 119, loss = 0.53723024\n",
            "Iteration 120, loss = 0.53608085\n",
            "Iteration 121, loss = 0.53487238\n",
            "Iteration 122, loss = 0.53371252\n",
            "Iteration 123, loss = 0.53261011\n",
            "Iteration 124, loss = 0.53141104\n",
            "Iteration 125, loss = 0.53031642\n",
            "Iteration 126, loss = 0.52910939\n",
            "Iteration 127, loss = 0.52798948\n",
            "Iteration 128, loss = 0.52667537\n",
            "Iteration 129, loss = 0.52548327\n",
            "Iteration 130, loss = 0.52435746\n",
            "Iteration 131, loss = 0.52324619\n",
            "Iteration 132, loss = 0.52216700\n",
            "Iteration 133, loss = 0.52115523\n",
            "Iteration 134, loss = 0.52012872\n",
            "Iteration 135, loss = 0.51911684\n",
            "Iteration 136, loss = 0.51806149\n",
            "Iteration 137, loss = 0.51721104\n",
            "Iteration 138, loss = 0.51621748\n",
            "Iteration 139, loss = 0.51537112\n",
            "Iteration 140, loss = 0.51448768\n",
            "Iteration 141, loss = 0.51357471\n",
            "Iteration 142, loss = 0.51273560\n",
            "Iteration 143, loss = 0.51184688\n",
            "Iteration 144, loss = 0.51099858\n",
            "Iteration 145, loss = 0.51026490\n",
            "Iteration 146, loss = 0.50944149\n",
            "Iteration 147, loss = 0.50861417\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.80906624\n",
            "Iteration 2, loss = 0.80646249\n",
            "Iteration 3, loss = 0.80382112\n",
            "Iteration 4, loss = 0.80108736\n",
            "Iteration 5, loss = 0.79848390\n",
            "Iteration 6, loss = 0.79582106\n",
            "Iteration 7, loss = 0.79314244\n",
            "Iteration 8, loss = 0.79031256\n",
            "Iteration 9, loss = 0.78767235\n",
            "Iteration 10, loss = 0.78489371\n",
            "Iteration 11, loss = 0.78213846\n",
            "Iteration 12, loss = 0.77948579\n",
            "Iteration 13, loss = 0.77671329\n",
            "Iteration 14, loss = 0.77392822\n",
            "Iteration 15, loss = 0.77127276\n",
            "Iteration 16, loss = 0.76847264\n",
            "Iteration 17, loss = 0.76572811\n",
            "Iteration 18, loss = 0.76303894\n",
            "Iteration 19, loss = 0.76013384\n",
            "Iteration 20, loss = 0.75733304\n",
            "Iteration 21, loss = 0.75451255\n",
            "Iteration 22, loss = 0.75167049\n",
            "Iteration 23, loss = 0.74877121\n",
            "Iteration 24, loss = 0.74578354\n",
            "Iteration 25, loss = 0.74293066\n",
            "Iteration 26, loss = 0.74008821\n",
            "Iteration 27, loss = 0.73714077\n",
            "Iteration 28, loss = 0.73413096\n",
            "Iteration 29, loss = 0.73103307\n",
            "Iteration 30, loss = 0.72813229\n",
            "Iteration 31, loss = 0.72474767\n",
            "Iteration 32, loss = 0.72161509\n",
            "Iteration 33, loss = 0.71836807\n",
            "Iteration 34, loss = 0.71528633\n",
            "Iteration 35, loss = 0.71185555\n",
            "Iteration 36, loss = 0.70871390\n",
            "Iteration 37, loss = 0.70563008\n",
            "Iteration 38, loss = 0.70236335\n",
            "Iteration 39, loss = 0.69918196\n",
            "Iteration 40, loss = 0.69610628\n",
            "Iteration 41, loss = 0.69274045\n",
            "Iteration 42, loss = 0.68969015\n",
            "Iteration 43, loss = 0.68666052\n",
            "Iteration 44, loss = 0.68366511\n",
            "Iteration 45, loss = 0.68073146\n",
            "Iteration 46, loss = 0.67758704\n",
            "Iteration 47, loss = 0.67473695\n",
            "Iteration 48, loss = 0.67191601\n",
            "Iteration 49, loss = 0.66907889\n",
            "Iteration 50, loss = 0.66630261\n",
            "Iteration 51, loss = 0.66364041\n",
            "Iteration 52, loss = 0.66075459\n",
            "Iteration 53, loss = 0.65792606\n",
            "Iteration 54, loss = 0.65528695\n",
            "Iteration 55, loss = 0.65247816\n",
            "Iteration 56, loss = 0.64983896\n",
            "Iteration 57, loss = 0.64701049\n",
            "Iteration 58, loss = 0.64425677\n",
            "Iteration 59, loss = 0.64162703\n",
            "Iteration 60, loss = 0.63888389\n",
            "Iteration 61, loss = 0.63617444\n",
            "Iteration 62, loss = 0.63343828\n",
            "Iteration 63, loss = 0.63068141\n",
            "Iteration 64, loss = 0.62802914\n",
            "Iteration 65, loss = 0.62541540\n",
            "Iteration 66, loss = 0.62275210\n",
            "Iteration 67, loss = 0.62019586\n",
            "Iteration 68, loss = 0.61749116\n",
            "Iteration 69, loss = 0.61481092\n",
            "Iteration 70, loss = 0.61227606\n",
            "Iteration 71, loss = 0.60959123\n",
            "Iteration 72, loss = 0.60710319\n",
            "Iteration 73, loss = 0.60442162\n",
            "Iteration 74, loss = 0.60188908\n",
            "Iteration 75, loss = 0.59927269\n",
            "Iteration 76, loss = 0.59659367\n",
            "Iteration 77, loss = 0.59418470\n",
            "Iteration 78, loss = 0.59184849\n",
            "Iteration 79, loss = 0.58941973\n",
            "Iteration 80, loss = 0.58729783\n",
            "Iteration 81, loss = 0.58512269\n",
            "Iteration 82, loss = 0.58300918\n",
            "Iteration 83, loss = 0.58109495\n",
            "Iteration 84, loss = 0.57891220\n",
            "Iteration 85, loss = 0.57701140\n",
            "Iteration 86, loss = 0.57510275\n",
            "Iteration 87, loss = 0.57312355\n",
            "Iteration 88, loss = 0.57130096\n",
            "Iteration 89, loss = 0.56946693\n",
            "Iteration 90, loss = 0.56764421\n",
            "Iteration 91, loss = 0.56597078\n",
            "Iteration 92, loss = 0.56414255\n",
            "Iteration 93, loss = 0.56253024\n",
            "Iteration 94, loss = 0.56077599\n",
            "Iteration 95, loss = 0.55921504\n",
            "Iteration 96, loss = 0.55761321\n",
            "Iteration 97, loss = 0.55603148\n",
            "Iteration 98, loss = 0.55440974\n",
            "Iteration 99, loss = 0.55287546\n",
            "Iteration 100, loss = 0.55131794\n",
            "Iteration 101, loss = 0.54977941\n",
            "Iteration 102, loss = 0.54818975\n",
            "Iteration 103, loss = 0.54663783\n",
            "Iteration 104, loss = 0.54508239\n",
            "Iteration 105, loss = 0.54342024\n",
            "Iteration 106, loss = 0.54180920\n",
            "Iteration 107, loss = 0.54020384\n",
            "Iteration 108, loss = 0.53849132\n",
            "Iteration 109, loss = 0.53685048\n",
            "Iteration 110, loss = 0.53530416\n",
            "Iteration 111, loss = 0.53367766\n",
            "Iteration 112, loss = 0.53201640\n",
            "Iteration 113, loss = 0.53040964\n",
            "Iteration 114, loss = 0.52884834\n",
            "Iteration 115, loss = 0.52718315\n",
            "Iteration 116, loss = 0.52566031\n",
            "Iteration 117, loss = 0.52432868\n",
            "Iteration 118, loss = 0.52288631\n",
            "Iteration 119, loss = 0.52141582\n",
            "Iteration 120, loss = 0.52010249\n",
            "Iteration 121, loss = 0.51865518\n",
            "Iteration 122, loss = 0.51731885\n",
            "Iteration 123, loss = 0.51605136\n",
            "Iteration 124, loss = 0.51471301\n",
            "Iteration 125, loss = 0.51349093\n",
            "Iteration 126, loss = 0.51227939\n",
            "Iteration 127, loss = 0.51107223\n",
            "Iteration 128, loss = 0.50997036\n",
            "Iteration 129, loss = 0.50878103\n",
            "Iteration 130, loss = 0.50767432\n",
            "Iteration 131, loss = 0.50658271\n",
            "Iteration 132, loss = 0.50546754\n",
            "Iteration 133, loss = 0.50437139\n",
            "Iteration 134, loss = 0.50322969\n",
            "Iteration 135, loss = 0.50207400\n",
            "Iteration 136, loss = 0.50081980\n",
            "Iteration 137, loss = 0.49969963\n",
            "Iteration 138, loss = 0.49843314\n",
            "Iteration 139, loss = 0.49724957\n",
            "Iteration 140, loss = 0.49612810\n",
            "Iteration 141, loss = 0.49503303\n",
            "Iteration 142, loss = 0.49401861\n",
            "Iteration 143, loss = 0.49296377\n",
            "Iteration 144, loss = 0.49192489\n",
            "Iteration 145, loss = 0.49087480\n",
            "Iteration 146, loss = 0.48983411\n",
            "Iteration 147, loss = 0.48882600\n",
            "Iteration 148, loss = 0.48793772\n",
            "Iteration 149, loss = 0.48705294\n",
            "Iteration 150, loss = 0.48621148\n",
            "Iteration 151, loss = 0.48535312\n",
            "Iteration 152, loss = 0.48448193\n",
            "Iteration 153, loss = 0.48371619\n",
            "Iteration 154, loss = 0.48290784\n",
            "Iteration 155, loss = 0.48218189\n",
            "Iteration 156, loss = 0.48143789\n",
            "Iteration 157, loss = 0.48073715\n",
            "Iteration 158, loss = 0.48006287\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.81000931\n",
            "Iteration 2, loss = 0.80738801\n",
            "Iteration 3, loss = 0.80465611\n",
            "Iteration 4, loss = 0.80197175\n",
            "Iteration 5, loss = 0.79927705\n",
            "Iteration 6, loss = 0.79657595\n",
            "Iteration 7, loss = 0.79388492\n",
            "Iteration 8, loss = 0.79109240\n",
            "Iteration 9, loss = 0.78837559\n",
            "Iteration 10, loss = 0.78567479\n",
            "Iteration 11, loss = 0.78292662\n",
            "Iteration 12, loss = 0.78027597\n",
            "Iteration 13, loss = 0.77752181\n",
            "Iteration 14, loss = 0.77473852\n",
            "Iteration 15, loss = 0.77215871\n",
            "Iteration 16, loss = 0.76930043\n",
            "Iteration 17, loss = 0.76652445\n",
            "Iteration 18, loss = 0.76383528\n",
            "Iteration 19, loss = 0.76092619\n",
            "Iteration 20, loss = 0.75811092\n",
            "Iteration 21, loss = 0.75524036\n",
            "Iteration 22, loss = 0.75240135\n",
            "Iteration 23, loss = 0.74939780\n",
            "Iteration 24, loss = 0.74635586\n",
            "Iteration 25, loss = 0.74341408\n",
            "Iteration 26, loss = 0.74046391\n",
            "Iteration 27, loss = 0.73755026\n",
            "Iteration 28, loss = 0.73435806\n",
            "Iteration 29, loss = 0.73138638\n",
            "Iteration 30, loss = 0.72844657\n",
            "Iteration 31, loss = 0.72515347\n",
            "Iteration 32, loss = 0.72202090\n",
            "Iteration 33, loss = 0.71891643\n",
            "Iteration 34, loss = 0.71578695\n",
            "Iteration 35, loss = 0.71241130\n",
            "Iteration 36, loss = 0.70937344\n",
            "Iteration 37, loss = 0.70626716\n",
            "Iteration 38, loss = 0.70309814\n",
            "Iteration 39, loss = 0.69996426\n",
            "Iteration 40, loss = 0.69703810\n",
            "Iteration 41, loss = 0.69371048\n",
            "Iteration 42, loss = 0.69082444\n",
            "Iteration 43, loss = 0.68785617\n",
            "Iteration 44, loss = 0.68495262\n",
            "Iteration 45, loss = 0.68214767\n",
            "Iteration 46, loss = 0.67924218\n",
            "Iteration 47, loss = 0.67649202\n",
            "Iteration 48, loss = 0.67383702\n",
            "Iteration 49, loss = 0.67107109\n",
            "Iteration 50, loss = 0.66855721\n",
            "Iteration 51, loss = 0.66592565\n",
            "Iteration 52, loss = 0.66329371\n",
            "Iteration 53, loss = 0.66068800\n",
            "Iteration 54, loss = 0.65813938\n",
            "Iteration 55, loss = 0.65549547\n",
            "Iteration 56, loss = 0.65300627\n",
            "Iteration 57, loss = 0.65031014\n",
            "Iteration 58, loss = 0.64762750\n",
            "Iteration 59, loss = 0.64511696\n",
            "Iteration 60, loss = 0.64246061\n",
            "Iteration 61, loss = 0.63991967\n",
            "Iteration 62, loss = 0.63726910\n",
            "Iteration 63, loss = 0.63465191\n",
            "Iteration 64, loss = 0.63218457\n",
            "Iteration 65, loss = 0.62961020\n",
            "Iteration 66, loss = 0.62712830\n",
            "Iteration 67, loss = 0.62472491\n",
            "Iteration 68, loss = 0.62223832\n",
            "Iteration 69, loss = 0.61985068\n",
            "Iteration 70, loss = 0.61745451\n",
            "Iteration 71, loss = 0.61504446\n",
            "Iteration 72, loss = 0.61276335\n",
            "Iteration 73, loss = 0.61040739\n",
            "Iteration 74, loss = 0.60803955\n",
            "Iteration 75, loss = 0.60574847\n",
            "Iteration 76, loss = 0.60332145\n",
            "Iteration 77, loss = 0.60112579\n",
            "Iteration 78, loss = 0.59885667\n",
            "Iteration 79, loss = 0.59647394\n",
            "Iteration 80, loss = 0.59419189\n",
            "Iteration 81, loss = 0.59182218\n",
            "Iteration 82, loss = 0.58959620\n",
            "Iteration 83, loss = 0.58762474\n",
            "Iteration 84, loss = 0.58552516\n",
            "Iteration 85, loss = 0.58357491\n",
            "Iteration 86, loss = 0.58162537\n",
            "Iteration 87, loss = 0.57960655\n",
            "Iteration 88, loss = 0.57777615\n",
            "Iteration 89, loss = 0.57602775\n",
            "Iteration 90, loss = 0.57415962\n",
            "Iteration 91, loss = 0.57247899\n",
            "Iteration 92, loss = 0.57069706\n",
            "Iteration 93, loss = 0.56902887\n",
            "Iteration 94, loss = 0.56725437\n",
            "Iteration 95, loss = 0.56571370\n",
            "Iteration 96, loss = 0.56411897\n",
            "Iteration 97, loss = 0.56259461\n",
            "Iteration 98, loss = 0.56097603\n",
            "Iteration 99, loss = 0.55952141\n",
            "Iteration 100, loss = 0.55812973\n",
            "Iteration 101, loss = 0.55665593\n",
            "Iteration 102, loss = 0.55521410\n",
            "Iteration 103, loss = 0.55379219\n",
            "Iteration 104, loss = 0.55242621\n",
            "Iteration 105, loss = 0.55101411\n",
            "Iteration 106, loss = 0.54969026\n",
            "Iteration 107, loss = 0.54830409\n",
            "Iteration 108, loss = 0.54698632\n",
            "Iteration 109, loss = 0.54562686\n",
            "Iteration 110, loss = 0.54427595\n",
            "Iteration 111, loss = 0.54298843\n",
            "Iteration 112, loss = 0.54158258\n",
            "Iteration 113, loss = 0.54019278\n",
            "Iteration 114, loss = 0.53879930\n",
            "Iteration 115, loss = 0.53743088\n",
            "Iteration 116, loss = 0.53615634\n",
            "Iteration 117, loss = 0.53476631\n",
            "Iteration 118, loss = 0.53340783\n",
            "Iteration 119, loss = 0.53207277\n",
            "Iteration 120, loss = 0.53068902\n",
            "Iteration 121, loss = 0.52936509\n",
            "Iteration 122, loss = 0.52806251\n",
            "Iteration 123, loss = 0.52676955\n",
            "Iteration 124, loss = 0.52544839\n",
            "Iteration 125, loss = 0.52417990\n",
            "Iteration 126, loss = 0.52289209\n",
            "Iteration 127, loss = 0.52154318\n",
            "Iteration 128, loss = 0.52028280\n",
            "Iteration 129, loss = 0.51899943\n",
            "Iteration 130, loss = 0.51777332\n",
            "Iteration 131, loss = 0.51653140\n",
            "Iteration 132, loss = 0.51534948\n",
            "Iteration 133, loss = 0.51416202\n",
            "Iteration 134, loss = 0.51295770\n",
            "Iteration 135, loss = 0.51181366\n",
            "Iteration 136, loss = 0.51069834\n",
            "Iteration 137, loss = 0.50960652\n",
            "Iteration 138, loss = 0.50842865\n",
            "Iteration 139, loss = 0.50729161\n",
            "Iteration 140, loss = 0.50614313\n",
            "Iteration 141, loss = 0.50509621\n",
            "Iteration 142, loss = 0.50396661\n",
            "Iteration 143, loss = 0.50282395\n",
            "Iteration 144, loss = 0.50174074\n",
            "Iteration 145, loss = 0.50058216\n",
            "Iteration 146, loss = 0.49945791\n",
            "Iteration 147, loss = 0.49835061\n",
            "Iteration 148, loss = 0.49728129\n",
            "Iteration 149, loss = 0.49617478\n",
            "Iteration 150, loss = 0.49511749\n",
            "Iteration 151, loss = 0.49406729\n",
            "Iteration 152, loss = 0.49300793\n",
            "Iteration 153, loss = 0.49206009\n",
            "Iteration 154, loss = 0.49107713\n",
            "Iteration 155, loss = 0.49015276\n",
            "Iteration 156, loss = 0.48924466\n",
            "Iteration 157, loss = 0.48847377\n",
            "Iteration 158, loss = 0.48766424\n",
            "Iteration 159, loss = 0.48685487\n",
            "Iteration 160, loss = 0.48609746\n",
            "Iteration 161, loss = 0.48523566\n",
            "Iteration 162, loss = 0.48445167\n",
            "Iteration 163, loss = 0.48363577\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.80941565\n",
            "Iteration 2, loss = 0.80678054\n",
            "Iteration 3, loss = 0.80409827\n",
            "Iteration 4, loss = 0.80143754\n",
            "Iteration 5, loss = 0.79879846\n",
            "Iteration 6, loss = 0.79615747\n",
            "Iteration 7, loss = 0.79345403\n",
            "Iteration 8, loss = 0.79079708\n",
            "Iteration 9, loss = 0.78804873\n",
            "Iteration 10, loss = 0.78534467\n",
            "Iteration 11, loss = 0.78270445\n",
            "Iteration 12, loss = 0.77995803\n",
            "Iteration 13, loss = 0.77726329\n",
            "Iteration 14, loss = 0.77450001\n",
            "Iteration 15, loss = 0.77191533\n",
            "Iteration 16, loss = 0.76900110\n",
            "Iteration 17, loss = 0.76630226\n",
            "Iteration 18, loss = 0.76352476\n",
            "Iteration 19, loss = 0.76065310\n",
            "Iteration 20, loss = 0.75786750\n",
            "Iteration 21, loss = 0.75507999\n",
            "Iteration 22, loss = 0.75222893\n",
            "Iteration 23, loss = 0.74934592\n",
            "Iteration 24, loss = 0.74635605\n",
            "Iteration 25, loss = 0.74344013\n",
            "Iteration 26, loss = 0.74053503\n",
            "Iteration 27, loss = 0.73766657\n",
            "Iteration 28, loss = 0.73458290\n",
            "Iteration 29, loss = 0.73170655\n",
            "Iteration 30, loss = 0.72878584\n",
            "Iteration 31, loss = 0.72552511\n",
            "Iteration 32, loss = 0.72247030\n",
            "Iteration 33, loss = 0.71944685\n",
            "Iteration 34, loss = 0.71622914\n",
            "Iteration 35, loss = 0.71288705\n",
            "Iteration 36, loss = 0.70976636\n",
            "Iteration 37, loss = 0.70668791\n",
            "Iteration 38, loss = 0.70340471\n",
            "Iteration 39, loss = 0.70018759\n",
            "Iteration 40, loss = 0.69731340\n",
            "Iteration 41, loss = 0.69387881\n",
            "Iteration 42, loss = 0.69097909\n",
            "Iteration 43, loss = 0.68790573\n",
            "Iteration 44, loss = 0.68497738\n",
            "Iteration 45, loss = 0.68197050\n",
            "Iteration 46, loss = 0.67902932\n",
            "Iteration 47, loss = 0.67620481\n",
            "Iteration 48, loss = 0.67350165\n",
            "Iteration 49, loss = 0.67066173\n",
            "Iteration 50, loss = 0.66802200\n",
            "Iteration 51, loss = 0.66524098\n",
            "Iteration 52, loss = 0.66252127\n",
            "Iteration 53, loss = 0.65976381\n",
            "Iteration 54, loss = 0.65716094\n",
            "Iteration 55, loss = 0.65434326\n",
            "Iteration 56, loss = 0.65171150\n",
            "Iteration 57, loss = 0.64897315\n",
            "Iteration 58, loss = 0.64613005\n",
            "Iteration 59, loss = 0.64355477\n",
            "Iteration 60, loss = 0.64083339\n",
            "Iteration 61, loss = 0.63812107\n",
            "Iteration 62, loss = 0.63548525\n",
            "Iteration 63, loss = 0.63271806\n",
            "Iteration 64, loss = 0.63018216\n",
            "Iteration 65, loss = 0.62758298\n",
            "Iteration 66, loss = 0.62494768\n",
            "Iteration 67, loss = 0.62244002\n",
            "Iteration 68, loss = 0.61973441\n",
            "Iteration 69, loss = 0.61707213\n",
            "Iteration 70, loss = 0.61445838\n",
            "Iteration 71, loss = 0.61185901\n",
            "Iteration 72, loss = 0.60928016\n",
            "Iteration 73, loss = 0.60664859\n",
            "Iteration 74, loss = 0.60404697\n",
            "Iteration 75, loss = 0.60152557\n",
            "Iteration 76, loss = 0.59882629\n",
            "Iteration 77, loss = 0.59639464\n",
            "Iteration 78, loss = 0.59412113\n",
            "Iteration 79, loss = 0.59175979\n",
            "Iteration 80, loss = 0.58968028\n",
            "Iteration 81, loss = 0.58753408\n",
            "Iteration 82, loss = 0.58550199\n",
            "Iteration 83, loss = 0.58364844\n",
            "Iteration 84, loss = 0.58159332\n",
            "Iteration 85, loss = 0.57975165\n",
            "Iteration 86, loss = 0.57788871\n",
            "Iteration 87, loss = 0.57603929\n",
            "Iteration 88, loss = 0.57427724\n",
            "Iteration 89, loss = 0.57262643\n",
            "Iteration 90, loss = 0.57085526\n",
            "Iteration 91, loss = 0.56923558\n",
            "Iteration 92, loss = 0.56753871\n",
            "Iteration 93, loss = 0.56593262\n",
            "Iteration 94, loss = 0.56425947\n",
            "Iteration 95, loss = 0.56277298\n",
            "Iteration 96, loss = 0.56125642\n",
            "Iteration 97, loss = 0.55974630\n",
            "Iteration 98, loss = 0.55826330\n",
            "Iteration 99, loss = 0.55676643\n",
            "Iteration 100, loss = 0.55537730\n",
            "Iteration 101, loss = 0.55392868\n",
            "Iteration 102, loss = 0.55249368\n",
            "Iteration 103, loss = 0.55112040\n",
            "Iteration 104, loss = 0.54972539\n",
            "Iteration 105, loss = 0.54832765\n",
            "Iteration 106, loss = 0.54701991\n",
            "Iteration 107, loss = 0.54565305\n",
            "Iteration 108, loss = 0.54436448\n",
            "Iteration 109, loss = 0.54300701\n",
            "Iteration 110, loss = 0.54169439\n",
            "Iteration 111, loss = 0.54033429\n",
            "Iteration 112, loss = 0.53896294\n",
            "Iteration 113, loss = 0.53756664\n",
            "Iteration 114, loss = 0.53616064\n",
            "Iteration 115, loss = 0.53473870\n",
            "Iteration 116, loss = 0.53333041\n",
            "Iteration 117, loss = 0.53188319\n",
            "Iteration 118, loss = 0.53039781\n",
            "Iteration 119, loss = 0.52894378\n",
            "Iteration 120, loss = 0.52737418\n",
            "Iteration 121, loss = 0.52581398\n",
            "Iteration 122, loss = 0.52424136\n",
            "Iteration 123, loss = 0.52266017\n",
            "Iteration 124, loss = 0.52100959\n",
            "Iteration 125, loss = 0.51942974\n",
            "Iteration 126, loss = 0.51790794\n",
            "Iteration 127, loss = 0.51643707\n",
            "Iteration 128, loss = 0.51494030\n",
            "Iteration 129, loss = 0.51350875\n",
            "Iteration 130, loss = 0.51218717\n",
            "Iteration 131, loss = 0.51097157\n",
            "Iteration 132, loss = 0.50985527\n",
            "Iteration 133, loss = 0.50865585\n",
            "Iteration 134, loss = 0.50749508\n",
            "Iteration 135, loss = 0.50641523\n",
            "Iteration 136, loss = 0.50524946\n",
            "Iteration 137, loss = 0.50429047\n",
            "Iteration 138, loss = 0.50322910\n",
            "Iteration 139, loss = 0.50224963\n",
            "Iteration 140, loss = 0.50128044\n",
            "Iteration 141, loss = 0.50037009\n",
            "Iteration 142, loss = 0.49948887\n",
            "Iteration 143, loss = 0.49853848\n",
            "Iteration 144, loss = 0.49757310\n",
            "Iteration 145, loss = 0.49651535\n",
            "Iteration 146, loss = 0.49550045\n",
            "Iteration 147, loss = 0.49448929\n",
            "Iteration 148, loss = 0.49352671\n",
            "Iteration 149, loss = 0.49258843\n",
            "Iteration 150, loss = 0.49169191\n",
            "Iteration 151, loss = 0.49080294\n",
            "Iteration 152, loss = 0.48991083\n",
            "Iteration 153, loss = 0.48924919\n",
            "Iteration 154, loss = 0.48835143\n",
            "Iteration 155, loss = 0.48756754\n",
            "Iteration 156, loss = 0.48673720\n",
            "Iteration 157, loss = 0.48597564\n",
            "Iteration 158, loss = 0.48525003\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.80833078\n",
            "Iteration 2, loss = 0.80559793\n",
            "Iteration 3, loss = 0.80287047\n",
            "Iteration 4, loss = 0.80008284\n",
            "Iteration 5, loss = 0.79735284\n",
            "Iteration 6, loss = 0.79468986\n",
            "Iteration 7, loss = 0.79187667\n",
            "Iteration 8, loss = 0.78913855\n",
            "Iteration 9, loss = 0.78635110\n",
            "Iteration 10, loss = 0.78353123\n",
            "Iteration 11, loss = 0.78075219\n",
            "Iteration 12, loss = 0.77792982\n",
            "Iteration 13, loss = 0.77507745\n",
            "Iteration 14, loss = 0.77226565\n",
            "Iteration 15, loss = 0.76954387\n",
            "Iteration 16, loss = 0.76661549\n",
            "Iteration 17, loss = 0.76374781\n",
            "Iteration 18, loss = 0.76087147\n",
            "Iteration 19, loss = 0.75792772\n",
            "Iteration 20, loss = 0.75493001\n",
            "Iteration 21, loss = 0.75203635\n",
            "Iteration 22, loss = 0.74900858\n",
            "Iteration 23, loss = 0.74595622\n",
            "Iteration 24, loss = 0.74271408\n",
            "Iteration 25, loss = 0.73971295\n",
            "Iteration 26, loss = 0.73660440\n",
            "Iteration 27, loss = 0.73353750\n",
            "Iteration 28, loss = 0.73026304\n",
            "Iteration 29, loss = 0.72722684\n",
            "Iteration 30, loss = 0.72412849\n",
            "Iteration 31, loss = 0.72071697\n",
            "Iteration 32, loss = 0.71758517\n",
            "Iteration 33, loss = 0.71450925\n",
            "Iteration 34, loss = 0.71117934\n",
            "Iteration 35, loss = 0.70779914\n",
            "Iteration 36, loss = 0.70470532\n",
            "Iteration 37, loss = 0.70163137\n",
            "Iteration 38, loss = 0.69822983\n",
            "Iteration 39, loss = 0.69498040\n",
            "Iteration 40, loss = 0.69204146\n",
            "Iteration 41, loss = 0.68872594\n",
            "Iteration 42, loss = 0.68571624\n",
            "Iteration 43, loss = 0.68267234\n",
            "Iteration 44, loss = 0.67969973\n",
            "Iteration 45, loss = 0.67667501\n",
            "Iteration 46, loss = 0.67376721\n",
            "Iteration 47, loss = 0.67095574\n",
            "Iteration 48, loss = 0.66825498\n",
            "Iteration 49, loss = 0.66554873\n",
            "Iteration 50, loss = 0.66291587\n",
            "Iteration 51, loss = 0.66027102\n",
            "Iteration 52, loss = 0.65760245\n",
            "Iteration 53, loss = 0.65496530\n",
            "Iteration 54, loss = 0.65243543\n",
            "Iteration 55, loss = 0.64983655\n",
            "Iteration 56, loss = 0.64724464\n",
            "Iteration 57, loss = 0.64464594\n",
            "Iteration 58, loss = 0.64195819\n",
            "Iteration 59, loss = 0.63953980\n",
            "Iteration 60, loss = 0.63696442\n",
            "Iteration 61, loss = 0.63440686\n",
            "Iteration 62, loss = 0.63190710\n",
            "Iteration 63, loss = 0.62936364\n",
            "Iteration 64, loss = 0.62695631\n",
            "Iteration 65, loss = 0.62455625\n",
            "Iteration 66, loss = 0.62213057\n",
            "Iteration 67, loss = 0.61985059\n",
            "Iteration 68, loss = 0.61755395\n",
            "Iteration 69, loss = 0.61529989\n",
            "Iteration 70, loss = 0.61300183\n",
            "Iteration 71, loss = 0.61069403\n",
            "Iteration 72, loss = 0.60844583\n",
            "Iteration 73, loss = 0.60610608\n",
            "Iteration 74, loss = 0.60374874\n",
            "Iteration 75, loss = 0.60145773\n",
            "Iteration 76, loss = 0.59896004\n",
            "Iteration 77, loss = 0.59671781\n",
            "Iteration 78, loss = 0.59446972\n",
            "Iteration 79, loss = 0.59214517\n",
            "Iteration 80, loss = 0.58997003\n",
            "Iteration 81, loss = 0.58785090\n",
            "Iteration 82, loss = 0.58582272\n",
            "Iteration 83, loss = 0.58398973\n",
            "Iteration 84, loss = 0.58202950\n",
            "Iteration 85, loss = 0.58026253\n",
            "Iteration 86, loss = 0.57846955\n",
            "Iteration 87, loss = 0.57674256\n",
            "Iteration 88, loss = 0.57500389\n",
            "Iteration 89, loss = 0.57342560\n",
            "Iteration 90, loss = 0.57174929\n",
            "Iteration 91, loss = 0.57015342\n",
            "Iteration 92, loss = 0.56847031\n",
            "Iteration 93, loss = 0.56693233\n",
            "Iteration 94, loss = 0.56530158\n",
            "Iteration 95, loss = 0.56388298\n",
            "Iteration 96, loss = 0.56244729\n",
            "Iteration 97, loss = 0.56101866\n",
            "Iteration 98, loss = 0.55961645\n",
            "Iteration 99, loss = 0.55822937\n",
            "Iteration 100, loss = 0.55692857\n",
            "Iteration 101, loss = 0.55561897\n",
            "Iteration 102, loss = 0.55427248\n",
            "Iteration 103, loss = 0.55301496\n",
            "Iteration 104, loss = 0.55171686\n",
            "Iteration 105, loss = 0.55039097\n",
            "Iteration 106, loss = 0.54912453\n",
            "Iteration 107, loss = 0.54778448\n",
            "Iteration 108, loss = 0.54648734\n",
            "Iteration 109, loss = 0.54511355\n",
            "Iteration 110, loss = 0.54375479\n",
            "Iteration 111, loss = 0.54229430\n",
            "Iteration 112, loss = 0.54079705\n",
            "Iteration 113, loss = 0.53927405\n",
            "Iteration 114, loss = 0.53780443\n",
            "Iteration 115, loss = 0.53624219\n",
            "Iteration 116, loss = 0.53472441\n",
            "Iteration 117, loss = 0.53332463\n",
            "Iteration 118, loss = 0.53188160\n",
            "Iteration 119, loss = 0.53052160\n",
            "Iteration 120, loss = 0.52912859\n",
            "Iteration 121, loss = 0.52774900\n",
            "Iteration 122, loss = 0.52633546\n",
            "Iteration 123, loss = 0.52504959\n",
            "Iteration 124, loss = 0.52368746\n",
            "Iteration 125, loss = 0.52249210\n",
            "Iteration 126, loss = 0.52130201\n",
            "Iteration 127, loss = 0.52013774\n",
            "Iteration 128, loss = 0.51903841\n",
            "Iteration 129, loss = 0.51792846\n",
            "Iteration 130, loss = 0.51680556\n",
            "Iteration 131, loss = 0.51574959\n",
            "Iteration 132, loss = 0.51473131\n",
            "Iteration 133, loss = 0.51370625\n",
            "Iteration 134, loss = 0.51268951\n",
            "Iteration 135, loss = 0.51166297\n",
            "Iteration 136, loss = 0.51067110\n",
            "Iteration 137, loss = 0.50979239\n",
            "Iteration 138, loss = 0.50882713\n",
            "Iteration 139, loss = 0.50793062\n",
            "Iteration 140, loss = 0.50708042\n",
            "Iteration 141, loss = 0.50619242\n",
            "Iteration 142, loss = 0.50538285\n",
            "Iteration 143, loss = 0.50448994\n",
            "Iteration 144, loss = 0.50361619\n",
            "Iteration 145, loss = 0.50271945\n",
            "Iteration 146, loss = 0.50183219\n",
            "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.88893094\n",
            "Iteration 2, loss = 0.88406969\n",
            "Iteration 3, loss = 0.87913574\n",
            "Iteration 4, loss = 0.87423250\n",
            "Iteration 5, loss = 0.86936865\n",
            "Iteration 6, loss = 0.86469367\n",
            "Iteration 7, loss = 0.86031994\n",
            "Iteration 8, loss = 0.85570645\n",
            "Iteration 9, loss = 0.85141335\n",
            "Iteration 10, loss = 0.84693757\n",
            "Iteration 11, loss = 0.84267396\n",
            "Iteration 12, loss = 0.83843028\n",
            "Iteration 13, loss = 0.83432011\n",
            "Iteration 14, loss = 0.83018907\n",
            "Iteration 15, loss = 0.82595684\n",
            "Iteration 16, loss = 0.82188267\n",
            "Iteration 17, loss = 0.81808522\n",
            "Iteration 18, loss = 0.81397517\n",
            "Iteration 19, loss = 0.80998156\n",
            "Iteration 20, loss = 0.80591468\n",
            "Iteration 21, loss = 0.80180183\n",
            "Iteration 22, loss = 0.79792759\n",
            "Iteration 23, loss = 0.79394292\n",
            "Iteration 24, loss = 0.79010097\n",
            "Iteration 25, loss = 0.78601342\n",
            "Iteration 26, loss = 0.78214657\n",
            "Iteration 27, loss = 0.77812290\n",
            "Iteration 28, loss = 0.77442727\n",
            "Iteration 29, loss = 0.77024939\n",
            "Iteration 30, loss = 0.76654866\n",
            "Iteration 31, loss = 0.76256410\n",
            "Iteration 32, loss = 0.75872778\n",
            "Iteration 33, loss = 0.75504757\n",
            "Iteration 34, loss = 0.75123326\n",
            "Iteration 35, loss = 0.74751457\n",
            "Iteration 36, loss = 0.74386063\n",
            "Iteration 37, loss = 0.74025415\n",
            "Iteration 38, loss = 0.73686244\n",
            "Iteration 39, loss = 0.73360510\n",
            "Iteration 40, loss = 0.73015063\n",
            "Iteration 41, loss = 0.72686433\n",
            "Iteration 42, loss = 0.72391488\n",
            "Iteration 43, loss = 0.72065333\n",
            "Iteration 44, loss = 0.71802969\n",
            "Iteration 45, loss = 0.71508227\n",
            "Iteration 46, loss = 0.71259270\n",
            "Iteration 47, loss = 0.70992691\n",
            "Iteration 48, loss = 0.70737650\n",
            "Iteration 49, loss = 0.70510680\n",
            "Iteration 50, loss = 0.70287996\n",
            "Iteration 51, loss = 0.70056899\n",
            "Iteration 52, loss = 0.69854648\n",
            "Iteration 53, loss = 0.69647718\n",
            "Iteration 54, loss = 0.69445842\n",
            "Iteration 55, loss = 0.69267225\n",
            "Iteration 56, loss = 0.69085010\n",
            "Iteration 57, loss = 0.68910030\n",
            "Iteration 58, loss = 0.68740210\n",
            "Iteration 59, loss = 0.68557205\n",
            "Iteration 60, loss = 0.68405450\n",
            "Iteration 61, loss = 0.68238401\n",
            "Iteration 62, loss = 0.68079130\n",
            "Iteration 63, loss = 0.67912528\n",
            "Iteration 64, loss = 0.67762583\n",
            "Iteration 65, loss = 0.67598103\n",
            "Iteration 66, loss = 0.67442768\n",
            "Iteration 67, loss = 0.67280707\n",
            "Iteration 68, loss = 0.67120045\n",
            "Iteration 69, loss = 0.66961578\n",
            "Iteration 70, loss = 0.66800832\n",
            "Iteration 71, loss = 0.66637771\n",
            "Iteration 72, loss = 0.66465875\n",
            "Iteration 73, loss = 0.66308195\n",
            "Iteration 74, loss = 0.66135013\n",
            "Iteration 75, loss = 0.65964685\n",
            "Iteration 76, loss = 0.65797409\n",
            "Iteration 77, loss = 0.65618971\n",
            "Iteration 78, loss = 0.65450729\n",
            "Iteration 79, loss = 0.65281411\n",
            "Iteration 80, loss = 0.65111161\n",
            "Iteration 81, loss = 0.64927350\n",
            "Iteration 82, loss = 0.64761753\n",
            "Iteration 83, loss = 0.64597752\n",
            "Iteration 84, loss = 0.64416148\n",
            "Iteration 85, loss = 0.64253272\n",
            "Iteration 86, loss = 0.64083085\n",
            "Iteration 87, loss = 0.63906306\n",
            "Iteration 88, loss = 0.63741273\n",
            "Iteration 89, loss = 0.63581457\n",
            "Iteration 90, loss = 0.63420668\n",
            "Iteration 91, loss = 0.63250513\n",
            "Iteration 92, loss = 0.63082737\n",
            "Iteration 93, loss = 0.62924656\n",
            "Iteration 94, loss = 0.62764787\n",
            "Iteration 95, loss = 0.62604754\n",
            "Iteration 96, loss = 0.62435953\n",
            "Iteration 97, loss = 0.62281340\n",
            "Iteration 98, loss = 0.62123462\n",
            "Iteration 99, loss = 0.61967448\n",
            "Iteration 100, loss = 0.61803222\n",
            "Iteration 101, loss = 0.61644448\n",
            "Iteration 102, loss = 0.61486157\n",
            "Iteration 103, loss = 0.61320475\n",
            "Iteration 104, loss = 0.61172191\n",
            "Iteration 105, loss = 0.61013155\n",
            "Iteration 106, loss = 0.60850702\n",
            "Iteration 107, loss = 0.60694624\n",
            "Iteration 108, loss = 0.60532096\n",
            "Iteration 109, loss = 0.60375026\n",
            "Iteration 110, loss = 0.60222980\n",
            "Iteration 111, loss = 0.60062875\n",
            "Iteration 112, loss = 0.59904153\n",
            "Iteration 113, loss = 0.59746221\n",
            "Iteration 114, loss = 0.59591851\n",
            "Iteration 115, loss = 0.59425585\n",
            "Iteration 116, loss = 0.59275071\n",
            "Iteration 117, loss = 0.59122107\n",
            "Iteration 118, loss = 0.58969516\n",
            "Iteration 119, loss = 0.58811032\n",
            "Iteration 120, loss = 0.58651185\n",
            "Iteration 121, loss = 0.58496340\n",
            "Iteration 122, loss = 0.58340525\n",
            "Iteration 123, loss = 0.58191983\n",
            "Iteration 124, loss = 0.58042430\n",
            "Iteration 125, loss = 0.57888966\n",
            "Iteration 126, loss = 0.57729922\n",
            "Iteration 127, loss = 0.57574894\n",
            "Iteration 128, loss = 0.57427362\n",
            "Iteration 129, loss = 0.57272341\n",
            "Iteration 130, loss = 0.57124191\n",
            "Iteration 131, loss = 0.56970816\n",
            "Iteration 132, loss = 0.56821710\n",
            "Iteration 133, loss = 0.56666303\n",
            "Iteration 134, loss = 0.56504980\n",
            "Iteration 135, loss = 0.56359108\n",
            "Iteration 136, loss = 0.56205736\n",
            "Iteration 137, loss = 0.56045598\n",
            "Iteration 138, loss = 0.55890915\n",
            "Iteration 139, loss = 0.55742067\n",
            "Iteration 140, loss = 0.55581191\n",
            "Iteration 141, loss = 0.55420160\n",
            "Iteration 142, loss = 0.55266139\n",
            "Iteration 143, loss = 0.55102123\n",
            "Iteration 144, loss = 0.54943743\n",
            "Iteration 145, loss = 0.54774117\n",
            "Iteration 146, loss = 0.54613232\n",
            "Iteration 147, loss = 0.54435340\n",
            "Iteration 148, loss = 0.54270529\n",
            "Iteration 149, loss = 0.54103826\n",
            "Iteration 150, loss = 0.53915012\n",
            "Iteration 151, loss = 0.53735266\n",
            "Iteration 152, loss = 0.53558884\n",
            "Iteration 153, loss = 0.53364297\n",
            "Iteration 154, loss = 0.53185264\n",
            "Iteration 155, loss = 0.52985834\n",
            "Iteration 156, loss = 0.52796041\n",
            "Iteration 157, loss = 0.52612373\n",
            "Iteration 158, loss = 0.52419865\n",
            "Iteration 159, loss = 0.52230841\n",
            "Iteration 160, loss = 0.52066988\n",
            "Iteration 161, loss = 0.51878330\n",
            "Iteration 162, loss = 0.51709204\n",
            "Iteration 163, loss = 0.51532500\n",
            "Iteration 164, loss = 0.51375627\n",
            "Iteration 165, loss = 0.51219013\n",
            "Iteration 166, loss = 0.51076024\n",
            "Iteration 167, loss = 0.50921734\n",
            "Iteration 168, loss = 0.50782065\n",
            "Iteration 169, loss = 0.50641024\n",
            "Iteration 170, loss = 0.50494294\n",
            "Iteration 171, loss = 0.50374920\n",
            "Iteration 172, loss = 0.50238482\n",
            "Iteration 173, loss = 0.50127340\n",
            "Iteration 174, loss = 0.49993726\n",
            "Iteration 175, loss = 0.49882682\n",
            "Iteration 176, loss = 0.49763531\n",
            "Iteration 177, loss = 0.49652820\n",
            "Iteration 178, loss = 0.49549294\n",
            "Iteration 179, loss = 0.49444357\n",
            "Iteration 180, loss = 0.49344165\n",
            "Iteration 181, loss = 0.49242326\n",
            "Iteration 182, loss = 0.49143384\n",
            "Iteration 183, loss = 0.49046680\n",
            "Iteration 184, loss = 0.48956501\n",
            "Iteration 185, loss = 0.48864747\n",
            "Iteration 186, loss = 0.48772390\n",
            "Iteration 187, loss = 0.48686134\n",
            "Iteration 188, loss = 0.48603692\n",
            "Iteration 189, loss = 0.48512792\n",
            "Iteration 190, loss = 0.48429538\n",
            "Iteration 191, loss = 0.48351520\n",
            "Iteration 192, loss = 0.48290539\n",
            "Iteration 193, loss = 0.48196744\n",
            "Iteration 194, loss = 0.48123946\n",
            "Iteration 195, loss = 0.48054517\n",
            "Iteration 196, loss = 0.47981491\n",
            "Iteration 197, loss = 0.47909407\n",
            "Iteration 198, loss = 0.47847767\n",
            "Iteration 199, loss = 0.47789193\n",
            "Iteration 200, loss = 0.47719241\n",
            "Iteration 201, loss = 0.47650545\n",
            "Iteration 202, loss = 0.47588105\n",
            "Iteration 203, loss = 0.47533998\n",
            "Iteration 204, loss = 0.47469251\n",
            "Iteration 205, loss = 0.47411761\n",
            "Iteration 206, loss = 0.47357304\n",
            "Iteration 207, loss = 0.47299823\n",
            "Iteration 208, loss = 0.47248709\n",
            "Iteration 209, loss = 0.47200515\n",
            "Iteration 210, loss = 0.47145037\n",
            "Iteration 211, loss = 0.47092359\n",
            "Iteration 212, loss = 0.47047446\n",
            "Iteration 213, loss = 0.46992277\n",
            "Iteration 214, loss = 0.46950085\n",
            "Iteration 215, loss = 0.46898987\n",
            "Iteration 216, loss = 0.46860880\n",
            "Iteration 217, loss = 0.46817191\n",
            "Iteration 218, loss = 0.46766689\n",
            "Iteration 219, loss = 0.46723417\n",
            "Iteration 220, loss = 0.46685727\n",
            "Iteration 221, loss = 0.46644630\n",
            "Iteration 222, loss = 0.46605927\n",
            "Iteration 223, loss = 0.46563770\n",
            "Iteration 224, loss = 0.46523756\n",
            "Iteration 225, loss = 0.46491817\n",
            "Iteration 226, loss = 0.46449854\n",
            "Iteration 227, loss = 0.46410734\n",
            "Iteration 228, loss = 0.46383252\n",
            "Iteration 229, loss = 0.46346045\n",
            "Iteration 230, loss = 0.46317950\n",
            "Iteration 231, loss = 0.46279443\n",
            "Iteration 232, loss = 0.46248890\n",
            "Iteration 233, loss = 0.46207698\n",
            "Iteration 234, loss = 0.46177671\n",
            "Iteration 235, loss = 0.46143899\n",
            "Iteration 236, loss = 0.46113772\n",
            "Iteration 237, loss = 0.46083969\n",
            "Iteration 238, loss = 0.46053043\n",
            "Iteration 239, loss = 0.46026241\n",
            "Iteration 240, loss = 0.45996305\n",
            "Iteration 241, loss = 0.45964003\n",
            "Iteration 242, loss = 0.45930102\n",
            "Iteration 243, loss = 0.45908732\n",
            "Iteration 244, loss = 0.45879460\n",
            "Iteration 245, loss = 0.45854099\n",
            "Iteration 246, loss = 0.45822786\n",
            "Iteration 247, loss = 0.45800004\n",
            "Iteration 248, loss = 0.45771956\n",
            "Iteration 249, loss = 0.45742029\n",
            "Iteration 250, loss = 0.45716565\n",
            "Iteration 251, loss = 0.45694820\n",
            "Iteration 252, loss = 0.45669038\n",
            "Iteration 253, loss = 0.45643542\n",
            "Iteration 254, loss = 0.45618828\n",
            "Iteration 255, loss = 0.45590990\n",
            "Iteration 256, loss = 0.45569760\n",
            "Iteration 257, loss = 0.45541651\n",
            "Iteration 258, loss = 0.45519886\n",
            "Iteration 259, loss = 0.45498265\n",
            "Iteration 260, loss = 0.45473995\n",
            "Iteration 261, loss = 0.45470889\n",
            "Iteration 262, loss = 0.45441131\n",
            "Iteration 263, loss = 0.45413125\n",
            "Iteration 264, loss = 0.45384525\n",
            "Iteration 265, loss = 0.45359465\n",
            "Iteration 266, loss = 0.45348187\n",
            "Iteration 267, loss = 0.45319606\n",
            "Iteration 268, loss = 0.45304495\n",
            "Iteration 269, loss = 0.45284078\n",
            "Iteration 270, loss = 0.45255473\n",
            "Iteration 271, loss = 0.45240892\n",
            "Iteration 272, loss = 0.45217289\n",
            "Iteration 273, loss = 0.45199786\n",
            "Iteration 274, loss = 0.45174851\n",
            "Iteration 275, loss = 0.45153323\n",
            "Iteration 276, loss = 0.45140732\n",
            "Iteration 277, loss = 0.45112868\n",
            "Iteration 278, loss = 0.45096649\n",
            "Iteration 279, loss = 0.45076734\n",
            "Iteration 280, loss = 0.45057577\n",
            "Iteration 281, loss = 0.45038331\n",
            "Iteration 282, loss = 0.45023763\n",
            "Iteration 283, loss = 0.45001166\n",
            "Iteration 284, loss = 0.44985226\n",
            "Iteration 285, loss = 0.44969345\n",
            "Iteration 286, loss = 0.44948152\n",
            "Iteration 287, loss = 0.44924379\n",
            "Iteration 288, loss = 0.44912838\n",
            "Iteration 289, loss = 0.44891922\n",
            "Iteration 290, loss = 0.44871759\n",
            "Iteration 291, loss = 0.44861776\n",
            "Iteration 292, loss = 0.44834077\n",
            "Iteration 293, loss = 0.44833037\n",
            "Iteration 294, loss = 0.44808688\n",
            "Iteration 295, loss = 0.44784838\n",
            "Iteration 296, loss = 0.44779585\n",
            "Iteration 297, loss = 0.44751640\n",
            "Iteration 298, loss = 0.44735759\n",
            "Iteration 299, loss = 0.44720079\n",
            "Iteration 300, loss = 0.44717393\n",
            "Iteration 301, loss = 0.44696940\n",
            "Iteration 302, loss = 0.44673646\n",
            "Iteration 303, loss = 0.44660739\n",
            "Iteration 304, loss = 0.44649505\n",
            "Iteration 305, loss = 0.44623649\n",
            "Iteration 306, loss = 0.44602192\n",
            "Iteration 307, loss = 0.44587611\n",
            "Iteration 308, loss = 0.44574298\n",
            "Iteration 309, loss = 0.44556066\n",
            "Iteration 310, loss = 0.44545109\n",
            "Iteration 311, loss = 0.44526989\n",
            "Iteration 312, loss = 0.44509109\n",
            "Iteration 313, loss = 0.44491886\n",
            "Iteration 314, loss = 0.44473896\n",
            "Iteration 315, loss = 0.44460560\n",
            "Iteration 316, loss = 0.44443763\n",
            "Iteration 317, loss = 0.44440888\n",
            "Iteration 318, loss = 0.44430060\n",
            "Iteration 319, loss = 0.44402853\n",
            "Iteration 320, loss = 0.44386477\n",
            "Iteration 321, loss = 0.44370858\n",
            "Iteration 322, loss = 0.44360559\n",
            "Iteration 323, loss = 0.44340663\n",
            "Iteration 324, loss = 0.44329223\n",
            "Iteration 325, loss = 0.44313310\n",
            "Iteration 326, loss = 0.44296542\n",
            "Iteration 327, loss = 0.44284514\n",
            "Iteration 328, loss = 0.44272115\n",
            "Iteration 329, loss = 0.44253998\n",
            "Iteration 330, loss = 0.44240742\n",
            "Iteration 331, loss = 0.44226061\n",
            "Iteration 332, loss = 0.44212453\n",
            "Iteration 333, loss = 0.44203206\n",
            "Iteration 334, loss = 0.44194777\n",
            "Iteration 335, loss = 0.44173622\n",
            "Iteration 336, loss = 0.44165677\n",
            "Iteration 337, loss = 0.44153217\n",
            "Iteration 338, loss = 0.44131491\n",
            "Iteration 339, loss = 0.44122612\n",
            "Iteration 340, loss = 0.44108591\n",
            "Iteration 341, loss = 0.44092625\n",
            "Iteration 342, loss = 0.44083266\n",
            "Iteration 343, loss = 0.44068738\n",
            "Iteration 344, loss = 0.44055803\n",
            "Iteration 345, loss = 0.44042261\n",
            "Iteration 346, loss = 0.44045512\n",
            "Iteration 347, loss = 0.44018188\n",
            "Iteration 348, loss = 0.44005076\n",
            "Iteration 349, loss = 0.43996067\n",
            "Iteration 350, loss = 0.43977307\n",
            "Iteration 351, loss = 0.43965033\n",
            "Iteration 352, loss = 0.43957572\n",
            "Iteration 353, loss = 0.43944517\n",
            "Iteration 354, loss = 0.43932676\n",
            "Iteration 355, loss = 0.43922513\n",
            "Iteration 356, loss = 0.43907107\n",
            "Iteration 357, loss = 0.43895936\n",
            "Iteration 358, loss = 0.43885535\n",
            "Iteration 359, loss = 0.43871056\n",
            "Iteration 360, loss = 0.43859606\n",
            "Iteration 361, loss = 0.43854427\n",
            "Iteration 362, loss = 0.43846789\n",
            "Iteration 363, loss = 0.43829622\n",
            "Iteration 364, loss = 0.43813378\n",
            "Iteration 365, loss = 0.43808526\n",
            "Iteration 366, loss = 0.43796444\n",
            "Iteration 367, loss = 0.43780170\n",
            "Iteration 368, loss = 0.43785691\n",
            "Iteration 369, loss = 0.43758693\n",
            "Iteration 370, loss = 0.43748330\n",
            "Iteration 371, loss = 0.43735955\n",
            "Iteration 372, loss = 0.43727839\n",
            "Iteration 373, loss = 0.43711707\n",
            "Iteration 374, loss = 0.43700927\n",
            "Iteration 375, loss = 0.43699229\n",
            "Iteration 376, loss = 0.43683955\n",
            "Iteration 377, loss = 0.43673573\n",
            "Iteration 378, loss = 0.43665050\n",
            "Iteration 379, loss = 0.43650771\n",
            "Iteration 380, loss = 0.43640916\n",
            "Iteration 381, loss = 0.43629720\n",
            "Iteration 382, loss = 0.43620601\n",
            "Iteration 383, loss = 0.43615678\n",
            "Iteration 384, loss = 0.43601501\n",
            "Iteration 385, loss = 0.43594559\n",
            "Iteration 386, loss = 0.43578067\n",
            "Iteration 387, loss = 0.43575969\n",
            "Iteration 388, loss = 0.43562256\n",
            "Iteration 389, loss = 0.43550274\n",
            "Iteration 390, loss = 0.43543667\n",
            "Iteration 391, loss = 0.43540531\n",
            "Iteration 392, loss = 0.43528240\n",
            "Iteration 393, loss = 0.43519456\n",
            "Iteration 394, loss = 0.43500638\n",
            "Iteration 395, loss = 0.43498119\n",
            "Iteration 396, loss = 0.43496454\n",
            "Iteration 397, loss = 0.43482831\n",
            "Iteration 398, loss = 0.43463917\n",
            "Iteration 399, loss = 0.43455789\n",
            "Iteration 400, loss = 0.43449864\n",
            "Iteration 401, loss = 0.43440619\n",
            "Iteration 402, loss = 0.43433351\n",
            "Iteration 403, loss = 0.43422789\n",
            "Iteration 404, loss = 0.43412746\n",
            "Iteration 405, loss = 0.43403457\n",
            "Iteration 406, loss = 0.43395771\n",
            "Iteration 407, loss = 0.43385191\n",
            "Iteration 408, loss = 0.43379262\n",
            "Iteration 409, loss = 0.43367865\n",
            "Iteration 410, loss = 0.43370478\n",
            "Iteration 411, loss = 0.43352434\n",
            "Iteration 412, loss = 0.43341940\n",
            "Iteration 413, loss = 0.43336241\n",
            "Iteration 414, loss = 0.43327880\n",
            "Iteration 415, loss = 0.43319268\n",
            "Iteration 416, loss = 0.43312676\n",
            "Iteration 417, loss = 0.43303133\n",
            "Iteration 418, loss = 0.43294546\n",
            "Iteration 419, loss = 0.43288977\n",
            "Iteration 420, loss = 0.43282051\n",
            "Iteration 421, loss = 0.43265755\n",
            "Iteration 422, loss = 0.43263908\n",
            "Iteration 423, loss = 0.43263161\n",
            "Iteration 424, loss = 0.43245415\n",
            "Iteration 425, loss = 0.43242147\n",
            "Iteration 426, loss = 0.43230985\n",
            "Iteration 427, loss = 0.43230420\n",
            "Iteration 428, loss = 0.43213759\n",
            "Iteration 429, loss = 0.43218886\n",
            "Iteration 430, loss = 0.43206435\n",
            "Iteration 431, loss = 0.43189533\n",
            "Iteration 432, loss = 0.43184659\n",
            "Iteration 433, loss = 0.43178973\n",
            "Iteration 434, loss = 0.43176987\n",
            "Iteration 435, loss = 0.43161501\n",
            "Iteration 436, loss = 0.43154669\n",
            "Iteration 437, loss = 0.43147286\n",
            "Iteration 438, loss = 0.43144996\n",
            "Iteration 439, loss = 0.43137947\n",
            "Iteration 440, loss = 0.43123780\n",
            "Iteration 441, loss = 0.43119491\n",
            "Iteration 442, loss = 0.43118698\n",
            "Iteration 443, loss = 0.43103135\n",
            "Iteration 444, loss = 0.43102785\n",
            "Iteration 445, loss = 0.43087519\n",
            "Iteration 446, loss = 0.43081887\n",
            "Iteration 447, loss = 0.43073736\n",
            "Iteration 448, loss = 0.43066666\n",
            "Iteration 449, loss = 0.43062082\n",
            "Iteration 450, loss = 0.43055728\n",
            "Iteration 451, loss = 0.43044909\n",
            "Iteration 452, loss = 0.43045719\n",
            "Iteration 453, loss = 0.43035746\n",
            "Iteration 454, loss = 0.43037884\n",
            "Iteration 455, loss = 0.43020213\n",
            "Iteration 456, loss = 0.43018667\n",
            "Iteration 457, loss = 0.43005613\n",
            "Iteration 458, loss = 0.43000167\n",
            "Iteration 459, loss = 0.42992744\n",
            "Iteration 460, loss = 0.42986695\n",
            "Iteration 461, loss = 0.42981056\n",
            "Iteration 462, loss = 0.42975816\n",
            "Iteration 463, loss = 0.42967969\n",
            "Iteration 464, loss = 0.42962837\n",
            "Iteration 465, loss = 0.42967405\n",
            "Iteration 466, loss = 0.42958079\n",
            "Iteration 467, loss = 0.42943632\n",
            "Iteration 468, loss = 0.42938455\n",
            "Iteration 469, loss = 0.42932619\n",
            "Iteration 470, loss = 0.42926411\n",
            "Iteration 471, loss = 0.42917773\n",
            "Iteration 472, loss = 0.42929345\n",
            "Iteration 473, loss = 0.42905909\n",
            "Iteration 474, loss = 0.42895717\n",
            "Iteration 475, loss = 0.42892711\n",
            "Iteration 476, loss = 0.42888190\n",
            "Iteration 477, loss = 0.42886003\n",
            "Iteration 478, loss = 0.42878987\n",
            "Iteration 479, loss = 0.42871515\n",
            "Iteration 480, loss = 0.42862828\n",
            "Iteration 481, loss = 0.42858225\n",
            "Iteration 482, loss = 0.42852388\n",
            "Iteration 483, loss = 0.42844256\n",
            "Iteration 484, loss = 0.42843354\n",
            "Iteration 485, loss = 0.42836397\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Melhores hiperparâmetros encontrados através do Random Search:\n",
            "{'tol': 0.0001, 'solver': 'adam', 'max_iter': 2000, 'activation': 'tanh'}\n",
            "Melhor pontuação (acurácia) encontrada através do Random Search: 0.788\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ajuste do modelo MLP aos dados de treinamento\n",
        "rnaf_best_random = MLPClassifier(**best_params_random)\n",
        "rnaf_best_random.fit(X_train_oversampled, y_train_oversampled)\n",
        "\n",
        "# Predições nos dados de teste usando o modelo com melhores hiperparâmetros encontrados pelo Random Search\n",
        "pred_randomf = rnaf_best_random.predict(X_test)"
      ],
      "metadata": {
        "id": "Q7PP1YiUz3p8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cálculo e impressão da acurácia nos dados de teste\n",
        "accuracy_random = accuracy_score(y_test, pred_randomf)\n",
        "print(\"Acurácia do modelo MLP com melhores hiperparâmetros pelo Random Search:\", accuracy_random)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3740751-5c69-485c-fcb1-0e789b6a7ffb",
        "id": "wLMlN9h0z3p9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acurácia do modelo MLP com melhores hiperparâmetros pelo Random Search: 0.7835820895522388\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Matriz de confusão para o modelo com melhores hiperparâmetros pelo Random Search\n",
        "print(\"Matriz de Confusão - Random Search\")\n",
        "cm_rnaf_random = confusion_matrix(y_test, pred_randomf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "548dbf9b-eea1-4c92-ba16-4c0bdea42de2",
        "id": "mauCdoq8z3p9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matriz de Confusão - Random Search\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtém a média das acurácias (10 folds) referente ao conjunto treino\n",
        "rnaf_random = r_results.loc[r_search.best_index_,'mean_test_score']"
      ],
      "metadata": {
        "id": "OuTwfyOMz3p9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1CmQjmp4IMb"
      },
      "source": [
        "## RESULTADOS\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Resultados dos classificadores:\n",
        "\n",
        "print(\"EXPERIMENTO COM AS CABINES: \")\n",
        "print(\"********************************\")\n",
        "print(\"ACURACIAS COM DECISION TREE:\")\n",
        "print(\"Acuracia bruta -\", accuracy_tree)\n",
        "print(\"GridSearch -\", Acurácias_Tree_Grid)\n",
        "print(\"RandomSearch -\", Acurácias_Tree_Random)\n",
        "print(\"Matriz confusao:\")\n",
        "print(confusion_tree)\n",
        "print(\"---------------------------------------\")\n",
        "print(\"ACURACIAS COM KNN:\")\n",
        "print(\"Acuracia bruta -\", acuracia_knn)\n",
        "print(\"GridSearch -\", knn_grid)\n",
        "print(\"RandomSearch -\", knn_random)\n",
        "print(\"Matriz confusao:\")\n",
        "print(cm_knn)\n",
        "print(\"---------------------------------------\")\n",
        "print(\"ACURACIAS COM RANDOM FOREST:\")\n",
        "print(\"Acuracia bruta -\", accuracy_tree)\n",
        "print(\"GridSearch -\", forest_grid)\n",
        "print(\"RandomSearch -\", forest_random)\n",
        "print(\"Matriz confusao:\")\n",
        "print(cm_forest)\n",
        "print(\"---------------------------------------\")\n",
        "print(\"ACURACIAS COM REDE NEURAL (Scikit) - 1 camada:\")\n",
        "print(\"Acuracia bruta teste -\", rna_teste)\n",
        "print(\"Matriz confusão do teste:\")\n",
        "print(cm_rna_teste)\n",
        "print(\"Acuracia bruta treino -\", rna_treino)\n",
        "print(\"Matriz confusão do treino:\")\n",
        "print(cm_rna_treino)\n",
        "print(\"GridSearch -\", rna1_grid)\n",
        "print(\"Matriz confusão do GridSearch:\")\n",
        "print(cm_rna_grid)\n",
        "print(\"RandomSearch -\", rna1_random)\n",
        "print(\"Matriz confusão do RandomSearch:\")\n",
        "print(cm_rna_random)\n",
        "print(\"---------------------------------------\")\n",
        "print(\"ACURACIAS COM REDE NEURAL (Scikit) - 2 camadas:\")\n",
        "print(\"Acuracia bruta teste -\", rna2_teste)\n",
        "print(\"Matriz confusão do teste:\")\n",
        "print(cm_rna2_teste)\n",
        "print(\"Acuracia bruta treino -\", rna2_treino)\n",
        "print(\"Matriz confusão do treino:\")\n",
        "print(cm_rna2_treino)\n",
        "print(\"GridSearch -\", rna2_grid)\n",
        "print(\"Matriz confusão do GridSearch:\")\n",
        "print(cm_rna2_grid)\n",
        "print(\"RandomSearch -\", rna2_random)\n",
        "print(\"Matriz confusão do RandomSearch:\")\n",
        "print(cm_rna2_random)\n",
        "print(\"---------------------------------------\")\n",
        "print(\"ACURACIAS COM REDE NEURAL (Scikit) - 3 camadas:\")\n",
        "print(\"Acuracia bruta teste -\", rna3_teste)\n",
        "print(\"Matriz confusão do teste:\")\n",
        "print(cm_rna3_teste)\n",
        "print(\"Acuracia bruta treino -\", rna3_treino)\n",
        "print(\"Matriz confusão do treino:\")\n",
        "print(cm_rna3_treino)\n",
        "print(\"GridSearch -\", rna3_grid)\n",
        "print(\"Matriz confusão do GridSearch:\")\n",
        "print(cm_rna3_grid)\n",
        "print(\"RandomSearch -\", rna3_random)\n",
        "print(\"Matriz confusão do RandomSearch:\")\n",
        "print(cm_rna3_random)\n",
        "print(\"---------------------------------------\")\n",
        "print(\"ACURACIAS COM REDE NEURAL Keras:\")\n",
        "print(\"Acuracia -\", acuracia_keras)\n",
        "print(\"---------------------------------------\")\n",
        "print(\"EXPERIMENTO SEM AS CABINES: \")\n",
        "print(\"********************************\")\n",
        "print(\"---------------------------------------\")\n",
        "print(\"ACURACIAS COM REDE NEURAL (Scikit) - 2 camadas:\")\n",
        "print(\"Acuracia bruta teste -\", rnaf_teste)\n",
        "print(\"Matriz confusão do teste:\")\n",
        "print(cm_rnaf_teste)\n",
        "print(\"Acuracia bruta treino -\", rnaf_treino)\n",
        "print(\"Matriz confusão do treino:\")\n",
        "print(cm_rnaf_treino)\n",
        "print(\"GridSearch -\", rnaf_grid)\n",
        "print(\"Matriz confusão do GridSearch:\")\n",
        "print(cm_rnaf_grid)\n",
        "print(\"RandomSearch -\", rnaf_random)\n",
        "print(\"Matriz confusão do RandomSearch:\")\n",
        "print(cm_rnaf_random)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qayjotOt5cWn",
        "outputId": "291d88d0-4a2c-488d-d363-ae89a385fd9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EXPERIMENTO COM AS CABINES: \n",
            "********************************\n",
            "ACURACIAS COM DECISION TREE:\n",
            "Acuracia bruta - 0.7649253731343284\n",
            "GridSearch - 0.8355263157894737\n",
            "RandomSearch - 0.8263157894736841\n",
            "Matriz confusao:\n",
            "[[141  28]\n",
            " [ 35  64]]\n",
            "---------------------------------------\n",
            "ACURACIAS COM KNN:\n",
            "Acuracia bruta - 0.7126865671641791\n",
            "GridSearch - 0.8355263157894737\n",
            "RandomSearch - 0.8342105263157894\n",
            "Matriz confusao:\n",
            "[[128  41]\n",
            " [ 36  63]]\n",
            "---------------------------------------\n",
            "ACURACIAS COM RANDOM FOREST:\n",
            "Acuracia bruta - 0.7649253731343284\n",
            "GridSearch - 0.843421052631579\n",
            "RandomSearch - 0.8328947368421054\n",
            "Matriz confusao:\n",
            "[[141  28]\n",
            " [ 35  64]]\n",
            "---------------------------------------\n",
            "ACURACIAS COM REDE NEURAL (Scikit) - 1 camada:\n",
            "Acuracia bruta teste - 0.7276119402985075\n",
            "Matriz confusão do teste:\n",
            "[[124  45]\n",
            " [ 28  71]]\n",
            "Acuracia bruta treino - 0.8105263157894737\n",
            "Matriz confusão do treino:\n",
            "[[313  67]\n",
            " [ 77 303]]\n",
            "GridSearch - 0.8355263157894738\n",
            "Matriz confusão do GridSearch:\n",
            "[[139  30]\n",
            " [ 33  66]]\n",
            "RandomSearch - 0.8210526315789475\n",
            "Matriz confusão do RandomSearch:\n",
            "[[142  27]\n",
            " [ 36  63]]\n",
            "---------------------------------------\n",
            "ACURACIAS COM REDE NEURAL (Scikit) - 2 camadas:\n",
            "Acuracia bruta teste - 0.7723880597014925\n",
            "Matriz confusão do teste:\n",
            "[[141  28]\n",
            " [ 33  66]]\n",
            "Acuracia bruta treino - 0.8013157894736842\n",
            "Matriz confusão do treino:\n",
            "[[333  47]\n",
            " [104 276]]\n",
            "GridSearch - 0.8315789473684211\n",
            "Matriz confusão do GridSearch:\n",
            "[[141  28]\n",
            " [ 39  60]]\n",
            "RandomSearch - 0.8065789473684211\n",
            "Matriz confusão do RandomSearch:\n",
            "[[144  25]\n",
            " [ 39  60]]\n",
            "---------------------------------------\n",
            "ACURACIAS COM REDE NEURAL (Scikit) - 3 camadas:\n",
            "Acuracia bruta teste - 0.6082089552238806\n",
            "Matriz confusão do teste:\n",
            "[[155  14]\n",
            " [ 91   8]]\n",
            "Acuracia bruta treino - 0.5105263157894737\n",
            "Matriz confusão do treino:\n",
            "[[356  24]\n",
            " [348  32]]\n",
            "GridSearch - 0.8355263157894738\n",
            "Matriz confusão do GridSearch:\n",
            "[[140  29]\n",
            " [ 34  65]]\n",
            "RandomSearch - 0.8210526315789475\n",
            "Matriz confusão do RandomSearch:\n",
            "[[138  31]\n",
            " [ 34  65]]\n",
            "---------------------------------------\n",
            "ACURACIAS COM REDE NEURAL Keras:\n",
            "Acuracia - 0.8022388059701493\n",
            "---------------------------------------\n",
            "EXPERIMENTO SEM AS CABINES: \n",
            "********************************\n",
            "---------------------------------------\n",
            "ACURACIAS COM REDE NEURAL (Scikit) - 2 camadas:\n",
            "Acuracia bruta teste - 0.7947761194029851\n",
            "Matriz confusão do teste:\n",
            "[[151  23]\n",
            " [ 32  62]]\n",
            "Acuracia bruta treino - 0.796\n",
            "Matriz confusão do treino:\n",
            "[[332  43]\n",
            " [110 265]]\n",
            "GridSearch - 0.8236842105263158\n",
            "Matriz confusão do GridSearch:\n",
            "[[140  34]\n",
            " [ 25  69]]\n",
            "RandomSearch - 0.8065789473684211\n",
            "Matriz confusão do RandomSearch:\n",
            "[[148  26]\n",
            " [ 32  62]]\n"
          ]
        }
      ]
    }
  ]
}